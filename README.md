# Table of Contents
1. [Large Language Models Papers](#large-language-models-papers)
2. [Other topics](#other-topics)
3. [Large Language Models Papers with Code](#large-language-models-papers-with-code)


## Large Language Models Papers
This GitHub repository contains an updated list of Federated Learning papers as of **July 27, 2025**. 

- The resources are collected from various sources, including arXiv, NeurIPS, ICML, ICLR, ACL, EMNLP, AAAI, IJCAI, KDD, CVPR, ICCV, ECCV, NIPS, IEEE, ACM, Springer, ScienceDirect, Wiley, Nature, Science, and other top AI/ML conferences and journals.
- For a better reading experience, visit the [Shinyapps website](https://mtuann.shinyapps.io/research-papers/).

---
# Other Topics
Explore additional research papers on the following topics:
- For **Large Language Models** papers, please visit the [**LLM Repository**](https://github.com/mtuann/llm-updated-papers).
- For **Backdoor Learning** papers, please visit the [**Backdoor Learning Repository**](https://github.com/mtuann/backdoor-ai-resources).
- For **Federated Learning** papers, please visit the [**Federated Learning Repository**](https://github.com/mtuann/federated-learning-updated-papers).
- For **Machine Unlearning** papers, please visit the [**Machine Unlearning Repository**](https://github.com/mtuann/machine-unlearning-papers).

---

For contributions, inquiries, or suggestions, feel free to reach out via [email](mailto:tuannm0312@gmail.com).

---

If you find this application helpful and would like to support its development, you can buy me a coffee using one of the following methods:
- **Techcombank (Vietnam):** 5877 5555 55 (Nguyen Thi Lan Phuong)
- **PayPal or Credit/Debit Card:** [https://ko-fi.com/miutheladycat](https://ko-fi.com/miutheladycat)

---

## Large Language Models Papers with Code
Due to GitHub repository limitations, this section includes only those papers that provide accompanying code, sorted by publish date. For access to the full list of papers, please visit the [Shinyapps website](https://mtuann.shinyapps.io/research-papers/).

---


|No.|Title|Authors|Publish Date|Venue|Code|URL|
|---|---|---|---|---|---|---|
|1|In BLOOM: Creativity and Affinity in Artificial Lyrics and Art|Evan Crothers, Herna L. Viktor, Nathalie Japkowicz|<li><span style="color:#FF5733;">None</span></li>|creativeAI|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/in-bloom-creativity-and-affinity-in/code)|https://openreview.net/pdf/3d502829f7e86330802059674fac7b55dfb63091.pdf|
|2|A Simple, Yet Effective Approach to Finding Biases in Code Generation|Spyridon Mouselinos, Mateusz Malinowski, Henryk Michalewski|<li><span style="color:#FF5733;">None</span></li>|OpenReview|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/a-simple-yet-effective-approach-to-finding/code)|https://openreview.net/pdf/dc913e6b5396ddf78d74195871197392db78fa41.pdf|
|3|A large language model for predicting neurotoxic peptides and neurotoxins|Anand Singh Rathore, Saloni Jain, Shubham Choudhury, Gajendra P. S. Raghava|<li><span style="color:#FF5733;">2025-08-01</span></li>|PubMed|https://github.com/raghavagps/ntxpred2|https://pubmed.ncbi.nlm.nih.gov/40671295|
|4|BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining|Baqer M. Merzah, Tania Taami, Salman Asoudeh, Amir reza Hossein pour, Saeed Mirzaee, Amir Ali Bengari|<li><span style="color:#FF5733;">2025-07-21</span></li>|OpenAlex|https://github.com/amirap80/BioPars|https://doi.org/10.21203/rs.3.rs-6823379/v1|
|5|textToKnowledgeGraph: Generation of Molecular Interaction Knowledge Graphs Using Large Language Models for Exploration in Cytoscape|Favour James, Christopher Churas, Dexter Pratt, Augustin Luna|<li><span style="color:#FF5733;">2025-07-21</span></li>|OpenAlex|https://github.com/ndexbio/llm-text-to-knowledge-graph|https://doi.org/10.1101/2025.07.17.664328|
|6|Empowering Universal Robot Programming with Fine-Tuned Large Language Models|Tien Dat Le, Minhhuy Le|<li><span style="color:#FF5733;">2025-07-15</span></li>|EAI Endorsed Transactions on AI and Robotics|https://github.com/t1end4t/llm-robotics|https://doi.org/10.4108/airo.8983|
|7|A Survey on the Memory Mechanism of Large Language Model based Agents|Zeyu Zhang, Quanyu Dai, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen|<li><span style="color:#FF5733;">2025-07-11</span></li>|ACM transactions on office information systems|https://github.com/nuster1128/LLM_Agent_Memory_Survey|https://doi.org/10.48550/arXiv.2404.13501|
|8|Conversational health agents: a personalized large language model-powered agent framework|Mahyar Abbasian, Iman Azimi, Amir M. Rahmani, Ramesh Jain|<li><span style="color:#FF5733;">2025-07-03</span></li>|JAMIA Open|https://github.com/Institute4FutureHealth/CHA|https://doi.org/10.1093/jamiaopen/ooaf067|
|9|Analysis of Image-and-Text Uncertainty Propagation in Multimodal Large
  Language Models with Cardiac MR-Based Applications|Yucheng Tang, Yunguan Fu, Weixi Yi, Yipei Wang, Daniel C. Alexander, Rhodri Davies, Yipeng Hu|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/yucheng722/MUPM.|http://arxiv.org/abs/2507.12945v1|
|10|Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding|Feng Xiao, Jicong Fan|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark|http://arxiv.org/abs/2507.12295v1|
|11|spaLLM: enhancing spatial domain analysis in multi-omics data through large language model integration|Longyi Li, Liyan Dong, Hao Zhang, Dong Xu, Yongli Li|<li><span style="color:#FF5733;">2025-07-01</span></li>|Briefings in Bioinformatics|https://github.com/liiilongyi/spaLLM.|https://doi.org/10.1093/bib/bbaf304|
|12|Warehouse Spatial Question Answering with LLM Agent|Hsiang-Wei Huang, Jen-Hao Cheng, Kuang-Ming Chen, Cheng-Yen Yang, Bahaa Alattar, Yi-Ru Lin, Pyongkun Kim, Sangwon Kim, K...|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/hsiangwei0903/SpatialAgent|http://arxiv.org/abs/2507.10778v1|
|13|The benefits of query-based KGQA systems for complex and temporal
  questions in LLM era|Artem Alekseev, Mikhail Chaichuk, Miron Butko, Alexander Panchenko, Elena Tutubalina, Oleg Somov|<li><span style="color:#FF5733;">2025-07-01</span></li>||https://github.com/ar2max/NLDB-KGQA-System|http://arxiv.org/abs/2507.11954v1|
|14|The Evolving Role of Large Language Models in Scientific Innovation:
  Evaluator, Collaborator, and Scientist|Haoxuan Zhang, Ruochi Li, Yang Zhang, Ting Xiao, Jiangping Chen, Junhua Ding, Haihua Chen|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/haoxuan-unt2024/llm4innovation.|http://arxiv.org/abs/2507.11810v1|
|15|The Devil behind the mask: An emergent safety vulnerability of Diffusion
  LLMs|Zichen Wen, Jiashu Qu, Dongrui Liu, Zhiyuan Liu, Ruixi Wu, Yicun Yang, Xiangqi Jin, Haoyun Xu, Xuyang Liu, Weijia Li, Ch...|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/ZichenWen1/DIJA.|http://arxiv.org/abs/2507.11097v1|
|16|DSSD: Efficient Edge-Device LLM Deployment and Collaborative Inference
  via Distributed Split Speculative Decoding|Jiahong Ning, Ce Zheng, Tingting Yang|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/JasonNing96/DSSD-Efficient-Edge-Computing|http://arxiv.org/abs/2507.12000v2|
|17|ParaStudent: Generating and Evaluating Realistic Student Code by
  Teaching LLMs to Struggle|Mihran Miroyan, Rose Niousha, Joseph E. Gonzalez, Gireeja Ranade, Narges Norouzi|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/mmiroyan/ParaStudent|http://arxiv.org/abs/2507.12674v1|
|18|First-Order Error Matters: Accurate Compensation for Quantized Large
  Language Models|Xingyu Zheng, Haotong Qin, Yuye Li, Jiakai Wang, Jinyang Guo, Michele Magno, Xianglong Liu|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/Xingyu-Zheng/FOEM.|http://arxiv.org/abs/2507.11017v1|
|19|Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study|Ritesh Maurya, Nikhil Kumar Rajput, M G Diviit, Satyajit Mahapatra, Manish Kumar Ojha|<li><span style="color:#FF5733;">2025-07-01</span></li>|Scientific Reports|https://github.com/diviitmg03/Comparative-analysis-of-LLMs-.git|https://doi.org/10.1038/s41598-025-05012-1|
|20|Mono-InternVL-1.5: Towards Cheaper and Faster Monolithic Multimodal
  Large Language Models|Gen Luo, Wenhan Dou, Wenhao Li, Zhaokai Wang, Xue Yang, Changyao Tian, Hao Li, Weiyun Wang, Wenhai Wang, Xizhou Zhu, Yu ...|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/OpenGVLab/Mono-InternVL.|http://arxiv.org/abs/2507.12566v1|
|21|DrafterBench: Benchmarking Large Language Models for Tasks Automation in
  Civil Engineering|Yinsheng Li, Zhen Dong, Yi Shao|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/Eason-Li-AIS/DrafterBench|http://arxiv.org/abs/2507.11527v1|
|22|Internal Value Alignment in Large Language Models through Controlled
  Value Vector Activation|Haoran Jin, Meng Li, Xiting Wang, Zhihao Xu, Minlie Huang, Yantao Jia, Defu Lian|<li><span style="color:#FF5733;">2025-07-01</span></li>|ACL|https://github.com/hr-jin/ConVA.|https://aclanthology.org/2025.acl-long.1326/|
|23|Marco-Bench-MIF: On Multilingual Instruction-Following Capability of
  Large Language Models|Bo Zeng, Chenyang Lyu, Sinuo Liu, Mingyan Zeng, Minghao Wu, Xuanfan Ni, Tianqi Shi, Yu Zhao, Yefeng Liu, Chenyu Zhu, Rui...|<li><span style="color:#FF5733;">2025-07-01</span></li>|arXiv|https://github.com/AIDC-AI/Marco-Bench-MIF.|http://arxiv.org/abs/2507.11882v1|
|24|Leveraging large language models to predict antibiotic resistance in <i>Mycobacterium tuberculosis</i>|Conrad Testagrose, Sakshi Pandey, Mohammadali Serajian, Simone Marini, Mattia Prosperi, Christina Boucher|<li><span style="color:#FF5733;">2025-07-01</span></li>|Bioinformatics|https://github.com/ctestagrose/LLMTB.|https://doi.org/10.1093/bioinformatics/btaf232|
|25|DrugTar Improves Druggability Prediction by Integrating Large Language Models and Gene Ontologies|Niloofar Borhani, Iman Izadi, Ali Motahharynia, Mahsa Sheikholeslami, Yousof Gheisari|<li><span style="color:#FF5733;">2025-06-24</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/NBorhani/DrugTar.|https://doi.org/10.1093/bioinformatics/btaf360|
|26|LANG: A Lesson Plan Generation Framework via Multi-Form Interaction with Large Language Models|Yong Ouyang, Jinhao Quan, Huan-Wen Wang, Yawen Zeng, Lingyu Chen|<li><span style="color:#FF5733;">2025-06-17</span></li>|Research Square (Research Square)|https://github.com/ssakana/LANG.|https://doi.org/10.21203/rs.3.rs-6808103/v1|
|27|Prime the search: Using large language models for guiding geometric task and motion planning by warm-starting tree search|Dongryung Lee, Se June Joo, Kimin Lee, Beomjoon Kim|<li><span style="color:#FF5733;">2025-06-06</span></li>|The International Journal of Robotics Research|https://github.com/iMSquared/prime-the-search|https://doi.org/10.1177/02783649251347307|
|28|Survey on Factuality in Large Language Models|Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Qipeng Guo, Xiangkun Hu, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao...|<li><span style="color:#FF5733;">2025-06-02</span></li>|ACM Computing Surveys|https://github.com/wangcunxiang/LLM-Factuality-Survey.|https://doi.org/10.1145/3742420|
|29|Improving drug-drug interaction prediction via in-context learning and judging with large language models|He Qi, Xiaoqiang Li, Chengcheng Zhang, Tianyi Zhao|<li><span style="color:#FF5733;">2025-06-02</span></li>|Frontiers in Pharmacology|https://github.com/zcc1203/ddi-judge.|https://doi.org/10.3389/fphar.2025.1589788|
|30|SummArIzeR: Simplifying cross-database enrichment result clustering and annotation via large language models|Marie Brinkmann, Michael Bonelli, Anela Tosevska|<li><span style="color:#FF5733;">2025-06-01</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/bonellilab/SummArIzeR.|https://doi.org/10.1101/2025.05.28.656331|
|31|The accuracy and efficiency of large language models for chart review in cancer genetics|James Dickerson, Margaret Shaw, Mina Satoyoshi, Sonia Rios‐Ventura, Kerry Kingham, Allison W. Kurian, Jennifer L. Caswel...|<li><span style="color:#FF5733;">2025-05-28</span></li>|Journal of Clinical Oncology|https://github.com/MrJimb0/ASCO2025|https://doi.org/10.1200/jco.2025.43.16_suppl.e22603|
|32|Mitigating Age-Related Bias in Large Language Models: Strategies for Responsible Artificial Intelligence Development|Zhuang Liu, S. Qian, Shuirong Cao, Tianyu Shi|<li><span style="color:#FF5733;">2025-05-21</span></li>|INFORMS journal on computing|https://github.com/INFORMSJoC/2024.0645|https://doi.org/10.1287/ijoc.2024.0645|
|33|ProtFun: A Protein Function Prediction Model Using Graph Attention Networks with a Protein Large Language Model|Muhammed Talo, Serdar Bozdag|<li><span style="color:#FF5733;">2025-05-17</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/bozdaglab/ProtFun|https://doi.org/10.1101/2025.05.13.653854|
|34|Social determinants of health extraction from clinical notes across institutions using large language models|Vipina K. Keloth, Salih Selek, Qingyu Chen, Christopher Gilman, Sunyang Fu, Yifang Dang, Xinghan Chen, Xinyue Hu, Yujia ...|<li><span style="color:#FF5733;">2025-05-17</span></li>|npj Digital Medicine|https://github.com/BIDS-Xu-Lab/LLMs4SDoH|https://doi.org/10.1038/s41746-025-01645-8|
|35|Exploring Zero-Shot Cross-Lingual Biomedical Concept Normalization via Large Language Models|Hossein Rouhizadeh, Anthony Yazdani, Boya Zhang, Douglas Teodoro|<li><span style="color:#FF5733;">2025-05-15</span></li>|Studies in health technology and informatics|https://github.com/hrouhizadeh/zsh_cl_bcn.|https://doi.org/10.1101/2025.02.27.25323007|
|36|Leveraging Large Language Models for Literature-Driven Prioritization of Protein Binding Pockets|Roman Stratiichuk, Mykola Melnychenko, Ihor Koleiev, Taras Voitsitskyi, Husak Vladyslav, Наталія Анатоліївна Шевчук, Zak...|<li><span style="color:#FF5733;">2025-05-15</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/MelnychenkoM/LLM-benchmark-dataset.|https://doi.org/10.1101/2025.05.13.653394|
|37|VirNucPro: an identifier for the identification of viral short sequences using six-frame translation and large language models|Jing Li, Jia Mi, Wei Lin, Fengjuan Tian, Jing Wan, Jingyang Gao, Yigang Tong|<li><span style="color:#FF5733;">2025-05-01</span></li>|Briefings in Bioinformatics|https://github.com/Li-Jing-1997/VirNucPro.|https://doi.org/10.1093/bib/bbaf224|
|38|UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models|Yu Zheng, Longyi Liu, Yuming Lin, Jie Feng, Guozhen Zhang, Depeng Jin, Yong Li|<li><span style="color:#FF5733;">2025-04-30</span></li>|Research Square (Research Square)|https://github.com/tsinghua-fib-lab/PlanBench|https://doi.org/10.21203/rs.3.rs-6551071/v1|
|39|Evaluating Personality Traits of Large Language Models Through Scenario-based Interpretive Benchmarking|Alessandro Berti|<li><span style="color:#FF5733;">2025-04-09</span></li>|OpenAlex|https://github.com/fit-alessandro-berti/llm-dreams-benchmark.|https://doi.org/10.20944/preprints202504.0435.v1|
|40|Improving Text-to-Sql Conversion for Low-Resource Languages Using Large Language Models|Emır Öztürk|<li><span style="color:#FF5733;">2025-03-26</span></li>|Bitlis Eren Üniversitesi Fen Bilimleri Dergisi|https://github.com/emirozturk/TT2SQL.|https://doi.org/10.17798/bitlisfen.1561298|
|41|Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification|Iain J. Cruickshank, Lynnette Hui Xian Ng|<li><span style="color:#FF5733;">2025-03-25</span></li>|ACM Transactions on Intelligent Systems and Technology|https://github.com/ijcruic/LLM-Stance-Labeling|https://doi.org/10.1145/3725816|
|42|Enhancing Gene Set Overrepresentation Analysis with Large Language Models|Jianjun Zhu, Rebecca Y. Wang, Xiaoting Wang, Ricardo B. R. Azevedo, Alexander Moreno, Julia Kuhn, Zia Khan|<li><span style="color:#FF5733;">2025-03-13</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/Alector-BIO/llm2geneset|https://doi.org/10.1101/2024.11.11.621189|
|43|NTxPred2: A large language model for predicting neurotoxic peptides and neurotoxins|Anand Singh Rathore, Saloni Jain, Shubham Choudhury, Gajendra P. S. Raghava|<li><span style="color:#FF5733;">2025-03-07</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/raghavagps/ntxpred2|https://doi.org/10.1101/2025.03.01.640936|
|44|Automatic recognition of cross-language classic entities based on large language models|Qiankun Xu, Yutong Liu, Dongbo Wang, Huang Shuiqing|<li><span style="color:#FF5733;">2025-03-03</span></li>|OpenAlex|https://github.com/Xunzi-LLM-of-Chinese-classics/XunziALLM|https://doi.org/10.1038/s40494-025-01624-y|
|45|SensitiveCancerGPT: Leveraging Generative Large Language Model on Structured Omics Data to Optimize Drug Sensitivity Prediction|Shaika Chowdhury, Sivaraman Rajaganapathy, Lichao Sun, Liewei Wang, Ping Yang, James R. Cerhan, Nansu Zong|<li><span style="color:#FF5733;">2025-02-28</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/bioIKEA/SensitiveCancerGPT.|https://doi.org/10.1101/2025.02.27.640661|
|46|CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems
  Based on Large Language Models|Zhenhong Zhou, Zherui Li, Jie Zhang, Yuanhe Zhang, Kun Wang, Yang Liu, Qing Guo|<li><span style="color:#FF5733;">2025-02-20</span></li>|arXiv|https://github.com/zhrli324/Corba.|https://doi.org/10.48550/arXiv.2502.14529|
|47|Dynamic Low-Rank Sparse Adaptation for Large Language Models|Weizhong Huang, Yuxin Zhang, Xiawu Zheng, Yang Liu, Jing Lin, Yiwu Yao, Rongrong Ji|<li><span style="color:#FF5733;">2025-02-20</span></li>|ICLR|https://github.com/wzhuang-xmu/LoSA.|https://openreview.net/forum?id=oXh0939Zzq|
|48|TritonBench: Benchmarking Large Language Model Capabilities for
  Generating Triton Operators|Jianling Li, Shaohui Li, Zhihao Gao, Qi Shi, Yuxuan Li, Zefan Wang, Jie Huang, Haojie Wang, Jianrong Wang, Xu Han, Zhiyu...|<li><span style="color:#FF5733;">2025-02-20</span></li>|arXiv|https://github.com/thunlp/TritonBench.|https://doi.org/10.48550/arXiv.2502.14752|
|49|AI-Empowered Catalyst Discovery: A Survey from Classical Machine
  Learning Approaches to Large Language Models|Yuanyuan Xu, Hanchen Wang, Wenjie Zhang, Lexing Xie, Yin Chen, Flora D. Salim, Ying Zhang, J. Justin Gooding, Toby Walsh|<li><span style="color:#FF5733;">2025-02-19</span></li>|arXiv|https://github.com/LuckyGirl-XU/Awesome-Artificial-Intelligence-Empowered-Catalyst-Discovery.|https://doi.org/10.48550/arXiv.2502.13626|
|50|Collaborative Retrieval for Large Language Model-based Conversational
  Recommender Systems|Yaochen Zhu, Chao Wan, Harald Steck, Dawen Liang, Yesu Feng, Nathan Kallus, Jundong Li|<li><span style="color:#FF5733;">2025-02-19</span></li>|OpenAlex|https://github.com/yaochenzhu/CRAG.|https://doi.org/10.48550/arXiv.2502.14137|
|51|Lost in Sequence: Do Large Language Models Understand Sequential
  Recommendation?|Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian J. McAuley, Chanyoung Par...|<li><span style="color:#FF5733;">2025-02-19</span></li>|arXiv|https://github.com/Sein-Kim/LLM-SRec.|https://doi.org/10.48550/arXiv.2502.13909|
|52|On the logical skills of large language models: evaluations using
  arbitrarily complex first-order logic problems|Shokhrukh Ibragimov, Arnulf Jentzen, Benno Kuckuck|<li><span style="color:#FF5733;">2025-02-19</span></li>|arXiv|https://github.com/bkuckuck/logical-skills-of-llms.|https://doi.org/10.48550/arXiv.2502.14180|
|53|PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training
  Quantization Methods for Large Language Models|Jiaqi Zhao, Miao Zhang, Ming Wang, Yuzhang Shang, Kaihao Zhang, Weili Guan, Yaowei Wang, Danshi Wang|<li><span style="color:#FF5733;">2025-02-18</span></li>|arXiv|https://github.com/zjq0455/PTQ1.61.|https://aclanthology.org/2025.acl-long.225/|
|54|SEA: Low-Resource Safety Alignment for Multimodal Large Language Models
  via Synthetic Embeddings|Weikai Lu, Hao Peng, Huiping Zhuang, Cen Chen, Ziqian Zeng|<li><span style="color:#FF5733;">2025-02-18</span></li>|arXiv|https://github.com/ZeroNLP/SEA.|https://aclanthology.org/2025.acl-long.1212/|
|55|G-Refer: Graph Retrieval-Augmented Large Language Model for Explainable
  Recommendation|Yuhan Li, Xinni Zhang, Linhao Luo, Heng Chang, Yuxiang Ren, Irwin King, Jia Li|<li><span style="color:#FF5733;">2025-02-18</span></li>|OpenAlex|https://github.com/Yuhan1i/G-Refer.|https://doi.org/10.48550/arXiv.2502.12586|
|56|$\mathttGeLLM^3O$: Generalizing Large Language Models for
  Multi-property Molecule Optimization|Vishal Dey, Xiao Hu, Xia Ning|<li><span style="color:#FF5733;">2025-02-18</span></li>|arXiv (Cornell University)|https://github.com/ninglab/GeLLMO.|http://arxiv.org/abs/2502.13398|
|57|Evaluation of ChatGPT and Gemini large language models for pharmacometrics with NONMEM|Euibeom Shin, Yifan Yu, Robert R. Bies, Murali Ramanathan|<li><span style="color:#FF5733;">2025-02-18</span></li>|Journal of Pharmacokinetics and Pharmacodynamics|https://github.com/metrumresearchgroup/mrgsolve20.|https://doi.org/10.21203/rs.3.rs-4189234/v1|
|58|Evaluation of Large Language Models for an AI Chat Assistant Focused on Pumas and Pharmacometrics|Juan Javier González Barbosa, Agastya Vinchhi, Vijay Ivaturi|<li><span style="color:#FF5733;">2025-02-18</span></li>|OpenAlex|https://github.com/explodinggradients/ragas|https://doi.org/10.70534/jnza2834|
|59|VRoPE: Rotary Position Embedding for Video Large Language Models|Zikang Liu, Longteng Guo, Yepeng Tang, Junxian Cai, Kai Ma, Xi Chen, Jing Liu|<li><span style="color:#FF5733;">2025-02-17</span></li>|arXiv|https://github.com/johncaged/VRoPE|https://doi.org/10.48550/arXiv.2502.11664|
|60|Idiosyncrasies in Large Language Models|Ming-Jie Sun, Yue Yin, Zeshui Xu, J. Zico Kolter, Zhuang Liu|<li><span style="color:#FF5733;">2025-02-17</span></li>|arXiv|https://github.com/locuslab/llm-idiosyncrasies.|https://doi.org/10.48550/arXiv.2502.12150|
|61|RIDE: Enhancing Large Language Model Alignment through Restyled
  In-Context Learning Demonstration Exemplars|Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari|<li><span style="color:#FF5733;">2025-02-17</span></li>|arXiv|https://github.com/AnonymousCode-ComputerScience/RIDE.|https://doi.org/10.48550/arXiv.2502.11681|
|62|A Survey of Personalized Large Language Models: Progress and Future
  Directions|Jiahong Liu, Zexuan Qiu, Zhongyang Li, Quanyu Dai, Jieming Zhu, Minda Hu, Menglin Yang, Irwin King|<li><span style="color:#FF5733;">2025-02-17</span></li>|arXiv|https://github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models.|https://doi.org/10.48550/arXiv.2502.11528|
|63|BoT: Breaking Long Thought Processes of o1-like Large Language Models
  through Backdoor Attack|Zihao Zhu, Hongbao Zhang, Mingda Zhang, Ruotong Wang, Guanzong Wu, Ke Xu, Baoyuan Wu|<li><span style="color:#FF5733;">2025-02-16</span></li>|arXiv|https://github.com/zihao-ai/BoT|https://doi.org/10.48550/arXiv.2502.12202|
|64|CORDIAL: Can Multimodal Large Language Models Effectively Understand
  Coherence Relationships?|Aashish Anantha Ramakrishnan, Aadarsh Anantha Ramakrishnan, Dongwon Lee|<li><span style="color:#FF5733;">2025-02-16</span></li>|arXiv|https://github.com/aashish2000/CORDIAL.|https://aclanthology.org/2025.acl-long.1033/|
|65|Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical
  Abilities in Large Language Models|Haoyang Li, Xuejia Chen, Zhanchao Xu, Darian Li, Nicole Hu, Fei Teng, Yiming Li, Luyu Qiu, Chen Jason Zhang, Qing Li, Le...|<li><span style="color:#FF5733;">2025-02-16</span></li>|arXiv|https://github.com/TreeAI-Lab/NumericBench.|https://doi.org/10.48550/arXiv.2502.11075|
|66|Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on
  Large Language Models|Zonghao Ying, Deyue Zhang, Zonglei Jing, Yisong Xiao, Quanchen Zou, Aishan Liu, Siyuan Liang, Xiangzheng Zhang, Xianglon...|<li><span style="color:#FF5733;">2025-02-16</span></li>|arXiv|https://github.com/NY1024/RACE|https://doi.org/10.48550/arXiv.2502.11054|
|67|SURGE: On the Potential of Large Language Models as General-Purpose
  Surrogate Code Executors|Bo Lyu, Susan S. Huang, Zhengzhao Liang|<li><span style="color:#FF5733;">2025-02-16</span></li>|arXiv|https://github.com/Imbernoulli/SURGE.|https://doi.org/10.48550/arXiv.2502.11167|
|68|Utilizing Pretrained Vision Transformers and Large Language Models for Epileptic Seizure Prediction|Paras Parani, Umair Mohammad, Fahad Saeed|<li><span style="color:#FF5733;">2025-02-16</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/pcdslab/UtilLLM_EPS|https://doi.org/10.1109/cdma61895.2025.00028|
|69|Injecting Domain-Specific Knowledge into Large Language Models: A
  Comprehensive Survey|Zhihua Song, Bin Yan, Yuhan Liu, Miao Fang, Mingzhe Li, Rui Yan, Xiuying Chen|<li><span style="color:#FF5733;">2025-02-15</span></li>|arXiv|https://github.com/abilliyb/Knowledge_Injection_Survey_Papers|https://doi.org/10.48550/arXiv.2502.10708|
|70|LANTERN: Leveraging Large Language Models and Transformers for Enhanced Molecular Interactions|Cong Nga Ha, Phuong Viet Pham, Truong Son Hy|<li><span style="color:#FF5733;">2025-02-15</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/HySonLab/LANTERN|https://doi.org/10.1101/2025.02.10.637522|
|71|SQuARE: Sequential Question Answering Reasoning Engine for Enhanced
  Chain-of-Thought in Large Language Models|Daniel Fleischer, Moshe Berchansky, George Markovits, Moshe Wasserblat|<li><span style="color:#FF5733;">2025-02-13</span></li>|arXiv|https://github.com/IntelLabs/RAG-FiT|https://doi.org/10.48550/arXiv.2502.09390|
|72|Data Augmentation to Improve Large Language Models in Food Hazard and
  Product Detection|Areeg Fahad Rasheed, Mahdi Zarkoosh, Shimam Amer Chasib, Safa F. Abbas|<li><span style="color:#FF5733;">2025-02-12</span></li>|arXiv|https://github.com/AREEG94FAHAD/food-hazard-prdouct-cls|https://doi.org/10.48550/arXiv.2502.08687|
|73|Do Large Language Models have Spatial Cognitive Abilities?|Ruoling Wu, Danhuai Guo|<li><span style="color:#FF5733;">2025-02-11</span></li>|ACM Transactions on Intelligent Systems and Technology|https://github.com/LLING000/SCABenchmark|https://doi.org/10.1145/3716855|
|74|DrugImproverGPT: A Large Language Model for Drug Optimization with
  Fine-Tuning via Structured Policy Optimization|Xuefeng Liu, Songhao Jiang, Siyu Chen, Zhuoran Yang, Yuxin Chen, Ian T. Foster, Rick Stevens|<li><span style="color:#FF5733;">2025-02-10</span></li>|arXiv|https://github.com/xuefeng-cs/DrugImproverGPT.|https://doi.org/10.48550/arXiv.2502.07237|
|75|Large Language Models Meet Symbolic Provers for Logical Reasoning
  Evaluation|Chengwen Qi, Ren Ma, Bowen Li, He Du, Binyuan Hui, Jinwang Wu, Yuanjun Laili, Conghui He|<li><span style="color:#FF5733;">2025-02-10</span></li>|ICLR|https://github.com/opendatalab/ProverGen|https://openreview.net/forum?id=C25SgeXWjE|
|76|Large Language Models in Software Security: A Survey of Vulnerability
  Detection Techniques and Insights|Ze Sheng, Zhicheng Chen, Shanqiang Gu, Heqing Huang, Guofei Gu, Jeff Huang|<li><span style="color:#FF5733;">2025-02-10</span></li>|arXiv (Cornell University)|https://github.com/OwenSanzas/LLM-For-Vulnerability-Detection|http://arxiv.org/abs/2502.07049|
|77|RALLRec: Improving Retrieval Augmented Large Language Model
  Recommendation with Representation Learning|Jian Xu, Sichun Luo, Xiangyu Chen, Haifeng Huang, Hanxu Hou, Linqi Song|<li><span style="color:#FF5733;">2025-02-09</span></li>|OpenAlex|https://github.com/JianXu95/RALLRec.|https://doi.org/10.48550/arXiv.2502.06101|
|78|XiHeFusion: Harnessing Large Language Models for Science Communication
  in Nuclear Fusion|Xiao Wang, Qingquan Yang, Fuling Wang, Qiang Chen, Wann‐Yih Wu, Yu Jin, Jun Jiang, Liang Jin, Bo Jiang, Dengdi Sun, Wenz...|<li><span style="color:#FF5733;">2025-02-08</span></li>|arXiv|https://github.com/Event-AHU/XiHeFusion.|https://doi.org/10.48550/arXiv.2502.05615|
|79|Top-DTI: Integrating Topological Deep Learning and Large Language Models for Drug Target Interaction Prediction|Muhammed Talo, Serdar Bozdag|<li><span style="color:#FF5733;">2025-02-08</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/bozdaglab/Top_DTI|https://doi.org/10.1093/bioinformatics/btaf183|
|80|Predicting Large Language Model Capabilities on Closed-Book QA Tasks
  Using Only Information Available Prior to Training|Changhao Jiang, Ming Zhang, Junjie Ye, Xiaoran Fan, Yifei Cao, Jiajun Sun, Zhiheng Xi, Shihan Dou, Yi Dong, Yujiong Shen...|<li><span style="color:#FF5733;">2025-02-06</span></li>|arXiv|https://github.com/yuhui1038/SMI.|https://doi.org/10.48550/arXiv.2502.04066|
|81|Intent Representation Learning with Large Language Model for
  Recommendation|Yu Wang, Lei Sang, Yi Zhang, Yiwen Zhang|<li><span style="color:#FF5733;">2025-02-05</span></li>|Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/wangyu0627/IRLLRec.|https://doi.org/10.1145/3726302.3730011|
|82|Knowledge Distillation from Large Language Models for Household Energy
  Modeling|Mohannad Takrouri, Nicolas Mauricio Cuadrado, Martin Takáč|<li><span style="color:#FF5733;">2025-02-05</span></li>|arXiv|https://github.com/Singularity-AI-Lab/LLM-Energy-Knowledge-Distillation|https://doi.org/10.48550/arXiv.2502.03034|
|83|Reinforced Prompt Personalization for Recommendation with Large Language Models|Wenyu Mao, Jiancan Wu, Weijian Chen, Chongming Gao, Xiang Wang, Xiangnan He|<li><span style="color:#FF5733;">2025-02-04</span></li>|ACM transactions on office information systems|https://github.com/maowenyu-11/RPP|https://doi.org/10.48550/arXiv.2407.17115|
|84|Risk-Aware Driving Scenario Analysis with Large Language Models|Y. S. Gao, Mattia Piccinini, Johannes Betz|<li><span style="color:#FF5733;">2025-02-04</span></li>|arXiv|https://github.com/yuangao-tum/Riskaware-Scenario-analyse|https://doi.org/10.48550/arXiv.2502.02145|
|85|SAISA: Towards Multimodal Large Language Models with Both Training and
  Inference Efficiency|Qianhao Yuan, Yanjiang Liu, Yaojie Lu, Hongyu Lin, Ben He, Xianpei Han, Le Sun|<li><span style="color:#FF5733;">2025-02-04</span></li>|arXiv|https://github.com/icip-cas/SAISA.|https://doi.org/10.48550/arXiv.2502.02458|
|86|AdaSVD: Adaptive Singular Value Decomposition for Large Language Models|Zhiteng Li, Mingyuan Xia, Jingyuan Zhang, Hui Zheng, Linghe Kong, Yulun Zhang, Xiaokang Yang|<li><span style="color:#FF5733;">2025-02-03</span></li>|arXiv|https://github.com/ZHITENGLI/AdaSVD.|https://doi.org/10.48550/arXiv.2502.01403|
|87|AtmosSci-Bench: Evaluating the Recent Advance of Large Language Model
  for Atmospheric Science|Chenyue Li, Wen Deng, Mengqian Lu, Binhang Yuan|<li><span style="color:#FF5733;">2025-02-03</span></li>|arXiv|https://github.com/Relaxed-System-Lab/AtmosSci-Bench.|https://doi.org/10.48550/arXiv.2502.01159|
|88|Robust-LLaVA: On the Effectiveness of Large-Scale Robust Image Encoders
  for Multi-modal Large Language Models|Hashmat Shadab Malik, Fahad Shamshad, Muzammal Naseer, Karthik Nandakumar, Fahad Shahbaz Khan, Salman Khan|<li><span style="color:#FF5733;">2025-02-03</span></li>|arXiv|https://github.com/HashmatShadab/Robust-LLaVA.|https://doi.org/10.48550/arXiv.2502.01576|
|89|sciLaMA: A Single-Cell Representation Learning Framework to Leverage Prior Knowledge from Large Language Models|Hongru Hu, Shuwen Zhang, Yongin Choi, Venkat S. Malladi, Gerald Quon|<li><span style="color:#FF5733;">2025-02-03</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/microsoft/sciLaMA.|https://doi.org/10.1101/2025.01.28.635153|
|90|Breaking Focus: Contextual Distraction Curse in Large Language Models|Yue Huang, Yanbo Wang, Zixiang Xu, Chujie Gao, Siyuan Wu, Jiayi Ye, Xiuying Chen, Pin-Yu Chen, Xiangliang Zhang|<li><span style="color:#FF5733;">2025-02-03</span></li>|arXiv|https://github.com/wyf23187/LLM_CDV.|https://doi.org/10.48550/arXiv.2502.01609|
|91|LIBRA: Measuring Bias of Large Language Model from a Local Context|B. Y. Pang, Tingrui Qiao, Caroline Walker, Chris Cunningham, Yun Sing Koh|<li><span style="color:#FF5733;">2025-02-01</span></li>|Lecture notes in computer science|https://github.com/ipangbo/LIBRA.|https://doi.org/10.1007/978-3-031-88708-6_1|
|92|MetaOpenFOAM 2.0: Large Language Model Driven Chain of Thought for
  Automating CFD Simulation and Post-Processing|Yuxuan Chen, Xu Zhu, Hua Zhou, Zhuyin Ren|<li><span style="color:#FF5733;">2025-02-01</span></li>|arXiv|https://github.com/Terry-cyx/MetaOpenFOAM|https://doi.org/10.48550/arXiv.2502.00498|
|93|Speculative Ensemble: Fast Large Language Model Ensemble via Speculation|Jiale Fu, Yuchu Jiang, Junkai Chen, Jiaming Fan, Peng Geng, Yang Xu|<li><span style="color:#FF5733;">2025-02-01</span></li>|arXiv|https://github.com/Kamichanw/Speculative-Ensemble|https://doi.org/10.48550/arXiv.2502.01662|
|94|LLMDet: Learning Strong Open-Vocabulary Object Detectors under the
  Supervision of Large Language Models|Shenghao Fu, Qize Yang, Qijie Mo, Junkai Yan, Xihan Wei, Jingke Meng, Xiaohua Xie, Wei-Shi Zheng|<li><span style="color:#FF5733;">2025-01-31</span></li>|CVPR|https://github.com/iSEE-Laboratory/LLMDet.|https://openaccess.thecvf.com/content/CVPR2025/html/Fu_LLMDet_Learning_Strong_Open-Vocabulary_Object_Detectors_under_the_Supervision_of_CVPR_2025_paper.html|
|95|Panacea: Mitigating Harmful Fine-tuning for Large Language Models via
  Post-fine-tuning Perturbation|Yibo Wang, Tiansheng Huang, Li Shen, Huanjin Yao, Haotian Luo, Rui Liu, Naiqiang Tan, Jiaxing Huang, Dacheng Tao|<li><span style="color:#FF5733;">2025-01-29</span></li>|arXiv|https://github.com/w-yibo/Panacea|https://doi.org/10.48550/arXiv.2501.18100|
|96|Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing
  Guardrail Moderation|Tiansheng Huang, Sihao Hu, Fatih İlhan, Selim Furkan Tekin, Ling Liu|<li><span style="color:#FF5733;">2025-01-29</span></li>|arXiv|https://github.com/git-disl/Virus|https://doi.org/10.48550/arXiv.2501.17433|
|97|SafeRAG: Benchmarking Security in Retrieval-Augmented Generation of
  Large Language Model|Xun Liang, Simin Niu, Zhiyu Li, Sensen Zhang, Hanyu Wang, Feiyu Xiong, Jianqing Fan, Bo Tang, Shichao Song, Mengwei Wang...|<li><span style="color:#FF5733;">2025-01-28</span></li>|arXiv|https://github.com/IAAR-Shanghai/SafeRAG.|https://aclanthology.org/2025.acl-long.230/|
|98|Large Language Model Critics for Execution-Free Evaluation of Code
  Changes|Aashish Yadavally, Hoan Anh Nguyen, Laurent Callot, Gauthier Guinet|<li><span style="color:#FF5733;">2025-01-27</span></li>|arXiv|https://github.com/amazon-science/code-agent-eval.|https://doi.org/10.48550/arXiv.2501.16655|
|99|Analyzing and Boosting the Power of Fine-Grained Visual Recognition for
  Multi-modal Large Language Models|Hulingxiao He, Geng Li, Zengmin Geng, Jinglin Xu, Yuxin Peng|<li><span style="color:#FF5733;">2025-01-25</span></li>|ICLR|https://github.com/PKU-ICST-MIPL/Finedefics_ICLR2025.|https://openreview.net/forum?id=p3NKpom1VL|
|100|JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning
  in Large Language Models|Michael K. Chen, Xikun Zhang, Dacheng Tao|<li><span style="color:#FF5733;">2025-01-24</span></li>|arXiv|https://github.com/michaelchen-lab/JustLogic|https://doi.org/10.48550/arXiv.2501.14851|
|101|Softplus Attention with Re-weighting Boosts Length Extrapolation in
  Large Language Models|Bo Gao, Michael W. Spratling|<li><span style="color:#FF5733;">2025-01-23</span></li>|arXiv|https://github.com/iminfine/freeatten.|https://doi.org/10.48550/arXiv.2501.13428|
|102|Can Large Language Models Understand Preferences in Personalized
  Recommendation?|Zhaoxuan Tan, Zinan Zeng, Qingkai Zeng, Zhenyu Wu, Zheyuan Liu, Fengran Mo, Meng Jiang|<li><span style="color:#FF5733;">2025-01-23</span></li>|arXiv|https://github.com/TamSiuhin/PerRecBench|https://doi.org/10.48550/arXiv.2501.13391|
|103|OstQuant: Refining Large Language Model Quantization with Orthogonal and
  Scaling Transformations for Better Distribution Fitting|Xing Hu, Yuan Cheng, Dawei Yang, Zhixuan Chen, Zukang Xu, Jiangyong Yu, Chen Xu, Zhihang Yuan, Zhe Jiang, Sifan Zhou|<li><span style="color:#FF5733;">2025-01-23</span></li>|ICLR|https://github.com/BrotherHappy/OSTQuant|https://openreview.net/forum?id=rAcgDBdKnP|
|104|Can open source large language models be used for tumor documentation in Germany? - An evaluation on urological doctors&apos; notes|Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Meike Ressing, Torsten Panholzer|<li><span style="color:#FF5733;">2025-01-21</span></li>|BioData Mining|https://github.com/stefan-m-lenz/UroLlmEval.|https://doi.org/10.1186/s13040-025-00463-8|
|105|Distillation Quantification for Large Language Models|Sunbowen Lee, Junting Zhou, Chang Ao, Kaige Li, Xinrun Du, Sirui He, Jiaheng Liu, Min Yang, Zhoufutu Wen, Shiwen Ni|<li><span style="color:#FF5733;">2025-01-21</span></li>|arXiv|https://github.com/Aegis1863/LLMs-Distillation-Quantification.|https://doi.org/10.48550/arXiv.2501.12619|
|106|An Empirical Characterization of Outages and Incidents in Public
  Services for Large Language Models|Xiaoyu Chu, Sacheendra Talluri, Qingxian Lu, Alexandru Iosup|<li><span style="color:#FF5733;">2025-01-21</span></li>|OpenAlex|https://github.com/atlarge-research/llm-service-analysis.|https://doi.org/10.48550/arXiv.2501.12469|
|107|ESCARGOT: An AI Agent Leveraging Large Language Models, Dynamic Graph of Thoughts, and Biomedical Knowledge Graphs for Enhanced Reasoning|Nicholas Matsumoto, Hyun‐Jun Choi, Jay Moran, Miguel Hernandez, Mythreye Venkatesan, Xi Li, Jui-Hsuan Chang, Paul P. Wan...|<li><span style="color:#FF5733;">2025-01-20</span></li>|Bioinformatics|https://github.com/EpistasisLab/ESCARGOT.|https://doi.org/10.1093/bioinformatics/btaf031|
|108|InsQABench: Benchmarking Chinese Insurance Domain Question Answering
  with Large Language Models|Jing Ding, Feng Kai, Binbin Lin, J. G. Cai, Qiushi Wang, Y. G. Xie, Xiaojin Zhang, Zhongyu Wei, Wei Chen|<li><span style="color:#FF5733;">2025-01-18</span></li>|arXiv|https://github.com/HaileyFamo/InsQABench.git.|https://doi.org/10.48550/arXiv.2501.10943|
|109|CXR-LLaVA: a multimodal large language model for interpreting chest X-ray images|Seowoo Lee, M. D., Jiwon Youn, Mansu Kim D., Soon Ho Yoon, M. D. D|<li><span style="color:#FF5733;">2025-01-15</span></li>|European Radiology|https://github.com/ECOFRI/CXR_LLAVA.|https://doi.org/10.1007/s00330-024-11339-6|
|110|PokerBench: Training Large Language Models to become Professional Poker
  Players|Richard Zhuang, Akshat Gupta, Chunhui Yang, Aniket Rahane, Zhengyu Li, Gopala Anumanchipalli|<li><span style="color:#FF5733;">2025-01-14</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/pokerllm/pokerbench|https://doi.org/10.1609/aaai.v39i24.34814|
|111|LLM4SR: A Survey on Large Language Models for Scientific Research|Zhongling Luo, Zonglin Yang, Zheng Xu, Wei Yang, Xinya Du|<li><span style="color:#FF5733;">2025-01-08</span></li>|arXiv|https://github.com/du-nlp-lab/LLM4SR|https://doi.org/10.48550/arXiv.2501.04306|
|112|Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of
  Large Language Models|Qianchen Ren, Jie Zeng, Qianyu He, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Z. J. Sun, F. Richard Yu|<li><span style="color:#FF5733;">2025-01-08</span></li>|arXiv|https://github.com/Rainier-rq/FollowSoftConstraints.|https://doi.org/10.48550/arXiv.2501.04945|
|113|ChronoSense: Exploring Temporal Understanding in Large Language Models
  with Time Intervals of Events|Duygu Sezen Islakoglu, Jan-Christoph Kalo|<li><span style="color:#FF5733;">2025-01-06</span></li>|arXiv|https://github.com/duyguislakoglu/chronosense.|https://doi.org/10.48550/arXiv.2501.03040|
|114|Visual Large Language Models for Generalized and Specialized
  Applications|Yifan Li, Zhixin Lai, Wentao Bao, Zhen Tan, Anh Dao, Kewei Sui, Jiayi Shen, Dong Liu, Huan Liu, Yu Kong|<li><span style="color:#FF5733;">2025-01-06</span></li>|arXiv|https://github.com/JackYFL/awesome-VLLMs.|https://doi.org/10.48550/arXiv.2501.02765|
|115|MIRAGE: Exploring How Large Language Models Perform in Complex Social
  Interactive Environments|Cai Yin, Gu Zhouhong, Du Zhaohan, Ye Zheyu, Cao Shaosheng, Xu Yiqian, Feng Hongwei, Ping Chen|<li><span style="color:#FF5733;">2025-01-03</span></li>|arXiv|https://github.com/lime728/MIRAGE|https://doi.org/10.48550/arXiv.2501.01652|
|116|Cold-Start Recommendation towards the Era of Large Language Models
  (LLMs): A Comprehensive Survey and Roadmap|Wei Zhang, Yuanchen Bei, Liangwei Yang, Henry Peng Zou, Peilin Zhou, Aiwei Liu, Yinghui Li, Liqiao Chen, Jian-Ling Wang,...|<li><span style="color:#FF5733;">2025-01-03</span></li>|arXiv|https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.|https://doi.org/10.48550/arXiv.2501.01945|
|117|Aligning Large Language Models for Faithful Integrity Against Opposing
  Argument|Yong Zhao, Yang Deng, See-Kiong Ng, Tat‐Seng Chua|<li><span style="color:#FF5733;">2025-01-02</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/zhaoy777/AFICE.git|https://doi.org/10.1609/aaai.v39i26.34990|
|118|Predicting differentially methylated cytosines in TET and DNMT3 knockout mutants via a large language model|Stefano Lonardi, Stefano Lonardi|<li><span style="color:#FF5733;">2025-01-01</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/ucrbioinfo/dmc_prediction.|https://doi.org/10.1101/2024.05.02.592257|
|119|Labels Generated by Large Language Model Helps Measuring People&apos;s Empathy in Vitro|Md. Rakibul Hasan, Yue Yao, Md. Zakir Hossain, Aneesh Krishna, Imre J. Rudas, Shafin Rahman, Tom Gedeon|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/hasan-rakibul/LLMPathy|https://doi.org/10.48550/arXiv.2501.00691|
|120|Large language models open new way of AI-assisted molecule design for chemists|Shoichi Ishida, Tomohiro Sato, Teruki Honma, Kei Terayama|<li><span style="color:#FF5733;">2025-01-01</span></li>|Journal of Cheminformatics|https://github.com/molecule-generator-collection/ChatChemTS.|https://doi.org/10.26434/chemrxiv-2024-1p82f|
|121|Medical Graph RAG: Evidence-based Medical Large Language Model via Graph Retrieval-Augmented Generation|Junde Wu, Jiayuan Zhu, Yunli Qi, Jingkun Chen, Min Xu, Filippo Menolascina, Yueming Jin, Vicente Grau|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/MedicineToken/Medical-Graph-RAG|https://aclanthology.org/2025.acl-long.1381/|
|122|Multi-Agent Systems Powered by Large Language Models: Applications in Swarm Intelligence|Cristian Jimenez-Romero, Alper Yegenoglu, Christian Blum|<li><span style="color:#FF5733;">2025-01-01</span></li>|Frontiers in Artificial Intelligence|https://github.com/crjimene/swarm_gpt|https://doi.org/10.48550/arXiv.2503.03800|
|123|Neuron based Personality Trait Induction in Large Language Models|Jia Deng, Tianyi Tang, Yanbin Yin, Wenhao Yang, Wayne Xin Zhao, Ji-Rong Wen|<li><span style="color:#FF5733;">2025-01-01</span></li>|ICLR|https://github.com/RUCAIBox/NPTI.|https://openreview.net/forum?id=LYHEY783Np|
|124|PAT: Pruning-Aware Tuning for Large Language Models|Yijiang Liu, Huanrui Yang, Youxin Chen, Rongyu Zhang, Miao Wang, Yuan Du, Li Du|<li><span style="color:#FF5733;">2025-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/kriskrisliu/PAT_Pruning-Aware-Tuning|https://doi.org/10.1609/aaai.v39i23.34649|
|125|PIP: Perturbation-based Iterative Pruning for Large Language Models|Yi Cao, Wei-Jie Xu, Yucheng Shen, Weijie Shi, Chi-Min Chan, Jiajie Xu|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/caoyiiiiii/PIP.|https://doi.org/10.48550/arXiv.2501.15278|
|126|Pipeline to explore information on genome editing using large language models and genome editing meta-database|Takayuki Suzuki, Hidemasa Bono|<li><span style="color:#FF5733;">2025-01-01</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/szktkyk/extract_geinfo|https://doi.org/10.1101/2024.10.16.617154|
|127|PointLLM-V2: Empowering Large Language Models to Better Understand Point Clouds|Runsen Xu, Shuai Yang, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin|<li><span style="color:#FF5733;">2025-01-01</span></li>|IEEE Transactions on Pattern Analysis and Machine Intelligence|https://github.com/OpenRobotLab/PointLLM.|https://doi.org/10.1007/978-3-031-72698-9_8|
|128|Taming Unleashed Large Language Models With Blockchain for Massive Personalized Reliable Healthcare|Lianshan Sun, Diandong Liu, Maoxue Wang, Yongyi Han, Yanqing Zhang, Biwei Zhou, Yi Ren, Peng zhu|<li><span style="color:#FF5733;">2025-01-01</span></li>|IEEE Journal of Biomedical and Health Informatics|https://github.com/LDDLQ/ChatCBD.|https://doi.org/10.1109/JBHI.2025.3528526|
|129|QuickLLaMA: Query-aware Inference Acceleration for Large Language Models|Jingyao Li, Han Shi, Sitong Wu, Chuanyang Zheng, Zhenguo Li, Xin Jiang, Hong Xu, Jiaya Jia|<li><span style="color:#FF5733;">2025-01-01</span></li>|COLING|https://github.com/dvlab-research/Q-LLM.|https://aclanthology.org/2025.coling-main.34/|
|130|REEF: Representation Encoding Fingerprints for Large Language Models|Jie Zhang, Dongrui Liu, Chen Qian, Linfeng Zhang, Yong Liu, Yu Qiao, Jing Shao|<li><span style="color:#FF5733;">2025-01-01</span></li>|ICLR|https://github.com/tmylla/REEF.|https://openreview.net/forum?id=SnDmPkOJ0T|
|131|ReLearn: Unlearning via Learning for Large Language Models|Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Mengru Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu ...|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/zjunlp/unlearn.|https://aclanthology.org/2025.acl-long.297/|
|132|Reliable Academic Conference Question Answering: A Study Based on Large Language Model|Zhiwei Huang, Long Jin, Junjie Wang, Mingchen Tu, Hua Yin, Zhiqiang Liu, Jiawei Meng, Huajun Chen, Wen Zhang|<li><span style="color:#FF5733;">2025-01-01</span></li>|Communications in computer and information science|https://github.com/zjukg/ConferenceQA.|https://doi.org/10.1007/978-981-96-1809-5_14|
|133|SCAR: Data Selection via Style Consistency-Aware Response Ranking for Efficient Instruction-Tuning of Large Language Models|Zhuang Li, Yuncheng Hua, Thuy-Trang Vu, Haolan Zhan, Lizhen Qu, Gholamreza Haffari|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/zhuang-li/SCAR|https://aclanthology.org/2025.acl-long.625/|
|134|SPRI: Aligning Large Language Models with Context-Situated Principles|Hongli Zhan, Muneeza Azmat, Raya Horesh, Junyi Jessy Li, Mikhail Yurochkin|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/honglizhan/SPRI-public.|https://doi.org/10.48550/arXiv.2502.03397|
|135|Systematic Outliers in Large Language Models|Yongqi An, Xu Zhao, Tao Yu, Ming Tang, Jinqiao Wang|<li><span style="color:#FF5733;">2025-01-01</span></li>|ICLR|https://github.com/an-yongqi/systematic-outliers.|https://openreview.net/forum?id=rLX7Vyyzus|
|136|LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large Language Models|Xiaohao Yang, He Zhao, Dinh Q. Phung, Wray L. Buntine, Lan Du|<li><span style="color:#FF5733;">2025-01-01</span></li>|Transactions of the Association for Computational Linguistics|https://github.com/Xiaohao-Yang/Topic_Model_Evaluation.|https://doi.org/10.48550/arXiv.2406.09008|
|137|TypedThinker: Diversify Large Language Model Reasoning with Typed Thinking|Danqing Wang, Jianxin Ma, Fei Fang, Lei Li|<li><span style="color:#FF5733;">2025-01-01</span></li>|ICLR|https://github.com/dqwang122/ThinkHub.|https://openreview.net/forum?id=VIUisLx8lQ|
|138|User Behavior Simulation with Large Language Model-based Agents for Recommender Systems|Lei Wang, Jingsen Zhang, Hao Yang, Zhiyuan Chen, Jiakai Tang, Zeyu Zhang, Xu Chen, Yankai Lin, Hao Sun, Ruihua Song, Xin...|<li><span style="color:#FF5733;">2025-01-01</span></li>|ACM transactions on office information systems|https://github.com/RUC-GSAI/YuLan-Rec|https://doi.org/10.1145/3708985|
|139|Veracity-Oriented Context-Aware Large Language Models-Based Prompting Optimization for Fake News Detection|Weiqiang Jin, Yang Gao, Tao Tao, Xiujun Wang, Ningwei Wang, Baohai Wu, Biao Zhao|<li><span style="color:#FF5733;">2025-01-01</span></li>|International Journal of Intelligent Systems|https://github.com/albert-jin/CAPE-FND|https://doi.org/10.1155/int/5920142|
|140|WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct|Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-Guang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yanson...|<li><span style="color:#FF5733;">2025-01-01</span></li>|ICLR|https://github.com/nlpxucan/WizardLM|https://openreview.net/forum?id=mMPMHWOdOy|
|141|LMCBert: An Automatic Academic Paper Rating Model Based on Large Language Models and Contrastive Learning|Chuanbin Liu, Xiaowu Zhang, Hongfei Zhao, Zhijie Liu, Xi Xi, Lean Yu|<li><span style="color:#FF5733;">2025-01-01</span></li>|IEEE Transactions on Cybernetics|https://github.com/iioSnail/LMCBert.|https://doi.org/10.1109/TCYB.2025.3550203|
|142|How Can Recommender Systems Benefit from Large Language Models: A Survey|Jianghao Lin, Xinyi Dai, Yunjia Xi, Weiwen Liu, Bo Chen, Hao Zhang, Yong Liu, Chuhan Wu, Xiangyang Li, Chenxu Zhu, Huife...|<li><span style="color:#FF5733;">2025-01-01</span></li>|ACM transactions on office information systems|https://github.com/CHIANGEL/Awesome-LLM-for-RecSys|https://doi.org/10.48550/arXiv.2306.05817|
|143|Knowledge-Driven Feature Selection and Engineering for Genotype Data with Large Language Models|Joseph Lee, Shuhua Yang, Jae Young Baik, Xiaoxi Liu, Zhen Tan, Dawei Li, Zixuan Wen, Bojian Hou, Duy Duong‐Tran, Tianlon...|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv (Cornell University)|https://github.com/PennShenLab/FREEFORM.|http://arxiv.org/abs/2410.01795|
|144|CFBenchmark-MM: Chinese Financial Assistant Benchmark for Multimodal Large Language Model|Lei Yang, Jiangtong Li, Ming Jiang, Junjie Hu, Dawei Cheng, Zhijun Ding, Changjun Jiang|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/TongjiFinLab/CFBenchmark.|https://doi.org/10.48550/arXiv.2506.13055|
|145|A Closer Look at Machine Unlearning for Large Language Models|Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin|<li><span style="color:#FF5733;">2025-01-01</span></li>|ICLR|https://github.com/sail-sg/closer-look-LLM-unlearning.|https://openreview.net/forum?id=Q1MHvGmhyT|
|146|APEER: Automatic Prompt Engineering Enhances Large Language Model
  Reranking|Can Jin, Hongwu Peng, Shiyu Zhao, Zhenting Wang, Wujiang Xu, Ligong Han, Jiahui Zhao, Kai Zhong, Sanguthevar Rajasekaran...|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/jincan333/APEER.|https://doi.org/10.48550/arXiv.2406.14449|
|147|ARB-LLM: Alternating Refined Binarizations for Large Language Models|Zhiteng Li, Xianglong Yan, Tianao Zhang, Haotong Qin, Dong Xie, Jiang Tian, Zhongchao Shi, Linghe Kong, Yulun Zhang, Xia...|<li><span style="color:#FF5733;">2025-01-01</span></li>|ICLR|https://github.com/ZHITENGLI/ARB-LLM.|https://openreview.net/forum?id=ZU8OdDLTts|
|148|Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities|Jinhua Liang, Xubo Liu, Wenwu Wang, Mark D. Plumbley, Huy P. Phan, Emmanouil Benetos|<li><span style="color:#FF5733;">2025-01-01</span></li>|IEEE Transactions on Audio Speech and Language Processing|https://github.com/JinhuaLiang/APT.|https://doi.org/10.1109/taslpro.2025.3533375|
|149|AgentGym: Evaluating and Training Large Language Model-based Agents across Diverse Environments|Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Xin Guo, Dingwen Yang, Chenyang Liao, Wei ...|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/WooooDyy/AgentGym.|https://aclanthology.org/2025.acl-long.1355/|
|150|Harnessing Multi-modal Large Language Models for Measuring and Interpreting Color Differences|Zhihua Wang, Long Yu, Qiuping Jiang, Chao Huang, Xiaochun Cao|<li><span style="color:#FF5733;">2025-01-01</span></li>|IEEE Transactions on Image Processing|https://github.com/LongYu-LY/CD-Reasoning.|https://doi.org/10.1109/tip.2024.3522802|
|151|An Empirical Analysis of Uncertainty in Large Language Model Evaluations|Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang|<li><span style="color:#FF5733;">2025-01-01</span></li>|ICLR|https://github.com/hasakiXie123/LLM-Evaluator-Uncertainty.|https://openreview.net/forum?id=J4xLuCt2kg|
|152|Benchmarking DNA large language models on quadruplexes|Oleksandr Cherednichenko, Alan Herbert, Maria Poptsova|<li><span style="color:#FF5733;">2025-01-01</span></li>|Computational and Structural Biotechnology Journal|https://github.com/powidla/G4s-FMs.|https://doi.org/10.1016/j.csbj.2025.03.007|
|153|Beyond Graphs: Can Large Language Models Comprehend Hypergraphs?|Yifan Feng, Chengwu Yang, Xingliang Hou, Shaoyi Du, Shihui Ying, Zongze Wu, Yue Gao|<li><span style="color:#FF5733;">2025-01-01</span></li>|ICLR|https://github.com/iMoonLab/LLM4Hypergraph.|https://openreview.net/forum?id=28qOQwjuma|
|154|CALM: Curiosity-Driven Auditing for Large Language Models|Xiaoyu Zheng, Longxiang Wang, Yi Liu, Xingjun Ma, Chao Shen, Cong Wang|<li><span style="color:#FF5733;">2025-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/x-zheng16/CALM.git.|https://doi.org/10.1609/aaai.v39i26.34991|
|155|Aligning, Autoencoding and Prompting Large Language Models for Novel Disease Reporting|Fenglin Liu, Xian Wu, Jinfa Huang, Bang Yang, Kim Branson, Patrick Schwab, Lei Clifton, Ping Zhang, Jiebo Luo, Yefeng Zh...|<li><span style="color:#FF5733;">2025-01-01</span></li>|IEEE Transactions on Pattern Analysis and Machine Intelligence|https://github.com/ai-in-health/PromptLLM.|https://doi.org/10.1109/tpami.2025.3534586|
|156|Causal Intervention Is What Large Language Models Need for Spatio-Temporal Forecasting|Shijie Li, He Li, Xiaojing Li, Yong Xu, Zhenhong Lin, Huaiguang Jiang|<li><span style="color:#FF5733;">2025-01-01</span></li>|IEEE Transactions on Cybernetics|https://github.com/lishijie15/STCInterLLM.|https://doi.org/10.1109/tcyb.2025.3569333|
|157|Enhancing Herbal Medicine-Drug Interaction Prediction Using Large Language Models|Sisi Yuan, Zhecheng Zhou, Xinyuan Jin, Linlin Zhuo, Keqin Li|<li><span style="color:#FF5733;">2025-01-01</span></li>|IEEE Journal of Biomedical and Health Informatics|https://github.com/sisyyuan/HDI.|https://doi.org/10.1109/jbhi.2025.3558667|
|158|GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest|Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, Ping Luo|<li><span style="color:#FF5733;">2025-01-01</span></li>|Lecture notes in computer science|https://github.com/jshilong/GPT4RoI.|https://doi.org/10.1007/978-3-031-91813-1_4|
|159|Comparative Analysis of Demonstration Selection Algorithms for In-Context Learning in Large Language Models (Student Abstract)|Dong Wook Shu, Mengnan Du|<li><span style="color:#FF5733;">2025-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/Tizzzzy/Demonstration_Selection_Overview.|https://doi.org/10.1609/aaai.v39i28.35299|
|160|Exploring Concept Depth: How Large Language Models Acquire Knowledge and Concept at Different Layers?|Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaiz...|<li><span style="color:#FF5733;">2025-01-01</span></li>|COLING|https://github.com/Luckfort/CD|https://aclanthology.org/2025.coling-main.37/|
|161|Evaluating the Prompt Steerability of Large Language Models|Erik Miehling, Michael Desmond, Karthikeyan Natesan Ramamurthy, Elizabeth Daly, Kush R. Varshney, Eitan Farchi, Pierre D...|<li><span style="color:#FF5733;">2025-01-01</span></li>|OpenAlex|https://github.com/IBM/prompt-steering.|https://doi.org/10.18653/v1/2025.naacl-long.400|
|162|From continuous pre-training to alignment: A comprehensive toolkit for large language models in federated learning|Zhuo Zhang, Yukun Zhang, Guanzhong Chen, Lizhen Qu, Xun Zhou, Hui Wang, Zenglin Xu|<li><span style="color:#FF5733;">2025-01-01</span></li>|Neurocomputing|https://github.com/iezhuozhuo/f4llm.|https://doi.org/10.1016/j.neucom.2025.130572|
|163|Dual Adapter Tuning of Vision-Language Models Using Large Language Models|Mohammad Reza Zarei, Abbas Akkasi, Majid Komeili|<li><span style="color:#FF5733;">2025-01-01</span></li>|International Journal of Computational Intelligence Systems|https://github.com/mrzarei5/DATViL.|https://doi.org/10.1007/s44196-025-00853-0|
|164|Do Large Language Model Benchmarks Test Reliability?|Joshua Vendrow, Edward Vendrow, Sara Beery, Aleksander Madry|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/MadryLab/platinum-benchmarks|https://doi.org/10.48550/arXiv.2502.03461|
|165|Disentangling Memory and Reasoning Ability in Large Language Models|Mingyu Jin, Weidi Luo, Sitao Cheng, Xinyi Wang, Wenyue Hua, Ruixiang Tang, William Yang Wang, Yongfeng Zhang|<li><span style="color:#FF5733;">2025-01-01</span></li>|ACL|https://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.|https://aclanthology.org/2025.acl-long.84/|
|166|DesignQA: A Multimodal Benchmark for Evaluating Large Language Models&apos; Understanding of Engineering Do cumentation|Anna C. Doris, Daniele Grandi, Ryan Tomich, Md Ferdous Alam, Mohammadmehdi Ataei, Hyunmin Cheong, Faez Ahmed|<li><span style="color:#FF5733;">2025-01-01</span></li>|Journal of Computing and Information Science in Engineering|https://github.com/anniedoris/design_qa|https://doi.org/10.48550/arXiv.2404.07917|
|167|Comparing Bad Apples to Good Oranges: Aligning Large Language Models via
  Joint Preference Optimization|Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, Aditya Grover|<li><span style="color:#FF5733;">2025-01-01</span></li>|arXiv|https://github.com/Hritikbansal/dove.|https://aclanthology.org/2025.findings-acl.39/|
|168|MentalQLM: A lightweight large language model for mental healthcare based on instruction tuning and dual LoRA modules|Jiayu Shi, Zexiao Wang, Jian Zhou, Chengyu Liu, Poly Z. H. Sun, Erying Zhao, Lei Lü|<li><span style="color:#FF5733;">2024-12-30</span></li>|medRxiv (Cold Spring Harbor Laboratory)|https://github.com/tortorish/MentalQLM.|https://doi.org/10.1101/2024.12.29.24319755|
|169|An Engorgio Prompt Makes Large Language Model Babble on|Jianshuo Dong, Ziyuan Zhang, Qingjie Zhang, Hanxun Qiu, Tianwei Zhang, Hao Wang, Hewu Li, Qi Li, Chao Zhang, Ke Xu|<li><span style="color:#FF5733;">2024-12-26</span></li>|ICLR|https://github.com/jianshuod/Engorgio-prompt.|https://openreview.net/forum?id=m4eXBo0VNc|
|170|MLLM-SUL: Multimodal Large Language Model for Semantic Scene
  Understanding and Localization in Traffic Scenarios|Jiaqi Fan, Jianhua Wu, Jincheng Gao, Jianhao Yu, Yafei Wang, Hongqing Chu, Bingzhao Gao|<li><span style="color:#FF5733;">2024-12-26</span></li>|arXiv (Cornell University)|https://github.com/fjq-tongji/MLLM-SUL.|http://arxiv.org/abs/2412.19406|
|171|Survey and Improvement Strategies for Gene Prioritization with Large Language Models|Matthew B. Neeley, Guantong Qi, Guanchao Wang, Ruixiang Tang, Dongxue Mao, Chaozhong Liu, Sasidhar Pasupuleti, Bo Yuan, ...|<li><span style="color:#FF5733;">2024-12-26</span></li>|Bioinformatics Advances|https://github.com/LiuzLab/GPT-Diagnosis.|https://doi.org/10.48550/arXiv.2501.18794|
|172|Task Preference Optimization: Improving Multimodal Large Language Models
  with Vision Task Alignment|Ziang Yan, Zhilin Li, Yinan He, Chenting Wang, Kunchang Li, Xinhao Li, Xiangyu Zeng, Zhong Lin Wang, Yali Wang, Yu Qiao,...|<li><span style="color:#FF5733;">2024-12-26</span></li>|CVPR|https://github.com/OpenGVLab/TPO|https://openaccess.thecvf.com/content/CVPR2025/html/Yan_Task_Preference_Optimization_Improving_Multimodal_Large_Language_Models_with_Vision_CVPR_2025_paper.html|
|173|A Survey on Large Language Model Acceleration based on KV Cache
  Management|Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen|<li><span style="color:#FF5733;">2024-12-26</span></li>|Trans. Mach. Learn. Res.|https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management|https://openreview.net/forum?id=z3JZzu9EA3|
|174|3DGraphLLM: Combining Semantic Graphs and Large Language Models for 3D
  Scene Understanding|Tatiana Zemskova, Dmitry Yudin|<li><span style="color:#FF5733;">2024-12-24</span></li>|arXiv (Cornell University)|https://github.com/CognitiveAISystems/3DGraphLLM.|http://arxiv.org/abs/2412.18450|
|175|ICM-Assistant: Instruction-tuning Multimodal Large Language Models for
  Rule-based Explainable Image Content Moderation|Mengyang Wu, Yuzhi Zhao, Jialun Cao, Mingjie Xu, Zhongming Jiang, Xuehui Wang, Qinbin Li, Guangneng Hu, Shengchao Qin, C...|<li><span style="color:#FF5733;">2024-12-24</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/zhaoyuzhi/ICM-Assistant.|https://doi.org/10.1609/aaai.v39i8.32908|
|176|Investigating Large Language Models for Code Vulnerability Detection: An
  Experimental Study|Xuefeng Jiang, L. H. Wu, Sheng Sun, Jia Li, Jingjing Xue, Yuwei Wang, Tingting Wu, Min Liu|<li><span style="color:#FF5733;">2024-12-24</span></li>|arXiv (Cornell University)|https://github.com/SakiRinn/LLM4CVD|http://arxiv.org/abs/2412.18260|
|177|Large Language Model Safety: A Holistic Survey|Dan Shi, Tianhao Shen, Yufei Huang, Zhigen Li, Yongqi Leng, Renren Jin, Chuang Liu, Xinwei Wu, Zishan Guo, Linhao Yu, Li...|<li><span style="color:#FF5733;">2024-12-23</span></li>|arXiv (Cornell University)|https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers.|http://arxiv.org/abs/2412.17686|
|178|Property Enhanced Instruction Tuning for Multi-task Molecule Generation
  with Large Language Models|Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu|<li><span style="color:#FF5733;">2024-12-23</span></li>|arXiv (Cornell University)|https://github.com/chenlong164/PEIT.|http://arxiv.org/abs/2412.18084|
|179|Large Language Model Can Be a Foundation for Hidden Rationale-Based
  Retrieval|Luo Ji, Fulai Guo, Teng Chen, Qing Gu, Xiaoyu Wang, Ningyuan Xi, Yihong Wang, Peng Yu, Yue Zhao, Hongyang Lei, Zhonglin ...|<li><span style="color:#FF5733;">2024-12-21</span></li>|Lecture notes in computer science|https://github.com/flyfree5/LaHoRe.|https://doi.org/10.1007/978-3-031-88714-7_27|
|180|Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models
  into Assembly Code Obfuscation|Seyedreza Mohseni, Seyedali Mohammadi, Deepa Tilwani, Yash Saxena, Gerald Ketu Ndawula, Sriram Vema, Edward Raff, Manas ...|<li><span style="color:#FF5733;">2024-12-20</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/mohammadi-ali/MetamorphASM.|https://doi.org/10.1609/aaai.v39i23.34672|
|181|PruneVid: Visual Token Pruning for Efficient Video Large Language Models|Xiaohu Huang, Hao Zhou, K. L. Han|<li><span style="color:#FF5733;">2024-12-20</span></li>|arXiv (Cornell University)|https://github.com/Visual-AI/PruneVid.|http://arxiv.org/abs/2412.16117|
|182|Sliding Windows Are Not the End: Exploring Full Ranking with
  Long-Context Large Language Models|Wenhan Liu, Xinyu Ma, Yutao Zhu, Ziliang Zhao, Shuaiqiang Wang, Dawei Yin, Zhicheng Dou|<li><span style="color:#FF5733;">2024-12-19</span></li>|ACL|https://github.com/8421BCD/fullrank|https://aclanthology.org/2025.acl-long.8/|
|183|Mitigating Social Bias in Large Language Models: A Multi-Objective
  Approach within a Multi-Agent Framework|Zhenjie Xu, Wenqing Chen, Yi Tang, Xuanying Li, Cheng Hu, Zhixuan Chu, Kui Ren, Zibin Zheng, Zhichao Lu|<li><span style="color:#FF5733;">2024-12-19</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/Cortantse/MOMA.|https://doi.org/10.1609/aaai.v39i24.34748|
|184|Unveiling Uncertainty: A Deep Dive into Calibration and Performance of
  Multimodal Large Language Models|Zijun Chen, Wenbo Hu, Guande He, Zhijie Deng, Zheng Zhang, Richang Hong|<li><span style="color:#FF5733;">2024-12-19</span></li>|COLING|https://github.com/hfutml/Calibration-MLLM|https://aclanthology.org/2025.coling-main.208/|
|185|ResQ: Mixed-Precision Quantization of Large Language Models with
  Low-Rank Residuals|Utkarsh Saxena, Sayeh Sharify, Kaushik Roy, Xin Wang|<li><span style="color:#FF5733;">2024-12-18</span></li>|arXiv (Cornell University)|https://github.com/utkarsh-dmx/project-resq.|http://arxiv.org/abs/2412.14363|
|186|InstructSeg: Unifying Instructed Visual Segmentation with Multi-modal
  Large Language Models|Cong Wei, Yujie Zhong, Haoxian Tan, Yingsen Zeng, Yong Liu, Zheng Zhao, Yujiu Yang|<li><span style="color:#FF5733;">2024-12-18</span></li>|arXiv (Cornell University)|https://github.com/congvvc/InstructSeg.|http://arxiv.org/abs/2412.14006|
|187|ORBIT: Cost-Effective Dataset Curation for Large Language Model Domain
  Adaptation with an Astronomy Case Study|Eric Modesitt, Ke Yang, Spencer Hulsey, Xin Liu, ChengXiang Zhai, Volodymyr V. Kindratenko|<li><span style="color:#FF5733;">2024-12-18</span></li>|ACL|https://github.com/ModeEric/ORBIT-Llama|https://aclanthology.org/2025.findings-acl.51/|
|188|DateLogicQA: Benchmarking Temporal Biases in Large Language Models|Gagan Bhatia, MingZe Tang, Cristina Mahanta, Madiha Kazi|<li><span style="color:#FF5733;">2024-12-17</span></li>|OpenAlex|https://github.com/gagan3012/EAIS-Temporal-Bias|https://doi.org/10.18653/v1/2025.naacl-srw.32|
|189|BlenderLLM: Training Large Language Models for Computer-Aided Design
  with Self-improvement|Yinwei Du, Shunian Chen, Wenbo Zan, Peizhao Li, Mingxuan Wang, Dongwoon Song, Bo Li, Yan Hu, Benyou Wang|<li><span style="color:#FF5733;">2024-12-16</span></li>|arXiv (Cornell University)|https://github.com/FreedomIntelligence/BlenderLLM|http://arxiv.org/abs/2412.14203|
|190|SPaR: Self-Play with Tree-Search Refinement to Improve
  Instruction-Following in Large Language Models|Jiale Cheng, Xiao Liu, Cunxiang Wang, Xiaotao Gu, Yida Lu, Dan Zhang, Yuxiao Dong, Jie Tang, Hongning Wang, Minlie Huang|<li><span style="color:#FF5733;">2024-12-16</span></li>|ICLR|https://github.com/thu-coai/SPaR.|https://openreview.net/forum?id=9chRqsPOGL|
|191|Can Large Language Models Understand You Better? An MBTI Personality
  Detection Dataset Aligned with Population Traits|Bohan Li, J Guan, Longxu Dou, Yun‐Long Feng, Dingzirui Wang, Yang Xu, Enbo Wang, Qiguang Chen, Bichen Wang, Xiao Xu, Yim...|<li><span style="color:#FF5733;">2024-12-16</span></li>|COLING|https://github.com/Personality-NLP/MbtiBench.|https://aclanthology.org/2025.coling-main.339/|
|192|Assessing the Limitations of Large Language Models in Clinical Fact
  Decomposition|Monica Munnangi, Akshay Swaminathan, Jason Fries, Jenelle Jindal, Sanjana Narayanan, Iván López, Lucia Tu, Philip Chung,...|<li><span style="color:#FF5733;">2024-12-16</span></li>|arXiv (Cornell University)|https://github.com/som-shahlab/factehr|http://arxiv.org/abs/2412.12422|
|193|NLSR: Neuron-Level Safety Realignment of Large Language Models Against
  Harmful Fine-Tuning|Xin Yi, Shunfan Zheng, Linlin Wang, Gerard de Melo, Xiaoling Wang, Liang He|<li><span style="color:#FF5733;">2024-12-16</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/xinykou/NLSR|https://doi.org/10.1609/aaai.v39i24.34762|
|194|RetroLLM: Empowering Large Language Models to Retrieve Fine-grained
  Evidence within Generation|Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Ye Qi, Zhicheng Dou|<li><span style="color:#FF5733;">2024-12-16</span></li>|ACL|https://github.com/sunnynexus/RetroLLM|https://aclanthology.org/2025.acl-long.819/|
|195|A survey on LoRA of large language models|Yuren Mao, Yuhang Ge, Yijiang Fan, Wenyi Xu, Yu Mi, Zhonghao Hu, Yunjun Gao|<li><span style="color:#FF5733;">2024-12-14</span></li>|Frontiers of Computer Science|https://github.com/ZJU-LLMs/Awesome-LoRAs.git|https://doi.org/10.1007/s11704-024-40663-9|
|196|B-VLLM: A Vision Large Language Model with Balanced Spatio-Temporal
  Tokens|Zhuqiang Lu, Zhenfei Yin, Meilin He, Zhihui Wang, Zicheng Liu, Zhiyong Wang, Kun Hu|<li><span style="color:#FF5733;">2024-12-13</span></li>|arXiv (Cornell University)|https://github.com/zhuqiangLu/B-VLLM.|http://arxiv.org/abs/2412.09919|
|197|Enhancing Multimodal Large Language Models Complex Reason via Similarity
  Computation|Xiaofeng Zhang, Fanshuo Zeng, Yihao Quan, Zheng Hui, Jiawei Yao|<li><span style="color:#FF5733;">2024-12-12</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/FanshuoZeng/Simignore|https://doi.org/10.1609/aaai.v39i10.33107|
|198|Filter-then-Generate: Large Language Models with Structure-Text Adapter
  for Knowledge Graph Completion|Ben Liu, Jihai Zhang, Fangquan Lin, Cheng Yang, Min Peng|<li><span style="color:#FF5733;">2024-12-12</span></li>|COLING|https://github.com/LB0828/FtG|https://aclanthology.org/2025.coling-main.740/|
|199|Simulate Scientific Reasoning with Multiple Large Language Models: An Application to Alzheimer’s Disease Combinatorial Therapy|Qidi Xu, Xiaozhong Liu, Xiaoqian Jiang, Yejin Kim|<li><span style="color:#FF5733;">2024-12-12</span></li>|medRxiv (Cold Spring Harbor Laboratory)|https://github.com/QidiXu96/Coated-LLM|https://doi.org/10.1101/2024.12.10.24318800|
|200|Towards a Multimodal Large Language Model with Pixel-Level Insight for
  Biomedicine|Xiaoshuang Huang, Lingdong Shen, Jia Liu, Fangxin Shang, Hongxiang Li, Haifeng Huang, Yehui Yang|<li><span style="color:#FF5733;">2024-12-12</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/ShawnHuang497/MedPLIB.|https://doi.org/10.1609/aaai.v39i4.32394|
|201|Exploiting the Index Gradients for Optimization-Based Jailbreaking on
  Large Language Models|Jiahui Li, Yongchang Hao, Hanqiu Xu, Xing Wang, Yu Hong|<li><span style="color:#FF5733;">2024-12-11</span></li>|COLING|https://github.com/jiah-li/magic.|https://aclanthology.org/2025.coling-main.305/|
|202|Concept Bottleneck Large Language Models|Chung-En Sun, Tuomas P. Oikarinen, Berk Ustun, Tsui-Wei Weng|<li><span style="color:#FF5733;">2024-12-10</span></li>|ICLR|https://github.com/Trustworthy-ML-Lab/CB-LLMs.|https://openreview.net/forum?id=RC5FPYVQaH|
|203|IntellectSeeker: A Personalized Literature Management System with the
  Probabilistic Model and Large Language Model|Weizhen Bian, Siyan Liu, Yubo Zhou, Dezhi Chen, Yijie Liao, Zhenzhen Fan, Aobo Wang|<li><span style="color:#FF5733;">2024-12-10</span></li>|Lecture notes in computer science|https://github.com/LuckyBian/ISY5001|https://doi.org/10.1007/978-981-97-5489-2_24|
|204|PediaBench: A Comprehensive Chinese Pediatric Dataset for Benchmarking
  Large Language Models|Qian Zhang, Panfeng Chen, Jiali Li, Shuo Feng, Shuyu Liu, Mei Chen, Hui Li, Yanhao Wang|<li><span style="color:#FF5733;">2024-12-09</span></li>|arXiv (Cornell University)|https://github.com/ACMISLab/PediaBench.|http://arxiv.org/abs/2412.06287|
|205|KaSA: Knowledge-Aware Singular-Value Adaptation of Large Language Models|Fan Wang, Jing-Jiang Jiang, C.Y. Park, Sunghun Kim, Jing Tang|<li><span style="color:#FF5733;">2024-12-08</span></li>|ICLR|https://github.com/juyongjiang/KaSA.|https://openreview.net/forum?id=OQqNieeivq|
|206|Mitigating Entity-Level Hallucination in Large Language Models|Weihang Su, Yichen Tang, Qingyao Ai, Changyue Wang, Zhijing Wu, Yiqun Liu|<li><span style="color:#FF5733;">2024-12-08</span></li>|OpenAlex|https://github.com/oneal2000/EntityHallucination.|https://doi.org/10.48550/arXiv.2407.09417|
|207|NJGPT: A Large Language Model-Driven, User-Friendly Solution for Phylogenetic Tree Construction|Zhixuan Wang, Haoyuan Huang, Teng Li, Allen G. Rodrigo|<li><span style="color:#FF5733;">2024-12-04</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/ZWan622/NJGPT1.0.git|https://doi.org/10.1101/2024.12.02.626464|
|208|Improving Linguistic Diversity of Large Language Models with Possibility
  Exploration Fine-Tuning|Long Mai, Julie Carson-Berndsen|<li><span style="color:#FF5733;">2024-12-04</span></li>|arXiv (Cornell University)|https://github.com/mailong25/peft_diversity|http://arxiv.org/abs/2412.03343|
|209|From Individual to Society: A Survey on Social Simulation Driven by
  Large Language Model-based Agents|Xinyi Mou, Xuanwen Ding, Qi He, Liang Wang, Jingcong Liang, Xinnong Zhang, Libo Sun, Jiayu Lin, Jie Zhou, Xuanjing Huang...|<li><span style="color:#FF5733;">2024-12-04</span></li>|arXiv (Cornell University)|https://github.com/FudanDISC/SocialAgent|http://arxiv.org/abs/2412.03563|
|210|Fine-Grained Behavior Simulation with Role-Playing Large Language Model
  on Social Media|Kun Li, C. H. Dai, Zhou We, Songlin Hu|<li><span style="color:#FF5733;">2024-12-04</span></li>|arXiv (Cornell University)|https://github.com/linkseed18612254945/FineRob|http://arxiv.org/abs/2412.03148|
|211|Beyond Labels: Aligning Large Language Models with Human-Like Reasoning|Muhammad Rafsan Kabir, Rafeed Mohammad Sultan, Ihsanul Haque Asif, Jason M. E. Ahad, Fuad Rahman, Mohammad Ruhul Amin, N...|<li><span style="color:#FF5733;">2024-12-02</span></li>|Lecture notes in computer science|https://github.com/apurba-nsu-rnd-lab/DFAR.|https://doi.org/10.1007/978-3-031-78172-8_16|
|212|Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic
  Vision-language Context Sparsification|Wenxuan Huang, Zijie Zhai, Yunhang Shen, Shaosheng Cao, Fei Zhao, Xiangfeng Xu, Zheyu Ye, Shaohui Lin|<li><span style="color:#FF5733;">2024-12-01</span></li>|arXiv|https://github.com/Osilly/dynamic_llava|https://openreview.net/forum?id=hzVpZDrW73|
|213|Woodpecker: hallucination correction for multimodal large language models|Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Bill Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, Enhong Chen|<li><span style="color:#FF5733;">2024-12-01</span></li>|Science China Information Sciences|https://github.com/BradyFU/Woodpecker.|https://doi.org/10.1007/s11432-024-4251-x|
|214|Accelerating Multimodal Large Language Models via Dynamic Visual-Token
  Exit and the Empirical Findings|Qiong Wu, Wei Lin, Weihao Ye, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji|<li><span style="color:#FF5733;">2024-11-29</span></li>|arXiv (Cornell University)|https://github.com/DoubtedSteam/DyVTE.|http://arxiv.org/abs/2411.19628|
|215|CovidLLM: A Robust Large Language Model with Missing Value Adaptation
  and Multi-Objective Learning Strategy for Predicting Disease Severity and
  Clinical Outcomes in COVID-19 Pa..|Shengjun Zhu, Siyu Liu, Yang Li, Qing Lei, Hongyan Hou, He‐wei Jiang, Shujuan Guo, Feng Wang, Rongshang Chen, Xionglin F...|<li><span style="color:#FF5733;">2024-11-28</span></li>|Current Proteomics|https://github.com/sysll/CovidLLM|https://doi.org/10.2174/0115701646366019250304064012|
|216|AdaShield: Safeguarding Multimodal Large Language Models from
  Structure-based Attack via Adaptive Shield Prompting|Yu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, Chaowei Xiao|<li><span style="color:#FF5733;">2024-11-26</span></li>|Lecture notes in computer science|https://github.com/rain305f/AdaShield.|https://doi.org/10.1007/978-3-031-72661-3_5|
|217|Leveraging Large Language Models and Topic Modeling for Toxicity
  Classification|Haniyeh Ehsani Oskouie, Christina Chance, Claire Huang, Margaret Capetz, Elizabeth Eyeson, Majid Sarrafzadeh|<li><span style="color:#FF5733;">2024-11-26</span></li>|2016 International Conference on Computing, Networking and Communications (ICNC)|https://github.com/aheldis/Toxicity-Classification.git.|https://doi.org/10.1109/ICNC64010.2025.10994061|
|218|VaxLLM: Leveraging Fine-tuned Large Language Model for automated annotation of Brucella Vaccines|Xingxian Li, Yuping Zheng, Jie Hu, Jie Zheng, Zhigang Wang, Yongqun He|<li><span style="color:#FF5733;">2024-11-26</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/xingxianli/VaxLLM|https://doi.org/10.1101/2024.11.25.625209|
|219|CS-Eval: A Comprehensive Large Language Model Benchmark for
  CyberSecurity|Zhengmin Yu, Jiutian Zeng, Siyi Chen, Wenhan Xu, Dandan Xu, Xiangyu Liu, Zonghao Ying, Nan Wang, Yuan Zhang, Min Yang|<li><span style="color:#FF5733;">2024-11-25</span></li>|arXiv (Cornell University)|https://github.com/CS-EVAL/CS-Eval.|http://arxiv.org/abs/2411.16239|
|220|"Moralized" Multi-Step Jailbreak Prompts: Black-Box Testing of
  Guardrails in Large Language Models for Verbal Attacks|Libo Wang|<li><span style="color:#FF5733;">2024-11-23</span></li>|arXiv (Cornell University)|https://github.com/brucewang123456789/GeniusTrail.git.|http://arxiv.org/abs/2411.16730|
|221|Large Language Model with Region-guided Referring and Grounding for CT
  Report Generation|Zhixuan Chen, Yequan Bie, Huanying Jin, Hao Chen|<li><span style="color:#FF5733;">2024-11-23</span></li>|IEEE Transactions on Medical Imaging|https://github.com/zhi-xuan-chen/Reg2RG.|https://doi.org/10.1109/tmi.2025.3559923|
|222|DRPruning: Efficient Large Language Model Pruning through
  Distributionally Robust Optimization|Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Jing Li, Min Zhang, Zhaopeng Tu|<li><span style="color:#FF5733;">2024-11-21</span></li>|ACL|https://github.com/hexuandeng/DRPruning.|https://aclanthology.org/2025.acl-long.1414/|
|223|SemiKong: Curating, Training, and Evaluating A Semiconductor
  Industry-Specific Large Language Model|Christopher Nguyen, William Nguyen, Atsushi Suzuki, Daisuke Oku, Hong An Phan, Dinh Viet Sang, Zooey Nguyen, Ha Cam Anh,...|<li><span style="color:#FF5733;">2024-11-20</span></li>|arXiv (Cornell University)|https://github.com/aitomatic/semikong|http://arxiv.org/abs/2411.13802|
|224|On the Consistency of Video Large Language Models in Temporal
  Comprehension|Minjoon Jung, Junbin Xiao, Byoung‐Tak Zhang, Angela Yao|<li><span style="color:#FF5733;">2024-11-19</span></li>|CVPR|https://github.com/minjoong507/Consistency-of-Video-LLM.|https://openaccess.thecvf.com/content/CVPR2025/html/Jung_On_the_Consistency_of_Video_Large_Language_Models_in_Temporal_CVPR_2025_paper.html|
|225|FLAME: Frozen Large Language Models Enable Data-Efficient Language-Image
  Pre-training|Anjia Cao, Xing Wei, Zhiheng Ma|<li><span style="color:#FF5733;">2024-11-18</span></li>|arXiv|https://github.com/MIV-XJTU/FLAME|https://openaccess.thecvf.com/content/CVPR2025/html/Cao_FLAME_Frozen_Large_Language_Models_Enable_Data-Efficient_Language-Image_Pre-training_CVPR_2025_paper.html|
|226|Multilingual Large Language Models: A Systematic Survey|Shaolin Zhu, Supryadi, Shaoyang Xu, Haoran Sun, Leiyu Pan, Menglong Cui, Jiangcun Du, Renren Jin, António Branco, Deyi X...|<li><span style="color:#FF5733;">2024-11-17</span></li>|arXiv (Cornell University)|https://github.com/tjunlp-lab/Awesome-Multilingual-LLMs-Papers.|http://arxiv.org/abs/2411.11072|
|227|TS-LLaVA: Constructing Visual Tokens through Thumbnail-and-Sampling for
  Training-Free Video Large Language Models|Tingyu Qu, Mingxiao Li, Tinne Tuytelaars, Marie‐Francine Moens|<li><span style="color:#FF5733;">2024-11-17</span></li>|arXiv (Cornell University)|https://github.com/tingyu215/TS-LLaVA.|http://arxiv.org/abs/2411.11066|
|228|BianCang: A Traditional Chinese Medicine Large Language Model|Sibo Wei, Xueping Peng, Yifei Wang, Jiasheng Si, Weiyu Zhang, Wenpeng Lü, Xiaoming Wu, Yinglong Wang|<li><span style="color:#FF5733;">2024-11-17</span></li>|arXiv (Cornell University)|https://github.com/QLU-NLP/BianCang.|http://arxiv.org/abs/2411.11027|
|229|Orca: Enhancing Role-Playing Abilities of Large Language Models by
  Integrating Personality Traits|Yu-Chih Huang|<li><span style="color:#FF5733;">2024-11-15</span></li>|arXiv (Cornell University)|https://github.com/Aipura/Orca.|http://arxiv.org/abs/2411.10006|
|230|Evaluating Creativity and Deception in Large Language Models: A
  Simulation Framework for Multi-Agent Balderdash|Parsa Hejabi, Elnaz Rahmati, Alireza S. Ziabari, Preni Golazizian, Jesse Thomason, Morteza Dehghani|<li><span style="color:#FF5733;">2024-11-15</span></li>|arXiv (Cornell University)|https://github.com/ParsaHejabi/Simulation-Framework-for-Multi-Agent-Balderdash|http://arxiv.org/abs/2411.10422|
|231|LHRS-Bot-Nova: Improved Multimodal Large Language Model for Remote
  Sensing Vision-Language Interpretation|Zhenshi Li, Dilxat Muhtar, Feng Gu, Yibei He, Xueliang Zhang, Pengfeng Xiao, Guangjun He, Xiao Xiang Zhu|<li><span style="color:#FF5733;">2024-11-14</span></li>|ISPRS Journal of Photogrammetry and Remote Sensing|https://github.com/NJU-LHRS/LHRS-Bot.|https://doi.org/10.1016/j.isprsjprs.2025.06.003|
|232|DROJ: A Prompt-Driven Attack against Large Language Models|Longfei Hu, B. Wang|<li><span style="color:#FF5733;">2024-11-13</span></li>|arXiv (Cornell University)|https://github.com/Leon-Leyang/LLM-Safeguard.|http://arxiv.org/abs/2411.09125|
|233|Verbosity $\neq$ Veracity: Demystify Verbosity Compensation Behavior of
  Large Language Models|Yusen Zhang, Sarkar Snigdha Sarathi Das, Rui Zhang|<li><span style="color:#FF5733;">2024-11-12</span></li>|arXiv (Cornell University)|https://github.com/psunlpgroup/VerbosityLLM.|http://arxiv.org/abs/2411.07858|
|234|Benchmarking Large Language Models for NL-to-SQL: A Comprehensive Evaluation of Accuracy, Cost and Throughput|Adithya Narasimhan, Amit Kumar Bhamboo, Abiram Devnathan, Jeyaraj Vellaisamy, Vineeth Vijayaraghavan|<li><span style="color:#FF5733;">2024-11-10</span></li>|OpenAlex|https://github.com/petavue/NL2SQL-Benchmark.|https://doi.org/10.36227/techrxiv.173121325.56335825/v1|
|235|Golden Touchstone: A Comprehensive Bilingual Benchmark for Evaluating
  Financial Large Language Models|Xiaojun Wu, Junxi Liu, Hong Su, Zhouchi Lin, Yiyan Qi, Chengjin Xu, Jiajun Su, Jiajie Zhong, Fuwei Wang, Saizhuo Wang, F...|<li><span style="color:#FF5733;">2024-11-09</span></li>|arXiv (Cornell University)|https://github.com/IDEA-FinAI/Golden-Touchstone|http://arxiv.org/abs/2411.06272|
|236|TourSynbio-Search: A Large Language Model Driven Agent Framework for
  Unified Search Method for Protein Engineering|Yungeng Liu, Zan Chen, Yu Guang Wang, Yiqing Shen|<li><span style="color:#FF5733;">2024-11-08</span></li>|2021 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)|https://github.com/tsynbio/Toursynbio-Search|https://doi.org/10.1109/bibm62325.2024.10822318|
|237|AutoProteinEngine: A Large Language Model Driven Agent Framework for
  Multimodal AutoML in Protein Engineering|Y. Liu, Zan Chen, Yu Guang Wang, Yiqing Shen|<li><span style="color:#FF5733;">2024-11-07</span></li>|arXiv (Cornell University)|https://github.com/tsynbio/AutoPE.|http://arxiv.org/abs/2411.04440|
|238|Measuring short-form factuality in large language models|Jason Lee, Nguyen Karina, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, William Fedus|<li><span style="color:#FF5733;">2024-11-06</span></li>|arXiv (Cornell University)|https://github.com/openai/simple-evals.|http://arxiv.org/abs/2411.04368|
|239|QUILL: Quotation Generation Enhancement of Large Language Models|Jin Xiao, Bowei Zhang, Qianyu He, Jiaqing Liang, Wei Feng, Jinglei Chen, Zujie Liang, Deqing Yang, Yanghua Xiao|<li><span style="color:#FF5733;">2024-11-06</span></li>|arXiv (Cornell University)|https://github.com/GraceXiaoo/QUILL.|http://arxiv.org/abs/2411.03675|
|240|Evaluating Large Language Models: A Comprehensive Survey|Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi X...|<li><span style="color:#FF5733;">2024-11-05</span></li>|International Journal of Latest Engineering and Management Research (IJLEMR)|https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.|https://doi.org/10.56581/ijlemr.9.10.05-16|
|241|FlexCAD: Unified and Versatile Controllable CAD Generation with
  Fine-tuned Large Language Models|Zhanwei Zhang, Shizhao Sun, Wenxiao Wang, Deng Cai, Jiang Bian|<li><span style="color:#FF5733;">2024-11-05</span></li>|ICLR|https://github.com/microsoft/CADGeneration|https://openreview.net/forum?id=Z0eiiV3Yyh|
|242|SMoA: Improving Multi-agent Large Language Models with Sparse
  Mixture-of-Agents|Dawei Li, Zhen Tan, Peijia Qian, Yifan Li, Kumar Satvik Chaudhary, Lijie Hu, Jiayi Shen|<li><span style="color:#FF5733;">2024-11-05</span></li>|Lecture notes in computer science|https://github.com/David-Li0406/SMoA.|https://doi.org/10.1007/978-981-96-8180-8_5|
|243|Leveraging Large Language Models in Code Question Answering: Baselines
  and Issues|Georgy Andryushchenko, Vladimir Ivanov, Vladimir Makharev, Elizaveta Tukhtina, Aidar Valeev|<li><span style="color:#FF5733;">2024-11-05</span></li>|Communications in computer and information science|https://github.com/IU-AES-AI4Code/CodeQuestionAnswering.|https://doi.org/10.1007/978-3-031-97019-1_1|
|244|DeeR-VLA: Dynamic Inference of Multimodal Large Language Models for
  Efficient Robot Execution|Yang Yue, Yulin Wang, Bingyi Kang, Yizeng Han, Shenzhi Wang, Shiji Song, Jiashi Feng, Gao Huang|<li><span style="color:#FF5733;">2024-11-04</span></li>|NeurIPS|https://github.com/yueyang130/DeeR-VLA.|http://papers.nips.cc/paper_files/paper/2024/hash/67b0e7c7c2a5780aeefe3b79caac106e-Abstract-Conference.html|
|245|SQL Injection Jailbreak: a structural disaster of large language models|Jiawei Zhao, Kejiang Chen, Weiming Zhang, Nenghai Yu|<li><span style="color:#FF5733;">2024-11-03</span></li>|ACL|https://github.com/weiyezhimeng/SQL-Injection-Jailbreak|https://aclanthology.org/2025.findings-acl.358/|
|246|Fish-Speech: Leveraging Large Language Models for Advanced Multilingual
  Text-to-Speech Synthesis|Shijia Liao, Yuxuan Wang, Tianyu Li, Yifan Cheng, Ruoyi Zhang, Ran Zhou, Yun-Xuan Xing|<li><span style="color:#FF5733;">2024-11-02</span></li>|arXiv (Cornell University)|https://github.com/fishaudio/fish-speech|http://arxiv.org/abs/2411.01156|
|247|Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in
  Automatic Evaluation by Large Language Models|Aliyah R. Hsu, James Zhu, Zhichao Wang, Bin Bi, Shubham Mehrotra, Shiva Pentyala, Katherine Tan, Xiang-Bo Mao, Roshanak ...|<li><span style="color:#FF5733;">2024-11-02</span></li>|arXiv (Cornell University)|https://github.com/adelaidehsu/REC|http://arxiv.org/abs/2411.02448|
|248|MoD: A Distribution-Based Approach for Merging Large Language Models|Quy-Anh Dang, Chong‐Wah Ngo|<li><span style="color:#FF5733;">2024-11-01</span></li>|arXiv (Cornell University)|https://github.com/knovel-eng/mod.|http://arxiv.org/abs/2411.00406|
|249|BitStack: Fine-Grained Size Control for Compressed Large Language Models
  in Variable Memory Environments|Xinghao Wang, Pengyu Wang, Bo Wang, Dong Zhang, Yunhua Zhou, Xipeng Qiu|<li><span style="color:#FF5733;">2024-10-31</span></li>|arXiv (Cornell University)|https://github.com/xinghaow99/BitStack.|http://arxiv.org/abs/2410.23918|
|250|End-to-End Ontology Learning with Large Language Models|Andrea Lo, Albert Q. Jiang, Wenda Li, Mateja Jamnik|<li><span style="color:#FF5733;">2024-10-30</span></li>|NeurIPS|https://github.com/andylolu2/ollm.|http://papers.nips.cc/paper_files/paper/2024/hash/9e89f068a62f6858c661a8abecf5bb0a-Abstract-Conference.html|
|251|NewTerm: Benchmarking Real-Time New Terms for Large Language Models with
  Annual Updates|Hexuan Deng, Wenxiang Jiao, Xuebo Liu, Zhang Min, Zhaopeng Tu|<li><span style="color:#FF5733;">2024-10-28</span></li>|NeurIPS|https://github.com/hexuandeng/NewTerm.|http://papers.nips.cc/paper_files/paper/2024/hash/3eec719ab86712d32b065c5977f94ad0-Abstract-Datasets_and_Benchmarks_Track.html|
|252|LLMCBench: Benchmarking Large Language Model Compression for Efficient
  Deployment|Ge Yang, Changyi He, Jinyang Guo, Jianyu Wu, Yifu Ding, Aishan Liu, Haotong Qin, Pengliang Ji, Xianglong Liu|<li><span style="color:#FF5733;">2024-10-28</span></li>|NeurIPS|https://github.com/AboveParadise/LLMCBench.|http://papers.nips.cc/paper_files/paper/2024/hash/9f4cc62d0632911c63163ea3d9ec19bd-Abstract-Datasets_and_Benchmarks_Track.html|
|253|GCoder: Improving Large Language Model for Generalized Graph Problem
  Solving|Qifan Zhang, Xiaobin Hong, Jianheng Tang, Nuo Chen, Yuhan Li, Wenzhong Li, Jing Tang, Jia Li|<li><span style="color:#FF5733;">2024-10-24</span></li>|arXiv (Cornell University)|https://github.com/Bklight999/WWW25-GCoder|http://arxiv.org/abs/2410.19084|
|254|GraphTeam: Facilitating Large Language Model-based Graph Analysis via
  Multi-Agent Collaboration|Xin Li, Qi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zhi Yu, Weize Chen, Chen Qian, Chuan Shi, Cheng Yang|<li><span style="color:#FF5733;">2024-10-23</span></li>|arXiv (Cornell University)|https://github.com/BUPT-GAMMA/GraphTeam.|http://arxiv.org/abs/2410.18032|
|255|Cross-model Control: Improving Multiple Large Language Models in
  One-time Training|Jiayi Wu, Hao Sun, Hengyi Cai, Lixin Su, Shuaiqiang Wang, Dawei Yin, Xiang Li, Ming Gao|<li><span style="color:#FF5733;">2024-10-23</span></li>|NeurIPS|https://github.com/wujwyi/CMC.|http://papers.nips.cc/paper_files/paper/2024/hash/9856b5d30ac61ab744fdab6f67d874e4-Abstract-Conference.html|
|256|DEAN: Deactivating the Coupled Neurons to Mitigate Fairness-Privacy
  Conflicts in Large Language Models|Qian Chen, Dongrui Liu, Jie Zhang, Yong Liu, Jing Shao|<li><span style="color:#FF5733;">2024-10-22</span></li>|arXiv (Cornell University)|https://github.com/ChnQ/DEAN|http://arxiv.org/abs/2410.16672|
|257|ETHIC: Evaluating Large Language Models on Long-Context Tasks with High
  Information Coverage|Taewhoo Lee, Chanwoong Yoon, Kyochul Jang, Donghyeon Lee, Myung-Ha Song, Hyunjae Kim, Jaewoo Kang|<li><span style="color:#FF5733;">2024-10-22</span></li>|OpenAlex|https://github.com/dmis-lab/ETHIC.|https://doi.org/10.18653/v1/2025.naacl-long.283|
|258|Improving Causal Reasoning in Large Language Models: A Survey|Lu Yu, Delin Chen, Siheng Xiong, Qingyang Wu, Qingzhen Liu, Dawei Li, Zhikai Chen, Xiaoze Liu, Liangming Pan|<li><span style="color:#FF5733;">2024-10-22</span></li>|arXiv (Cornell University)|https://github.com/chendl02/Awesome-LLM-causal-reasoning.|http://arxiv.org/abs/2410.16676|
|259|Benchmarking Large Language Models for Image Classification of Marine
  Mammals|Yijiashun Qi, Shuzhang Cai, Zunduo Zhao, Jiaming Li, Yanbin Lin, Zhiqiang Wang|<li><span style="color:#FF5733;">2024-10-21</span></li>|OpenAlex|https://github.com/yeyimilk/LLM-Vision-Marine-Animals.git.|https://doi.org/10.1109/ICKG63256.2024.00040|
|260|Comprehensive benchmarking of large language models for RNA secondary
  structure prediction|Luciano I Zablocki, Leandro A. Bugnon, M. Gérard, Leandro E. Di Persia, Georgina Stegmayer, Diego H. Milone|<li><span style="color:#FF5733;">2024-10-21</span></li>|Briefings in Bioinformatics|https://github.com/sinc-lab/rna-llm-folding|https://doi.org/10.1093/bib/bbaf137|
|261|LLaVA-KD: A Framework of Distilling Multimodal Large Language Models|Yuxuan Cai, Jiangning Zhang, Haoyang He, Xinwei He, Ao Tong, Zhenye Gan, Chengjie Wang, Xiang Bai|<li><span style="color:#FF5733;">2024-10-21</span></li>|arXiv (Cornell University)|https://github.com/Fantasyele/LLaVA-KD.|http://arxiv.org/abs/2410.16236|
|262|Boosting Jailbreak Transferability for Large Language Models|Hanqing Liu, Lifeng Zhou, Huanqian Yan|<li><span style="color:#FF5733;">2024-10-21</span></li>|arXiv (Cornell University)|https://github.com/HqingLiu/SI-GCG.|http://arxiv.org/abs/2410.15645|
|263|Advances in Citation Text Generation: Leveraging Multi-Source Seq2Seq Models and Large Language Models|Avinash Anand, Ashwin R. Nair, Kritarth Prasad, Vrinda Narayan, Naman Lal, Debanjan Mahata, Yaman Singla, Rajiv Ratn Sha...|<li><span style="color:#FF5733;">2024-10-20</span></li>|OpenAlex|https://github.com/midas-research/M-CTG|https://doi.org/10.1145/3627673.3679783|
|264|Evaluating Deep Unlearning in Large Language Models|Ruihan Wu, Chhavi Yadav, Russ R. Salakhutdinov, Kamalika Chaudhuri|<li><span style="color:#FF5733;">2024-10-19</span></li>|arXiv (Cornell University)|https://github.com/wrh14/deep_unlearning.|http://arxiv.org/abs/2410.15153|
|265|Explaining Graph Neural Networks with Large Language Models: A
  Counterfactual Perspective for Molecular Property Prediction|Yinhan He, Zaiyi Zheng, Patrick Soga, Yuemin Zhu, Yushun Dong, Jundong Li|<li><span style="color:#FF5733;">2024-10-19</span></li>|arXiv (Cornell University)|https://github.com/YinhanHe123/new|http://arxiv.org/abs/2410.15165|
|266|GlitchMiner: Mining Glitch Tokens in Large Language Models via
  Gradient-based Discrete Optimization|Zihui Wu, Haichang Gao, Ping Wang, Shudong Zhang, Zhaoxiang Liu, Shiguo Lian|<li><span style="color:#FF5733;">2024-10-19</span></li>|arXiv (Cornell University)|https://github.com/wooozihui/GlitchMiner.|http://arxiv.org/abs/2410.15052|
|267|CoMAL: Collaborative Multi-Agent Large Language Models for
  Mixed-Autonomy Traffic|Huaiyuan Yao, Longchao Da, Vishnu Nandam, Justin Turnau, Zhiwei Liu, Linsey Pang, Hua Wei|<li><span style="color:#FF5733;">2024-10-18</span></li>|Society for Industrial and Applied Mathematics eBooks|https://github.com/Hyan-Yao/CoMAL.|https://doi.org/10.1137/1.9781611978520.43|
|268|Towards Faithful Natural Language Explanations: A Study Using Activation
  Patching in Large Language Models|Wei Jie Yeo, Ranjan Satapathy, Erik Cambria|<li><span style="color:#FF5733;">2024-10-17</span></li>|arXiv|https://github.com/wj210/Causal-Faithfulness|https://doi.org/10.48550/arXiv.2410.14155|
|269|Automatically Interpreting Millions of Features in Large Language Models|Gonçalo Paulo, Alex Mallen, C. H. Juang, Nora Belrose|<li><span style="color:#FF5733;">2024-10-17</span></li>|arXiv (Cornell University)|https://github.com/EleutherAI/sae-auto-interp|http://arxiv.org/abs/2410.13928|
|270|Do LLMs Overcome Shortcut Learning? An Evaluation of Shortcut Challenges
  in Large Language Models|Yu Yuan, Lili Zhao, Kai Zhang, Guangting Zheng, Qi Liu|<li><span style="color:#FF5733;">2024-10-17</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/yyhappier/ShortcutSuite.git.|https://doi.org/10.18653/v1/2024.emnlp-main.679|
|271|HerO at AVeriTeC: The Herd of Open Large Language Models for Verifying
  Real-World Claims|Yejun Yoon, Jaeyoon Jung, Seunghyun Yoon, Kunwoo Park|<li><span style="color:#FF5733;">2024-10-16</span></li>|OpenAlex|https://github.com/ssu-humane/HerO.|https://doi.org/10.18653/v1/2024.fever-1.15|
|272|Data Defenses Against Large Language Models|William S. Agnew, Harry H. Jiang, Cella Sum, Maarten Sap, Sauvik Das|<li><span style="color:#FF5733;">2024-10-16</span></li>|arXiv (Cornell University)|https://github.com/wagnew3/LLMDataDefenses|http://arxiv.org/abs/2410.13138|
|273|Bridging the Language Gaps in Large Language Models with Inference-Time
  Cross-Lingual Intervention|Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch|<li><span style="color:#FF5733;">2024-10-16</span></li>|arXiv|https://github.com/weixuan-wang123/INCLINE.|https://aclanthology.org/2025.acl-long.270/|
|274|Subspace Optimization for Large Language Models with Convergence
  Guarantees|Yutong He, Peichao Li, Yinggang Hu, C.L. Philip Chen, Kun Yuan|<li><span style="color:#FF5733;">2024-10-15</span></li>|arXiv (Cornell University)|https://github.com/pkumelon/Golore.|http://arxiv.org/abs/2410.11289|
|275|LLM4THP: a computing tool to identify tumor homing peptides by molecular and sequence representation of large language model based on two-layer ensemble model strategy|Sen Yang, Piao Xu|<li><span style="color:#FF5733;">2024-10-15</span></li>|Amino Acids|https://github.com/abcair/LLM4THP.|https://doi.org/10.1007/s00726-024-03422-5|
|276|Layer-wise Importance Matters: Less Memory for Better Performance in
  Parameter-efficient Fine-tuning of Large Language Models|Kai Yao, Penglei Gao, Lichun Li, Yuan Zhao, Xiaofeng Wang, Wei Wang, Jianke Zhu|<li><span style="color:#FF5733;">2024-10-15</span></li>|OpenAlex|https://github.com/Kaiseem/IST.|https://doi.org/10.18653/v1/2024.findings-emnlp.109|
|277|Large Language Model Evaluation via Matrix Nuclear-Norm|Y. S. Li, Tingyu Xia, Yi Chang, Yuan Chieh Wu|<li><span style="color:#FF5733;">2024-10-14</span></li>|arXiv (Cornell University)|https://github.com/MLGroupJLU/MatrixNuclearNorm.|http://arxiv.org/abs/2410.10672|
|278|MentalGLM Series: Explainable Large Language Models for Mental Health
  Analysis on Chinese Social Media|Wei Zhai, Nan Bai, Qing Zhao, Jianqiang Li, Fan Wang, Hongzhi Qi, Meng Jiang, Xiaoqin Wang, Bing Xiang Yang, Guanghui Fu|<li><span style="color:#FF5733;">2024-10-14</span></li>|arXiv (Cornell University)|https://github.com/zwzzzQAQ/MentalGLM.|http://arxiv.org/abs/2410.10323|
|279|One Language, Many Gaps: Evaluating Dialect Fairness and Robustness of
  Large Language Models in Reasoning Tasks|Fangru Lin, Shaoguang Mao, Emanuele La Malfa, Valentin Hofmann, Adrian de Wynter, Jing Yao, Siqing Chen, Michael Wooldri...|<li><span style="color:#FF5733;">2024-10-14</span></li>|arXiv|https://github.com/fangru-lin/redial_dialect_robustness_fairness.git.|https://doi.org/10.48550/arXiv.2410.11005|
|280|Automatically Generating Visual Hallucination Test Cases for Multimodal
  Large Language Models|Zhongye Liu, Hongbin Liu, Yuepeng Hu, Zedian Shao, Neil Zhenqiang Gong|<li><span style="color:#FF5733;">2024-10-14</span></li>|arXiv (Cornell University)|https://github.com/lycheeefish/VHExpansion.|http://arxiv.org/abs/2410.11242|
|281|Denial-of-Service Poisoning Attacks against Large Language Models|Kuofeng Gao, Tianyu Pang, Chao Du, Yong Yang, Shu-Tao Xia, Min Lin|<li><span style="color:#FF5733;">2024-10-14</span></li>|arXiv (Cornell University)|https://github.com/sail-sg/P-DoS.|http://arxiv.org/abs/2410.10760|
|282|AlphaPruning: Using Heavy-Tailed Self Regularization Theory for Improved
  Layer-wise Pruning of Large Language Models|Haiquan Lu, Yefan Zhou, Shiwei Liu, Zhangyang Wang, Michael W. Mahoney, Yaoqing Yang|<li><span style="color:#FF5733;">2024-10-13</span></li>|NeurIPS|https://github.com/haiquanlu/AlphaPruning.|http://papers.nips.cc/paper_files/paper/2024/hash/10fc83943b4540a9524af6fc67a23fef-Abstract-Conference.html|
|283|Benchmarking Cell Type Annotation by Large Language Models with AnnDictionary|George Crowley, Stephen R. Quake|<li><span style="color:#FF5733;">2024-10-13</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/ggit12/anndictionary|https://doi.org/10.1101/2024.10.10.617605|
|284|Targeted Vaccine: Safety Alignment for Large Language Models against
  Harmful Fine-Tuning via Layer-wise Perturbation|G. H. Liu, Weiwei Lin, Tiansheng Huang, Ruichao Mo, Qi Mu, Li Shen|<li><span style="color:#FF5733;">2024-10-13</span></li>|arXiv (Cornell University)|https://github.com/Lslland/T-Vaccine.|http://arxiv.org/abs/2410.09760|
|285|Dual-AEB: Synergizing Rule-Based and Multimodal Large Language Models
  for Effective Emergency Braking|Wei Zhang, Peng-Fei Li, Junli Wang, B. Sun, Jin Qian, Guozhang Bao, Shao‐Shi Rui, Yu Yang, Wenchao Ding, Peng Li, Yilun ...|<li><span style="color:#FF5733;">2024-10-11</span></li>|arXiv (Cornell University)|https://github.com/ChipsICU/Dual-AEB.|http://arxiv.org/abs/2410.08616|
|286|Extracting and Transferring Abilities For Building Multi-lingual
  Ability-enhanced Large Language Models|Zhipeng Chen, Liang Song, Kun Zhou, Wayne Xin Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen|<li><span style="color:#FF5733;">2024-10-10</span></li>|arXiv (Cornell University)|https://github.com/RUCAIBox/MAET|http://arxiv.org/abs/2410.07825|
|287|Key-Value Cache Quantization in Large Language Models: A Safety Benchmark|Timothy Liu|<li><span style="color:#FF5733;">2024-10-10</span></li>|International Journal of Computer Science and Information Technology|https://github.com/TimochiL/llm_benchmark.|https://doi.org/10.62051/ijcsit.v4n2.16|
|288|Teaching-Inspired Integrated Prompting Framework: A Novel Approach for
  Enhancing Reasoning in Large Language Models|Wenting Tan, Dongxiao Chen, Jieting Xue, Zihao Wang, Tianqiao Chen|<li><span style="color:#FF5733;">2024-10-10</span></li>|arXiv (Cornell University)|https://github.com/SallyTan13/Teaching-Inspired-Prompting.|http://arxiv.org/abs/2410.08068|
|289|Understanding the Interplay between Parametric and Contextual Knowledge
  for Large Language Models|Sitao Cheng, Liangming Pan, Xunjian Yin, Xinyi Wang, William Yang Wang|<li><span style="color:#FF5733;">2024-10-10</span></li>|arXiv (Cornell University)|https://github.com/sitaocheng/Knowledge_Interplay|http://arxiv.org/abs/2410.08414|
|290|Dissecting Fine-Tuning Unlearning in Large Language Models|Yihuai Hong, Yuelin Zou, Lijie Hu, Ziqian Zeng, Di Wang, Haiqin Yang|<li><span style="color:#FF5733;">2024-10-09</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/yihuaihong/Dissecting-FT-Unlearning.|https://doi.org/10.18653/v1/2024.emnlp-main.228|
|291|Synthesizing Interpretable Control Policies through Large Language Model
  Guided Search|Carlo Bosio, Mark W. Mueller|<li><span style="color:#FF5733;">2024-10-07</span></li>|arXiv (Cornell University)|https://github.com/muellerlab/synthesizing_interpretable_control_policies.git|http://arxiv.org/abs/2410.05406|
|292|On the Reliability of Large Language Models to Misinformed and
  Demographically-Informed Prompts|Toluwani Aremu, Oluwakemi Akinwehinmi, Chukwuemeka Nwagu, Syed Ishtiaque Ahmed, Rita Orji, Pedro Arnau Del Amo, Abdulmot...|<li><span style="color:#FF5733;">2024-10-06</span></li>|Research Square (Research Square)|https://github.com/tolusophy/Edge|https://doi.org/10.21203/rs.3.rs-5258646/v1|
|293|Leveraging Large Language Models for Suicide Detection on Social Media
  with Limited Labels|Vy Nguyen, Chau Pham|<li><span style="color:#FF5733;">2024-10-06</span></li>|2021 IEEE International Conference on Big Data (Big Data)|https://github.com/khanhvynguyen/Suicide_Detection_LLMs.|https://doi.org/10.1109/BigData62323.2024.10825313|
|294|MindScope: Exploring cognitive biases in large language models through
  Multi-Agent Systems|Zhentao Xie, Jiabao Zhao, Yilei Wang, Jinxin Shi, Yanhong Bai, Xingjiao Wu, Liang He|<li><span style="color:#FF5733;">2024-10-06</span></li>|Frontiers in artificial intelligence and applications|https://github.com/2279072142/MindScope.|https://doi.org/10.3233/FAIA240879|
|295|Neuron-Level Sequential Editing for Large Language Models|Houcheng Jiang, Junfeng Fang, Tianyu Zhang, Baolong Bi, An Zhang, Ruipeng Wang, Tao Liang, Xiang Wang|<li><span style="color:#FF5733;">2024-10-05</span></li>|ACL|https://github.com/jianghoucheng/NSE|https://aclanthology.org/2025.acl-long.815/|
|296|CS4: Measuring the Creativity of Large Language Models Automatically by
  Controlling the Number of Story-Writing Constraints|Anirudh Atmakuru, Jatin Nainani, Rohith Siddhartha Reddy Bheemreddy, Anirudh Lakkaraju, Zonghai Yao, Hamed Zamani, Haw-S...|<li><span style="color:#FF5733;">2024-10-05</span></li>|arXiv (Cornell University)|https://github.com/anirudhlakkaraju/cs4_benchmark.|http://arxiv.org/abs/2410.04197|
|297|Output Scouting: Auditing Large Language Models for Catastrophic
  Responses|Colin Bell, João Eurico Fonseca|<li><span style="color:#FF5733;">2024-10-04</span></li>|arXiv (Cornell University)|https://github.com/joaopfonseca/outputscouting|http://arxiv.org/abs/2410.05305|
|298|POSIX: A Prompt Sensitivity Index For Large Language Models|Anwoy Chatterjee, H. S. V. N. S. Kowndinya Renduchintala, Sumit Bhatia, Tanmoy Chakraborty|<li><span style="color:#FF5733;">2024-10-03</span></li>|OpenAlex|https://github.com/kowndinya-renduchintala/POSIX.|https://doi.org/10.18653/v1/2024.findings-emnlp.852|
|299|CommonIT: Commonality-Aware Instruction Tuning for Large Language Models
  via Data Partitions|Jun Rao, Xuebo Liu, Lian Lian, Shengjun Cheng, Yunjie Liao, Min Zhang|<li><span style="color:#FF5733;">2024-10-03</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/raojay7/CommonIT|https://doi.org/10.18653/v1/2024.emnlp-main.561|
|300|Basis Sharing: Cross-Layer Parameter Sharing for Large Language Model
  Compression|Jingcun Wang, Yu-Guang Chen, Ing-Chao Lin, Bing Li, Grace Li Zhang|<li><span style="color:#FF5733;">2024-10-02</span></li>|ICLR|https://github.com/TUDa-HWAI/Basis_Sharing|https://openreview.net/forum?id=gp32jvUquq|
|301|CodeJudge: Evaluating Code Generation with Large Language Models|Weiwei Tong, Tianyi Zhang|<li><span style="color:#FF5733;">2024-10-02</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/VichyTong/CodeJudge.|https://doi.org/10.18653/v1/2024.emnlp-main.1118|
|302|DLP-LoRA: Efficient Task-Specific LoRA Fusion with a Dynamic,
  Lightweight Plugin for Large Language Models|Yuxuan Zhang, Ruizhe Li|<li><span style="color:#FF5733;">2024-10-02</span></li>|arXiv (Cornell University)|https://github.com/MeCuping/DLP-LoRA.|http://arxiv.org/abs/2410.01497|
|303|AMR-Evol: Adaptive Modular Response Evolution Elicits Better Knowledge
  Distillation for Large Language Models in Code Generation|Ziyang Luo, Xin Li, Hongzhan Lin, Jing Ma, Lidong Bing|<li><span style="color:#FF5733;">2024-10-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/ChiYeungLaw/AMR-Evol.|https://doi.org/10.18653/v1/2024.emnlp-main.66|
|304|Do Influence Functions Work on Large Language Models?|Zhe Li, Wei Zhao, Yige Li, Jun Sun|<li><span style="color:#FF5733;">2024-09-30</span></li>|arXiv (Cornell University)|https://github.com/plumprc/Failures-of-Influence-Functions-in-LLMs.|http://arxiv.org/abs/2409.19998|
|305|Can Large Language Models Analyze Graphs like Professionals? A
  Benchmark, Datasets and Models|Xin Li, Weize Chen, Qi Chu, Haopeng Li, Zhaojun Sun, Ran Li, Qian Chen, Yiwei Wei, Z. Liu, Chuan Shi, Maosong Sun, Cheng...|<li><span style="color:#FF5733;">2024-09-29</span></li>|NeurIPS|https://github.com/BUPT-GAMMA/ProGraph.|http://papers.nips.cc/paper_files/paper/2024/hash/ff417c3993894694e88ffc4d3f53d28b-Abstract-Datasets_and_Benchmarks_Track.html|
|306|Identifying Knowledge Editing Types in Large Language Models|Xiao Peng Li, Shangwen Wang, Shezheng Song, Bin Ji, Huijun Liu, Shasha Li, Jun Ma, Jie Yu|<li><span style="color:#FF5733;">2024-09-29</span></li>|arXiv (Cornell University)|https://github.com/xpq-tech/KETI.|http://arxiv.org/abs/2409.19663|
|307|RouterDC: Query-Based Router by Dual Contrastive Learning for Assembling
  Large Language Models|Shuhao Chen, Weisen Jiang, Baijiong Lin, James T. Kwok, Yu Zhang|<li><span style="color:#FF5733;">2024-09-29</span></li>|NeurIPS|https://github.com/shuhao02/RouterDC.|http://papers.nips.cc/paper_files/paper/2024/hash/7a641b8ec86162fc875fb9f6456a542f-Abstract-Conference.html|
|308|OpenSep: Leveraging Large Language Models with Textual Inversion for
  Open World Audio Separation|Tanvir Mahmud, Diana Marculescu|<li><span style="color:#FF5733;">2024-09-28</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/tanvir-utexas/OpenSep.git|https://doi.org/10.18653/v1/2024.emnlp-main.735|
|309|Align$^2$LLaVA: Cascaded Human and Large Language Model Preference
  Alignment for Multi-modal Instruction Curation|Hongzhe Huang, Zhewen Yu, Jiang Liu, Li Cai, Dian Jiao, Wenqiao Zhang, Siliang Tang, Juncheng Li, Hao Jiang, Haoyuan Li,...|<li><span style="color:#FF5733;">2024-09-27</span></li>|arXiv (Cornell University)|https://github.com/DCDmllm/Align2LLaVA.|http://arxiv.org/abs/2409.18541|
|310|Harmful Fine-tuning Attacks and Defenses for Large Language Models: A
  Survey|Tiansheng Huang, Sihao Hu, Fatih İlhan, Selim Furkan Tekin, Ling Liu|<li><span style="color:#FF5733;">2024-09-26</span></li>|arXiv (Cornell University)|https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers|http://arxiv.org/abs/2409.18169|
|311|CurricuLLM: Automatic Task Curricula Design for Learning Complex Robot
  Skills using Large Language Models|Kanghyun Ryu, Qiayuan Liao, Zhongyu Li, Koushil Sreenath, Negar Mehr|<li><span style="color:#FF5733;">2024-09-26</span></li>|arXiv (Cornell University)|https://github.com/labicon/CurricuLLM|http://arxiv.org/abs/2409.18382|
|312|Control Industrial Automation System with Large Language Models|Yuchen Xia, Nasser Jazdi, J. Zhang, C.L. Shah, Michael Weyrich|<li><span style="color:#FF5733;">2024-09-26</span></li>|arXiv (Cornell University)|https://github.com/YuchenXia/LLM4IAS|http://arxiv.org/abs/2409.18009|
|313|RED QUEEN: Safeguarding Large Language Models against Concealed
  Multi-Turn Jailbreaking|Yifan Jiang, Kriti Aggarwal, Tanmay Laud, Kashif Munir, Jay Pujara, Subhabrata Mukherjee|<li><span style="color:#FF5733;">2024-09-25</span></li>|arXiv (Cornell University)|https://github.com/kriti-hippo/red_queen.|http://arxiv.org/abs/2409.17458|
|314|LLM-CARD: Towards a Description and Landscape of Large Language Models|Shengwei Tian, Lifeng Han, Erick Mendez Guzman, Goran Nenadić|<li><span style="color:#FF5733;">2024-09-25</span></li>|arXiv (Cornell University)|https://github.com/shengwei-tian/dependency-parser-visualization|http://arxiv.org/abs/2409.17011|
|315|XTRUST: On the Multilingual Trustworthiness of Large Language Models|Y. S. Li, Yi Wang, Yi Chang, Guowu Yuan|<li><span style="color:#FF5733;">2024-09-24</span></li>|arXiv (Cornell University)|https://github.com/LluckyYH/XTRUST.|http://arxiv.org/abs/2409.15762|
|316|Pretraining Data Detection for Large Language Models: A Divergence-based
  Calibration Method|Weichao Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng|<li><span style="color:#FF5733;">2024-09-23</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/zhang-wei-chao/DC-PDD|https://doi.org/10.18653/v1/2024.emnlp-main.300|
|317|Interpreting Arithmetic Mechanism in Large Language Models through
  Comparative Neuron Analysis|Zeping Yu, Sophia Ananiadou|<li><span style="color:#FF5733;">2024-09-21</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/zepingyu0512/arithmetic-mechanism.|https://doi.org/10.18653/v1/2024.emnlp-main.193|
|318|StateAct: State Tracking and Reasoning for Acting and Planning with
  Large Language Models|Nikolai Rozanov, Marek Rei|<li><span style="color:#FF5733;">2024-09-21</span></li>|arXiv (Cornell University)|https://github.com/ai-nikolai/StateAct|http://arxiv.org/abs/2410.02810|
|319|ShizishanGPT: An Agricultural Large Language Model Integrating Tools and
  Resources|Shuting Yang, Zehui Liu, Wolfgang Mayer, Ningpei Ding, Ying Wang, Yu Huang, Pengfei Wu, Wanli Li, Li Lin, Hongyu Zhang, ...|<li><span style="color:#FF5733;">2024-09-20</span></li>|Lecture notes in computer science|https://github.com/Zaiwen/CropGPT.|https://doi.org/10.1007/978-981-96-0573-6_21|
|320|An adapted large language model facilitates multiple medical tasks in
  diabetes care|Wei Lai, Zhen Ying, M. He, Yutong Chen, Qian Yang, Hong Ye, Jiaping Lu, Xiaoying Li, Weiran Huang, Ying Chen|<li><span style="color:#FF5733;">2024-09-19</span></li>|arXiv (Cornell University)|https://github.com/waltonfuture/Diabetica.|http://arxiv.org/abs/2409.13191|
|321|CLAIR-A: Leveraging Large Language Models to Judge Audio Captions|Tsung-Han Wu, Joseph E. Gonzalez, Trevor Darrell, David M. Chan|<li><span style="color:#FF5733;">2024-09-19</span></li>|arXiv (Cornell University)|https://github.com/DavidMChan/clair-a.|http://arxiv.org/abs/2409.12962|
|322|From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal
  Reasoning with Large Language Models|Shengsheng Qian, Zuyi Zhou, Dizhan Xue, Bing Wang, Changsheng Xu|<li><span style="color:#FF5733;">2024-09-18</span></li>|arXiv (Cornell University)|https://github.com/ZuyiZhou/Awesome-Cross-modal-Reasoning-with-LLMs|http://arxiv.org/abs/2409.18996|
|323|Benchmarking Large Language Model Uncertainty for Prompt Optimization|Pei-Fu Guo, Yun-Da Tsai, Shou-De Lin|<li><span style="color:#FF5733;">2024-09-16</span></li>|arXiv (Cornell University)|https://github.com/0Frett/PO-Uncertainty-Benchmarking.|http://arxiv.org/abs/2409.10044|
|324|Do Large Language Models Need a Content Delivery Network?|Yihua Cheng, Kuntai Du, Jiayi Yao, Junchen Jiang|<li><span style="color:#FF5733;">2024-09-16</span></li>|arXiv (Cornell University)|https://github.com/LMCache/LMCache.|http://arxiv.org/abs/2409.13761|
|325|Fit and Prune: Fast and Training-free Visual Token Pruning for
  Multi-modal Large Language Models|Weihao Ye, Qiong Wu, Wenhao Lin, Yiyi Zhou|<li><span style="color:#FF5733;">2024-09-16</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/ywh187/FitPrune.|https://doi.org/10.1609/aaai.v39i21.34366|
|326|The Two Word Test as a semantic benchmark for large language models|Nicholas Riccardi, Xuan Yang, Rutvik H Desai|<li><span style="color:#FF5733;">2024-09-16</span></li>|Scientific Reports|https://github.com/NickRiccardi/two-word-test|https://doi.org/10.1038/s41598-024-72528-3|
|327|Can Large Language Models Grasp Event Signals? Exploring Pure Zero-Shot
  Event-based Recognition|Zongyou Yu, Qiang Qu, Xiaoming Chen, Chen Wang|<li><span style="color:#FF5733;">2024-09-15</span></li>|ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/ChrisYu-Zz/Pure-event-based-recognition-based-LLM|https://doi.org/10.1109/ICASSP49660.2025.10887714|
|328|Towards Data Contamination Detection for Modern Large Language Models:
  Limitations, Inconsistencies, and Oracle Challenges|Vinay Samuel, Yue Zhou, Henry Peng Zou|<li><span style="color:#FF5733;">2024-09-15</span></li>|COLING|https://github.com/vsamuel2003/data-contamination.|https://aclanthology.org/2025.coling-main.338/|
|329|DrLLM: Prompt-Enhanced Distributed Denial-of-Service Resistance Method
  with Large Language Models|Zhenyu Yin, Shang Liu, Guangyuan Xu|<li><span style="color:#FF5733;">2024-09-11</span></li>|ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/liuup/DrLLM.|https://doi.org/10.1109/ICASSP49660.2025.10889735|
|330|Towards Democratizing Multilingual Large Language Models For Medicine
  Through A Two-Stage Instruction Fine-tuning Approach|Meng Zhou, Surajsinh Parmar, Anubhav Bhatti|<li><span style="color:#FF5733;">2024-09-09</span></li>|arXiv (Cornell University)|https://github.com/SpassMed/Med-Llama3|http://arxiv.org/abs/2409.05732|
|331|FLoRA: Federated Fine-Tuning Large Language Models with Heterogeneous
  Low-Rank Adaptations|Ziyao Wang, Zheyu Shen, Yexiao He, Guoheng Sun, Hongyi Wang, Lingjuan Lyu, Ang Li|<li><span style="color:#FF5733;">2024-09-09</span></li>|NeurIPS|https://github.com/ATP-1010/FederatedLLM.|http://papers.nips.cc/paper_files/paper/2024/hash/28312c9491d60ed0c77f7fff4ad86dd1-Abstract-Conference.html|
|332|Benchmarking Chinese Knowledge Rectification in Large Language Models|Tianhe Lu, J. Fang, Yunzhi Yao, Xin Xu, Ningyu Zhang, Huajun Chen|<li><span style="color:#FF5733;">2024-09-09</span></li>|arXiv (Cornell University)|https://github.com/zjunlp/EasyEdit.|http://arxiv.org/abs/2409.05806|
|333|Large Language Model-Based Agents for Software Engineering: A Survey|Junwei Liu, Kaixin Wang, Yixuan Chen, Xin Peng, Zhenpeng Chen, Lingming Zhang, Yiling Lou|<li><span style="color:#FF5733;">2024-09-04</span></li>|arXiv (Cornell University)|https://github.com/FudanSELab/Agent4SE-Paper-List.|http://arxiv.org/abs/2409.02977|
|334|Exploiting the Vulnerability of Large Language Models via Defense-Aware
  Architectural Backdoor|Abdullah Arafat Miah, Yu Bi|<li><span style="color:#FF5733;">2024-09-03</span></li>|arXiv (Cornell University)|https://github.com/SiSL-URI/Arch_Backdoor_LLM.|http://arxiv.org/abs/2409.01952|
|335|Foundations of Large Language Model Compression -- Part 1: Weight
  Quantization|Sean I. Young|<li><span style="color:#FF5733;">2024-09-03</span></li>|arXiv (Cornell University)|https://github.com/seannz/cvxq.|http://arxiv.org/abs/2409.02026|
|336|FlashFlex: Accommodating Large Language Model Training over
  Heterogeneous Environment|Ran Yan, Youhe Jiang, Wangcheng Tao, Xiaonan Nie, Bin Cui, Binhang Yuan|<li><span style="color:#FF5733;">2024-09-02</span></li>|arXiv (Cornell University)|https://github.com/Relaxed-System-Lab/FlashFlex.|http://arxiv.org/abs/2409.01143|
|337|Booster: Tackling Harmful Fine-tuning for Large Language Models via
  Attenuating Harmful Perturbation|Tiansheng Huang, Sihao Hu, Fatih İlhan, Selim Furkan Tekin, Ling Liu|<li><span style="color:#FF5733;">2024-09-02</span></li>|ICLR|https://github.com/git-disl/Booster|https://openreview.net/forum?id=tTPHgb0EtV|
|338|Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals
  in Large Language Models|Bang An, Sicheng Zhu, Ruiyi Zhang, Michael-Andrei Panaitescu-Liess, Yuancheng Xu, Furong Huang|<li><span style="color:#FF5733;">2024-08-31</span></li>|arXiv (Cornell University)|https://github.com/umd-huang-lab/FalseRefusal|http://arxiv.org/abs/2409.00598|
|339|AgentMove: Predicting Human Mobility Anywhere Using Large Language Model
  based Agentic Framework|Jie Feng, Yuwei Du, Jie Zhao, Yong Li|<li><span style="color:#FF5733;">2024-08-25</span></li>|arXiv (Cornell University)|https://github.com/tsinghua-fib-lab/AgentMove.|http://arxiv.org/abs/2408.13986|
|340|DesignQA: Benchmarking Multimodal Large Language Models on Questions Grounded in Engineering Documentation|Anna C. Doris, Daniele Grandi, Ryan Tomich, Md Ferdous Alam, Hyunmin Cheong, Faez Ahmed|<li><span style="color:#FF5733;">2024-08-25</span></li>|OpenAlex|https://github.com/anniedoris/design_qa|https://doi.org/10.1115/detc2024-139024|
|341|AutoWebGLM: Bootstrap And Reinforce A Large Language Model-based Web
  Navigating Agent|Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, H. H. Zhang, Xiaohan Zhang, Yuxiao D...|<li><span style="color:#FF5733;">2024-08-24</span></li>|Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/THUDM/AutoWebGLM.|https://doi.org/10.48550/arXiv.2404.03648|
|342|IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model
  with Multimodal Capabilities|Bin Wang, Chunyu Xie, Dawei Leng, Yuhui Yin|<li><span style="color:#FF5733;">2024-08-23</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/360CVGroup/Inner-Adaptor-Architecture.|https://doi.org/10.1609/aaai.v39i20.35400|
|343|LIMP: Large Language Model Enhanced Intent-aware Mobility Prediction|Songwei Li, Jie Feng, Jiawei Chi, Xinyuan Hu, Xiaomeng Zhao, Fengli Xu|<li><span style="color:#FF5733;">2024-08-23</span></li>|arXiv (Cornell University)|https://github.com/tsinghua-fib-lab/LIMP|http://arxiv.org/abs/2408.12832|
|344|CLLMFS: A Contrastive Learning enhanced Large Language Model Framework
  for Few-Shot Named Entity Recognition|Yafeng Zhang, Zilan Yu, Yuang Huang, Jing Tang|<li><span style="color:#FF5733;">2024-08-23</span></li>|Frontiers in artificial intelligence and applications|https://github.com/yuzilan/CLLMFS|https://doi.org/10.3233/FAIA240714|
|345|Controllable Text Generation for Large Language Models: A Survey|Xun Liang, Hanyu Wang, Yezhaohui Wang, Shichao Song, Jiawei Yang, Simin Niu, Jie Hu, Dan Liu, Shunyu Yao, Feiyu Xiong, Z...|<li><span style="color:#FF5733;">2024-08-22</span></li>|arXiv (Cornell University)|https://github.com/IAAR-Shanghai/CTGSurvey.|http://arxiv.org/abs/2408.12599|
|346|Enhanced Fine-Tuning of Lightweight Domain-Specific Q&amp;A Model Based on Large Language Models|Shenglin Zhang, Pengtian Zhu, Minghua Ma, Jia‐Gang Wang, Yongqian Sun, Dongwen Li, Jingyu Wang, Qianying Guo, Xiaolei Hu...|<li><span style="color:#FF5733;">2024-08-22</span></li>|OpenAlex|https://github.com/Zero-Pointer/Self-Evolution.|https://doi.org/10.1109/ISSREW63542.2024.00048|
|347|GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender
  Bias in Large Language Models|Kunsheng Tang, Wenbo Zhou, Jie Zhang, Aishan Liu, Gelei Deng, Shuai Li, Pengyu Qi, Weiming Zhang, Tianwei Zhang, Nenghai...|<li><span style="color:#FF5733;">2024-08-22</span></li>|OpenAlex|https://github.com/kstanghere/GenderCARE-ccs24.|https://doi.org/10.1145/3658644.3670284|
|348|Geolocation Representation from Large Language Models are Generic
  Enhancers for Spatio-Temporal Learning|Junlin He, Tong Nie, Wei Ma|<li><span style="color:#FF5733;">2024-08-22</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/Umaruchain/LLMGeovec|https://doi.org/10.1609/aaai.v39i16.33879|
|349|Towards Evaluating and Building Versatile Large Language Models for
  Medicine|Chaoyi Wu, Pengcheng Qiu, Jinxin Liu, Hongfei Gu, Na Li, Ya Zhang, Yanfeng Wang, Weidi Xie|<li><span style="color:#FF5733;">2024-08-22</span></li>|npj Digital Medicine|https://github.com/MAGIC-AI4Med/MedS-Ins.|https://doi.org/10.1038/s41746-024-01390-4|
|350|Personality Alignment of Large Language Models|Minjun Zhu, Yixuan Weng, Linyi Yang, Yue Zhang|<li><span style="color:#FF5733;">2024-08-21</span></li>|ICLR|https://github.com/zhu-minjun/PAlign|https://openreview.net/forum?id=0DZEs8NpUH|
|351|MoE-LPR: Multilingual Extension of Large Language Models through
  Mixture-of-Experts with Language Priors Routing|Hao Zhou, Zhijun Wang, Shujian Huang, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Weihua Luo, Jiajun Chen|<li><span style="color:#FF5733;">2024-08-21</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/zjwang21/MoE-LPR.git.|https://doi.org/10.48550/arXiv.2408.11396|
|352|SORSA: Singular Values and Orthonormal Regularized Singular Vectors
  Adaptation of Large Language Models|Yang Cao|<li><span style="color:#FF5733;">2024-08-21</span></li>|arXiv (Cornell University)|https://github.com/Gunale0926/SORSA.|http://arxiv.org/abs/2409.00055|
|353|CodeJudge-Eval: Can Large Language Models be Good Judges in Code
  Understanding?|Yuwei Zhao, Ziyang Luo, Yuchen Tian, Hongzhan Lin, Weixiang Yan, Annan Li, Jing Ma|<li><span style="color:#FF5733;">2024-08-20</span></li>|COLING|https://github.com/CodeLLM-Research/CodeJudge-Eval|https://aclanthology.org/2025.coling-main.7/|
|354|SysBench: Can Large Language Models Follow System Messages?|Yanzhao Qin, Tao Zhang, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng Chen, Zenan Zhou...|<li><span style="color:#FF5733;">2024-08-20</span></li>|arXiv (Cornell University)|https://github.com/PKU-Baichuan-MLSystemLab/SysBench.|http://arxiv.org/abs/2408.10943|
|355|LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for
  Large Language Models|Yupeng Su, Ziyi Guan, Xiaoqun Liu, Tianlai Jin, Dongkuan Wu, Graziano Chesi, Ngai Wong, Hao Yu|<li><span style="color:#FF5733;">2024-08-20</span></li>|arXiv (Cornell University)|https://github.com/YupengSu/LLM-Barber.|http://arxiv.org/abs/2408.10631|
|356|Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion
  for Efficient Inference Intervention in Large Language Model|Chenhan Yuan, Fei Huang, Ru Peng, Keming Lu, Bowen Yu, Chang Zhou, Jingren Zhou|<li><span style="color:#FF5733;">2024-08-20</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/chenhan97/Otter|https://doi.org/10.18653/v1/2024.emnlp-main.316|
|357|Large Language Models for Multimodal Deformable Image Registration|Mingrui Ma, Weijie Wang, Jie Ning, Jianfeng He, Nicu Sebe, Bruno Lepri|<li><span style="color:#FF5733;">2024-08-20</span></li>|arXiv (Cornell University)|https://github.com/ninjannn/LLM-Morph.|http://arxiv.org/abs/2408.10703|
|358|R2GenCSR: Retrieving Context Samples for Large Language Model based
  X-ray Medical Report Generation|Xiao Wang, Yuehang Li, Fuling Wang, Shiao Wang, Chuanfu Li, Bo Jiang|<li><span style="color:#FF5733;">2024-08-19</span></li>|arXiv (Cornell University)|https://github.com/Event-AHU/Medical_Image_Analysis|http://arxiv.org/abs/2408.09743|
|359|HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon
  Agent Tasks with Large Language Model|Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, Ping Luo|<li><span style="color:#FF5733;">2024-08-18</span></li>|ACL|https://github.com/HiAgent2024/HiAgent|https://aclanthology.org/2025.acl-long.1575/|
|360|Can Large Language Models Improve the Adversarial Robustness of Graph
  Neural Networks?|Zhongjian Zhang, Xiao Wang, Huichi Zhou, Yue Yu, Mengmei Zhang, Cheng Yang, Chuan Shi|<li><span style="color:#FF5733;">2024-08-16</span></li>|OpenAlex|https://github.com/zhongjian-zhang/LLM4RGNN.|https://doi.org/10.1145/3690624.3709256|
|361|A Survey on Benchmarks of Multimodal Large Language Models|Li Jian, Lu Weisheng, Fei Hao, Meng Luo, Ming Dai, Min Xia, Yizhang Jin, Zhenye Gan, Ding Qi, Chaoyou Fu, Ying Tai, Wank...|<li><span style="color:#FF5733;">2024-08-16</span></li>|arXiv (Cornell University)|https://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.|http://arxiv.org/abs/2408.08632|
|362|Prefix Guidance: A Steering Wheel for Large Language Models to Defend
  Against Jailbreak Attacks|Jiawei Zhao, Kejiang Chen, Xiaojian Yuan, Weiming Zhang|<li><span style="color:#FF5733;">2024-08-15</span></li>|arXiv (Cornell University)|https://github.com/weiyezhimeng/Prefix-Guidance.|http://arxiv.org/abs/2408.08924|
|363|ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal
  Knowledge in Large Language Models|Faris Hijazi, Somayah Alharbi, A Abouzeid Alhussein, Harethah Abu Shairah, Reem Alzahrani, Hebah Alshamlan, Omar Knio, G...|<li><span style="color:#FF5733;">2024-08-15</span></li>|ArabicNLP|https://github.com/Thiqah/ArabLegalEval|https://doi.org/10.18653/v1/2024.arabicnlp-1.20|
|364|Evaluating Large Language Model based Personal Information Extraction
  and Countermeasures|Yupei Liu, Y. Jia, Jinyuan Jia, Neil Zhenqiang Gong|<li><span style="color:#FF5733;">2024-08-14</span></li>|arXiv (Cornell University)|https://github.com/liu00222/LLM-Based-Personal-Profile-Extraction|http://arxiv.org/abs/2408.07291|
|365|Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge
  Editing for Large Language Models|Chenhui Hu, Pengfei Cao, Yubo Chen, Liu Kang, Zhao Jun|<li><span style="color:#FF5733;">2024-08-14</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/ChenhuiHu/knowledge_in_superposition.|https://doi.org/10.1609/aaai.v39i22.34583|
|366|Causal Agent based on Large Language Model|KaiYi Han, Kun Kuang, Ziyu Zhao, Junjian Ye, Fei Wu|<li><span style="color:#FF5733;">2024-08-13</span></li>|arXiv (Cornell University)|https://github.com/Kairong-Han/Causal_Agent.|http://arxiv.org/abs/2408.06849|
|367|Large Language Models: A Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects|Muhammad Usman Hadi, Qasem Al Tashi, Abbas Shah, Rizwan Qureshi, Amgad Muneer, Muhammad Irfan, Anas Zafar, Muhammad Bila...|<li><span style="color:#FF5733;">2024-08-12</span></li>|OpenAlex|https://github.com/anas-zafar/LLM-Survey|https://doi.org/10.36227/techrxiv.23589741.v6|
|368|BA-LoRA: Bias-Alleviating Low-Rank Adaptation to Mitigate Catastrophic
  Inheritance in Large Language Models|Yupeng Chang, Yi Chang, Yuan Wu|<li><span style="color:#FF5733;">2024-08-08</span></li>|arXiv (Cornell University)|https://github.com/cyp-jlu-ai/BA-LoRA.|http://arxiv.org/abs/2408.04556|
|369|Enhancing the Readability of Preoperative Patient Instructions Using Large Language Models|Hyo Jung Hong, Clifford A. Schmiesing, Alex J. Goodell|<li><span style="color:#FF5733;">2024-08-08</span></li>|Anesthesiology|https://github.com/stanfordaimlab/anesthesia-literacy|https://doi.org/10.1097/aln.0000000000005122|
|370|Advancing Multimodal Large Language Models with Quantization-Aware Scale
  Learning for Efficient Adaptation|Jingjing Xie, Yuxin Zhang, Mingbao Lin, Liujuan Cao, Rongrong Ji|<li><span style="color:#FF5733;">2024-08-07</span></li>|ACM Multimedia|https://github.com/xjjxmu/QSLAW.|https://doi.org/10.48550/arXiv.2408.03735|
|371|CodexGraph: Bridging Large Language Models and Code Repositories via
  Code Graph Databases|Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Qizhe Shieh, Wenmeng Zhou|<li><span style="color:#FF5733;">2024-08-07</span></li>|OpenAlex|https://github.com/modelscope/modelscope-agent|https://doi.org/10.18653/v1/2025.naacl-long.7|
|372|ULLME: A Unified Framework for Large Language Model Embeddings with
  Generation-Augmented Learning|Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Thien Huu Nguyen|<li><span style="color:#FF5733;">2024-08-06</span></li>|OpenAlex|https://github.com/nlp-uoregon/ullme.|https://doi.org/10.18653/v1/2024.emnlp-demo.24|
|373|Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in
  Customized Large Language Models|Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, Haoyang Li|<li><span style="color:#FF5733;">2024-08-05</span></li>|arXiv|https://github.com/liangzid/PromptExtractionEval|https://doi.org/10.48550/arXiv.2408.02416|
|374|UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks
  With Large Language Model|Zhaowei Li, Wei Wang, Yiqing Cai, Xu Qi, Pengyu Wang, Dong Zhang, Hang Song, Botian Jiang, Zhida Huang, Tao Wang|<li><span style="color:#FF5733;">2024-08-05</span></li>|Findings of the Association for Computational Linguistics: NAACL 2022|https://github.com/lzw-lzw/UnifiedMLLM|https://doi.org/10.18653/v1/2025.findings-naacl.19|
|375|Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models
  within Perturbed Inputs|Peng Ding, Jingyu Wu, Jun Kuang, Dan Ma, Xuezhi Cao, Xunliang Cai, Shi Chen, Jiajun Chen, Shujian Huang|<li><span style="color:#FF5733;">2024-08-02</span></li>|ACM Multimedia|https://github.com/NJUNLP/Hallu-PI.|https://doi.org/10.48550/arXiv.2408.01355|
|376|Correcting Negative Bias in Large Language Models through Negative
  Attention Score Alignment|Sangwon Yu, Jongyoon Song, Bongkyu Hwang, Hoyoung Kang, Sooah Cho, Junhwa Choi, Seongho Joe, Tae-Hee Lee, Youngjune Gwon...|<li><span style="color:#FF5733;">2024-07-31</span></li>|OpenAlex|https://github.com/ysw1021/NASA|https://doi.org/10.18653/v1/2025.naacl-long.503|
|377|Advancing Multimodal Large Language Models in Chart Question Answering
  with Visualization-Referenced Instruction Tuning|Xingchen Zeng, Haichuan Lin, Yilin Ye, Wei Zeng|<li><span style="color:#FF5733;">2024-07-29</span></li>|IEEE Transactions on Visualization and Computer Graphics|https://github.com/zengxingchen/ChartQA-MLLM.|https://doi.org/10.48550/arXiv.2407.20174|
|378|CollectiveSFT: Scaling Large Language Models for Chinese Medical
  Benchmark with Collective Instructions in Healthcare|Jingwei Zhu, Minghuan Tan, Min Yang, Ruixue Li, Hamid Alinejad‐Rokny|<li><span style="color:#FF5733;">2024-07-29</span></li>|Lecture notes in computer science|https://github.com/CAS-SIAT-XinHai/CollectiveSFT|https://doi.org/10.1007/978-981-96-1151-5_6|
|379|Optimizing Numerical Estimation and Operational Efficiency in the Legal
  Domain through Large Language Models|Jia-Hong Huang, Chao-Chun Yang, Yixian Shen, Alessio M. Pacces, Evangelos Kanoulas|<li><span style="color:#FF5733;">2024-07-26</span></li>|OpenAlex|https://github.com/Jhhuangkay/Optimizing-Numerical-Estimation-and-Operational-Efficiency-in-the-Legal-Domain.|https://doi.org/10.48550/arXiv.2407.19041|
|380|A Role-specific Guided Large Language Model for Ophthalmic Consultation
  Based on Stylistic Differentiation|Laiyi Fu, Binbin Fan, Hongkai Du, Yanxiang Feng, LI Chun-hua, Haiwen Song|<li><span style="color:#FF5733;">2024-07-25</span></li>|arXiv|https://github.com/sperfu/EyeDoc.|https://doi.org/10.48550/arXiv.2407.18483|
|381|IgnitionInnovators at &quot;Discharge Me!&quot;: Chain-of-Thought Instruction Finetuning Large Language Models for Discharge Summaries|An Quang Tang, Xiuzhen Zhang, Minh Ngoc Dinh|<li><span style="color:#FF5733;">2024-07-24</span></li>|BioNLP@ACL|https://github.com/antangrocket1312/Discharge_LLM|https://doi.org/10.18653/v1/2024.bionlp-1.65|
|382|Structure-aware Domain Knowledge Injection for Large Language Models|Kai Liu, Ze Chen, Zhihang Fu, Wei Zhang, Rongxin Jiang, Fan Zhou, Yaowu Chen, Yue Wu, Jieping Ye|<li><span style="color:#FF5733;">2024-07-23</span></li>|ACL|https://github.com/alibaba/struxgpt.|https://aclanthology.org/2025.acl-long.1425/|
|383|Do Large Language Models Have Compositional Ability? An Investigation
  into Limitations and Scalability|Zhuoyan Xu, Zhenmei Shi, Yingyu Liang|<li><span style="color:#FF5733;">2024-07-22</span></li>|arXiv|https://github.com/OliverXUZY/LLM_Compose|https://doi.org/10.48550/arXiv.2407.15720|
|384|Leveraging Large Language Models to Geolocate Linguistic Variations in
  Social Media Posts|Davide Savarro, Davide Zago, Stefano Zoia|<li><span style="color:#FF5733;">2024-07-22</span></li>|arXiv|https://github.com/dawoz/geolingit-biss2024.|https://doi.org/10.48550/arXiv.2407.16047|
|385|Lingdan: enhancing encoding of traditional Chinese medicine knowledge for clinical reasoning tasks with large language models|Rui Hua, Dong Xin, Yu Wei, Zixin Shu, Pengcheng Yang, Yunhui Hu, Shuiping Zhou, He Sun, Kaijing Yan, Xijun Yan, Kai Chan...|<li><span style="color:#FF5733;">2024-07-22</span></li>|Journal of the American Medical Informatics Association|https://github.com/TCMAI-BJTU/LingdanLLM.|https://doi.org/10.1093/jamia/ocae087|
|386|Knowledge Acquisition Disentanglement for Knowledge-based Visual
  Question Answering with Large Language Models|Wenbin An, Feng Tian, Jiahao Nie, Wenkai Shi, Haonan Lin, Yan Chen, Qianying Wang, Yaqiang Wu, Guang Dai, Ping Chen|<li><span style="color:#FF5733;">2024-07-21</span></li>|arXiv|https://github.com/Lackel/DKA|https://doi.org/10.48550/arXiv.2407.15346|
|387|Internal Consistency and Self-Feedback in Large Language Models: A
  Survey|Xun Liang, Shichao Song, Zifan Zheng, Hanyu Wang, Qingchen Yu, Xunkai Li, Yuanyuan Li, Feiyu Xiong, Zhiyu Li|<li><span style="color:#FF5733;">2024-07-19</span></li>|arXiv|https://github.com/IAAR-Shanghai/ICSFSurvey|https://doi.org/10.48550/arXiv.2407.14507|
|388|EarthMarker: A Visual Prompting Multi-modal Large Language Model for
  Remote Sensing|Wei Zhang, Miaoxin Cai, Tong Zhang, Junmin Li, Yin Zhuang, Xuerui Mao|<li><span style="color:#FF5733;">2024-07-18</span></li>|IEEE Transactions on Geoscience and Remote Sensing|https://github.com/wivizhang/EarthMarker.|https://doi.org/10.1109/tgrs.2024.3523505|
|389|LRQ: Optimizing Post-Training Quantization for Large Language Models by
  Learning Low-Rank Weight-Scaling Matrices|Jung Hyun Lee, Jeonghoon Kim, June Yong Yang, Se Jung Kwon, Eunho Yang, Kang Min Yoo, Dongsoo Lee|<li><span style="color:#FF5733;">2024-07-16</span></li>|OpenAlex|https://github.com/onliwad101/FlexRound_LRQ|https://doi.org/10.18653/v1/2025.naacl-long.393|
|390|Evaluating Large Language Models with fmeval|Pola Schwöbel, Luca Franceschi, Muhammad Bilal Zafar, Keerthan Vasist, A. Malhotra, Tomer Shenhar, Pinal Tailor, Pınar Y...|<li><span style="color:#FF5733;">2024-07-15</span></li>|arXiv|https://github.com/aws/fmeval.|https://doi.org/10.48550/arXiv.2407.12872|
|391|When AI Meets Finance (StockAgent): Large Language Model-based Stock
  Trading in Simulated Real-world Environments|Chong Zhang, Xinyi Liu, Mingyu Jin, Zhongmou Zhang, Lingyao Li, Zhenting Wang, Wenyue Hua, Shu Dong, Suiyuan Zhu, Xiaobo...|<li><span style="color:#FF5733;">2024-07-15</span></li>|arXiv|https://github.com/MingyuJ666/Stockagent.|https://doi.org/10.48550/arXiv.2407.18957|
|392|ChatLogic: Integrating Logic Programming with Large Language Models for
  Multi-Step Reasoning|Zhongsheng Wang, Jiamou Liu, Qiming Bao, Hongfei Rong, Jingfeng Zhang|<li><span style="color:#FF5733;">2024-07-14</span></li>|2022 International Joint Conference on Neural Networks (IJCNN)|https://github.com/Strong-AI-Lab/ChatLogic|https://doi.org/10.1109/IJCNN60899.2024.10650138|
|393|The Synergy between Data and Multi-Modal Large Language Models: A Survey
  from Co-Development Perspective|Zhentao Qin, Daoyuan Chen, Wenhao Zhang, Liuyi Yao, Yilun Huang, Bolin Ding, Yaliang Li, Shuiguang Deng|<li><span style="color:#FF5733;">2024-07-11</span></li>|IEEE Transactions on Pattern Analysis and Machine Intelligence|https://github.com/modelscope/data-juicer|https://doi.org/10.1109/tpami.2025.3576835|
|394|Hypergraph Multi-modal Large Language Model: Exploiting EEG and
  Eye-tracking Modalities to Evaluate Heterogeneous Responses for Video
  Understanding|Minghui Wu, Chenxu Zhao, Anyang Su, Donglin Di, Tianyu Fu, Da An, Min He, Ya Gao, Meng Ma, Kun Yan, Ping Wang|<li><span style="color:#FF5733;">2024-07-10</span></li>|ACM Multimedia|https://github.com/suay1113/HMLLM|https://doi.org/10.48550/arXiv.2407.08150|
|395|DebUnc: Mitigating Hallucinations in Large Language Model Agent
  Communication with Uncertainty Estimations|Luke Yoffe, Alfonso Amayuelas, William Yang Wang|<li><span style="color:#FF5733;">2024-07-08</span></li>|arXiv|https://github.com/lukeyoffe/debunc|https://doi.org/10.48550/arXiv.2407.06426|
|396|iLLM-TSC: Integration reinforcement learning and large language model
  for traffic signal control policy improvement|Aoyu Pang, Maonan Wang, Man-On Pun, Chung Shue Chen, Xi Xiong|<li><span style="color:#FF5733;">2024-07-08</span></li>|arXiv|https://github.com/Traffic-Alpha/iLLM-TSC|https://doi.org/10.48550/arXiv.2407.06025|
|397|GlyphDraw2: Automatic Generation of Complex Glyph Posters with Diffusion
  Models and Large Language Models|Jian Ma, Yonglin Deng, Chen Chen, Nanyang Du, Haonan Lu, Zhenyu Yang|<li><span style="color:#FF5733;">2024-07-02</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/OPPO-Mente-Lab/GlyphDraw2.|https://doi.org/10.48550/arXiv.2407.02252|
|398|Extracting and Encoding: Leveraging Large Language Models and Medical
  Knowledge to Enhance Radiological Text Representation|Pablo Messina, René Víctor Valqui Vidal, Denis Parra, Álvaro Soto, Vladimir Araujo|<li><span style="color:#FF5733;">2024-07-02</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/PabloMessina/CXR-Fact-Encoder|https://doi.org/10.18653/v1/2024.findings-acl.236|
|399|Enhancing the Capability and Robustness of Large Language Models through
  Reinforcement Learning-Driven Query Refinement|Zisu Huang, Xiaohua Wang, Feiran Zhang, Zhibo Xu, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang|<li><span style="color:#FF5733;">2024-07-01</span></li>|arXiv|https://github.com/Huangzisu/query-refinement|https://doi.org/10.48550/arXiv.2407.01461|
|400|GraphArena: Benchmarking Large Language Models on Graph Computational
  Problems|Jianheng Tang, Qifan Zhang, Yuhan Li, Jia Li|<li><span style="color:#FF5733;">2024-06-29</span></li>|arXiv|https://github.com/squareRoot3/GraphArena.|https://doi.org/10.48550/arXiv.2407.00379|
|401|STBench: Assessing the Ability of Large Language Models in
  Spatio-Temporal Analysis|Wenbin Li, Di Yao, Ruibo Zhao, Wenjie Chen, Zijie Xu, Chengxue Luo, Chang Gong, Quanliang Jing, Haining Tan, Jingping Bi|<li><span style="color:#FF5733;">2024-06-27</span></li>|OpenAlex|https://github.com/LwbXc/STBench.|https://doi.org/10.48550/arXiv.2406.19065|
|402|DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning
  Graph|Zhehao Zhang, Jiaao Chen, Diyi Yang|<li><span style="color:#FF5733;">2024-06-25</span></li>|NeurIPS|https://github.com/SALT-NLP/DARG.|http://papers.nips.cc/paper_files/paper/2024/hash/f5198bc255e1d5f959edd6d1d1a86fab-Abstract-Conference.html|
|403|D2LLM: Decomposed and Distilled Large Language Models for Semantic
  Search|Zihan Liao, Hang Yu, Jianguo Li, Jun Wang, Wei Zhang|<li><span style="color:#FF5733;">2024-06-25</span></li>|OpenAlex|https://github.com/codefuse-ai/D2LLM.|https://doi.org/10.18653/v1/2024.acl-long.791|
|404|Improving Arithmetic Reasoning Ability of Large Language Models through
  Relation Tuples, Verification and Dynamic Feedback|Zhongtao Miao, Kaiyan Zhao, Yoshimasa Tsuruoka|<li><span style="color:#FF5733;">2024-06-25</span></li>|arXiv|https://github.com/gpgg/art.|https://doi.org/10.48550/arXiv.2406.17873|
|405|Predicting the Big Five Personality Traits in Chinese Counselling
  Dialogues Using Large Language Models|Yan Yang, Leping Ma, Anqi Li, Jingsong Ma, Zhenzhong Lan|<li><span style="color:#FF5733;">2024-06-25</span></li>|arXiv|https://github.com/kuri-leo/BigFive-LLM-Predictor|https://doi.org/10.48550/arXiv.2406.17287|
|406|DemoRank: Selecting Effective Demonstrations for Large Language Models
  in Ranking Task|Wenhan Liu, Yutao Zhu, Zhicheng Dou|<li><span style="color:#FF5733;">2024-06-24</span></li>|arXiv|https://github.com/8421BCD/DemoRank|https://doi.org/10.48550/arXiv.2406.16332|
|407|Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability
  of Large Language Models|Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral|<li><span style="color:#FF5733;">2024-06-24</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/Mihir3009/Multi-LogiEval.|https://doi.org/10.18653/v1/2024.emnlp-main.1160|
|408|RES-Q: Evaluating Code-Editing Large Language Model Systems at the
  Repository Scale|Beck LaBash, August Rosedale, Alex Reents, Lucas Negritto, Colin Wiel|<li><span style="color:#FF5733;">2024-06-24</span></li>|arXiv|https://github.com/Qurrent-AI/RES-Q.|https://doi.org/10.48550/arXiv.2406.16801|
|409|AutoDetect: Towards a Unified Framework for Automated Weakness Detection
  in Large Language Models|Jiale Cheng, Yida Lu, Xiaotao Gu, Pei Ke, Xiao Liu, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang|<li><span style="color:#FF5733;">2024-06-24</span></li>|OpenAlex|https://github.com/thu-coai/AutoDetect.|https://doi.org/10.18653/v1/2024.findings-emnlp.397|
|410|EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge
  Devices via Layerwise Unified Compression and Adaptive Layer Tuning and
  Voting|Zhongzhi Yu, Zheng Wang, Yuhan Li, Haoran You, Ruijie Gao, Xiaoya Zhou, Sreenidhi Reedy Bommu, Yang Zhao, Yingyan Celine...|<li><span style="color:#FF5733;">2024-06-22</span></li>|OpenAlex|https://github.com/GATECH-EIC/Edge-LLM|https://doi.org/10.48550/arXiv.2406.15758|
|411|The Music Maestro or The Musically Challenged, A Massive Music
  Evaluation Benchmark for Large Language Models|Jiajia Li, Lu Yang, Mingni Tang, Chenchong Chenchong, Zuchao Li, Ping Wang, Hai Zhao|<li><span style="color:#FF5733;">2024-06-22</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/zcli-charlie/ZIQI-Eval|https://doi.org/10.18653/v1/2024.findings-acl.194|
|412|GIEBench: Towards Holistic Evaluation of Group Identity-based Empathy
  for Large Language Models|Leyan Wang, Yonggang Jin, Tianhao Shen, Tianyu Zheng, Xinrun Du, Chenchen Zhang, Wenhao Huang, Jiaheng Liu, Shi Wang, Ge...|<li><span style="color:#FF5733;">2024-06-21</span></li>|arXiv|https://github.com/GIEBench/GIEBench.|https://doi.org/10.48550/arXiv.2406.14903|
|413|Leveraging Passage Embeddings for Efficient Listwise Reranking with
  Large Language Models|Qi Liu, Bo Wang, Nan Wang, Jiaxin Mao|<li><span style="color:#FF5733;">2024-06-20</span></li>|OpenAlex|https://github.com/liuqi6777/pe_rank|https://doi.org/10.48550/arXiv.2406.14848|
|414|PneumoLLM: Harnessing the power of large language model for pneumoconiosis diagnosis|Meiyue Song, Jiarui Wang, Zhihua Yu, Jiaxin Wang, Le Yang, Yuting Lu, Baicun Li, Xue Gang Wang, Xiaoxu Wang, Qinghua Hua...|<li><span style="color:#FF5733;">2024-06-20</span></li>|Medical Image Analysis|https://github.com/CodeMonsterPHD/PneumoLLM|https://doi.org/10.1016/j.media.2024.103248|
|415|ReaLHF: Optimized RLHF Training for Large Language Models through
  Parameter Reallocation|Zhiyu Mei, Wei Fu, Kaiwei Li, Guangju Wang, Huanchen Zhang, Yi Wu|<li><span style="color:#FF5733;">2024-06-20</span></li>|arXiv|https://github.com/openpsi-project/ReaLHF|https://doi.org/10.48550/arXiv.2406.14088|
|416|CityBench: Evaluating the Capabilities of Large Language Model as World
  Model|Jie Feng, Jun Zhang, Junbo Yan, Xin Zhang, Tianjian Ouyang, Tianhui Liu, Yuwei Du, Siqi Guo, Yongle Li|<li><span style="color:#FF5733;">2024-06-19</span></li>|arXiv|https://github.com/tsinghua-fib-lab/CityBench|https://doi.org/10.48550/arXiv.2406.13945|
|417|CityGPT: Empowering Urban Spatial Cognition of Large Language Models|Jie Feng, Yuwei Du, Tianhui Liu, Siqi Guo, Yuming Lin, Li Yong|<li><span style="color:#FF5733;">2024-06-19</span></li>|arXiv|https://github.com/tsinghua-fib-lab/CityGPT.|https://doi.org/10.48550/arXiv.2406.13948|
|418|Self-play with Execution Feedback: Improving Instruction-following
  Capabilities of Large Language Models|Guanting Dong, Keming Lu, Cheng‐Peng Li, Tingyu Xia, Bowen Yu, Chang Zhou, Jingren Zhou|<li><span style="color:#FF5733;">2024-06-19</span></li>|arXiv|https://github.com/QwenLM/AutoIF.|https://openreview.net/forum?id=cRR0oDFEBC|
|419|Stealth edits for provably fixing or attacking large language models|Oliver J. Sutton, Qinghua Zhou, Wei Wang, Desmond J. Higham, Alexander N. Gorban, Alexander Bastounis, Ivan Yu. Tyukin|<li><span style="color:#FF5733;">2024-06-18</span></li>|arXiv|https://github.com/qinghua-zhou/stealth-edits.|https://doi.org/10.48550/arXiv.2406.12670|
|420|MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular
  Property Prediction|Yuyan Liu, Sirui Ding, Zhou Sheng, Wenqi Fan, Qiaoyu Tan|<li><span style="color:#FF5733;">2024-06-18</span></li>|arXiv|https://github.com/NYUSHCS/MolecularGPT.|https://doi.org/10.48550/arXiv.2406.12950|
|421|GeoGPT4V: Towards Geometric Multi-modal Large Language Models with
  Geometric Image Generation|Shihao Cai, Keqin Bao, Hangyu Guo, Jizhi Zhang, Jun Myung Song, Bo Zheng|<li><span style="color:#FF5733;">2024-06-17</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/Lanyu0303/GeoGPT4V_Project|https://doi.org/10.18653/v1/2024.emnlp-main.44|
|422|Interactive Evolution: A Neural-Symbolic Self-Training Framework For
  Large Language Models|Fangzhi Xu, Qiushi Sun, Kanzhi Cheng, Jun Liu, Yu Qiao, Zhiyong Wu|<li><span style="color:#FF5733;">2024-06-17</span></li>|arXiv|https://github.com/xufangzhi/ENVISIONS|https://aclanthology.org/2025.acl-long.635/|
|423|Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of
  Multimodal Large Language Models|Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, Hao Wang|<li><span style="color:#FF5733;">2024-06-17</span></li>|OpenAlex|https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack.|https://doi.org/10.18653/v1/2025.naacl-long.166|
|424|Towards an End-to-End Framework for Invasive Brain Signal Decoding with
  Large Language Models|Sheng Feng, Heyang Liu, Yu Wang, Yanfeng Wang|<li><span style="color:#FF5733;">2024-06-17</span></li>|Interspeech 2022|https://github.com/FsFrancis15/BrainLLM.|https://doi.org/10.21437/Interspeech.2024-382|
|425|GAugLLM: Improving Graph Contrastive Learning for Text-Attributed Graphs
  with Large Language Models|Yifang Ma, Dongzhe Fan, Daochen Zha, Qiaoyu Tan|<li><span style="color:#FF5733;">2024-06-17</span></li>|Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/NYUSHCS/GAugLLM.|https://doi.org/10.48550/arXiv.2406.11945|
|426|A Comprehensive Survey of Scientific Large Language Models and Their
  Applications in Scientific Discovery|Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Sheng Wang, Jiawei Han|<li><span style="color:#FF5733;">2024-06-16</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.|https://doi.org/10.18653/v1/2024.emnlp-main.498|
|427|Investigating Annotator Bias in Large Language Models for Hate Speech
  Detection|Amit Das, Zheng Zhang, Fatemeh Jamshidi, Vinija Jain, Aman Chadha, Nilanjana Raychawdhary, Mary J. Sandage, Lauramarie P...|<li><span style="color:#FF5733;">2024-06-16</span></li>|arXiv|https://github.com/AmitDasRup123/HateSpeechCorpus|https://doi.org/10.48550/arXiv.2406.11109|
|428|DIEKAE: Difference Injection for Efficient Knowledge Augmentation and
  Editing of Large Language Models|Alessio Galatolo, Meriem Beloucif, Katie Winkle|<li><span style="color:#FF5733;">2024-06-15</span></li>|arXiv|https://github.com/alessioGalatolo/DIEKAE.|https://doi.org/10.48550/arXiv.2406.10660|
|429|Large Language Models as Surrogate Models in Evolutionary Algorithms: A
  Preliminary Study|Hao Hao, Xiaoqun Zhang, Aimin Zhou|<li><span style="color:#FF5733;">2024-06-15</span></li>|Swarm and Evolutionary Computation|https://github.com/hhyqhh/LAEA.git|https://doi.org/10.1016/j.swevo.2024.101741|
|430|First Multi-Dimensional Evaluation of Flowchart Comprehension for
  Multimodal Large Language Models|Enming Zhang, Ruobing Yao, Huanyong Liu, Junhui Yu, Jiale Wang|<li><span style="color:#FF5733;">2024-06-14</span></li>|arXiv|https://github.com/360AILAB-NLP/FlowCE|https://doi.org/10.48550/arXiv.2406.10057|
|431|Living in the Moment: Can Large Language Models Grasp Co-Temporal
  Reasoning?|Zhaochen Su, Jun‐Tao Li, Jun Zhang, Tong Zhu, X. B. Qu, Zhou Pan, Bowen Yan, Yu Cheng, Min Zhang|<li><span style="color:#FF5733;">2024-06-13</span></li>|OpenAlex|https://github.com/zhaochen0110/Cotempqa.|https://doi.org/10.18653/v1/2024.acl-long.703|
|432|CS-Bench: A Comprehensive Benchmark for Large Language Models towards
  Computer Science Mastery|Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin...|<li><span style="color:#FF5733;">2024-06-12</span></li>|arXiv|https://github.com/csbench/csbench.|https://openreview.net/forum?id=fjEZ2LPceZ|
|433|Enhancing Psychotherapy Counseling: A Data Augmentation Pipeline
  Leveraging Large Language Models for Counseling Conversations|Jun-Woo Kim, Ji-Eun Han, Jun-Seok Koh, Hyeon-Tae Seo, Du-Seong Chang|<li><span style="color:#FF5733;">2024-06-12</span></li>|arXiv|https://github.com/jwkim-chat/A-Data-Augmentation-Pipeline-Leveraging-Large-Language-Models-for-Counseling-Conversations.|https://doi.org/10.48550/arXiv.2406.08718|
|434|TasTe: Teaching Large Language Models to Translate through
  Self-Reflection|Yutong Wang, Jiali Zeng, Xuebo Liu, Fandong Meng, Jie Zhou, Min Zhang|<li><span style="color:#FF5733;">2024-06-12</span></li>|OpenAlex|https://github.com/YutongWang1216/ReflectionLLMMT.|https://doi.org/10.18653/v1/2024.acl-long.333|
|435|VulDetectBench: Evaluating the Deep Capability of Vulnerability
  Detection with Large Language Models|Yu Liu, Lang Gao, Mingxin Yang, Yu Xie, Ping Chen, Xiaojin Zhang, Wei Chen|<li><span style="color:#FF5733;">2024-06-11</span></li>|arXiv|https://github.com/Sweetaroo/VulDetectBench.|https://doi.org/10.48550/arXiv.2406.07595|
|436|When Linear Attention Meets Autoregressive Decoding: Towards More
  Effective and Efficient Linearized Large Language Models|Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Yingyan Celine Lin|<li><span style="color:#FF5733;">2024-06-11</span></li>|ICML|https://github.com/GATECH-EIC/Linearized-LLM.|https://openreview.net/forum?id=7mFSaP6IiN|
|437|MoreauPruner: Robust Pruning of Large Language Models against Weight
  Perturbations|Zixiao Wang, Jingwei Zhang, Wenqian Zhao, Farzan Farnia, Bei Yu|<li><span style="color:#FF5733;">2024-06-11</span></li>|arXiv|https://github.com/ShiningSord/MoreauPruner|https://doi.org/10.48550/arXiv.2406.07017|
|438|Mitigating Boundary Ambiguity and Inherent Bias for Text Classification
  in the Era of Large Language Models|Zhenyi Lu, Jie Tian, Wei Wei, Xiaoye Qu, Yu Cheng, Wenfeng Xie, Dangyang Chen|<li><span style="color:#FF5733;">2024-06-11</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/Chuge0335/PC-CoT|https://doi.org/10.18653/v1/2024.findings-acl.467|
|439|Entropy-Reinforced Planning with Large Language Models for Drug
  Discovery|Xuefeng Liu, Chih-chan Tien, Peng Ding, Songhao Jiang, Rick L. Stevens|<li><span style="color:#FF5733;">2024-06-11</span></li>|ICML|https://github.com/xuefeng-cs/ERP.|https://openreview.net/forum?id=F3Ds71Xgo1|
|440|AutoSurvey: Large Language Models Can Automatically Write Surveys|Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, X H Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, We...|<li><span style="color:#FF5733;">2024-06-10</span></li>|NeurIPS|https://github.com/AutoSurveys/AutoSurvey|http://papers.nips.cc/paper_files/paper/2024/hash/d07a9fc7da2e2ec0574c38d5f504d105-Abstract-Conference.html|
|441|Can AI Beat Undergraduates in Entry-level Java Assignments? Benchmarking
  Large Language Models on JavaBench|Jialun Cao, Zhiyong Chen, Jia‐Rong Wu, Shing-Chi Cheung, Chang Xu|<li><span style="color:#FF5733;">2024-06-10</span></li>|arXiv|https://github.com/java-bench/JavaBench.|https://doi.org/10.48550/arXiv.2406.12902|
|442|SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context
  Large Language Models|Hengyu Zhang|<li><span style="color:#FF5733;">2024-06-09</span></li>|arXiv|https://github.com/Dexter-GT-86/SinkLoRA|https://doi.org/10.48550/arXiv.2406.05678|
|443|Large Language Model Assisted Adversarial Robustness Neural Architecture
  Search|Rui Zhong, Yang Cao, Jun Yu, Masaharu Munetomo|<li><span style="color:#FF5733;">2024-06-08</span></li>|OpenAlex|https://github.com/RuiZhong961230/LLMO|https://doi.org/10.1109/docs63458.2024.10704419|
|444|CRiskEval: A Chinese Multi-Level Risk Evaluation Benchmark Dataset for
  Large Language Models|Ling Shi, Deyi Xiong|<li><span style="color:#FF5733;">2024-06-07</span></li>|arXiv|https://github.com/lingshi6565/Risk_eval.|https://aclanthology.org/2025.acl-long.670/|
|445|An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal
  Large Language Models|Xiongtao Zhou, Jie He, Yuhua Ke, Guangyao Zhu, Víctor Gutiérrez-Basulto, Jeff Z. Pan|<li><span style="color:#FF5733;">2024-06-07</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/alenai97/PEFT-MLLM.git.|https://doi.org/10.18653/v1/2024.findings-acl.598|
|446|LawGPT: A Chinese Legal Knowledge-Enhanced Large Language Model|Zhi Qiang Zhou, Jiang-Xin Shi, Peng-Xiao Song, Xiao-Wen Yang, Yi-Xuan Jin, Lan-Zhe Guo, Yufeng Li|<li><span style="color:#FF5733;">2024-06-06</span></li>|arXiv|https://github.com/pengxiao-song/LaWGPT|https://doi.org/10.48550/arXiv.2406.04614|
|447|ValueBench: Towards Comprehensively Evaluating Value Orientations and
  Understanding of Large Language Models|Yuanyi Ren, Haoran Ye, Hanjun Fang, Xin Zhang, Guojie Song|<li><span style="color:#FF5733;">2024-06-06</span></li>|OpenAlex|https://github.com/Value4AI/ValueBench.|https://doi.org/10.18653/v1/2024.acl-long.111|
|448|Exploring User Retrieval Integration towards Large Language Models for
  Cross-Domain Sequential Recommendation|Tingjia Shen, Hao Wang, Jiaqing Zhang, Sirui Zhao, Liangyue Li, Zulong Chen, Defu Lian, Enhong Chen|<li><span style="color:#FF5733;">2024-06-05</span></li>|arXiv|https://github.com/TingJShen/URLLM|https://doi.org/10.48550/arXiv.2406.03085|
|449|Text-like Encoding of Collaborative Information in Large Language Models
  for Recommendation|Yang Zhang, Keqin Bao, Ming Yang, Wenjie Wang, Fuli Feng, Xiangnan He|<li><span style="color:#FF5733;">2024-06-05</span></li>|OpenAlex|https://github.com/zyang1580/BinLLM.|https://doi.org/10.18653/v1/2024.acl-long.497|
|450|Interactive Text-to-Image Retrieval with Large Language Models: A
  Plug-and-Play Approach|Saehyung Lee, Sangwon Yu, Junsung Park, Jihun Yi, Sungroh Yoon|<li><span style="color:#FF5733;">2024-06-05</span></li>|OpenAlex|https://github.com/Saehyung-Lee/PlugIR.|https://doi.org/10.18653/v1/2024.acl-long.46|
|451|Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown
  in State-Of-the-Art Large Language Models|Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, Jenia Jitsev|<li><span style="color:#FF5733;">2024-06-04</span></li>|arXiv|https://github.com/LAION-AI/AIW|https://doi.org/10.48550/arXiv.2406.02061|
|452|Disentangling Logic: The Role of Context in Large Language Model
  Reasoning Capabilities|Wenyue Hua, Kaijie Zhu, Lingyao Li, Lizhou Fan, Shuhang Lin, Mingyu Jin, Haochen Xue, Zelong Li, Jindong Wang, Yongfeng ...|<li><span style="color:#FF5733;">2024-06-04</span></li>|arXiv|https://github.com/agiresearch/ContextHub.|https://doi.org/10.48550/arXiv.2406.02787|
|453|CR-UTP: Certified Robustness against Universal Text Perturbations on
  Large Language Models|Qian Lou, Xin Liang, Jiaqi Xue, Yancheng Zhang, Rui Xie, Mengxin Zheng|<li><span style="color:#FF5733;">2024-06-03</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/UCFML-Research/CR-UTP|https://doi.org/10.18653/v1/2024.findings-acl.588|
|454|Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of
  Knowledge Editing in Large Language Models|Cheng-Hsun Hsueh, Paul Kuo-Ming Huang, Tzu-Han Lin, Che-Wei Liao, Hung-Chieh Fang, Chao-Wei Huang, Yun-Nung Chen|<li><span style="color:#FF5733;">2024-06-03</span></li>|OpenAlex|https://github.com/MiuLab/EditLLM-Survey|https://doi.org/10.18653/v1/2024.findings-emnlp.550|
|455|Envisioning Outlier Exposure by Large Language Models for
  Out-of-Distribution Detection|Chentao Cao, Zhun Zhong, Zhanke Zhou, Yang Liu, Tongliang Liu, Bo Han|<li><span style="color:#FF5733;">2024-06-02</span></li>|ICML|https://github.com/tmlr-group/EOE.|https://openreview.net/forum?id=xZO7SmM12y|
|456|Evaluating Mathematical Reasoning of Large Language Models: A Focus on
  Error Identification and Correction|Xiaoyuan Li, Wenjie Wang, Moxin Li, Junrong Guo, Shuicheng Yan, Fuli Feng|<li><span style="color:#FF5733;">2024-06-02</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/LittleCirc1e/EIC.|https://doi.org/10.18653/v1/2024.findings-acl.673|
|457|KRAGEN: a knowledge graph-enhanced RAG framework for biomedical problem solving using large language models|Nicholas Matsumoto, Jay Moran, Hyun‐Jun Choi, Miguel Hernandez, Mythreye Venkatesan, Paul P. Wang, Jason H. Moore|<li><span style="color:#FF5733;">2024-06-01</span></li>|Bioinformatics|https://github.com/EpistasisLab/KRAGEN.|https://doi.org/10.1093/bioinformatics/btae353|
|458|Large Language Models as Planning Domain Generators (Student Abstract)|James T. Oswald, Kavitha Srinivas, Harsha Kokel, Junkyu Lee, Michael Katz, Shirin Sohrabi|<li><span style="color:#FF5733;">2024-05-30</span></li>|Proceedings of the International Conference on Automated Planning and Scheduling|https://github.com/IBM/NL2PDDL.|https://doi.org/10.1609/icaps.v34i1.31502|
|459|Compressing Large Language Models using Low Rank and Low Precision
  Decomposition|Rajarshi Saha, Naomi Sagan, Varun Srivastava, Andrea J. Goldsmith, Mert Pilanci|<li><span style="color:#FF5733;">2024-05-29</span></li>|NeurIPS|https://github.com/pilancilab/caldera|http://papers.nips.cc/paper_files/paper/2024/hash/a20e8451ffb07ad25282c21945ad4f19-Abstract-Conference.html|
|460|Detecting Hallucinations in Large Language Model Generation: A Token
  Probability Approach|Ernesto Quevedo, Jorge Yero, Rachel Koerner, Pablo Rivas, Tomáš Černý|<li><span style="color:#FF5733;">2024-05-29</span></li>|Communications in computer and information science|https://github.com/Baylor-AI/HalluDetect.|https://openreview.net/pdf/afae86eb07b4a5ea8e6ec327ee2d1531d57578c5.pdf|
|461|Pipette: Automatic Fine-grained Large Language Model Training
  Configurator for Real-World Clusters|Jinkyu Yim, Jaeyong Song, Yerim Choi, Jaebeen Lee, Jaewon Jung, Hongsun Jang, Jinho Lee|<li><span style="color:#FF5733;">2024-05-28</span></li>|OpenAlex|https://github.com/yimjinkyu1/date2024_pipette.|https://doi.org/10.23919/DATE58400.2024.10546826|
|462|Lazy Safety Alignment for Large Language Models against Harmful
  Fine-tuning|Tiansheng Huang, Sihao Hu, Fatih İlhan, Selim Furkan Tekin, Ling Liu|<li><span style="color:#FF5733;">2024-05-28</span></li>|arXiv|https://github.com/git-disl/Lisa|https://doi.org/10.48550/arXiv.2405.18641|
|463|LLM experiments with simulation: Large Language Model Multi-Agent System
  for Process Simulation Parametrization in Digital Twins|Yuchen Xia, Daniel Dittler, Nasser Jazdi, Haonan Chen, Michael Weyrich|<li><span style="color:#FF5733;">2024-05-28</span></li>|2022 IEEE 27th International Conference on Emerging Technologies and Factory Automation (ETFA)|https://github.com/YuchenXia/LLMDrivenSimulation|https://doi.org/10.1109/ETFA61755.2024.10710900|
|464|Defending Large Language Models Against Jailbreak Attacks via
  Layer-specific Editing|Wei Zhao, Zhe Li, Yige Li, Ye Zhang, Jun Sun|<li><span style="color:#FF5733;">2024-05-28</span></li>|OpenAlex|https://github.com/ledllm/ledllm|https://doi.org/10.18653/v1/2024.findings-emnlp.293|
|465|C$^3$Bench: A Comprehensive Classical Chinese Understanding Benchmark
  for Large Language Models|Jiahuan Cao, Yongxin Shi, Dezhi Peng, Yang Liu, Lianwen Jin|<li><span style="color:#FF5733;">2024-05-27</span></li>|arXiv (Cornell University)|https://github.com/SCUT-DLVCLab/C3bench|http://arxiv.org/abs/2405.17732|
|466|Empowering Large Language Models to Set up a Knowledge Retrieval Indexer
  via Self-Learning|Xiang Liang, Simin Niu, Zhiyu Li, Sensen Zhang, Shichao Song, Hanyu Wang, Jiawei Yang, Feiyu Xiong, Bo Tang, Chenyang Xi|<li><span style="color:#FF5733;">2024-05-27</span></li>|arXiv|https://github.com/IAAR-Shanghai/PGRAG.|https://doi.org/10.48550/arXiv.2405.16933|
|467|Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal
  Large Language Models|Xijie Huang, Xinyuan Wang, Hantao Zhang, Jiawen Xi, Jingkun An, Hao Wang, Chengwei Pan|<li><span style="color:#FF5733;">2024-05-26</span></li>|arXiv|https://github.com/dirtycomputer/O2M_attack.|https://doi.org/10.48550/arXiv.2405.20775|
|468|Evaluating the Adversarial Robustness of Retrieval-Based In-Context
  Learning for Large Language Models|Simon Chi Lok U, Jie He, Pasquale Minervini, Jeff Z. Pan|<li><span style="color:#FF5733;">2024-05-24</span></li>|arXiv|https://github.com/simonucl/adv-retreival-icl|https://doi.org/10.48550/arXiv.2405.15984|
|469|Computational toolkit for predicting thickness of 2D materials using
  machine learning and autogenerated dataset by large language model|Chinedu E. Ekuma|<li><span style="color:#FF5733;">2024-05-23</span></li>|AIP Advances|https://github.com/gmp007/THICK2D|https://doi.org/10.1063/5.0241511|
|470|FinRobot: An Open-Source AI Agent Platform for Financial Applications
  using Large Language Models|Hongyang Yang, Boyu Zhang, Neng Wang, Cheng Guo, Xiaoli Zhang, Likun Lin, Junlin Wang, Tianyu Zhou, Mao Guan, Runjia Zha...|<li><span style="color:#FF5733;">2024-05-23</span></li>|SSRN Electronic Journal|https://github.com/AI4Finance-Foundation/FinRobot|https://doi.org/10.48550/arXiv.2405.14767|
|471|RefChecker: Reference-based Fine-grained Hallucination Checker and
  Benchmark for Large Language Models|Xiangkun Hu, Dongyu Ru, Lin Qiu, Qipeng Guo, Tianhang Zhang, Yang Xu, Yun Luo, Pengfei Liu, Yue Zhang, Zheng Zhang|<li><span style="color:#FF5733;">2024-05-23</span></li>|arXiv|https://github.com/amazon-science/RefChecker|https://doi.org/10.48550/arXiv.2405.14486|
|472|WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of
  Large Language Models|Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen|<li><span style="color:#FF5733;">2024-05-23</span></li>|NeurIPS|https://github.com/zjunlp/EasyEdit.|http://papers.nips.cc/paper_files/paper/2024/hash/60960ad78868fce5c165295fbd895060-Abstract-Conference.html|
|473|AutoCoder: Enhancing Code Large Language Model with
  \textscAIEV-Instruct|Bin Lei, Yuchen Li, Qiuwu Chen|<li><span style="color:#FF5733;">2024-05-22</span></li>|arXiv|https://github.com/bin123apple/AutoCoder|https://doi.org/10.48550/arXiv.2405.14906|
|474|BioInformatics Agent (BIA): Unleashing the Power of Large Language Models to Reshape Bioinformatics Workflow|Xin Qi, Quyu Kong, Hongyi Ji, Yue Shen, Yuqi Liu, Yan Sun, Zhilin Zhang, Zhaorong Li, Xunlong Xia, Bing Deng, Yinqi Bai|<li><span style="color:#FF5733;">2024-05-22</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/biagent-dev/biagent|https://doi.org/10.1101/2024.05.22.595240|
|475|LOGIN: A Large Language Model Consulted Graph Neural Network Training
  Framework|Yiran Qiao, Xiang Ao, Yang Liu, Jiarong Xu, Xiaoqian Sun, Qing He|<li><span style="color:#FF5733;">2024-05-22</span></li>|OpenAlex|https://github.com/QiaoYRan/LOGIN.|https://doi.org/10.48550/arXiv.2405.13902|
|476|Adapting Multi-modal Large Language Model to Concept Drift in the
  Long-tailed Open World|Xiaoyu Yang, Jie Lü, En Yu|<li><span style="color:#FF5733;">2024-05-22</span></li>|arXiv|https://github.com/Anonymous0Knight/ConceptDriftMLLMs.|https://doi.org/10.48550/arXiv.2405.13459|
|477|CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information
  Needs in Large Language Models|Tong Zhang, Peixin Qin, Yang Deng, Chen Huang, Wenqiang Lei, Junhong Liu, Dingnan Jin, Hongru Liang, Tat‐Seng Chua|<li><span style="color:#FF5733;">2024-05-20</span></li>|OpenAlex|https://github.com/zt991211/CLAMBER|https://doi.org/10.18653/v1/2024.acl-long.578|
|478|DOP: Diagnostic-Oriented Prompting for Large Language Models in
  Mathematical Correction|Hao Chen, Biaojie Zeng, Xin Lin, Liang He, Aimin Zhou|<li><span style="color:#FF5733;">2024-05-20</span></li>|arXiv|https://github.com/ChenhaoEcnuCS/Reason-Correct.|https://doi.org/10.48550/arXiv.2405.12100|
|479|xFinder: Robust and Pinpoint Answer Extraction for Large Language Models|Qingchen Yu, Zifan Zheng, Shichao Song, Zhiyu Li, Feiyu Xiong, Bo Tang, Chen Ding|<li><span style="color:#FF5733;">2024-05-20</span></li>|arXiv|https://github.com/IAAR-Shanghai/xFinder|https://doi.org/10.48550/arXiv.2405.11874|
|480|Efficient Multimodal Large Language Models: A Survey|Yizhang Jin, Jian Li, Y. Liu, Tianjun Gu, Kai Wu, Zhengkai Jiang, Muyang He, Bo Zhao, Xin Tan, Zhenye Gan, Yabiao Wang, ...|<li><span style="color:#FF5733;">2024-05-17</span></li>|arXiv|https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey.|https://doi.org/10.48550/arXiv.2405.10739|
|481|DuetSim: Building User Simulator with Dual Large Language Models for
  Task-Oriented Dialogues|Xianglong Luo, Zhiwen Tang, Jin Wang, Xuejie Zhang|<li><span style="color:#FF5733;">2024-05-16</span></li>|LREC/COLING|https://github.com/suntea233/DuetSim.|https://aclanthology.org/2024.lrec-main.481|
|482|When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks
  via Multi-modal Large Language Models|Xianzheng Ma, Yash Bhalgat, Brandon Smart, Shuai Chen, Xinghui Li, Jian Ding, Jindong Gu, Dave Zhenyu Chen, Songyou Peng...|<li><span style="color:#FF5733;">2024-05-16</span></li>|arXiv|https://github.com/ActiveVisionLab/Awesome-LLM-3D.|https://doi.org/10.48550/arXiv.2405.10255|
|483|Distilling Implicit Multimodal Knowledge into Large Language Models for
  Zero-Resource Dialogue Generation|Bo Zhang, Hui Ma, Jian Ding, Jian Wang, Bo Xu, Hongfei Lin|<li><span style="color:#FF5733;">2024-05-16</span></li>|Information Fusion|https://github.com/zhangbo-nlp/VIKDF.|https://doi.org/10.1016/j.inffus.2025.102985|
|484|A Systematic Investigation of Distilling Large Language Models into
  Cross-Encoders for Passage Re-ranking|Ferdinand Schlatt, Maik Fröbe, Harrisen Scells, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Benno Stein, Martin Pottha...|<li><span style="color:#FF5733;">2024-05-13</span></li>|arXiv|https://github.com/webis-de/msmarco-llm-distillation|https://doi.org/10.48550/arXiv.2405.07920|
|485|EMS-SD: Efficient Multi-sample Speculative Decoding for Accelerating
  Large Language Models|Yunsheng Ni, Chuanjian Liu, Yehui Tang, Kai Han, Yunhe Wang|<li><span style="color:#FF5733;">2024-05-13</span></li>|OpenAlex|https://github.com/niyunsheng/EMS-SD.|https://doi.org/10.18653/v1/2025.naacl-long.471|
|486|Boosting Multimodal Large Language Models with Visual Tokens Withdrawal
  for Rapid Inference|Zhihang Lin, Mingbao Lin, Luxi Lin, Rongrong Ji|<li><span style="color:#FF5733;">2024-05-09</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/lzhxmu/VTW.|https://doi.org/10.48550/arXiv.2405.05803|
|487|LLM-QBench: A Benchmark Towards the Best Practice for Post-training
  Quantization of Large Language Models|Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Huang, Yuncheng Zhang, Xianglong Liu, Dacheng Tao|<li><span style="color:#FF5733;">2024-05-09</span></li>|arXiv|https://github.com/ModelTC/llmc.|https://doi.org/10.48550/arXiv.2405.06001|
|488|Word2World: Generating Stories and Worlds through Large Language Models|Muhammad Umair Nasir, Steven James, Julian Togelius|<li><span style="color:#FF5733;">2024-05-06</span></li>|arXiv|https://github.com/umair-nasir14/Word2World.|https://doi.org/10.48550/arXiv.2405.06686|
|489|NegativePrompt: Leveraging Psychology for Large Language Models
  Enhancement via Negative Emotional Stimuli|Xu Wang, Cheng Li, Yi Ying Chang, Jindong Wang, Yuan Chieh Wu|<li><span style="color:#FF5733;">2024-05-05</span></li>|OpenAlex|https://github.com/wangxu0820/NegativePrompt.|https://www.ijcai.org/proceedings/2024/719|
|490|Enhancing Contextual Understanding in Large Language Models through
  Contrastive Decoding|Zheng Zhao, Emilio Monti, Jens Lehmann, Haytham Assem|<li><span style="color:#FF5733;">2024-05-04</span></li>|NAACL-HLT|https://github.com/amazon-science/ContextualUnderstanding-ContrastiveDecoding.|https://doi.org/10.18653/v1/2024.naacl-long.237|
|491|EDA Corpus: A Large Language Model Dataset for Enhanced Interaction with
  OpenROAD|Bing-Yue Wu, Utsav Sharma, Sai Rahul Dhanvi Kankipati, Ajay Singh Yadav, Bintu Kappil George, Sai Ritish Guntupalli, Aus...|<li><span style="color:#FF5733;">2024-05-04</span></li>|OpenAlex|https://github.com/OpenROAD-Assistant/EDA-Corpus.|https://doi.org/10.1109/lad62341.2024.10691774|
|492|Beyond Helpfulness and Harmlessness: Eliciting Diverse Behaviors from
  Large Language Models with Persona In-Context Learning|Hyeong Kyu Choi, Yixuan Li|<li><span style="color:#FF5733;">2024-05-03</span></li>|arXiv (Cornell University)|https://github.com/deeplearning-wisc/picle.|https://arxiv.org/abs/2405.02501|
|493|A Survey of Time Series Foundation Models: Generalizing Time Series
  Representation with Large Language Model|Jiexia Ye, Weiqi Zhang, Yi Ke, Yongzi Yu, Ziyue Li, Jia Li, Fugee Tsung|<li><span style="color:#FF5733;">2024-05-02</span></li>|arXiv|https://github.com/start2020/Awesome-TimeSeries-LLM-FM|https://doi.org/10.48550/arXiv.2405.02358|
|494|A Survey on Large Language Models for Critical Societal Domains:
  Finance, Healthcare, and Law|Zhiyu Zoey Chen, Jing Ma, Xinlu Zhang, Nan Hao, An Yan, Armineh Nourbakhsh, Xianjun Yang, McAuley Julian, Linda Petzold,...|<li><span style="color:#FF5733;">2024-05-02</span></li>|Trans. Mach. Learn. Res.|https://github.com/czyssrs/LLM_X_papers|https://openreview.net/forum?id=upAWnMgpnH|
|495|Characterising the Creative Process in Humans and Large Language Models|Surabhi S. Nath, Peter Dayan, Claire E. Stevenson|<li><span style="color:#FF5733;">2024-05-01</span></li>|ICCC|https://github.com/surabhisnath/Creative_Process|https://computationalcreativity.net/iccc24/papers/ICCC24_paper_179.pdf|
|496|Lp-slam: language-perceptive RGB-D SLAM framework exploiting large language model|Weiyi Zhang, Yushi Guo, Liting Niu, Peijun Li, Zeyu Wan, Fei Shao, Cheng Nian, Fasih Ud Din Farrukh, Debing Zhang, Chun ...|<li><span style="color:#FF5733;">2024-04-30</span></li>|Complex & Intelligent Systems|https://github.com/GroupOfLPSLAM/LP_SLAM|https://doi.org/10.1007/s40747-024-01408-0|
|497|SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with
  Text-Rich Visual Comprehension|Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, Ying Shan|<li><span style="color:#FF5733;">2024-04-25</span></li>|arXiv|https://github.com/AILab-CVC/SEED-Bench.|https://doi.org/10.48550/arXiv.2404.16790|
|498|Attacks on Third-Party APIs of Large Language Models|Wanru Zhao, Vidit Khazanchi, Haodi Xing, Xuanli He, Qiongkai Xu, Nicholas Donald Lane|<li><span style="color:#FF5733;">2024-04-24</span></li>|arXiv|https://github.com/vk0812/Third-Party-Attacks-on-LLMs.|https://doi.org/10.48550/arXiv.2404.16891|
|499|Chat2Scenario: Scenario Extraction From Dataset Through Utilization of
  Large Language Model|Yongqi Zhao, Wenbo Xiao, Tomislav Mihalj, Jia Hu, Arno Eichberger|<li><span style="color:#FF5733;">2024-04-24</span></li>|2022 IEEE Intelligent Vehicles Symposium (IV)|https://github.com/ftgTUGraz/Chat2Scenario.|https://doi.org/10.1109/IV55156.2024.10588843|
|500|Studying Large Language Model Behaviors Under Realistic Knowledge
  Conflicts|Evgenii Kortukov, Alexander Rubinstein, Elisa Nguyen, Seung‐June Oh|<li><span style="color:#FF5733;">2024-04-24</span></li>|arXiv|https://github.com/kortukov/realistic_knowledge_conflicts|https://doi.org/10.48550/arXiv.2404.16032|
|501|Evaluating Character Understanding of Large Language Models via
  Character Profiling from Fictional Works|Xinfeng Yuan, Siyu Yuan, Yuhan Cui, Tianhe Lin, Xintao Wang, Rui Xu, Jiangjie Chen, Deqing Yang|<li><span style="color:#FF5733;">2024-04-19</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/Joanna0123/character_profiling.|https://doi.org/10.18653/v1/2024.emnlp-main.456|
|502|Advancing the Robustness of Large Language Models through Self-Denoised
  Smoothing|Jiabao Ji, Bairu Hou, Zhen Zhang, Guanhua Zhang, Wenqi Fan, Qing Li, Yang Zhang, Gaowen Liu, Sijia Liu, Shiyu Chang|<li><span style="color:#FF5733;">2024-04-18</span></li>|OpenAlex|https://github.com/UCSB-NLP-Chang/SelfDenoise|https://doi.org/10.18653/v1/2024.naacl-short.23|
|503|Large Language Models in Targeted Sentiment Analysis for Russian|Nicolay Rusnachenko, Alexander Golubev, Natalia Loukachevitch|<li><span style="color:#FF5733;">2024-04-18</span></li>|Lobachevskii Journal of Mathematics|https://github.com/nicolay-r/Reasoning-for-Sentiment-Analysis-Framework|https://doi.org/10.48550/arXiv.2404.12342|
|504|Simultaneous Interpretation Corpus Construction by Large Language Models
  in Distant Language Pair|Yusuke Sakai, Mana Makinae, Hidetaka Kamigaito, Taro Watanabe|<li><span style="color:#FF5733;">2024-04-18</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/yusuke1997/LLM-SI-Corpus|https://doi.org/10.18653/v1/2024.emnlp-main.1248|
|505|Large Language Models meet Collaborative Filtering: An Efficient
  All-round LLM-based Recommender System|Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, Min-Chul Yang, Chanyoung Park|<li><span style="color:#FF5733;">2024-04-17</span></li>|Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/ghdtjr/A-LLMRec.|https://doi.org/10.48550/arXiv.2404.11343|
|506|Balancing Speciality and Versatility: a Coarse to Fine Framework for
  Supervised Fine-tuning Large Language Model|Hengyuan Zhang, Yanru Wu, Dawei Li, Zacc Yang, Rui Zhao, Yong Jiang, Fei Tan|<li><span style="color:#FF5733;">2024-04-16</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/rattlesnakey/CoFiTune.|https://openreview.net/pdf/a9aa26a4c2daa719e35412a023f3f70e83a742eb.pdf|
|507|MMCode: Evaluating Multi-Modal Code Large Language Models with Visually
  Rich Programming Problems|Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, Jing Ma|<li><span style="color:#FF5733;">2024-04-15</span></li>|arXiv|https://github.com/happylkx/MMCode.|https://doi.org/10.48550/arXiv.2404.09486|
|508|Memory Sharing for Large Language Model based Agents|Hang Gao, Yongfeng Zhang|<li><span style="color:#FF5733;">2024-04-15</span></li>|arXiv|https://github.com/GHupppp/MemorySharingLLM|https://doi.org/10.48550/arXiv.2404.09982|
|509|Demonstration of DB-GPT: Next Generation Data Interaction System
  Empowered by Large Language Models|Siqiao Xue, Danrui Qi, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting Chen, Hongjun Yang, Zhiping Zhang, Jianshan He, H...|<li><span style="color:#FF5733;">2024-04-15</span></li>|Proceedings of the VLDB Endowment|https://github.com/eosphoros-ai/DB-GPT|https://www.vldb.org/pvldb/vol17/p4365-chen.pdf|
|510|When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in
  Large Language Models|Yanhong Li, Chenghao Yang, Allyson Ettinger|<li><span style="color:#FF5733;">2024-04-13</span></li>|Findings of the Association for Computational Linguistics: NAACL 2022|https://github.com/yanhong-lbh/LLM-SelfReflection-Eval.|https://doi.org/10.18653/v1/2024.findings-naacl.237|
|511|LaVy: Vietnamese Multimodal Large Language Model|Chi T. Tran, Huong Le Thanh|<li><span style="color:#FF5733;">2024-04-11</span></li>|arXiv|https://github.com/baochi0212/LaVy|https://doi.org/10.48550/arXiv.2404.07922|
|512|Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on
  Graphs|Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Meng Y...|<li><span style="color:#FF5733;">2024-04-10</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/PeterGriffinJin/Graph-CoT.|https://doi.org/10.18653/v1/2024.findings-acl.11|
|513|Elephants Never Forget: Memorization and Learning of Tabular Data in
  Large Language Models|Sebastian Bordt, Harsha Nori, Vanessa Rodrigues, Besmira Nushi, Rich Caruana|<li><span style="color:#FF5733;">2024-04-09</span></li>|arXiv|https://github.com/interpretml/LLM-Tabular-Memorization-Checker|https://doi.org/10.48550/arXiv.2404.06209|
|514|LayoutLLM: Layout Instruction Tuning with Large Language Models for
  Document Understanding|Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Zheng Qi, Zhi Yu, Cong Yao|<li><span style="color:#FF5733;">2024-04-08</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/AlibabaResearch/AdvancedLiterateMachinery|https://doi.org/10.1109/CVPR52733.2024.01480|
|515|Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning
  Skills in Large Language Models|Yantao Liu, Zijun Yao, Xin Lv, Yuchen Fan, Shulin Cao, Jifan Yu, Lei Hou, Juanzi Li|<li><span style="color:#FF5733;">2024-04-04</span></li>|LREC/COLING|https://github.com/THU-KEG/KNOT|https://aclanthology.org/2024.lrec-main.1493|
|516|ChatGLM-Math: Improving Math Problem-Solving in Large Language Models
  with a Self-Critique Pipeline|Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao,...|<li><span style="color:#FF5733;">2024-04-03</span></li>|OpenAlex|https://github.com/THUDM/ChatGLM-Math|https://doi.org/10.18653/v1/2024.findings-emnlp.569|
|517|Evaluating the Factuality of Large Language Models using Large-Scale
  Knowledge Graphs|Xiaoze Liu, Feijie Wu, Tianyang Xu, Zhuo Chen, Yichi Zhang, Xiaoqian Wang, Jing Gao|<li><span style="color:#FF5733;">2024-04-01</span></li>|IEEE Data Eng. Bull.|https://github.com/xz-liu/GraphEval.|http://sites.computer.org/debull/A24dec/p87.pdf|
|518|Can LLMs Master Math? Investigating Large Language Models on Math Stack
  Exchange|Ankit Satpute, Noah Gießing, André Greiner-Petter, Moritz Schubotz, Olaf Teschke, Akiko Aizawa, Béla Gipp|<li><span style="color:#FF5733;">2024-03-30</span></li>|Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/gipplab/LLM-Investig-MathStackExchange|https://doi.org/10.48550/arXiv.2404.00344|
|519|MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of
  Large Language Models|Peng Ding, Jiading Fang, Peng Li, Kangrui Wang, Xiaochen Zhou, Mo Yu, Jing Li, Matthew R. Walter, Hongyuan Mei|<li><span style="color:#FF5733;">2024-03-28</span></li>|arXiv|https://github.com/oaklight/mango|https://doi.org/10.48550/arXiv.2403.19913|
|520|Evaluating large language models for annotating proteins|Rosario Vitale, Leandro A. Bugnon, Emilio Fenoy, Diego H. Milone, Georgina Stegmayer|<li><span style="color:#FF5733;">2024-03-27</span></li>|Briefings in Bioinformatics|https://github.com/sinc-lab/llm4pfam|https://doi.org/10.1093/bib/bbae177|
|521|How do Large Language Models understand Genes and Cells|Chen Fang, Yidong Wang, Yunze Song, Qingqing Long, Lu Wang, L. Chen, Pengfei Wang, Guihai Feng, Yuanchun Zhou, Xin Li|<li><span style="color:#FF5733;">2024-03-27</span></li>|ACM Transactions on Intelligent Systems and Technology|https://github.com/epang-ucas/Evaluate_LLMs_to_Genes|https://doi.org/10.1101/2024.03.23.586383|
|522|Generation of Asset Administration Shell with Large Language Model
  Agents: Toward Semantic Interoperability in Digital Twins in the Context of
  Industry 4.0|Yuchen Xia, Zhewen Xiao, Nasser Jazdi, Michael Weyrich|<li><span style="color:#FF5733;">2024-03-25</span></li>|IEEE Access|https://github.com/YuchenXia/AASbyLLM.|https://doi.org/10.1109/ACCESS.2024.3415470|
|523|SciEval: A Multi-Level Large Language Model Evaluation Benchmark for Scientific Research|Liangtai Sun, Han Yang, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, Kai Yu|<li><span style="color:#FF5733;">2024-03-24</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/OpenDFM/SciEval.|https://doi.org/10.1609/aaai.v38i17.29872|
|524|Imagination Augmented Generation: Learning to Imagine Richer Context for
  Question Answering over Large Language Models|Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, Jun Zhao|<li><span style="color:#FF5733;">2024-03-22</span></li>|arXiv|https://github.com/Xnhyacinth/IAG.|https://doi.org/10.48550/arXiv.2403.15268|
|525|AOPWIKI-EXPLORER: An interactive graph-based query engine leveraging large language models|Saurav Kumar, Deepika Deepika, Luke T. Slater, Vikas Kumar|<li><span style="color:#FF5733;">2024-03-22</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/Crispae/AOPWiki_Explorer|https://doi.org/10.1016/j.comtox.2024.100308|
|526|ChainLM: Empowering Large Language Models with Improved Chain-of-Thought
  Prompting|Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen|<li><span style="color:#FF5733;">2024-03-21</span></li>|LREC/COLING|https://github.com/RUCAIBox/ChainLM.|https://aclanthology.org/2024.lrec-main.265|
|527|Advancing entity recognition in biomedicine via instruction tuning of large language models|Vipina K. Keloth, Yan Hu, Qianqian Xie, Xueqing Peng, Yan Wang, Andrew Zheng, Melih Selek, Kalpana Raja, Chih-Hsuan Wei,...|<li><span style="color:#FF5733;">2024-03-21</span></li>|Bioinformatics|https://github.com/BIDS-Xu-Lab/BioNER-LLaMA.|https://doi.org/10.1093/bioinformatics/btae163|
|528|HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal
  Large Language Models|Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun Shu, Haoyuan Li, Lei Zhang, He Wanggui, Hao Zhou, Zheqi Lv, Hao Jiang, Ju...|<li><span style="color:#FF5733;">2024-03-20</span></li>|arXiv|https://github.com/DCDmllm/HyperLLaVA|https://doi.org/10.48550/arXiv.2403.13447|
|529|Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for
  Large Language Models|Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Chaoyu Chen, Feng Zhao|<li><span style="color:#FF5733;">2024-03-19</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/InternLM/Agent-FLAN.|https://doi.org/10.18653/v1/2024.findings-acl.557|
|530|Instruction Multi-Constraint Molecular Generation Using a
  Teacher-Student Large Language Model|Peng Zhou, Jianmin Wang, Chunyan Li, Zixu Wang, Yiping Liu, Siqi Sun, Jianxin Lin, Leyi Wei, Xibao Cai, Houtim Lai, Wei ...|<li><span style="color:#FF5733;">2024-03-19</span></li>|Research Square (Research Square)|https://github.com/HHW-zhou/TSMMG.|https://doi.org/10.21203/rs.3.rs-3845824/v1|
|531|ManipVQA: Injecting Robotic Affordance and Physically Grounded
  Information into Multi-Modal Large Language Models|Siyuan Huang, Iaroslav Ponomarenko, Zhengkai Jiang, Xiaoqi Li, Xiaobin Hu, Peng Gao, Hongsheng Li, Hao Dong|<li><span style="color:#FF5733;">2024-03-17</span></li>|2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)|https://github.com/SiyuanHuang95/ManipVQA.|https://doi.org/10.1109/IROS58592.2024.10801993|
|532|DRAGIN: Dynamic Retrieval Augmented Generation based on the Real-time
  Information Needs of Large Language Models|Weihang Su, Yichen Tang, Qingyao Ai, Zhijing Wu, Yiqun Liu|<li><span style="color:#FF5733;">2024-03-15</span></li>|OpenAlex|https://github.com/oneal2000/DRAGIN|https://doi.org/10.18653/v1/2024.acl-long.702|
|533|LLM-Assisted Light: Leveraging Large Language Model Capabilities for
  Human-Mimetic Traffic Signal Control in Complex Urban Environments|Maonan Wang, Aoyu Pang, Yuheng Kan, Man-On Pun, Chung Shue Chen, Bo Huang|<li><span style="color:#FF5733;">2024-03-13</span></li>|arXiv|https://github.com/Traffic-Alpha/LLM-Assisted-Light|https://doi.org/10.48550/arXiv.2403.08337|
|534|Empowering Robotics with Large Language Models: osmAG Map Comprehension
  with LLMs|Fujing Xie, Sören Schwertfeger|<li><span style="color:#FF5733;">2024-03-13</span></li>|arXiv|https://github.com/xiefujing/LLM-osmAG-Comprehension.|https://doi.org/10.48550/arXiv.2403.08228|
|535|Smart-Infinity: Fast Large Language Model Training using Near-Storage
  Processing on a Real System|Hongsun Jang, Jaeyong Song, Jaewon Jung, Jaeyoung Park, Youngsok Kim, Jinho Lee|<li><span style="color:#FF5733;">2024-03-11</span></li>|OpenAlex|https://github.com/AIS-SNU/smart-infinity.|https://doi.org/10.1109/HPCA57654.2024.00034|
|536|LLM4Decompile: Decompiling Binary Code with Large Language Models|Hanzhuo Tan, Q. Luo, Jing Li, Yuqun Zhang|<li><span style="color:#FF5733;">2024-03-08</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/albertan017/LLM4Decompile|https://doi.org/10.18653/v1/2024.emnlp-main.203|
|537|CAT: Enhancing Multimodal Large Language Model to Answer Questions in
  Dynamic Audio-Visual Scenarios|Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, Xiaochun Cao|<li><span style="color:#FF5733;">2024-03-07</span></li>|Lecture notes in computer science|https://github.com/rikeilong/Bay-CAT.|https://doi.org/10.1007/978-3-031-72684-2_9|
|538|GraphInstruct: Empowering Large Language Models with Graph Understanding
  and Reasoning Capability|Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jin-Qi Jiang, Xing Xie, Hai Jin|<li><span style="color:#FF5733;">2024-03-07</span></li>|arXiv|https://github.com/CGCL-codes/GraphInstruct.|https://doi.org/10.48550/arXiv.2403.04483|
|539|Benchmarking Hallucination in Large Language Models based on
  Unanswerable Math Word Problem|Yuhong Sun, Zhangyue Yin, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Zhao Hui|<li><span style="color:#FF5733;">2024-03-06</span></li>|LREC/COLING|https://github.com/Yuki-Asuuna/UMWP.|https://aclanthology.org/2024.lrec-main.196|
|540|Towards Efficient and Effective Unlearning of Large Language Models for
  Recommendation|Hangyu Wang, Jianghao Lin, Bo Chen, Yang Yang, Ruiming Tang, Weinan Zhang, Yong Yu|<li><span style="color:#FF5733;">2024-03-06</span></li>|Frontiers of Computer Science|https://github.com/justarter/E2URec|https://doi.org/10.1007/s11704-024-40044-2|
|541|InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated
  Large Language Model Agents|Qiusi Zhan, Zhixiang Liang, Zifan Ying, Daniel Kang|<li><span style="color:#FF5733;">2024-03-05</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/uiuc-kang-lab/InjecAgent.|https://doi.org/10.18653/v1/2024.findings-acl.624|
|542|Peacock: A Family of Arabic Multimodal Large Language Models and
  Benchmarks|Fakhraddin Alwajih, El Moatez Billah Nagoudi, Gagan Bhatia, Abdelrahman Mohamed, Muhammad Abdul-Mageed|<li><span style="color:#FF5733;">2024-03-01</span></li>|OpenAlex|https://github.com/UBC-NLP/peacock|https://doi.org/10.18653/v1/2024.acl-long.689|
|543|Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period
  of Large Language Models|Qian Chen, Jie Zhang, Wei Yao, Dongrui Liu, Zhenfei Yin, Yu Qiao, Yong Liu, Jing Shao|<li><span style="color:#FF5733;">2024-02-29</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/ChnQ/TracingLLM|https://doi.org/10.18653/v1/2024.findings-acl.290|
|544|Large Language Models are Learnable Planners for Long-Term
  Recommendation|Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi Zhang, Qifan Wang, Fuli Feng|<li><span style="color:#FF5733;">2024-02-29</span></li>|Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/jizhi-zhang/BiLLP.|https://doi.org/10.1145/3626772.3657683|
|545|Exploring Genomic Large Language Models: Bridging the Gap between Natural Language and Gene Sequences|Huaqing Liu, Shuxian Zhou, Pei‐Yi Chen, Jiahui Liu, Ku-Geng Huo, Lanqing Han|<li><span style="color:#FF5733;">2024-02-29</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/Huatsing-Lau/GenomicLLM|https://doi.org/10.1101/2024.02.26.581496|
|546|Token-Specific Watermarking with Enhanced Detectability and Semantic
  Coherence for Large Language Models|Mingjia Huo, Sai Ashish Somayajula, Youwei Liang, Ruisi Zhang, Farinaz Koushanfar, Pengtao Xie|<li><span style="color:#FF5733;">2024-02-28</span></li>|ICML|https://github.com/mignonjia/TS_watermark|https://openreview.net/forum?id=AqBz54aFyj|
|547|AskIt: Unified Programming Interface for Programming with Large Language Models|Katsumi Okuda, Saman P. Amarasinghe|<li><span style="color:#FF5733;">2024-02-28</span></li>|OpenAlex|https://github.com/katsumiok/ts-askit|https://doi.org/10.1109/cgo57630.2024.10444830|
|548|DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping
  Backward Propagation|Sunghyeon Woo, Baeseong Park, Byeongwook Kim, Minjung Jo, Se Jung Kwon, Dongsuk Jeon, Dongsoo Lee|<li><span style="color:#FF5733;">2024-02-27</span></li>|NeurIPS|https://github.com/WooSunghyeon/dropbp.|http://papers.nips.cc/paper_files/paper/2024/hash/240225294cdd2c9b692c2519d3278a08-Abstract-Conference.html|
|549|MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical
  Reasoning|Debrup Das, Debopriyo Banerjee, Somak Aditya, Ashish Kulkarni|<li><span style="color:#FF5733;">2024-02-27</span></li>|OpenReview|https://github.com/Debrup-61/MathSensei.|https://openreview.net/pdf/d862f1b31ddae37b60b2353aff5c5bcedf981569.pdf|
|550|DenseMamba: State Space Models with Dense Hidden Connection for
  Efficient Large Language Models|Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang|<li><span style="color:#FF5733;">2024-02-26</span></li>|OpenAlex|https://github.com/WailordHe/DenseSSM|https://doi.org/10.18653/v1/2025.naacl-long.467|
|551|MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in
  Intellectual Property|Shiwen Ni, Minghuan Tan, Yuelin Bai, Fuqiang Niu, Min Yang, Bowen Zhang, Ruifeng Xu, Xiaojun Chen, Chengming Li, Xiping ...|<li><span style="color:#FF5733;">2024-02-26</span></li>|LREC/COLING|https://github.com/AI-for-Science/MoZi|https://aclanthology.org/2024.lrec-main.1018|
|552|ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language
  Processing|Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu‐Chian Chen, Yuan Li, Yonghong Tian|<li><span style="color:#FF5733;">2024-02-26</span></li>|IEEE Transactions on Artificial Intelligence|https://github.com/Lyu6PosHao/ProLLaMA|https://doi.org/10.48550/arXiv.2402.16445|
|553|SelectIT: Selective Instruction Tuning for Large Language Models via
  Uncertainty-Aware Self-Reflection|Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, Min Zhang|<li><span style="color:#FF5733;">2024-02-26</span></li>|arXiv|https://github.com/Blue-Raincoat/SelectIT.|https://doi.org/10.48550/arXiv.2402.16705|
|554|Defending Large Language Models against Jailbreak Attacks via Semantic
  Smoothing|Jiabao Ji, Bairu Hou, Alexander Robey, George J. Pappas, Hamed Hassani, Shuicheng Yan, Eric Wong, Shiyu Chang|<li><span style="color:#FF5733;">2024-02-25</span></li>|arXiv|https://github.com/UCSB-NLP-Chang/SemanticSmooth.|https://doi.org/10.48550/arXiv.2402.16192|
|555|How Large Language Models Encode Context Knowledge? A Layer-Wise Probing
  Study|Tianjie Ju, Weiwei Sun, Wei Du, Xinwei Yuan, Zhaochun Ren, Gongshen Liu|<li><span style="color:#FF5733;">2024-02-25</span></li>|LREC/COLING|https://github.com/Jometeorie/probing_llama.|https://aclanthology.org/2024.lrec-main.722|
|556|SpaCCC: Large language model-based cell-cell communication inference for spatially resolved transcriptomic data|Boya Ji, Xiaoqi Wang, Debin Qiao, Liwen Xu, Shaoliang Peng|<li><span style="color:#FF5733;">2024-02-23</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/jiboyalab/SpaCCC.|https://doi.org/10.26599/bdma.2024.9020056|
|557|Not All Experts are Equal: Efficient Expert Pruning and Skipping for
  Mixture-of-Experts Large Language Models|Xudong Lü, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng Li|<li><span style="color:#FF5733;">2024-02-22</span></li>|OpenAlex|https://github.com/Lucky-Lance/Expert_Sparsity.|https://doi.org/10.18653/v1/2024.acl-long.334|
|558|CriticBench: Evaluating Large Language Models as Critic|Lü Tian, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Chaoyu Chen, Xian-Ling Mao|<li><span style="color:#FF5733;">2024-02-21</span></li>|arXiv|https://github.com/open-compass/CriticBench|https://doi.org/10.48550/arXiv.2402.13764|
|559|ARL2: Aligning Retrievers for Black-box Large Language Models via
  Self-guided Adaptive Relevance Labeling|Lingxi Zhang, Yue Yu, Kuan Wang, Chao Zhang|<li><span style="color:#FF5733;">2024-02-21</span></li>|OpenAlex|https://github.com/zhanglingxi-cs/ARL2|https://doi.org/10.18653/v1/2024.acl-long.203|
|560|Round Trip Translation Defence against Large Language Model Jailbreaking
  Attacks|Canaan Yung, Hadi Mohaghegh Dolatabadi, Sarah M. Erfani, Christopher Leckie|<li><span style="color:#FF5733;">2024-02-20</span></li>|Lecture notes in computer science|https://github.com/Cancanxxx/Round_Trip_Translation_Defence|https://doi.org/10.1007/978-981-96-8197-6_21|
|561|Large Language Model-based Human-Agent Collaboration for Complex Task
  Solving|Xueyang Feng, Zhiyuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan Liu, Ji-Rong Wen|<li><span style="color:#FF5733;">2024-02-20</span></li>|OpenReview|https://github.com/XueyangFeng/ReHAC.|https://openreview.net/pdf/1fe7194085f2d4ab21d2c0170a48e1737870317d.pdf|
|562|TreeEval: Benchmark-Free Evaluation of Large Language Models through
  Tree Planning|Xiang Li, Yunshi Lan, Chao Yang|<li><span style="color:#FF5733;">2024-02-20</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/Ashura5/TreeEval.|https://doi.org/10.48550/arXiv.2402.13125|
|563|Remember This Event That Year? Assessing Temporal Information and Understanding in Large Language Models|Himanshu Beniwal, Dishant Patel, Kowsik Nandagopan D, Hritik Ladia, Ankit Yadav, Mayank Singh|<li><span style="color:#FF5733;">2024-02-19</span></li>|OpenAlex|https://github.com/lingoiitgn/TempUN|https://doi.org/10.18653/v1/2024.findings-emnlp.953|
|564|Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of
  Large Language Models|Loka Li, Guangyi Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric P. Xing, Kun Zhang|<li><span style="color:#FF5733;">2024-02-19</span></li>|arXiv|https://github.com/MBZUAI-CLeaR/IoE-Prompting.git|https://doi.org/10.48550/arXiv.2402.12563|
|565|BESA: Pruning Large Language Models with Blockwise Parameter-Efficient
  Sparsity Allocation|Peng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao, Fengwei An, Yu Qiao, Ping Luo|<li><span style="color:#FF5733;">2024-02-18</span></li>|ICLR|https://github.com/OpenGVLab/LLMPrune-BESA|https://openreview.net/forum?id=gC6JTEU3jl|
|566|Large Language Model-driven Meta-structure Discovery in Heterogeneous
  Information Network|Lin Chen, Fengli Xu, Nian Li, Zhenyu Han, Meng Wang, Yong Li, Pan Hui|<li><span style="color:#FF5733;">2024-02-18</span></li>|Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/LinChen-65/ReStruct.|https://doi.org/10.48550/arXiv.2402.11518|
|567|Multi-Task Inference: Can Large Language Models Follow Multiple
  Instructions at Once?|Guijin Son, Sangwon Baek, Sangdae Nam, Ilgyun Jeong, Seungone Kim|<li><span style="color:#FF5733;">2024-02-18</span></li>|OpenAlex|https://github.com/guijinSON/MTI-Bench.|https://doi.org/10.18653/v1/2024.acl-long.304|
|568|Aligning Modalities in Vision Large Language Models via Preference
  Fine-tuning|Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, Huaxiu Yao|<li><span style="color:#FF5733;">2024-02-17</span></li>|arXiv|https://github.com/YiyangZhou/POVID.|https://doi.org/10.48550/arXiv.2402.11411|
|569|ToolSword: Unveiling Safety Issues of Large Language Models in Tool
  Learning Across Three Stages|Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang, Songyang Gao, Yi‐Long Wu, Qi Zhang, Tao Gui, Xuanjing Huang|<li><span style="color:#FF5733;">2024-02-16</span></li>|OpenAlex|https://github.com/Junjie-Ye/ToolSword.|https://doi.org/10.18653/v1/2024.acl-long.119|
|570|Uncertainty Decomposition and Quantification for In-Context Learning of
  Large Language Models|Ling Chen, Xujiang Zhao, Wei Cheng, Yanchi Liu, Yiyou Sun, Xuchao Zhang, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie ...|<li><span style="color:#FF5733;">2024-02-15</span></li>|arXiv|https://github.com/lingchen0331/UQ_ICL|https://doi.org/10.48550/arXiv.2402.10189|
|571|BreakGPT: A Large Language Model with Multi-stage Structure for
  Financial Breakout Detection|Kang Zhang, Osamu Yoshie, Weiran Huang|<li><span style="color:#FF5733;">2024-02-12</span></li>|arXiv|https://github.com/Neviim96/BreakGPT|https://doi.org/10.48550/arXiv.2402.07536|
|572|Verified Multi-Step Synthesis using Large Language Models and Monte
  Carlo Tree Search|David Brandfonbrener, S. Kumar Raja, Tarun Prasad, Chloe Loughridge, Jianang Yang, Simon Henniger, William E. Byrd, Robe...|<li><span style="color:#FF5733;">2024-02-12</span></li>|arXiv|https://github.com/namin/llm-verified-with-monte-carlo-tree-search|https://doi.org/10.48550/arXiv.2402.08147|
|573|GraphTranslator: Aligning Graph Model to Large Language Model for
  Open-ended Tasks|Mengmei Zhang, Mingwei Sun, Puying Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, Chuan Shi|<li><span style="color:#FF5733;">2024-02-11</span></li>|Proceedings of the ACM Web Conference 2022|https://github.com/alibaba/GraphTranslator|https://doi.org/10.48550/arXiv.2402.07197|
|574|Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems &amp; Hallucinations|Ankit Pal, Malaikannan Sankarasubbu|<li><span style="color:#FF5733;">2024-02-10</span></li>|ClinicalNLP@NAACL|https://github.com/promptslab/RosettaEval|https://doi.org/10.18653/v1/2024.clinicalnlp-1.3|
|575|OpenFedLLM: Training Large Language Models on Decentralized Private Data
  via Federated Learning|Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, Siheng Chen|<li><span style="color:#FF5733;">2024-02-10</span></li>|Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/rui-ye/OpenFedLLM.|https://doi.org/10.48550/arXiv.2402.06954|
|576|Understanding the Weakness of Large Language Model Agents within a
  Complex Android Environment|Mingzhe Xing, Rongkai Zhang, Hui Xue, Qi Chen, Fengtang Yang, Zhen Xiao|<li><span style="color:#FF5733;">2024-02-09</span></li>|Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/AndroidArenaAgent/AndroidArena.|https://doi.org/10.48550/arXiv.2402.06596|
|577|UrbanKGent: A Unified Large Language Model Agent Framework for Urban
  Knowledge Graph Construction|Yansong Ning, Hao Liu|<li><span style="color:#FF5733;">2024-02-09</span></li>|NeurIPS|https://github.com/usail-hkust/UrbanKGent.|http://papers.nips.cc/paper_files/paper/2024/hash/decd42d78c42cea59c95c7c3d40d5e0f-Abstract-Conference.html|
|578|InternLM-Math: Open Math Large Language Models Toward Verifiable
  Reasoning|Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi ...|<li><span style="color:#FF5733;">2024-02-09</span></li>|arXiv|https://github.com/InternLM/InternLM-Math|https://doi.org/10.48550/arXiv.2402.06332|
|579|Self-Alignment of Large Language Models via Monopolylogue-based Social
  Scene Simulation|Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng Wang, Siheng Chen|<li><span style="color:#FF5733;">2024-02-08</span></li>|ICML|https://github.com/pangxianghe/MATRIX.|https://openreview.net/forum?id=l7shXGuGBT|
|580|Identifying Reasons for Contraceptive Switching from Real-World Data
  Using Large Language Models|Brenda Y. Miao, Christopher Y. K. Williams, Ebenezer Chinedu-Eneh, Travis Zack, Emily Alsentzer, Atul J. Butte, Irene Y....|<li><span style="color:#FF5733;">2024-02-05</span></li>|arXiv|https://github.com/BMiao10/contraceptive-switching.|https://doi.org/10.48550/arXiv.2402.03597|
|581|KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce
  Programs over Low-resourced Knowledge Bases|Jiajie Zhang, Shulin Cao, Linmei Hu, Ling Feng, Lei Hou, Juanzi Li|<li><span style="color:#FF5733;">2024-02-02</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/THU-KEG/KB-Plugin.|https://doi.org/10.18653/v1/2024.emnlp-main.168|
|582|Leveraging Large Language Models for Structure Learning in Prompted Weak
  Supervision|Jinyan Su, Peilin Yu, Jieyu Zhang, Stephen H. Bach|<li><span style="color:#FF5733;">2024-02-02</span></li>|2021 IEEE International Conference on Big Data (Big Data)|https://github.com/BatsResearch/su-bigdata23-code.|https://doi.org/10.1109/BigData59044.2023.10386190|
|583|ReEvo: Large Language Models as Hyper-Heuristics with Reflective
  Evolution|Haoran Ye, Jiarui Wang, Zhiguang Cao, Federico Berto, Chuanbo Hua, Haeyeon Kim, Jinkyoo Park, Guojie Song|<li><span style="color:#FF5733;">2024-02-02</span></li>|NeurIPS|https://github.com/ai4co/LLM-as-HH.|http://papers.nips.cc/paper_files/paper/2024/hash/4ced59d480e07d290b6f29fc8798f195-Abstract-Conference.html|
|584|EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit
  Large Language Models|Xuchen Pan, Yanxi Chen, Yaliang Li, Bolin Ding, Jingren Zhou|<li><span style="color:#FF5733;">2024-02-01</span></li>|arXiv|https://github.com/pan-x-c/EE-LLM.|https://doi.org/10.48550/arXiv.2402.00518|
|585|Proximity QA: Unleashing the Power of Multi-Modal Large Language Models
  for Spatial Proximity Analysis|Jianing Li, Xi Nan, Ming Lu, Li Du, Shanghang Zhang|<li><span style="color:#FF5733;">2024-01-31</span></li>|arXiv|https://github.com/NorthSummer/ProximityQA.git|https://doi.org/10.48550/arXiv.2401.17862|
|586|Can Large Language Models be Trusted for Evaluation? Scalable
  Meta-Evaluation of LLMs as Evaluators via Agent Debate|Steffi Chern, Ethan Chern, Graham Neubig, Pengfei Liu|<li><span style="color:#FF5733;">2024-01-30</span></li>|arXiv|https://github.com/GAIR-NLP/scaleeval|https://doi.org/10.48550/arXiv.2401.16788|
|587|EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor
  Image Comprehension in Remote Sensing Domain|Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, Xuerui Mao|<li><span style="color:#FF5733;">2024-01-30</span></li>|IEEE Transactions on Geoscience and Remote Sensing|https://github.com/wivizhang/EarthGPT.|https://doi.org/10.48550/arXiv.2401.16822|
|588|Efficient Tuning and Inference for Large Language Models on Textual
  Graphs|Yun Zhu, Yaoke Wang, Haizhou Shi, Siliang Tang|<li><span style="color:#FF5733;">2024-01-28</span></li>|OpenAlex|https://github.com/ZhuYun97/ENGINE.|https://www.ijcai.org/proceedings/2024/634|
|589|Contextualization Distillation from Large Language Model for Knowledge
  Graph Completion|Dawei Li, Zhen Tan, Tianlong Chen, Huan Liu|<li><span style="color:#FF5733;">2024-01-28</span></li>|OpenReview|https://github.com/David-Li0406/Contextulization-Distillation|https://openreview.net/pdf/fe344955074309289a00c3511362bb14bad3e314.pdf|
|590|PROXYQA: An Alternative Framework for Evaluating Long-Form Text
  Generation with Large Language Models|Haochen Tan, Zhijiang Guo, Zhan Shi, Lu Xu, Zhili Liu, Yunlong Feng, Xiaoguang Li, Yasheng Wang, Lifeng Shang, Qun Liu, ...|<li><span style="color:#FF5733;">2024-01-26</span></li>|OpenAlex|https://github.com/Namco0816/ProxyQA|https://doi.org/10.18653/v1/2024.acl-long.368|
|591|Prompting Large Language Models for Zero-Shot Clinical Prediction with
  Structured Longitudinal Electronic Health Record Data|Yinghao Zhu, Zixiang Wang, Junyi Gao, Yuning Tong, Jingkun An, Weibin Liao, Ewen M. Harrison, Liantao Ma, Chengwei Pan|<li><span style="color:#FF5733;">2024-01-25</span></li>|arXiv|https://github.com/yhzhu99/llm4healthcare|https://doi.org/10.48550/arXiv.2402.01713|
|592|Replication package for "A Novel Approach for Automated Program Repair using Round-Trip Translation with Large Language Models"|Fernando Vallecillos Ruiz, Anastasiia Grishina, Max Hort, Leon Moonen|<li><span style="color:#FF5733;">2024-01-13</span></li>|Zenodo (CERN European Organization for Nuclear Research)|https://github.com/secureIT-project/RTT_for_APR.|https://zenodo.org/doi/10.5281/zenodo.10500593|
|593|Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models|Jianhui Pang, Fanghua Ye, Derek Fai Wong, Dian Yu, Shuming Shi, Zhaopeng Tu, Longyue Wang|<li><span style="color:#FF5733;">2024-01-07</span></li>|Transactions of the Association for Computational Linguistics|https://github.com/pangjh3/LLM4MT.|https://doi.org/10.48550/arXiv.2401.08350|
|594|QUIK: Towards End-to-end 4-Bit Inference on Generative Large Language Models|Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, Dan Alistarh|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/IST-DASLab/QUIK.|https://doi.org/10.18653/v1/2024.emnlp-main.197|
|595|Open-domain Implicit Format Control for Large Language Model Generation|Yiqun Yao, Wenjia Ma, Xuezhi Fang, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Jing Li, Aixin Sun, Yequan Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/cofe-ai/OIFC.|https://doi.org/10.48550/arXiv.2408.04392|
|596|OctoPack: Instruction Tuning Code Large Language Models|Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang,...|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICLR|https://github.com/bigcode-project/octopack.|https://openreview.net/forum?id=mw1PWNSWZP|
|597|OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models|Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok Park|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/xvyaward/owq.|https://doi.org/10.1609/aaai.v38i12.29237|
|598|ONCE: Boosting Content-based Recommendation with Both Open- and Closed-source Large Language Models|Qijiong Liu, Nuo Chen, Tetsuya Sakai, Xiao-Ming Wu|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/Jyonn/ONCE|https://doi.org/10.1145/3616855.3635845|
|599|ORLM: Training Large Language Models for Optimization Modeling|Zhengyang Tang, Chenyu Huang, Xin Zheng, Shixi Hu, Zizhuo Wang, Dongdong Ge, Benyou Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/Cardinal-Operations/ORLM|https://doi.org/10.48550/arXiv.2405.17743|
|600|OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models|Shuai Wang, Liang Ding, Li Shen, Yong Luo, Bo Du, Dacheng Tao|<li><span style="color:#FF5733;">2024-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/alphadl/OOP-eval.|https://doi.org/10.18653/v1/2024.findings-acl.808|
|601|Over-Reasoning and Redundant Calculation of Large Language Models|David Cheng-Han Chiang, Hung-Yi Lee|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/d223302/Over-Reasoning-of-LLMs|https://doi.org/10.18653/v1/2024.eacl-short.15|
|602|News Recommendation with Category Description by a Large Language Model|Yuki Yada, Hayato Yamana|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/yamanalab/gpt-augmented-news-recommendation.|https://doi.org/10.48550/arXiv.2405.13007|
|603|Orion-14B: Open-source Multilingual Large Language Models|Chen Du, Yi Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan, Leichao Xu, Dacheng Zhang, Zhipeng Zhang, Kun Han|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/OrionStarAI/Orion|https://doi.org/10.48550/arXiv.2401.12246|
|604|PLeak: Prompt Leaking Attacks against Large Language Model Applications|Bo Hui, Haolin Yuan, Neil Zhenqiang Gong, Philippe Burlina, Yinzhi Cao|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/BHui97/PLeak.|https://doi.org/10.48550/arXiv.2405.06823|
|605|PAL: Proxy-Guided Black-Box Attack on Large Language Models|Chawin Sitawarin, Norman Mu, David A. Wagner, Alexandre Araujo|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/chawins/pal.|https://doi.org/10.48550/arXiv.2402.09674|
|606|PLLaMa: An Open-source Large Language Model for Plant Science|Xianjun Yang, Junfeng Gao, Wenxin Xue, Erik Alexandersson|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/Xianjun-Yang/PLLaMa|https://doi.org/10.48550/arXiv.2401.01600|
|607|Multi-label Sequential Sentence Classification via Large Language Model|Mengfei Lan, Lecheng Zheng, Shufan Ming, Halil Kilicoglu|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/ScienceNLP-Lab/LLM-SSC.|https://doi.org/10.18653/v1/2024.findings-emnlp.944|
|608|PPTC-R benchmark: Towards Evaluating the Robustness of Large Language Models for PowerPoint Task Completion|Yiduo Guo, Zekai Zhang, Yaobo Liang, Dongyan Zhao, Nan Duan|<li><span style="color:#FF5733;">2024-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/gydpku/PPTC|https://doi.org/10.18653/v1/2024.findings-emnlp.722|
|609|Patch-Level Training for Large Language Models|Chenze Shao, Fandong Meng, Jie Zhou|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/shaochenze/PatchTrain|https://doi.org/10.48550/arXiv.2407.12665|
|610|PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering|Jinlong He, Pengfei Li, Gang Liu, Zixu Zhao, Shenjun Zhong|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/jinlHe/PeFoMed.|https://doi.org/10.48550/arXiv.2401.02797|
|611|Phased Instruction Fine-Tuning for Large Language Models|Wei Pang, Chuan Zhou, Xiao-Hua Zhou, Xiaojie Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/xubuvd/PhasedSFT.|https://doi.org/10.18653/v1/2024.findings-acl.341|
|612|Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models|Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, Carlo Vittorio Cannistraci|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning|https://doi.org/10.20944/preprints202310.1487.v2|
|613|PostMark: A Robust Blackbox Watermark for Large Language Models|Yapei Chang, Kalpesh Krishna, Amir Houmansadr, John Wieting, Mohit Iyyer|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/lilakk/PostMark.|https://doi.org/10.18653/v1/2024.emnlp-main.506|
|614|Privacy issues in Large Language Models: A survey|Hareem Kibriya, Wazir Zada Khan, Ayesha Siddiqa, Muhammad Khurrum Khan|<li><span style="color:#FF5733;">2024-01-01</span></li>|Computers & Electrical Engineering|https://github.com/safr-ml-lab/survey-llm.|https://doi.org/10.1016/j.compeleceng.2024.109698|
|615|Prompt-Time Ontology-Driven Symbolic Knowledge Capture with Large Language Models|Tolga Çöplü, Arto Bendiken, Andrii Skomorokhov, Eduard Bateiko, Stephen Cobb, Joshua J. Bouw|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/HaltiaAI/paper-PTSKC.|https://doi.org/10.48550/arXiv.2405.14012|
|616|PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking|Yuzhang Xie, Jiaying Lu, Joyce C. Ho, Fadi B. Nahab, Xiao Hu, Carl Yang|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/constantjxyz/PromptLink.|https://doi.org/10.48550/arXiv.2405.07500|
|617|Naive Bayes-based Context Extension for Large Language Models|Jianlin Su, Murtadha H. M. Ahmed, Bo Wen, Luo Ao, Mingren Zhu, Yunfeng Liu|<li><span style="color:#FF5733;">2024-01-01</span></li>|NAACL-HLT|https://github.com/amurtadha/NBCE-master|https://doi.org/10.18653/v1/2024.naacl-long.431|
|618|MentalLLaMA: Interpretable Mental Health Analysis on Social Media with Large Language Models|Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, Sophia Ananiadou|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the ACM Web Conference 2022|https://github.com/SteveKGYang/MentaLLaMA.|https://doi.org/10.48550/arXiv.2309.13567|
|619|MuGI: Enhancing Information Retrieval through Multi-Text Generation Intergration with Large Language Models|Le Zhang, Yihong Wu, Qian Yang, Jian-Yun Nie|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/lezhang7/Retrieval_MuGI.|https://doi.org/10.48550/arXiv.2401.06311|
|620|MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception|Yuhao Wang, Yusheng Liao, Heyang Liu, Hongcheng Liu, Yanfeng Wang, Yu Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/YHWmz/MM-SAP.|https://doi.org/10.18653/v1/2024.acl-long.498|
|621|Llumnix: Dynamic Scheduling for Large Language Model Serving|Biao Sun, Ziming Huang, Hanyu Zhao, Wencong Xiao, Xinyi Zhang, Yong Li, Wei Lin|<li><span style="color:#FF5733;">2024-01-01</span></li>|OSDI|https://github.com/AlibabaPAI/llumnix.|https://www.usenix.org/conference/osdi24/presentation/sun-biao|
|622|LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models|Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng He, Weizhu Chen, Tuo Zhao|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICLR|https://github.com/yxli2123/LoftQ.|https://openreview.net/forum?id=LzPWWPAdY4|
|623|Long-form factuality in large language models|Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Qu...|<li><span style="color:#FF5733;">2024-01-01</span></li>|NeurIPS|https://github.com/google-deepmind/long-form-factuality.|http://papers.nips.cc/paper_files/paper/2024/hash/937ae0e83eb08d2cb8627fe1def8c751-Abstract-Conference.html|
|624|LongAlign: A Recipe for Long Context Alignment of Large Language Models|Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, Juanzi Li|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/THUDM/LongAlign.|https://doi.org/10.18653/v1/2024.findings-emnlp.74|
|625|LongVLM: Efficient Long Video Understanding via Large Language Models|Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, Bohan Zhuang|<li><span style="color:#FF5733;">2024-01-01</span></li>|Lecture notes in computer science|https://github.com/ziplab/LongVLM|https://doi.org/10.1007/978-3-031-73414-4_26|
|626|Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy|Yao Zhao, Zhitian Xie, Chen Liang, Chenyi Zhuang, Jinjie Gu|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/alipay/PainlessInferenceAcceleration.|https://doi.org/10.48550/arXiv.2312.12728|
|627|Low-Redundant Optimization for Large Language Model Alignment|Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Jingyuan Wang, Ji-Rong Wen|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/RUCAIBox/ALLO|https://doi.org/10.48550/arXiv.2406.12606|
|628|MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration|Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See Kiong Ng, Jiashi Feng|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/cathyxl/MAgIC.|https://openreview.net/pdf/761061f68cb72433ef3df3c718557fc2ddd95681.pdf|
|629|MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning|Chenyu Wang, Weixin Luo, Qianyu Chen, Haonan Mai, Jindi Guo, Sixun Dong, Xiaohua, Xuan Xuan, Zhengxin Li, Ma Lin, Shengh...|<li><span style="color:#FF5733;">2024-01-01</span></li>|2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/MLLM-Tool/MLLM-Tool.|https://doi.org/10.1109/WACV61041.2025.00650|
|630|MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with Large Language Models|Haoxuan Li, Zhengmao Yang, Yunshan Ma, Yi Bin, Yang Yang, Tat-Seng Chua|<li><span style="color:#FF5733;">2024-01-01</span></li>|ACM Multimedia|https://github.com/LuminosityX/MM-Forecast.|https://doi.org/10.48550/arXiv.2408.04388|
|631|MOSS: An Open Conversational Large Language Model|Tianxiang Sun, Xiaotian Zhang, Zhengfu He, Peng Li, Qinyuan Cheng, Xiangyang Liu, Hang Yan, Yunfan Shao, Qiong Tang, Shi...|<li><span style="color:#FF5733;">2024-01-01</span></li>|Mach. Intell. Res.|https://github.com/OpenMOSS/MOSS|https://doi.org/10.1007/s11633-024-1502-8|
|632|Mitigating Hallucinations of Large Language Models via Knowledge Consistent Alignment|Fanqi Wan, Xinting Huang, Leyang Cui, Xiaojun Quan, Wei Bi, Shuming Shi|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/fanqiwan/KCA|https://doi.org/10.48550/arXiv.2401.10768|
|633|MPT4LM: Multi-Modal Prompt Tuning Makes Pre-Trained Large Language Models Better Vision-Language Learners|Yongzhu Miao, Jintao Tang, Shasha Li, Ting Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|Frontiers in artificial intelligence and applications|https://github.com/YzM1a0/MPT4LM.|https://doi.org/10.3233/FAIA240515|
|634|M\&apos;elange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity|Tyler Griggs, Xiaoxuan Liu, Jiaxiang Yu, Doyoung Kim, Wei-Lin Chiang, Alvin Cheung, Ion Stoica|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/tyler-griggs/melange-release.|https://doi.org/10.48550/arXiv.2404.14527|
|635|MacBehaviour: An R package for behavioural experimentation on large language models|Xufeng Duan, Shixuan Li, Zhenguang G. Cai|<li><span style="color:#FF5733;">2024-01-01</span></li>|Behavior Research Methods|https://github.com/xufengduan/MacBehaviour|https://doi.org/10.3758/s13428-024-02524-y|
|636|Making Large Language Models Perform Better in Knowledge Graph Completion|Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Wen Zhang, Huajun Chen|<li><span style="color:#FF5733;">2024-01-01</span></li>|ACM Multimedia|https://github.com/zjukg/KoPA|https://openreview.net/pdf/67e226a56851ba101e44daad3397a44c471cd28f.pdf|
|637|Manipulating Large Language Models to Increase Product Visibility|Aounon Kumar, Himabindu Lakkaraju|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/aounon/llm-rank-optimizer.|https://doi.org/10.48550/arXiv.2404.07981|
|638|MaskLLM: Learnable Semi-Structured Sparsity for Large Language Models|Gongfan Fang, Hongxu Yin, Saurav Muralidharan, Greg Heinrich, Jeff Pool, Jan Kautz, Pavlo Molchanov, Xinchao Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|NeurIPS|https://github.com/NVlabs/MaskLLM|http://papers.nips.cc/paper_files/paper/2024/hash/0e9a05f5ce62284c91e4a33498899124-Abstract-Conference.html|
|639|Me LLaMA: Foundation Large Language Models for Medical Applications|Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina ...|<li><span style="color:#FF5733;">2024-01-01</span></li>|Research Square (Research Square)|https://github.com/BIDS-Xu-Lab/Me-LLaMA.|https://doi.org/10.21203/rs.3.rs-4240043/v1|
|640|MedDoc-Bot: A Chat Tool for Comparative Analysis of Large Language Models in the Context of the Pediatric Hypertension Guideline|Mohamed Yaseen Jabarulla, Steffen Oeltze-Jafra, Philipp Beerbaum, Theodor Uden|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/yaseen28/MedDoc-Bot.|https://doi.org/10.1109/EMBC53108.2024.10781509|
|641|Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences|Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Fuxiao Liu, Gedas Ber...|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenReview|https://github.com/umd-huang-lab/Mementos.|https://openreview.net/pdf/b2807a9eabd437c96c558eaefa9a5b5b8ca148d8.pdf|
|642|Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models|Yiming Wang, Zhuosheng Zhang, Pei Zhang, Baosong Yang, Rui Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/Alsace08/Meta-Reasoning|https://doi.org/10.18653/v1/2024.findings-acl.34|
|643|Protein Design by Directed Evolution Guided by Large Language Models|Thanh V. T. Tran, Truong Son Hy|<li><span style="color:#FF5733;">2024-01-01</span></li>|IEEE Transactions on Evolutionary Computation|https://github.com/HySonLab/Directed_Evolution|https://doi.org/10.1109/tevc.2024.3439690|
|644|Towards Lifelong Learning of Large Language Models: A Survey|Junhao Zheng, Shengjie Qiu, Chengming Shi, Qianli Ma|<li><span style="color:#FF5733;">2024-01-01</span></li>|ACM Computing Surveys|https://github.com/qianlima-lab/awesome-lifelong-learning-methods-for-llm.|https://doi.org/10.48550/arXiv.2406.06391|
|645|Quantifying Emergence in Large Language Models|Hang Chen, Xinyu Yang, Jiaying Zhu, Wenya Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/Zodiark-ch/Emergence-of-LLMs|https://doi.org/10.48550/arXiv.2405.12617|
|646|VISA: Reasoning Video Object Segmentation via Large Language Models|Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, Efstratios Gavves|<li><span style="color:#FF5733;">2024-01-01</span></li>|Lecture notes in computer science|https://github.com/cilinyan/VISA.|https://doi.org/10.1007/978-3-031-72633-0_6|
|647|Temporal Blind Spots in Large Language Models|Jonas Wallat, Adam Jatowt, Avishek Anand|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/jwallat/temporalblindspots.|https://doi.org/10.48550/arXiv.2401.12078|
|648|The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models|Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/RUCAIBox/HaluEval-2.0.|https://doi.org/10.18653/v1/2024.acl-long.586|
|649|Tokenization Falling Short: On Subword Robustness in Large Language Models|Yekun Chai, Yewei Fang, Qiwei Peng, Xuhong Li|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/FloatAI/TKEval.|https://doi.org/10.18653/v1/2024.findings-emnlp.86|
|650|ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios|Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, X...|<li><span style="color:#FF5733;">2024-01-01</span></li>|COLING|https://github.com/Junjie-Ye/ToolEyes.|https://aclanthology.org/2025.coling-main.12/|
|651|Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models|Pengzhi Gao, Zhongjun He, Hua Wu, Haifeng Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/gpengzhi/CrossConST-LLM.|https://doi.org/10.48550/arXiv.2401.05861|
|652|Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform|Mingyue Cheng, Hao Zhang, Jiqian Yang, Qi Liu, Li Li, Xin Huang, Liwei Song, Zhi Li, Zhenya Huang, Enhong Chen|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/Mingyue-Cheng/Bingjian.|https://doi.org/10.48550/arXiv.2403.08305|
|653|Training-Free Long-Context Scaling of Large Language Models|Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, Lingpeng Kong|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICML|https://github.com/HKUNLP/ChunkLlama|https://openreview.net/forum?id=If4xW9vF7U|
|654|Unleashing the Emergent Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration|Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji|<li><span style="color:#FF5733;">2024-01-01</span></li>|NAACL-HLT|https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.|https://doi.org/10.18653/v1/2024.naacl-long.15|
|655|Unleashing the Retrieval Potential of Large Language Models in Conversational Recommender Systems|Ting Yang, Li Chen|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/yt556677/ReFICR.|https://doi.org/10.1145/3640457.3688146|
|656|Unlocking the Potential of Large Language Models for Explainable Recommendations|Yucong Luo, Mingyue Cheng, Hao Zhang, Junyu Lu, Qi Liu, Enhong Chen|<li><span style="color:#FF5733;">2024-01-01</span></li>|Lecture notes in computer science|https://github.com/GodFire66666/LLM_rec_explanation|https://doi.org/10.1007/978-981-97-5569-1_18|
|657|Vaccine: Perturbation-aware Alignment for Large Language Model|Tiansheng Huang, Sihao Hu, Ling Liu|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/git-disl/Vaccine|https://doi.org/10.48550/arXiv.2402.01109|
|658|Quokka: An Open-source Large Language Model ChatBot for Material Science|Xianjun Yang, Stephen D. Wilson, Linda R. Petzold|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/Xianjun-Yang/Quokka.|https://doi.org/10.48550/arXiv.2401.01089|
|659|ViLLa: Video Reasoning Segmentation with Large Language Model|Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, Hengshuang Zhao|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/rkzheng99/ViLLa.|https://doi.org/10.48550/arXiv.2407.14500|
|660|Visual Hallucinations of Multi-modal Large Language Models|Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong|<li><span style="color:#FF5733;">2024-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/wenhuang2000/VHTest.|https://doi.org/10.18653/v1/2024.findings-acl.573|
|661|Walert: Putting Conversational Information Seeking Knowledge into Action by Building and Evaluating a Large Language Model-Powered Chatbot|Sachin Pathiyan Cherumanal, Lin Tian, Futoon M. Abushaqra, Angel Felipe Magnossão de Paula, Kaixin Ji, Halil Ali, Danula...|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/rmit-ir/walert.|https://doi.org/10.48550/arXiv.2401.07216|
|662|Weak-to-Strong Jailbreaking on Large Language Models|Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao‐Hai Du, Lei Li, Yu-Xiang Wang, William Yang Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/XuandongZhao/weak-to-strong|https://doi.org/10.48550/arXiv.2401.17256|
|663|XRec: Large Language Models for Explainable Recommendation|Qiyao Ma, Xubin Ren, Chao Huang|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/HKUDS/XRec.|https://doi.org/10.18653/v1/2024.findings-emnlp.22|
|664|YuLan: An Open-source Large Language Model|Yutao Zhu, Kun Zhou, Kelong Mao, Wentong Chen, Yiding Sun, Zhipeng Chen, Qian Cao, Yihan Wu, Yushuo Chen, Feng Wang, Lei...|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/RUC-GSAI/YuLan-Chat.|https://doi.org/10.48550/arXiv.2406.19853|
|665|Zero and Few Short Learning Using Large Language Models for De-Identification of Medical Records|Yarasam Yashwanth, Rajashree Shettar|<li><span style="color:#FF5733;">2024-01-01</span></li>|IEEE Access|https://github.com/YashwanthYS/De-Identification-of-medical-Records.|https://doi.org/10.1109/ACCESS.2024.3439680|
|666|Zero-shot Model-based Reinforcement Learning using Large Language Models|Abdelhakim Benechehab, Youssef Attia El Hili, Ambroise Odonnat, Oussama Zekri, Albert Thomas, Giuseppe Paolo, Maurizio F...|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/abenechehab/dicl.|https://openreview.net/forum?id=uZFXpPrwSh|
|667|u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model|Jinjin Xu, Liwu Xu, Yuzhe Yang, Xiang Li, Fanyi Wang, Yanchun Xie, Yi-Jie Huang, Yaqian Li|<li><span style="color:#FF5733;">2024-01-01</span></li>|Frontiers in artificial intelligence and applications|https://github.com/OPPOMKLab/u-LLaVA.|https://doi.org/10.48550/arXiv.2311.05348|
|668|video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models|Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, Chao Zhang|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICML|https://github.com/bytedance/SALMONN|https://openreview.net/forum?id=nYsh5GFIqX|
|669|TechGPT-2.0: A large language model project to solve the task of knowledge graph construction|Jiaqi Wang, Yuying Chang, Zhong Li, Ning An, Ma Qi, Lei Hei, Haibo Luo, Yifei Lu, Feiliang Ren|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/neukg/TechGPT-2.0|https://doi.org/10.48550/arXiv.2401.04507|
|670|Teaching Large Language Models to Translate with Comparison|Jiali Zeng, Fandong Meng, Yongjing Yin, Jie Zhou|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/lemon0830/TIM.|https://doi.org/10.1609/aaai.v38i17.29920|
|671|Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study|Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, Dongmei Zhang|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/microsoft/TableProvider|https://doi.org/10.1145/3616855.3635752|
|672|Synergistic Interplay between Search and Large Language Models for Information Retrieval|Jiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen, Can Xu, Guodong Long, Dongyan Zhao, Daxin Jiang|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/Cyril-JZ/InteR|https://doi.org/10.18653/v1/2024.acl-long.517|
|673|R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models|Shangqing Tu, Yuanchun Wang, Jifan Yu, Yuyang Xie, Yaran Shi, Xiaozhi Wang, Jing Zhang, Lei Hou, Juanzi Li|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/THU-KEG/R-Eval|https://doi.org/10.48550/arXiv.2406.11681|
|674|RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems|Jianxun Lian, Yuxuan Lei, Xu Huang, Jing Yao, Wei Xu, Xing Xie|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/microsoft/RecAI|https://doi.org/10.48550/arXiv.2403.06465|
|675|RecExplainer: Aligning Large Language Models for Explaining Recommendation Models|Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, Xing Xie|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/microsoft/RecAI.|https://doi.org/10.1145/3637528.3671802|
|676|Refusing Safe Prompts for Multi-modal Large Language Models|Zedian Shao, Hongbin Liu, Yuepeng Hu, Neil Zhenqiang Gong|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/Sadcardation/MLLM-Refusal.|https://doi.org/10.48550/arXiv.2407.09050|
|677|Rethinking the Development of Large Language Models from the Causal Perspective: A Legal Text Prediction Case Study|Haotian Chen, Lingwei Zhang, Yiran Liu, Yang Yu|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/Carrot-Red/Rethink-LLM-development.|https://doi.org/10.1609/aaai.v38i19.30086|
|678|RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning|Junjie Ye, Yilong Wu, Songyang Gao, Caishuang Huang, Sixian Li, Guanyu Li, Xiaoran Fan, Qi Zhang, Tao Gui, Xuanjing Huan...|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/Junjie-Ye/RoTBench.|https://doi.org/10.18653/v1/2024.emnlp-main.19|
|679|Robust and Scalable Model Editing for Large Language Models|Yingfa Chen, Zhengyan Zhang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Chen Chen, Kuai Li, Tao Yang, Maosong Sun|<li><span style="color:#FF5733;">2024-01-01</span></li>|LREC/COLING|https://github.com/thunlp/EREN.|https://aclanthology.org/2024.lrec-main.1235|
|680|ST-LLM: Large Language Models Are Effective Temporal Learners|Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, Ge Li|<li><span style="color:#FF5733;">2024-01-01</span></li>|Lecture notes in computer science|https://github.com/TencentARC/ST-LLM.|https://doi.org/10.1007/978-3-031-72998-0_1|
|681|Safety of Multimodal Large Language Models on Images and Text|Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/isXinLiu/Awesome-MLLM-Safety|https://www.ijcai.org/proceedings/2024/901|
|682|SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions|Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jingyu Tang, Minlie Huang|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/thu-coai/SafetyBench.|https://doi.org/10.18653/v1/2024.acl-long.830|
|683|Scaling Sparse Fine-Tuning to Large Language Models|Alan Ansell, Ivan Vulić, Hannah Sterz, Anna Korhonen, Edoardo Maria Ponti|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/AlanAnsell/peft|https://doi.org/10.48550/arXiv.2401.16405|
|684|ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models|Yash Akhauri, Ahmed F. AbouElhamayed, Jordan Dotzel, Zhiru Zhang, Alexander M. Rush, Safeen Huda, Mohamed S. Abdelfattah|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/abdelfattah-lab/shadow_llm|https://doi.org/10.18653/v1/2024.emnlp-main.1068|
|685|Show, Don&apos;t Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay|Gonçalo Hora de Carvalho, Robert Pollice, Oscar Knap|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/child-play-neurips/child-play|https://doi.org/10.48550/arXiv.2407.11068|
|686|SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model|Yang Zhan, Zhitong Xiong, Yuan Yuan|<li><span style="color:#FF5733;">2024-01-01</span></li>|ISPRS Journal of Photogrammetry and Remote Sensing|https://github.com/ZhanYang-nwpu/SkyEyeGPT.|https://doi.org/10.1016/j.isprsjprs.2025.01.020|
|687|SliceGPT: Compress Large Language Models by Deleting Rows and Columns|Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari Do Nascimento, Torsten Hoefler, James Hensman|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICLR|https://github.com/microsoft/TransformerCompression|https://openreview.net/forum?id=vXxardq6db|
|688|Smart Expert System: Large Language Models as Text Classifiers|Zhiqiang Wang, Yiran Pang, Yanbin Lin|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/yeyimilk/llm-zero-shot-classifiers.|https://doi.org/10.48550/arXiv.2405.10523|
|689|Limited Out-of-Context Knowledge Reasoning in Large Language Models|Peng Hu, Changjiang Gao, Ruiqi Gao, Jiajun Chen, Shujian Huang|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/NJUNLP/ID-OCKR.|https://doi.org/10.48550/arXiv.2406.07393|
|690|Soft Prompting for Unlearning in Large Language Models|Karuna Bhaila, Minh-Hao Van, Xintao Wu|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/karuna-bhaila/llm_unlearning|https://doi.org/10.18653/v1/2025.naacl-long.204|
|691|Sparsity-Accelerated Training for Large Language Models|Da Ma, Lu Chen, Pengyu Wang, Hongshen Xu, Hanqi Li, Liangtai Sun, Su Zhu, Shuai Fan, Kai Yu|<li><span style="color:#FF5733;">2024-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/OpenDFM/SAT.|https://doi.org/10.18653/v1/2024.findings-acl.875|
|692|SpecInfer: Accelerating Generative Large Language Model Serving with Speculative Inference and Token Tree Verification|Xupeng Miao, G. Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, C...|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/flexflow/FlexFlow|https://doi.org/10.1145/3620666.3651335|
|693|Struc-Bench: Are Large Language Models Good at Generating Complex Structured Tabular Data?|Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, Mark Gerstein|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/gersteinlab/Struc-Bench.|https://doi.org/10.18653/v1/2024.naacl-short.2|
|694|Linearizing Large Language Models|Jean Mercat, Igor Vasiljevic, Sedrick Scott Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/TRI-ML/linear_open_lm.|https://doi.org/10.48550/arXiv.2405.06640|
|695|ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences|Yuanhe Tian, Ruyi Gan, Song Yan, Jiaxing Zhang, Yongdong Zhang|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/synlp/ChiMed-GPT.|https://doi.org/10.18653/v1/2024.acl-long.386|
|696|Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models|Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/OpenNLPLab/lightning-attention.|https://doi.org/10.48550/arXiv.2401.04658|
|697|Empowering Segmentation Ability to Multi-modal Large Language Models|Yuqi Yang, Peng-Tao Jiang, Jing Wang, Hao Zhang, Kai Zhao, Jinwei Chen, Bo Li|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/YuqiYang213/LLaVASeg.|https://doi.org/10.48550/arXiv.2403.14141|
|698|CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning|Weiqi Wang, Tianqing Fang, Chunyang Li, Haochen Shi, Wenxuan Ding, Baixuan Xu, Zhaowei Wang, Jiaxin Bai, Xin Liu, Cheng ...|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/HKUST-KnowComp/CANDLE.|https://doi.org/10.18653/v1/2024.acl-long.128|
|699|Can Large Language Models Identify Authorship?|Baixiang Huang, Canyu Chen, Kai Shu|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/baixianghuang/authorship-llm.|https://doi.org/10.18653/v1/2024.findings-emnlp.26|
|700|Can Large Language Models Understand Real-World Complex Instructions?|Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Lida Chen, Xintao Wang, Yuncheng Huang, ...|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/Abbey4799/CELLO.|https://doi.org/10.1609/aaai.v38i16.29777|
|701|Can large language models understand uncommon meanings of common words?|Jinyang Wu, Feihu Che, Xinxin Zheng, Shuai Zhang, Ruihan Jin, Shuai Nie, Pengpeng Shao, Jianhua Tao|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/jinyangwu/LeSC.|https://doi.org/10.48550/arXiv.2405.05741|
|702|Characteristic AI Agents via Large Language Models|Xi Wang, Hongliang Dai, Shen Gao, Piji Li|<li><span style="color:#FF5733;">2024-01-01</span></li>|LREC/COLING|https://github.com/nuaa-nlp/Character100.|https://aclanthology.org/2024.lrec-main.269|
|703|Characterizing Large Language Model Geometry Helps Solve Toxicity Detection and Generation|Randall Balestriero, Romain Cosentino, Sarath Shekkizhar|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICML|https://github.com/RandallBalestriero/SplineLLM|https://openreview.net/forum?id=glfcwSsks8|
|704|ChatVis: Automating Scientific Visualization with a Large Language Model|Tanwi Mallick, Orçun Yildiz, David E. Lenz, Tom Peterka|<li><span style="color:#FF5733;">2024-01-01</span></li>|SC Workshops|https://github.com/tanwimallick/ChatVis|https://doi.org/10.1109/SCW63240.2024.00014|
|705|Citekit: A Modular Toolkit for Large Language Model Citation Generation|Jiajun Shen, Tong Zhou, Suifeng Zhao, Yubo Chen, Kang Liu|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/SjJ1017/Citekit.|https://doi.org/10.48550/arXiv.2408.04662|
|706|ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model|Zhiwei Liu, Boyang Liu, Paul Thompson, Kailai Yang, Raghav Jain, Sophia Ananiadou|<li><span style="color:#FF5733;">2024-01-01</span></li>|Frontiers in artificial intelligence and applications|https://github.com/lzw108/ConspEmoLLM|https://doi.org/10.48550/arXiv.2403.06765|
|707|ConstraintChecker : a plugin for large language models to reason on commonsense knowledge bases|Quyet V. Do, Tianqing Fang, Shizhe Diao, Zhaowei Wang, Yangqiu Song|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/HKUST-KnowComp/ConstraintChecker|https://doi.org/10.14711/thesis-991013340455703412|
|708|Continual Learning of Large Language Models: A Comprehensive Survey|Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, Zifeng Wang, Sayna Ebrahimi, Hao Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|ACM Computing Surveys|https://github.com/Wang-ML-Lab/llm-continual-learning-survey.|https://doi.org/10.48550/arXiv.2404.16789|
|709|Datasets for Large Language Models: A Comprehensive Survey|Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, Lianwen Jin|<li><span style="color:#FF5733;">2024-01-01</span></li>|Research Square (Research Square)|https://github.com/lmmlzn/Awesome-LLMs-Datasets.|https://doi.org/10.21203/rs.3.rs-3996137/v1|
|710|Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization|Zhexin Zhang, Junxiao Yang, Pei Ke, Fei Mi, Hongning Wang, Minlie Huang|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/thu-coai/JailbreakDefense_GoalPriority|https://doi.org/10.18653/v1/2024.acl-long.481|
|711|Detoxifying Large Language Models via Knowledge Editing|Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun C...|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/zjunlp/EasyEdit.|https://doi.org/10.18653/v1/2024.acl-long.171|
|712|Developing Safe and Responsible Large Language Models -- A Comprehensive
  Framework|Shaina Raza, Oluwanifemi Bamgbose, Shardul Ghuge, Fatemeh Tavakoli, Deepak John Reji|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/shainarazavi/Safe-Responsible-LLM|https://doi.org/10.48550/arXiv.2404.01399|
|713|DiJiang: Efficient Large Language Models through Compact Kernelization|Hanting Chen, Liuzhi Cheng, Xutao Wang, Yuchuan Tian, Yunhe Wang|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICML|https://github.com/YuchuanTian/DiJiang.|https://openreview.net/forum?id=0uUHfhXdnH|
|714|Divide and Conquer for Large Language Models Reasoning|Zijie Meng, Yan Zhang, Zhaopeng Feng, Yang Feng, Gaoang Wang, Joey Tianyi Zhou, Jian Wu, Zuozhu Liu|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/AiMijie/Divide-and-Conquer|https://doi.org/10.48550/arXiv.2401.05190|
|715|Do Large Language Models Understand Conversational Implicature -- A case
  study with a chinese sitcom|Shisen Yue, Siyuan Song, Xinyuan Cheng, Hai Hu|<li><span style="color:#FF5733;">2024-01-01</span></li>|Lecture notes in computer science|https://github.com/sjtu-compling/llm-pragmatics.|https://doi.org/10.1007/978-981-97-8367-0_24|
|716|DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models (Exemplified as A Video Agent)|Zongxin Yang, Guikun Chen, Xiaodi Li, Wenguan Wang, Yi Yang|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/z-x-yang/DoraemonGPT.|https://openreview.net/forum?id=QMy2RLnxGN|
|717|DriveLLM: Charting the Path Toward Full Autonomous Driving With Large Language Models|Yaodong Cui, Shucheng Huang, Jiaming Zhong, Zhenan Liu, Yutong Wang, Chen Sun, Bai Li, Xiao Wang, Amir Khajepour|<li><span style="color:#FF5733;">2024-01-01</span></li>|IEEE Transactions on Intelligent Vehicles|https://github.com/DriveLLM/DriveLLM|https://doi.org/10.1109/TIV.2023.3327715|
|718|DrugAssist: A Large Language Model for Molecule Optimization|Geyan Ye, Xibao Cai, Houtim Lai, Xing Wang, Junhong Huang, Longyue Wang, Wei Liu, Xiangxiang Zeng|<li><span style="color:#FF5733;">2024-01-01</span></li>|Briefings in Bioinformatics|https://github.com/blazerye/DrugAssist|https://doi.org/10.48550/arXiv.2401.10334|
|719|EconNLI: Evaluating Large Language Models on Economics Reasoning|Yue Guo, Yi Yang|<li><span style="color:#FF5733;">2024-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/Irenehere/EconNLI.|https://doi.org/10.18653/v1/2024.findings-acl.58|
|720|EmoBench: Evaluating the Emotional Intelligence of Large Language Models|Sahand Sabour, Siyang Liu, Zheyuan Zhang, June Liu, Jinfeng Zhou, Alvionna S. Sunaryo, Tatia Lee, Rada Mihalcea, Minlie ...|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/Sahandfer/EmoBench.|https://doi.org/10.18653/v1/2024.acl-long.326|
|721|BioMedGPT: An Open Multimodal Large Language Model for BioMedicine|Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Massimo Hong, Yushuai Wu, Mu Qiao, Zaiqing Nie|<li><span style="color:#FF5733;">2024-01-01</span></li>|IEEE Journal of Biomedical and Health Informatics|https://github.com/PharMolix/OpenBioMed.|https://doi.org/10.1109/jbhi.2024.3505955|
|722|BioCoder: a benchmark for bioinformatics code generation with large language models|Xiangru Tang, Bill Qian, Rick Gao, Jiakang Chen, Xinyun Chen, Mark B. Gerstein|<li><span style="color:#FF5733;">2024-01-01</span></li>|Bioinformatics|https://github.com/gersteinlab/biocoder|https://doi.org/10.1093/bioinformatics/btae230|
|723|Beyond Text: Frozen Large Language Models in Visual Signal Comprehension|Lei Zhu, Fangyun Wei, Yanye Lu|<li><span style="color:#FF5733;">2024-01-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/zh460045050/V2L-Tokenizer.|https://doi.org/10.1109/CVPR52733.2024.02554|
|724|AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception|Yipo Huang, Quan Yuan, Xiangfei Sheng, Zhichao Yang, Haoning Wu, Pengfei Chen, Yuzhe Yang, Leida Li, Weisi Lin|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/yipoh/AesBench.|https://doi.org/10.48550/arXiv.2401.08276|
|725|A Closer Look into Mixture-of-Experts in Large Language Models|Ka Man Lo, Zeyu Huang, Zihan Qiu, Zili Wang, Jie Fu|<li><span style="color:#FF5733;">2024-01-01</span></li>|Findings of the Association for Computational Linguistics: NAACL 2022|https://github.com/kamanphoebe/Look-into-MoEs.|https://doi.org/10.18653/v1/2025.findings-naacl.251|
|726|Libra: Building Decoupled Vision System on Large Language Models|Yifan Xu, Xiaoshan Yang, Yaguang Song, Changsheng Xu|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICML|https://github.com/YifanXu74/Libra.|https://openreview.net/forum?id=F1drhMjN7s|
|727|A Dataset for Large Language Model-Driven AI Accelerator Generation|Mahmoud Nazzal, Deepak Vungarala, Mehrdad Morsali, Chao Zhang, Arnob Ghosh, Abdallah Khreishah, Shaahin Angizi|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/ACADLab/SA-DS.|https://doi.org/10.48550/arXiv.2404.10875|
|728|A Review of Large Language Models and Autonomous Agents in Chemistry|Mayk Caldas Ramos, Christopher J. Collison, Andrew Dickson White|<li><span style="color:#FF5733;">2024-01-01</span></li>|Chemical Science|https://github.com/ur-whitelab/LLMs-in-science.|https://doi.org/10.48550/arXiv.2407.01603|
|729|A Survey of Large Language Models for Graphs|Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh V. Chawla, Chao Huang|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/HKUDS/Awesome-LLM4Graph-Papers.|https://doi.org/10.48550/arXiv.2405.08011|
|730|A Survey on Knowledge Distillation of Large Language Models|Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, Tianyi Zhou|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/Tebmer/Awesome-Knowledge-Distillation-of-LLMs.|https://doi.org/10.48550/arXiv.2402.13116|
|731|A Survey on Large Language Model-Based Social Agents in Game-Theoretic
  Scenarios|Sihao Hu, Tiansheng Huang, Fatih İlhan, Selim Furkan Tekin, Gaowen Liu, Ramana Rao Kompella, Ling Liu|<li><span style="color:#FF5733;">2024-01-01</span></li>|Trans. Mach. Learn. Res.|https://github.com/git-disl/awesome-LLM-game-agent-papers.|https://openreview.net/forum?id=CsoSWpR5xC|
|732|A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models|Jiayin Wang, Fengran Mo, Weizhi Ma, Peijie Sun, Min Zhang, Jian-Yun Nie|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/Alice1998/URS.|https://doi.org/10.18653/v1/2024.emnlp-main.210|
|733|Active Prompting with Chain-of-Thought for Large Language Models|Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, Tong Zhang|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/shizhediao/active-prompt.|https://doi.org/10.18653/v1/2024.acl-long.73|
|734|Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts|Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICLR|https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict.|https://openreview.net/forum?id=auKAUJZMO6|
|735|AlignBench: Benchmarking Chinese Alignment of Large Language Models|Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yi Huang, Andrew Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiao...|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/THUDM/AlignBench|https://doi.org/10.18653/v1/2024.acl-long.624|
|736|Benchmarking Large Language Models on Controllable Generation under Diversified Instructions|Yihan Chen, Benfeng Xu, Quan Wang, Yi Liu, Zhendong Mao|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/Xt-cyh/CoDI-Eval.|https://doi.org/10.1609/aaai.v38i16.29734|
|737|Aligning Large Language Models by On-Policy Self-Judgment|Sangkyu Lee, Sungdong Kim, Ashkan Yousefpour, Minjoon Seo, Kang Min Yoo, Youngjae Yu|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/oddqueue/self-judge.|https://doi.org/10.18653/v1/2024.acl-long.617|
|738|An Unforgeable Publicly Verifiable Watermark for Large Language Models|Aiwei Liu, Leyi Pan, Xuming Hu, Shuang Li, Lijie Wen, Irwin King, Philip S. Yu|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICLR|https://github.com/THU-BPM/unforgeable_watermark|https://openreview.net/forum?id=gMLQwKDY3N|
|739|Arcee&apos;s MergeKit: A Toolkit for Merging Large Language Models|Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, Jaco...|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/arcee-ai/MergeKit.|https://doi.org/10.18653/v1/2024.emnlp-industry.36|
|740|Are Large Language Models Good at Utility Judgments?|Hengran Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/ict-bigdatalab/utility_judgments|https://doi.org/10.48550/arXiv.2403.19216|
|741|AugSumm: Towards Generalizable Speech Summarization Using Synthetic Labels from Large Language Models|Jee-weon Jung, Roshan S. Sharma, William Chen, Bhiksha Raj, Shinji Watanabe|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/Jungjee/AugSumm.|https://doi.org/10.1109/ICASSP48485.2024.10447328|
|742|AutoFlow: Automated Workflow Generation for Large Language Model Agents|Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, Yongfeng Zhang|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/agiresearch/AutoFlow.|https://doi.org/10.48550/arXiv.2407.12821|
|743|BIBench: Benchmarking Data Analysis Knowledge of Large Language Models|Liu Shu, Shangqing Zhao, Chenghao Jia, Xinlin Zhuang, Zhaoguang Long, Man Lan|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/cubenlp/BIBench|https://doi.org/10.48550/arXiv.2401.02982|
|744|BeHonest: Benchmarking Honesty of Large Language Models|Steffi Chern, Zhulin Hu, Yuqing Yang, Ethan Chern, Yuan Guo, Jiahe Jin, Binjie Wang, Pengfei Liu|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/GAIR-NLP/BeHonest|https://doi.org/10.48550/arXiv.2406.13261|
|745|Benchmarking Large Language Models for Molecule Prediction Tasks|Zhiqiang Zhong, Kuangyu Zhou, Davide Mottin|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/zhiqiangzhongddu/LLMaMol.|https://doi.org/10.48550/arXiv.2403.05075|
|746|Benchmarking Large Language Models on CFLUE -- A Chinese Financial
  Language Understanding Evaluation Dataset|Jie Zhu, Junhui Li, Yalong Wen, Lifan Guo|<li><span style="color:#FF5733;">2024-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/aliyun/cflue.|https://doi.org/10.18653/v1/2024.findings-acl.337|
|747|EmoLLMs: A Series of Emotional Large Language Models and Annotation Tools for Comprehensive Affective Analysis|Zhiwei Liu, Kailai Yang, Tianlin Zhang, Qianqian Xie, Zeping Yu, Sophia Ananiadou|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/lzw108/EmoLLMs|https://doi.org/10.48550/arXiv.2401.08508|
|748|CheX-GPT: Harnessing Large Language Models for Enhanced Chest X-ray Report Labeling|Jawook Gu, Han-Cheol Cho, Jiho Kim, Kihyun You, Eun Kyoung Hong, Byungseok Roh|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/kakaobrain/CheXGPT.|https://doi.org/10.48550/arXiv.2401.11505|
|749|Enabling action crossmodality for a pretrained large language model|Anton Caesar, Ozan Özdemir, Cornelius Weber, Stefan Wermter|<li><span style="color:#FF5733;">2024-01-01</span></li>|Natural Language Processing Journal|https://github.com/samsoneko/CrossT5.|https://doi.org/10.1016/j.nlp.2024.100072|
|750|LLM4EDA: Emerging Progress in Large Language Models for Electronic Design Automation|Ruizhe Zhong, Xingbo Du, Shixiong Kai, Zhentao Tang, Siyuan Xu, Hui-Ling Zhen, Jianye Hao, Qiang Xu, Mingxuan Yuan, Junc...|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/Thinklab-SJTU/Awesome-LLM4EDA.|https://doi.org/10.48550/arXiv.2401.12224|
|751|Information Re-Organization Improves Reasoning in Large Language Models|Xiaoxia Cheng, Zeqi Tan, Wei Xue, Weiming Lu|<li><span style="color:#FF5733;">2024-01-01</span></li>|NeurIPS|https://github.com/hustcxx/InfoRE.|http://papers.nips.cc/paper_files/paper/2024/hash/eb1a323fa10d4102ff13422476a744ff-Abstract-Conference.html|
|752|Instruct Large Language Models to Drive like Humans|Ruijun Zhang, Xianda Guo, Wenzhao Zheng, Chenming Zhang, Kurt Keutzer, Long Chen|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/bonbon-rj/InstructDriver.|https://doi.org/10.48550/arXiv.2406.07296|
|753|Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender|Yuqi Zhang, Liang Ding, Lefei Zhang, Dacheng Tao|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/alphadl/SafeLLM_with_IntentionAnalysis.|https://doi.org/10.48550/arXiv.2401.06561|
|754|Interpretable Online Log Analysis Using Large Language Models with Prompt Strategies|Yilun Liu, Shimin Tao, Weibin Meng, Jingyu Wang, Wenbing Ma, Yuhang Chen, Yanqing Zhao, Hao Yang, Yanfei Jiang|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/lunyiliu/LogPrompt.|https://doi.org/10.1145/3643916.3644408|
|755|Item-side Fairness of Large Language Model-based Recommendation System|Meng Jiang, Keqin Bao, Jizhi Zhang, Wenjie Wang, Zhengyi Yang, Fuli Feng, Xiangnan He|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the ACM Web Conference 2022|https://github.com/JiangM-C/IFairLRS.git.|https://doi.org/10.48550/arXiv.2402.15215|
|756|Knowledge Fusion of Large Language Models|Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, Shuming Shi|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICLR|https://github.com/fanqiwan/FuseLLM|https://openreview.net/forum?id=jiDsk12qcz|
|757|Knowledge-Aware Code Generation with Large Language Models|Tao Huang, Zhihong Sun, Zhi Jin, Ge Li, Chen Lyu|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/CodeGeneration3/KareCoder.|https://doi.org/10.48550/arXiv.2401.15940|
|758|LFED: A Literary Fiction Evaluation Dataset for Large Language Models|Linhao Yu, Qun Liu, Deyi Xiong|<li><span style="color:#FF5733;">2024-01-01</span></li>|LREC/COLING|https://github.com/tjunlp-lab/LFED.git|https://aclanthology.org/2024.lrec-main.915|
|759|LLASP: Fine-tuning Large Language Models for Answer Set Programming|Erica Coppolillo, Francesco Calimeri, Giuseppe Manco, Simona Perri, Francesco Ricca|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/EricaCoppolillo/LLASP.|https://doi.org/10.48550/arXiv.2407.18723|
|760|LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning|Junchi Wang, Lei Ke|<li><span style="color:#FF5733;">2024-01-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)|https://github.com/wangjunchi/LLMSeg.|https://doi.org/10.1109/CVPRW63382.2024.00183|
|761|LLMBox: A Comprehensive Library for Large Language Models|Tianyi Tang, Yiwen Hu, Bingqian Li, Wenyang Luo, Zijing Qin, Haoxiang Sun, Jiapeng Wang, Shiyi Xu, Xiaoxue Cheng, Geyang...|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/RUCAIBox/LLMBox.|https://doi.org/10.18653/v1/2024.acl-demos.37|
|762|In-context Autoencoder for Context Compression in a Large Language Model|Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, Furu Wei|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenReview|https://github.com/getao/icae.|https://openreview.nethttps://arxiv.org/pdf/2307.06945.pdf|
|763|LLaMo: Large Language Model-based Molecular Graph Assistant|Jinyoung Park, Minseong Bae, Dohwan Ko, Hyunwoo J. Kim|<li><span style="color:#FF5733;">2024-01-01</span></li>|NeurIPS|https://github.com/mlvlab/LLaMo.|http://papers.nips.cc/paper_files/paper/2024/hash/ee46288ab2aaf5c6e53aebebe719712c-Abstract-Conference.html|
|764|LMDrive: Closed-Loop End-to-End Driving with Large Language Models|Hao Shao, Yuxuan Hu, Letian Wang, Guanglu Song, Steven L. Waslander, Yu Liu, Hongsheng Li|<li><span style="color:#FF5733;">2024-01-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/opendilab/LMDrive|https://doi.org/10.1109/CVPR52733.2024.01432|
|765|Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents|Zihao Zhou, Bin Hu, Chenyang Zhao, Pu Zhang, Bin Liu|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/ZJLAB-AMMI/LLM4Teach.|https://www.ijcai.org/proceedings/2024/627|
|766|Large Language Model for Verilog Generation with Golden Code Feedback|Ning Wang, Bingkun Yao, Jie Zhou, Xi Wang, Zhe Jiang, Nan Guan|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/CatIIIIIIII/veriseek|https://doi.org/10.48550/arXiv.2407.18271|
|767|Large Language Models are Contrastive Reasoners|Liang Yao|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/yao8839836/cp|https://doi.org/10.48550/arXiv.2403.08211|
|768|Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment|Keming Lu, Bowen Yu, Chang Zhou, Jingren Zhou|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/OFA-Sys/Ditto.|https://doi.org/10.18653/v1/2024.acl-long.423|
|769|Large Language Models as Evaluators for Recommendation Explanations|Xiaoyu Zhang, Yishan Li, Jiayin Wang, Bowen Sun, Weizhi Ma, Peijie Sun, Min Zhang|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/Xiaoyu-SZ/LLMasEvaluator.|https://doi.org/10.48550/arXiv.2406.03248|
|770|Large Language Models for Data Annotation and Synthesis: A Survey|Zhen Tan, Dawei Li, Song Wang, Alimohammad Beigi, Bohan Jiang, Amrita Bhattacharjee, Mansooreh Karami, Jundong Li, Cheng...|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/Zhen-Tan-dmml/LLM4Annotation.git|https://doi.org/10.18653/v1/2024.emnlp-main.54|
|771|Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate|Liang Tian, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming Shi, Zhaopeng Tu|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/Skytliang/Multi-Agents-Debate|https://doi.org/10.18653/v1/2024.emnlp-main.992|
|772|Learning to Poison Large Language Models During Instruction Tuning|Yao Qiang, Xiangyu Zhou, Saleh Zare Zade, Mohammad Amin Roshani, Douglas Zytko, Dongxiao Zhu|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/RookieZxy/GBTL|https://doi.org/10.48550/arXiv.2402.13459|
|773|Inference Performance Optimization for Large Language Models on CPUs|Pujiang He, Shan Zhou, Wenhuan Huang, Changqing Li, Duyi Wang, Bin Guo, Chen Meng, Sheng Gui, Weifei Yu, Yi Xie|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/intel/xFasterTransformer.|https://doi.org/10.48550/arXiv.2407.07304|
|774|Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models|Ran Xu, Hejie Cui, Yue Yu, Xuan Kan, Wenqi Shi, Yuchen Zhuang, May Dongmei Wang, Wei Jin, Joyce C. Ho, Carl Yang|<li><span style="color:#FF5733;">2024-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/ritaranx/ClinGen|https://doi.org/10.18653/v1/2024.findings-acl.916|
|775|Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint|Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Junchen Wan, Fuzheng Zhang, Di Zhang, Ji-Rong Wen|<li><span style="color:#FF5733;">2024-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/RUCAIBox/RLMEC|https://doi.org/10.18653/v1/2024.findings-acl.338|
|776|Evaluation of Large Language Models on Code Obfuscation (Student Abstract)|Adrian Swindle, Derrick McNealy, Giri P. Krishnan, Ramyaa Ramyaa|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/SwindleA/LLMCodeObfuscation.|https://doi.org/10.1609/aaai.v38i21.30517|
|777|INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning|Yutao Zhu, Peitian Zhang, Chenghao Zhang, Yifei Chen, Binyu Xie, Zheng Liu, Ji-Rong Wen, Zhicheng Dou|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/DaoD/INTERS|https://doi.org/10.18653/v1/2024.acl-long.154|
|778|Evolving Subnetwork Training for Large Language Models|Hanqi Li, Lu Chen, Da Ma, Zijian Wu, Su Zhu, Kai Yu|<li><span style="color:#FF5733;">2024-01-01</span></li>|ICML|https://github.com/OpenDFM/EST.|https://openreview.net/forum?id=DbMm8pmoAP|
|779|Exploring Advanced Large Language Models with LLMsuite|Giorgio Roffo|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/giorgioroffo/large_language_models_open_suite|https://doi.org/10.48550/arXiv.2407.12036|
|780|Exploring Equation as a Better Intermediate Meaning Representation for Numerical Reasoning of Large Language Models|Dingzirui Wang, Longxu Dou, Wenbin Zhang, Junyu Zeng, Wanxiang Che|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/zirui-HIT/Bridge_for_Numerical_Reasoning|https://doi.org/10.1609/aaai.v38i17.29879|
|781|Exploring Model Kinship for Merging Large Language Models|Yedi Hu, Yunzhi Yao, Ningyu Zhang, Shumin Deng, Huajun Chen|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/zjunlp/ModelKinship.|https://doi.org/10.48550/arXiv.2410.12613|
|782|FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design|Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhua...|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/usyd-fsalab/fp6_llm.|https://doi.org/10.48550/arXiv.2401.14112|
|783|FactAlign: Long-form Factuality Alignment of Large Language Models|Chao-Wei Huang, Yun-Nung Chen|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/MiuLab/FactAlign|https://doi.org/10.18653/v1/2024.findings-emnlp.955|
|784|Fairness in Large Language Models in Three Hours|Thang Viet Doan, Zichong Wang, Nhat Nguyen Minh Hoang, Wenbin Zhang|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/LavinWong/Fairness-in-Large-Language-Models|https://doi.org/10.48550/arXiv.2408.00992|
|785|Fast and Effective Weight Update for Pruned Large Language Models|Vladimír Boza|<li><span style="color:#FF5733;">2024-01-01</span></li>|Trans. Mach. Learn. Res.|https://github.com/fmfi-compbio/admm-pruning.|https://openreview.net/forum?id=1hcpXd9Jir|
|786|Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models|Shi Lin, Rongchang Li, Xun Wang, Changting Lin, Wenpeng Xing, Meng Han|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/theshi-1128/ABJ-Attack.|https://doi.org/10.48550/arXiv.2407.16205|
|787|Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models|Raphael Tang, Crystina Zhang, Xueguang Ma, Jimmy Lin, Ferhan Türe|<li><span style="color:#FF5733;">2024-01-01</span></li>|NAACL-HLT|https://github.com/castorini/perm-sc.|https://doi.org/10.18653/v1/2024.naacl-long.129|
|788|From Static to Dynamic: Knowledge Metabolism for Large Language Models|Mingzhe Du, Anh Tuan Luu, Bin Ji, See-Kiong Ng|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/Elfsong/DynaMind.|https://doi.org/10.1609/aaai.v38i21.30564|
|789|From Supervised to Generative: A Novel Paradigm for Tabular Deep Learning with Large Language Models|Xumeng Wen, Han Zhang, Shun Zheng, Wei Xu, Jiang Bian|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/microsoft/Industrial-Foundation-Models.|https://doi.org/10.1145/3637528.3671975|
|790|Entity Alignment with Noisy Annotations from Large Language Models|Shengyuan Chen, Qinggang Zhang, Junnan Dong, Wen Hua, Qing Li, Xiao Huang|<li><span style="color:#FF5733;">2024-01-01</span></li>|NeurIPS|https://github.com/chensyCN/llm4ea_official.|http://papers.nips.cc/paper_files/paper/2024/hash/1b57aaddf85ab01a2445a79c9edc1f4b-Abstract-Conference.html|
|791|GATSol, an enhanced predictor of protein solubility through the synergy of 3D structure graph and large language modeling|Bin Li, Dengming Ming|<li><span style="color:#FF5733;">2024-01-01</span></li>|BMC Bioinformatics|https://github.com/binbinbinv/GATSol|https://doi.org/10.1186/s12859-024-05820-8|
|792|GLBench: A Comprehensive Benchmark for Graph with Large Language Models|Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng Cai, Wai Kin (Victor) Chan, Jia Li|<li><span style="color:#FF5733;">2024-01-01</span></li>|NeurIPS|https://github.com/NineAbyss/GLBench.|http://papers.nips.cc/paper_files/paper/2024/hash/4ab0bd666d034fcaa5566fc7d176daa6-Abstract-Datasets_and_Benchmarks_Track.html|
|793|GraphEdit: Large Language Models for Graph Structure Learning|Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang Pang, Tat-Seng Chua, Chao Huang|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/HKUDS/GraphEdit.|https://doi.org/10.48550/arXiv.2402.15183|
|794|Hallucination of Multimodal Large Language Models: A Survey|Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, Mike Zheng Shou|<li><span style="color:#FF5733;">2024-01-01</span></li>|arXiv|https://github.com/showlab/Awesome-MLLM-Hallucination.|https://doi.org/10.48550/arXiv.2404.18930|
|795|Having Beer after Prayer? Measuring Cultural Bias in Large Language Models|Tarek Naous, Michael J. Ryan, Alan Ritter, Wei Xu|<li><span style="color:#FF5733;">2024-01-01</span></li>|OpenAlex|https://github.com/tareknaous/camel|https://doi.org/10.18653/v1/2024.acl-long.862|
|796|ICE-Score: Instructing Large Language Models to Evaluate Code|Terry Yue Zhuo|<li><span style="color:#FF5733;">2024-01-01</span></li>|EACL|https://github.com/terryyz/ice-score|https://aclanthology.org/2024.findings-eacl.148|
|797|ICLEval: Evaluating In-Context Learning Ability of Large Language Models|Wen‐Tong Chen, Yankai Lin, ZhenHao Zhou, HongYun Huang, Yantao Jia, Zhao Cao, Ji-Rong Wen|<li><span style="color:#FF5733;">2024-01-01</span></li>|COLING|https://github.com/yiye3/ICLEval.|https://aclanthology.org/2025.coling-main.693/|
|798|Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection|Zekun Li, Baolin Peng, Pengcheng He, Xifeng Yan|<li><span style="color:#FF5733;">2024-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/Leezekun/instruction-following-robustness-eval|https://doi.org/10.18653/v1/2024.emnlp-main.33|
|799|Lost in Translation: A Study of Bugs Introduced by Large Language Models while Translating Code|Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele Merler, Boris Sobolev, Ra...|<li><span style="color:#FF5733;">2023-12-31</span></li>|Zenodo (CERN European Organization for Nuclear Research)|https://github.com/Intelligent-CAT-Lab/PLTranslationEmpirical|https://zenodo.org/doi/10.5281/zenodo.10447705|
|800|A Comparison of a Large Language Model vs Manual Chart Review for the Extraction of Data Elements From the Electronic Health Record|Jin Ge, Michael Li, Molly Delk, Jennifer C. Lai|<li><span style="color:#FF5733;">2023-12-25</span></li>|Gastroenterology|https://github.com/zhanghao-njmu/openapiDate|https://doi.org/10.1053/j.gastro.2023.12.019|
|801|LncPNdeep: A long non-coding RNA classifier based on Large Language Model with peptide and nucleotide embedding|Zongrui Dai, F. Deng|<li><span style="color:#FF5733;">2023-12-01</span></li>|bioRxiv (Cold Spring Harbor Laboratory)|https://github.com/yatoka233/LncPNdeep|https://doi.org/10.1101/2023.11.29.569323|
|802|Zero-shot Bilingual App Reviews Mining with Large Language Models|Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Binbin Xu, Pierre Louis Bernard, Gérard Dray|<li><span style="color:#FF5733;">2023-11-06</span></li>|OpenAlex|https://github.com/Jl-wei/mini-bar|https://doi.org/10.1109/ictai59109.2023.00135|
|803|Development of meta-prompts for Large Language Models to screen titles and abstracts for diagnostic test accuracy reviews|Yuki Kataoka, Ryuhei So, Masahiro Banno, Junji Kumasawa, Hidehiro Someko, Shunsuke Taito, Teruhiko Terasawa, Yasushi Tsu...|<li><span style="color:#FF5733;">2023-11-01</span></li>|medRxiv (Cold Spring Harbor Laboratory)|https://github.com/youkiti/ARE|https://doi.org/10.1101/2023.10.31.23297818|
|804|The Unequal Opportunities of Large Language Models: Examining Demographic Biases in Job Recommendations by ChatGPT and LLaMA|Abel Salinas, Parth Vipul Shah, Yuzhong Huang, Robert McCormack, Fred Morstatter|<li><span style="color:#FF5733;">2023-10-29</span></li>|OpenAlex|https://github.com/Abel2Code/Unequal-Opportunities-of-LLMs.|https://doi.org/10.1145/3617694.3623257|
|805|Using fine-tuned large language models to parse clinical notes in musculoskeletal pain disorders|Akhil Vaid, Isotta Landi, Girish N. Nadkarni, Ismail Nabeel|<li><span style="color:#FF5733;">2023-10-27</span></li>|The Lancet Digital Health|https://github.com/tatsu-lab/stanford_alpacaDate|https://doi.org/10.1016/s2589-7500(23)00202-9|
|806|SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models|Shanshan Zhong, Zhongzhan Huang, Weushao Wen, Jinghui Qin, Liang Lin|<li><span style="color:#FF5733;">2023-10-26</span></li>|ACM Multimedia|https://github.com/Qrange-group/SUR-adapter.|https://doi.org/10.48550/arXiv.2305.05189|
|807|A Comparative Evaluation on Melody Generation of Large Language Models|Kenta Suzuki, Jinyu Cai, Jialong Li, Takuto Yamauchi, Kenji Tei|<li><span style="color:#FF5733;">2023-10-23</span></li>|2022 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)|https://github.com/545659928/LLMMelody|https://doi.org/10.1109/icce-asia59966.2023.10326362|
|808|Efficient Memory Management for Large Language Model Serving with PagedAttention|Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoic...|<li><span style="color:#FF5733;">2023-10-03</span></li>|OpenAlex|https://github.com/vllm-project/vllm.|https://doi.org/10.48550/arXiv.2309.06180|
|809|Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition|Huy Ha, Pete Florence, Shuran Song|<li><span style="color:#FF5733;">2023-08-31</span></li>|CoRL 2023 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/scaling-up-and-distilling-down-language/code)|https://openreview.net/pdf/fc651c57b63f8554c325df33fa6f7f830d0d84d8.pdf|
|810|REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction|Zeyi Liu, Arpit Bahety, Shuran Song|<li><span style="color:#FF5733;">2023-08-31</span></li>|CoRL 2023 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/reflect-summarizing-robot-experiences-for/code)|https://openreview.net/pdf/55b4404e7bac9f0d8a2b022f42f708f2921aef16.pdf|
|811|Language-Guided Traffic Simulation via Scene-Level Diffusion|Ziyuan Zhong, Davis Rempe, Yuxiao Chen, Boris Ivanovic, Yulong Cao, Danfei Xu, Marco Pavone, Baishakhi Ray|<li><span style="color:#FF5733;">2023-08-31</span></li>|CoRL 2023 Oral|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/language-guided-traffic-simulation-via-scene/code)|https://openreview.net/pdf/99aaef8ada0a858b727eb6ecd6880e451efa156c.pdf|
|812|Language Conditioned Traffic Generation|Shuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone, Philipp Kraehenbuehl|<li><span style="color:#FF5733;">2023-08-31</span></li>|CoRL 2023 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/language-conditioned-traffic-generation/code)|https://openreview.net/pdf/34c183e547c1870a787c9b43b4bcf7ea496f0281.pdf|
|813|A Survey on Large Language Models: Applications, Challenges, Limitations, and Practical Usage|Muhammad Usman Hadi, qasem al tashi, Rizwan Qureshi, Abbas Shah, Amgad Muneer, Muhammad Irfan, Anas Zafar, Muhammad Bila...|<li><span style="color:#FF5733;">2023-07-10</span></li>|OpenAlex|https://github.com/anas-zafar/LLM-Survey|https://doi.org/10.36227/techrxiv.23589741.v1|
|814|Energy-based Models are Zero-Shot Planners for Compositional Scene Rearrangement|Nikolaos Gkanatsios, Ayush Jain, Zhou Xian, Yunchu Zhang, Christopher G Atkeson, Katerina Fragkiadaki|<li><span style="color:#FF5733;">2023-06-13</span></li>|RSS-23 LTAMP Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/energy-based-models-are-zero-shot-planners/code)|https://openreview.net/pdf/b1f6170bbe5a4558b2805e4d264a4063d8492cf1.pdf|
|815|Outlier Suppression+: Accurate quantization of large language models by
  equivalent and optimal shifting and scaling|Xiuying Wei, Yuncheng Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong Liu|<li><span style="color:#FF5733;">2023-04-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/ModelTC/Outlier_Suppression_Plus.|https://doi.org/10.18653/v1/2023.emnlp-main.102|
|816|Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam
  Detection|Maxime Labonne, Sean J. Moran|<li><span style="color:#FF5733;">2023-04-01</span></li>|arXiv|https://github.com/jpmorganchase/emailspamdetection.|https://doi.org/10.48550/arXiv.2304.01238|
|817|Pythia: A Suite for Analyzing Large Language Models Across Training and
  Scaling|Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, S...|<li><span style="color:#FF5733;">2023-04-01</span></li>|arXiv|https://github.com/EleutherAI/pythia|https://proceedings.mlr.press/v202/biderman23a.html|
|818|ParroT: Translating during Chat using Large Language Models tuned with
  Human Translation and Feedback|Wenxiang Jiao, Jen-tse Huang, Wenxuan Wang, Zhiwei He, Tian Liang, Xing Wang, Shuming Shi, Zhaopeng Tu|<li><span style="color:#FF5733;">2023-04-01</span></li>|OpenAlex|https://github.com/wxjiao/ParroT.|https://doi.org/10.18653/v1/2023.findings-emnlp.1001|
|819|LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of
  Large Language Models|Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, Roy Ka-Wei Lee|<li><span style="color:#FF5733;">2023-04-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/AGI-Edgerunners/LLM-Adapters.|https://doi.org/10.18653/v1/2023.emnlp-main.319|
|820|OpenAGI: When LLM Meets Domain Experts|Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang|<li><span style="color:#FF5733;">2023-04-01</span></li>|arXiv|https://github.com/agiresearch/OpenAGI.|http://arxiv.org/abs/2304.04370v6|
|821|Low-code LLM: Graphical User Interface over Large Language Models|Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Zehua Wang, Yaobo Liang, Tao Ge, Chenfei Wu, Wang You, Ting Song, Yan Xia, Jonatha...|<li><span style="color:#FF5733;">2023-04-01</span></li>|OpenAlex|https://github.com/moymix/TaskMatrix|https://doi.org/10.18653/v1/2024.naacl-demo.2|
|822|Multilingual Machine Translation with Large Language Models: Empirical
  Results and Analysis|Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Shujian Huang, Lingpeng Kong, Jiajun Chen, Lei Li|<li><span style="color:#FF5733;">2023-04-01</span></li>|Findings of the Association for Computational Linguistics: NAACL 2022|https://github.com/NJUNLP/MMT-LLM.|https://doi.org/10.18653/v1/2024.findings-naacl.176|
|823|A Survey for Biomedical Text Summarization: From Pre-trained to Large
  Language Models|Qianqian Xie, Zheheng Luo, Benyou Wang, Sophia Ananiadou|<li><span style="color:#FF5733;">2023-04-01</span></li>|arXiv (Cornell University)|https://github.com/KenZLuo/Biomedical-Text-Summarization-Survey|http://arxiv.org/abs/2304.08763v2|
|824|Does Human Collaboration Enhance the Accuracy of Identifying
  LLM-Generated Deepfake Texts?|Adaku Uchendu, Jooyoung Lee, Hua Shen, Thai Le, Ting-Hao 'Kenneth' Huang, Dongwon Lee|<li><span style="color:#FF5733;">2023-04-01</span></li>|arXiv|https://github.com/huashen218/llm-deepfake-human-study.git|http://arxiv.org/abs/2304.01002v3|
|825|GeneGPT: Augmenting Large Language Models with Domain Tools for Improved
  Access to Biomedical Information|Qiao Jin, Yifan Yang, Qingyu Chen, Zhiyong Lu|<li><span style="color:#FF5733;">2023-04-01</span></li>|Bioinformatics, 2024|https://github.com/ncbi/GeneGPT.|https://doi.org/10.1093/bioinformatics/btae075|
|826|Epigenomic Language Models Powered By Cerebras|Meredith Trotter, Cuong Quoc Nguyen, Stephen Young, Rob Woodruff, Kim Branson|<li><span style="color:#FF5733;">2023-03-06</span></li>|ICLR 2023 - MLDD Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/epigenomic-language-models-powered-by/code)|https://openreview.net/pdf/debceadfb2eab0de22ad367a6d835591de748500.pdf|
|827|Z-ICL: Zero-Shot In-Context Learning with Pseudo-Demonstrations|Xinxi Lyu, Sewon Min, Iz Beltagy, Luke Zettlemoyer, Hannaneh Hajishirzi|<li><span style="color:#FF5733;">2023-03-05</span></li>|ME-FoMo 2023 Poster|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/z-icl-zero-shot-in-context-learning-with/code)|https://openreview.net/pdf/5b1e023ca167c48edf529788e3d37576b69ad37e.pdf|
|828|Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the
  Question Answering Performance of the GPT LLM Family|Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, Guilin Qi|<li><span style="color:#FF5733;">2023-03-01</span></li>|arXiv|https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family.git|http://arxiv.org/abs/2303.07992v3|
|829|Cost-Effective Hyperparameter Optimization for Large Language Model
  Generation Inference|Chi Wang, Susan Xueqing Liu, Ahmed Hassan Awadallah|<li><span style="color:#FF5733;">2023-03-01</span></li>|AutoML 2023 MainTrack|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/cost-effective-hyperparameter-optimization/code)|https://openreview.net/pdf/e7c20057a629eea9988f07aec4b40d5d7d73a898.pdf|
|830|DERA: Enhancing Large Language Model Completions with Dialog-Enabled
  Resolving Agents|Varun Nair, Elliot Schumacher, Geoffrey J. Tso, Anitha Kannan|<li><span style="color:#FF5733;">2023-03-01</span></li>|ClinicalNLP@NAACL|https://github.com/curai/curai-research|https://doi.org/10.18653/v1/2024.clinicalnlp-1.12|
|831|Error Analysis Prompting Enables Human-Like Translation Evaluation in Large Language Models: A Case Study on ChatGPT|Qingyu Lu, Baopu Qiu, Liang Ding, Kanjian Zhang, Tom Kocmi, Dacheng Tao|<li><span style="color:#FF5733;">2023-03-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/Coldmist-Lu/ErrorAnalysis_Prompt.|https://doi.org/10.18653/v1/2024.findings-acl.520|
|832|FlexGen: High-Throughput Generative Inference of Large Language Models
  with a Single GPU|Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett...|<li><span style="color:#FF5733;">2023-03-01</span></li>|ICML|https://github.com/FMInference/FlexGen|https://proceedings.mlr.press/v202/sheng23a.html|
|833|Large Language Models Know Your Contextual Search Intent: A Prompting
  Framework for Conversational Search|Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou, Haonan Chen, Hongjin Qian|<li><span style="color:#FF5733;">2023-03-01</span></li>|OpenAlex|https://github.com/kyriemao/LLM4CS|https://doi.org/10.18653/v1/2023.findings-emnlp.86|
|834|Reward Design with Language Models|Minae Kwon, Sang Michael Xie, Kalesha Bullard, Dorsa Sadigh|<li><span style="color:#FF5733;">2023-02-02</span></li>|ICLR 2023 poster|[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/reward-design-with-language-models/code)|https://openreview.net/pdf/696171827b35dfe4e639dfe0644bf0f279f84c75.pdf|
|835|Explaining Patterns in Data  with  Language Models via Interpretable Autoprompting|Chandan Singh, John Xavier Morris, Jyoti Aneja, Alexander M Rush, Jianfeng Gao|<li><span style="color:#FF5733;">2023-02-02</span></li>|Submitted to ICLR 2023|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/explaining-patterns-in-data-with-language/code)|https://openreview.net/pdf/2607a064389bdde9d47bd61f38b952fb58cb49c2.pdf|
|836|Language models are multilingual chain-of-thought reasoners|Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastia...|<li><span style="color:#FF5733;">2023-02-02</span></li>|ICLR 2023 poster|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/language-models-are-multilingual-chain-of/code)|https://openreview.net/pdf/972d6eaf77336eece16b7ec5bdb9565b06423b8a.pdf|
|837|Binding Language Models in Symbolic Languages|Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendor...|<li><span style="color:#FF5733;">2023-02-02</span></li>|ICLR 2023 notable top 25%|[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/binding-language-models-in-symbolic-languages/code)|https://openreview.net/pdf/d226e827fb59bcd4253c7eb8ce07d339ef5d519d.pdf|
|838|Describe, Explain, Plan and Select: Interactive Planning with Large
  Language Models Enables Open-World Multi-Task Agents|Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang|<li><span style="color:#FF5733;">2023-02-01</span></li>|arXiv|https://github.com/CraftJarvis/MC-Planner.|https://doi.org/10.48550/arXiv.2302.01560|
|839|MarioGPT: Open-Ended Text2Level Generation through Large Language Models|Shyam Sudhakaran, Miguel González Duque, Claire Glanois, Matthias Freiberger, Elias Najarro, Sebastian Risi|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/shyamsn97/mario-gpt.|http://papers.nips.cc/paper_files/paper/2023/hash/a9bbeb2858dfbdbd4c19814e5d80ec60-Abstract-Conference.html|
|840|MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models|Yilin Wen, Zifeng Wang, Jimeng Sun|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/wyl-willing/MindMap.|https://doi.org/10.18653/v1/2024.acl-long.558|
|841|MindGames: Targeting Theory of Mind in Large Language Models with Dynamic Epistemic Modal Logic|Damien Sileo, Antoine Lernould|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/sileod/llm-theory-of-mind|https://doi.org/10.18653/v1/2023.findings-emnlp.303|
|842|MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use|Yi Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, L...|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/HowieHwong/MetaTool.|https://openreview.net/forum?id=R0c2qtalgG|
|843|MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning|Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, Mark Gerstein|<li><span style="color:#FF5733;">2023-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/gersteinlab/MedAgents|https://doi.org/10.18653/v1/2024.findings-acl.33|
|844|MiniChain: A Small Library for Coding with Large Language Models|Alexander M. Rush|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/srush/MiniChain|https://doi.org/10.18653/v1/2023.emnlp-demo.27|
|845|MathAttack: Attacking Large Language Models Towards Math Solving Ability|Zihao Zhou, Qiufeng Wang, Mingyu Jin, Jie Yao, Jianan Ye, Wei Liu, Wei Wang, Xiaowei Huang, Kaizhu Huang|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/zhouzihao501/MathAttack.|https://doi.org/10.48550/arXiv.2309.01686|
|846|Massive Editing for Large Language Models via Meta Learning|Chenmien Tan, Ge Zhang, Jie Fu|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/ChenmienTan/malmen.|https://openreview.net/forum?id=L6L1CJQ2PE|
|847|LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models|Jianxin Yang|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/yangjianxin1/LongQLoRA.|https://doi.org/10.48550/arXiv.2311.04879|
|848|Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning|Zhuo Huang, Chang Liu, Yinpeng Dong, Hang Su, Shibao Zheng, Tongliang Liu|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/tmllab/Machine_Vision_Therapy.|https://openreview.net/forum?id=LwOfVWgEzS|
|849|Machine Mindset: An MBTI Exploration of Large Language Models|Jiaxi Cui, Liuzhenghao Lv, Jing Wen, Rongsheng Wang, Jing Tang, Yonghong Tian, Li Yuan|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/PKU-YuanGroup/Machine-Mindset|https://doi.org/10.48550/arXiv.2312.12999|
|850|MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models|Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Lin Xu, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng,...|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models|https://doi.org/10.48550/arXiv.2306.13394|
|851|MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks|Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, Yuqing Yang|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/microsoft/CoML.|https://doi.org/10.18653/v1/2024.eacl-long.179|
|852|MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion|Pengyue Jia, Yiding Liu, Xiangyu Zhao, Xiaopeng Li, Changying Hao, Shuaiqiang Wang, Dawei Yin|<li><span style="color:#FF5733;">2023-01-01</span></li>|NAACL-HLT|https://github.com/Applied-Machine-Learning-Lab/MILL|https://doi.org/10.18653/v1/2024.naacl-long.138|
|853|M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models|Chuang Liu, Renren Jin, Yuqi Ren, Linhao Yu, Tianyu Dong, Xiaohan Peng, Shuting Zhang, Jianxiang Peng, Peiyi Zhang, Qing...|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/tjunlp-lab/M3KE.|https://doi.org/10.48550/arXiv.2305.10263|
|854|M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models|Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, Lidong Bing|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/DAMO-NLP-SG/M3Exam|http://papers.nips.cc/paper_files/paper/2023/hash/117c5c8622b0d539f74f6d1fb082a2e9-Abstract-Datasets_and_Benchmarks.html|
|855|ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models|Chenliang Li, He Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Ch...|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/modelscope/modelscope-agent|https://doi.org/10.18653/v1/2023.emnlp-demo.51|
|856|Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning|Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/teacherpeterpan/Logic-LLM.|https://doi.org/10.18653/v1/2023.findings-emnlp.248|
|857|LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery|Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, Luming Liang|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/microsoft/lorashear.|https://doi.org/10.48550/arXiv.2310.18356|
|858|MoTCoder: Elevating Large Language Models with Modular of Thought for Challenging Programming Tasks|Jingyao Li, Pengguang Chen, Jiaya Jia|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/dvlab-research/MoTCoder.|https://doi.org/10.48550/arXiv.2312.15960|
|859|OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation|Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu|<li><span style="color:#FF5733;">2023-01-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/shikiw/OPERA.|https://doi.org/10.1109/CVPR52733.2024.01274|
|860|MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model|Le Zhang, Yihong Wu, Fengran Mo, Jian-Yun Nie, Aishwarya Agrawal|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/lezhang7/MOQAGPT.|https://doi.org/10.18653/v1/2023.findings-emnlp.85|
|861|Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking|Shengyao Zhuang, Bing Liu, Bevan Koopman, Guido Zuccon|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/ielab/llm-qlm.|https://doi.org/10.18653/v1/2023.findings-emnlp.590|
|862|Lion: Adversarial Distillation of Proprietary Large Language Models|Yuxin Jiang, Chunkit Chan, Mingyang Chen, Wei Wang|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/YJiangcm/Lion.|https://doi.org/10.18653/v1/2023.emnlp-main.189|
|863|Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models|Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, Ee-Peng Lim|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting.|https://doi.org/10.18653/v1/2023.acl-long.147|
|864|Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging|Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, ...|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/joeljang/RLPHF.|https://doi.org/10.48550/arXiv.2310.11564|
|865|Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models|Hritik Bansal, John Dang, Aditya Grover|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/Hritikbansal/sparse_feedback.|https://openreview.net/forum?id=dKl6lMwbCy|
|866|Parallel Context Windows for Large Language Models|Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Br...|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/ai21labs/parallel-context-windows.|https://doi.org/10.18653/v1/2023.acl-long.352|
|867|PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records|Viktor Schlegel, Hao Li, Yuping Wu, Anand Subramanian, Thanh-Tung Nguyen, Abhinav Ramesh Kashyap, Daniel Beck, Xiao-Jun ...|<li><span style="color:#FF5733;">2023-01-01</span></li>|CLEF|https://github.com/yuping-wu/PULSAR.|https://ceur-ws.org/Vol-3497/paper-138.pdf|
|868|PEER: Empowering Writing with Large Language Models|Kathrin Seßler, Tao Xiang, Lukas Bogenrieder, Enkelejda Kasneci|<li><span style="color:#FF5733;">2023-01-01</span></li>|Lecture notes in computer science|https://github.com/Kasneci-Lab/AI-assisted-writing|https://doi.org/10.1007/978-3-031-42682-7_73|
|869|PB-LLM: Partially Binarized Large Language Models|Yuzhang Shang, Zhihang Yuan, Qiang Wu, Zhen Dong|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/hahnyuan/BinaryLLM.|https://openreview.net/forum?id=BifeBRhikU|
|870|OptiMUS: Optimization Modeling Using MIP Solvers and large language models|Ali AhmadiTeshnizi, Wenzhi Gao, Madeleine Udell|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/teshnizi/OptiMUS|https://doi.org/10.48550/arXiv.2310.06116|
|871|OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models|Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping Luo|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/OpenGVLab/OmniQuant|https://openreview.net/forum?id=8Wuvhh0LYW|
|872|Multi-Label Classification of COVID-Tweets Using Large Language Models|Aniket Deroy, Subhankar Maity|<li><span style="color:#FF5733;">2023-01-01</span></li>|FIRE|https://github.com/anonmous1981/AISOME|https://doi.org/10.48550/arXiv.2312.10748|
|873|Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback|Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen-Tran, Franck Dernoncourt, Ryan A. Rossi, Thien Huu Nguyen|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/nlp-uoregon/Okapi.|https://doi.org/10.18653/v1/2023.emnlp-demo.28|
|874|OceanGPT: A Large Language Model for Ocean Science Tasks|Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, Huajun Chen|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/zjunlp/KnowLM.|https://doi.org/10.18653/v1/2024.acl-long.184|
|875|OWQ: Lessons learned from activation outliers for weight quantization in large language models|Chang-Hun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok Park|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/xvyaward/owq|https://doi.org/10.48550/arXiv.2306.02272|
|876|Negated Complementary Commonsense using Large Language Models|Navid Rezaei, Marek Z. Reformat|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/navidre/negated_complementary_commonsense.|https://doi.org/10.48550/arXiv.2307.06794|
|877|NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models|Gengze Zhou, Yicong Hong, Qi Wu|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/GengzeZhou/NavGPT.|https://doi.org/10.48550/arXiv.2305.16986|
|878|Natural Language Dataset Generation Framework for Visualizations Powered by Large Language Models|Hyung-Kwon Ko, Hyeon Jeon, Gwanmo Park, Dae Hyun Kim, Nam Wook Kim, Juho Kim, Jinwook Seo|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/hyungkwonko/chart-llm.|https://doi.org/10.48550/arXiv.2309.10245|
|879|Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model|Mingxin Li, Richong Zhang, Zhijie Nie, Yongyi Mao|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/BDBC-KG-NLP/NGCSE.|https://doi.org/10.48550/arXiv.2309.06453|
|880|NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes|Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, Yongfeng Zhang, Libby Hemphill|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/casmlab/NPHardEval.|https://doi.org/10.18653/v1/2024.acl-long.225|
|881|Multilingual Jailbreak Challenges in Large Language Models|Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs|https://openreview.net/forum?id=vESNKdEMGp|
|882|LitSumm: Large language models for literature summarisation of non-coding RNAs|Andrew Green, Carlos Eduardo Ribas, Nancy Ontiveros-Palacios, Anton I. Petrov, Alex Bateman, Blake A. Sweeney|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/RNAcentral/litscan-summarization|https://doi.org/10.48550/arXiv.2311.03056|
|883|LLM4Drive: A Survey of Large Language Models for Autonomous Driving|Zhenjie Yang, Xiaosong Jia, Hongyang Li, Junchi Yan|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/Thinklab-SJTU/Awesome-LLM4AD.|https://doi.org/10.48550/arXiv.2311.01043|
|884|Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding|Mu Cai, Zeyi Huang, Yuheng Li, Haohan Wang, Yong Jae Lee|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/mu-cai/svg-llm.|https://doi.org/10.48550/arXiv.2306.06094|
|885|LLaRA: Aligning Large Language Models with Sequential Recommenders|Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, Xiang Wang, Xiangnan He|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/ljy0ustc/LLaRA.|https://doi.org/10.48550/arXiv.2312.02445|
|886|Large Language Models Need Holistically Thought in Medical Conversational QA|Yixuan Weng, Bin Li, Fei Xia, Minjun Zhu, Bin Sun, Shizhu He, Kang Liu, Jun Zhao|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/WENGSYX/HoT.|https://doi.org/10.48550/arXiv.2305.05410|
|887|Large Language Models Are Partially Primed in Pronoun Interpretation|Suet-Ying Lam, Qingcheng Zeng, Kexun Zhang, Chenyu You, Rob Voigt|<li><span style="color:#FF5733;">2023-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/zkx06111/llm_priming.|https://doi.org/10.18653/v1/2023.findings-acl.605|
|888|Large Language Models Are Latent Variable Models: Explaining and Finding
  Good Demonstrations for In-Context Learning|Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/WANGXinyiLinda/concept-based-demonstration-selection|http://papers.nips.cc/paper_files/paper/2023/hash/3255a7554605a88800f4e120b3a929e1-Abstract-Conference.html|
|889|Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives|Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Furkan Tekin, Ling Liu|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/git-disl/GPTLens.|https://doi.org/10.1109/TPS-ISA58951.2023.00044|
|890|Large Language Model is a Good Policy Teacher for Training Reinforcement Learning Agents|Zihao Zhou, Bin Hu, Pu Zhang, Chenyang Zhao, Bin Liu|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/ZJLAB-AMMI/LLM4Teach.|https://doi.org/10.48550/arXiv.2311.13373|
|891|Large Language Model for Multi-objective Evolutionary Optimization|Fei Liu, Xi Lin, Zhenkun Wang, Shunyu Yao, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/FeiLiu36/LLM4MOEA.|https://doi.org/10.48550/arXiv.2310.12541|
|892|Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias|Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander J. Ratner, Ranjay Krishna, Jiaming Shen, Chao Zhang|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/yueyu1030/AttrPrompt|http://papers.nips.cc/paper_files/paper/2023/hash/ae9500c4f5607caf2eff033c67daa9d7-Abstract-Datasets_and_Benchmarks.html|
|893|Large Language Model Guided Tree-of-Thought|Jieyi Long|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/jieyilong/tree-of-thought-puzzle-solver|https://doi.org/10.48550/arXiv.2305.08291|
|894|Large Language Model Can Interpret Latent Space of Sequential Recommender|Zhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang Wang, Xiangnan He|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenReview|https://github.com/YangZhengyi98/RecInterpreter.|https://openreview.net/pdf/2ba4543fcd486f7bb6ab0d2a7b7f9457aa1ff682.pdf|
|895|LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models|Yanwei Li, Chengyao Wang, Jiaya Jia|<li><span style="color:#FF5733;">2023-01-01</span></li>|Lecture notes in computer science|https://github.com/dvlab-research/LLaMA-VID|https://doi.org/10.1007/978-3-031-72952-2_19|
|896|Learning to Retrieve In-Context Examples for Large Language Models|Liang Wang, Nan Yang, Furu Wei|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/microsoft/LMOps|https://doi.org/10.18653/v1/2024.eacl-long.105|
|897|LLMatic: Neural Architecture Search via Large Language Models and Quality Diversity Optimization|Muhammad Umair Nasir, Sam Earle, Julian Togelius, Steven James, Christopher W. Cleghorn|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the Genetic and Evolutionary Computation Conference|https://github.com/umair-nasir14/LLMatic.|https://doi.org/10.48550/arXiv.2306.01102|
|898|LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning|Yunlong Tang, Jinrui Zhang, Xiangchen Wang, Teng Wang, Feng Zheng|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/zjr2000/LLMVA-GEBC|https://doi.org/10.48550/arXiv.2306.10354|
|899|LLMRec: Large Language Models with Graph Augmentation for Recommendation|Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/HKUDS/LLMRec.git.|https://doi.org/10.48550/arXiv.2311.00423|
|900|LLMRec: Benchmarking Large Language Models on Recommendation Task|Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong, Kang Zhou, Yueqi Xie, Yuwei Cao, Shoujin Wang, Chenyu You, ...|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/williamliujl/LLMRec.|https://doi.org/10.48550/arXiv.2308.12241|
|901|LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language
  Models|Patrik Puchert, Poonam Poonam, Christian van Onzenoodt, Timo Ropinski|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/viscom-ulm/LLMMaps|https://doi.org/10.48550/arXiv.2304.00457|
|902|LLMDet: A Third Party Large Language Models Generated Text Detection Tool|Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, Tat-Seng Chua|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/TrustedLLM/LLMDet.|https://doi.org/10.18653/v1/2023.findings-emnlp.139|
|903|LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models|Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Chukwunyere Osi, Parteek Sharma, Fan Chen, Lei Jiang|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/SotaroKaneda/MLCarbon|https://openreview.net/forum?id=aIok3ZD9to|
|904|Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models|Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, Yang Liu|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/PVIT-official/PVIT.|https://doi.org/10.48550/arXiv.2308.13437|
|905|LLM-Pruner: On the Structural Pruning of Large Language Models|Xinyin Ma, Gongfan Fang, Xinchao Wang|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/horseee/LLM-Pruner|http://papers.nips.cc/paper_files/paper/2023/hash/44956951349095f74492a5471128a7e0-Abstract-Conference.html|
|906|Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts|Zihao Li, Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Matthew Shardlow, Goran Nenadic|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/HECTA-UoM/PLABA-MU|https://doi.org/10.48550/arXiv.2309.13202|
|907|Large Language Models are Better Reasoners with Self-Verification|Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/WENGSYX/Self-Verification.|https://doi.org/10.18653/v1/2023.findings-emnlp.167|
|908|Large Language Models are Built-in Autoregressive Search Engines|Noah Ziems, Wenhao Yu, Zhihan Zhang, Meng Jiang|<li><span style="color:#FF5733;">2023-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/Ziems/llm-url.|https://doi.org/10.18653/v1/2023.findings-acl.167|
|909|Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset|Saeid Alavi Naeini, Raeid Saqur, Mozhgan Saeidi, John M. Giorgi, Babak Taati|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/TaatiTeam/OCW.|http://papers.nips.cc/paper_files/paper/2023/hash/11e3e0f1b29dcd31bd0952bfc1357f68-Abstract-Datasets_and_Benchmarks.html|
|910|LayoutPrompter: Awaken the Design Ability of Large Language Models|Jiawei Lin, Jiaqi Guo, Shizhao Sun, Zijiang James Yang, Jian-Guang Lou, Dongmei Zhang|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/microsoft/LayoutGeneration|http://papers.nips.cc/paper_files/paper/2023/hash/88a129e44f25a571ae8b838057c46855-Abstract-Conference.html|
|911|LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models|Zecheng Tang, Chenfei Wu, Juntao Li, Nan Duan|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/ProjectNUWA/LayoutNUWA.|https://openreview.net/forum?id=qCUWVT0Ayy|
|912|LawBench: Benchmarking Legal Knowledge of Large Language Models|Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Alan Huang, Songyang Zhang, Kai Chen, Zhixin Yin, Zongwen Sh...|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/open-compass/LawBench|https://doi.org/10.18653/v1/2024.emnlp-main.452|
|913|Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models|Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, Zhenzhong Lan|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/qiuhuachuan/latent-jailbreak.|https://doi.org/10.48550/arXiv.2307.08487|
|914|Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations|Qingyu Chen, Jingcheng Du, Yan Hu, Vipina Kuttichi Keloth, Xueqing Peng, Kalpana Raja, Rui Zhang, Zhiyong Lu, Hua Xu|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/qingyu-qc/gpt_bionlp_benchmark.|https://doi.org/10.48550/arXiv.2305.16326|
|915|Large language models illuminate a progressive pathway to artificial intelligent healthcare assistant|Mingze Yuan, Peng Bao, Jiajia Yuan, Yunhao Shen, Zifan Chen, Yi Xie, Jie Zhao, Quanzheng Li, Yang Chen, Li Zhang, Lin Sh...|<li><span style="color:#FF5733;">2023-01-01</span></li>|Medicine Plus|https://github.com/mingze-yuan/Awesome-LLM-Healthcare|https://doi.org/10.1016/j.medp.2024.100030|
|916|Large Language Models on Graphs: A Comprehensive Survey|Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, Jiawei Han|<li><span style="color:#FF5733;">2023-01-01</span></li>|IEEE Transactions on Knowledge and Data Engineering|https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.|https://doi.org/10.48550/arXiv.2312.02783|
|917|Large Language Models for Software Engineering: A Systematic Literature Review|Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John C. Grundy, Haoyu Wang|<li><span style="color:#FF5733;">2023-01-01</span></li>|ACM Transactions on Software Engineering and Methodology|https://github.com/xinyi-hou/LLM4SE_SLR|https://doi.org/10.48550/arXiv.2308.10620|
|918|Large Language Models for Generative Information Extraction: A Survey|Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Yang Wang, Enhong Chen|<li><span style="color:#FF5733;">2023-01-01</span></li>|Frontiers of Computer Science|https://github.com/quqxui/Awesome-LLM4IE-Papers|https://doi.org/10.1007/s11704-024-40555-y|
|919|Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering|Noah Hollmann, Samuel Müller, Frank Hutter|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/automl/CAAFE|http://papers.nips.cc/paper_files/paper/2023/hash/8c2df4c35cdbee764ebb9e9d0acd5197-Abstract-Conference.html|
|920|Large Language Models can be Guided to Evade AI-Generated Text Detection|Ning Lu, Shengcai Liu, Rui He, Yew-Soon Ong, Qi Wang, Ke Tang|<li><span style="color:#FF5733;">2023-01-01</span></li>|Trans. Mach. Learn. Res.|https://github.com/ColinLu50/Evade-GPT-Detector.|https://openreview.net/forum?id=lLE0mWzUrr|
|921|Large Language Models as Optimizers|Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/google-deepmind/opro.|https://openreview.net/forum?id=Bb4VGOWELI|
|922|Large Language Models as Corporate Lobbyists|John J. Nay|<li><span style="color:#FF5733;">2023-01-01</span></li>|SSRN Electronic Journal|https://github.com/JohnNay/llm-lobbyist|https://doi.org/10.48550/arXiv.2301.01181|
|923|Large Language Models are not Fair Evaluators|Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, Zhifan...|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/i-Eval/FairEval|https://doi.org/10.18653/v1/2024.acl-long.511|
|924|Large Language Models are Zero-Shot Rankers for Recommender Systems|Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian J. McAuley, Wayne Xin Zhao|<li><span style="color:#FF5733;">2023-01-01</span></li>|Lecture notes in computer science|https://github.com/RUCAIBox/LLMRank.|https://doi.org/10.1007/978-3-031-56060-6_24|
|925|Large Language Models are Versatile Decomposers: Decompose Evidence and
  Questions for Table-based Reasoning|Yunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei Huang, Yongbin Li|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/AlibabaResearch/DAMO-ConvAI.|https://doi.org/10.48550/arXiv.2301.13808|
|926|Large Language Models are Temporal and Causal Reasoners for Video Question Answering|Dohwan Ko, Ji Soo Lee, Woo-Young Kang, Byungseok Roh, Hyunwoo J. Kim|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/mlvlab/Flipped-VQA.|https://doi.org/10.18653/v1/2023.emnlp-main.261|
|927|Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners|Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, Muhan Zhang|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/XiaojuanTang/ICSR|https://doi.org/10.48550/arXiv.2305.14825|
|928|Large Language Models are Good Prompt Learners for Low-Shot Image Classification|Zhaoheng Zheng, Jingmin Wei, Xuefeng Hu, Haidong Zhu, Ram Nevatia|<li><span style="color:#FF5733;">2023-01-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/zhaohengz/LLaMP.|https://doi.org/10.1109/CVPR52733.2024.02688|
|929|Planning with Large Language Models for Code Generation|Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, Chuang Gan|<li><span style="color:#FF5733;">2023-01-01</span></li>|FMDM@NeurIPS2022|[![CatalyzeX](/images/catalyzex_icon.svg) 5 code implementations](https://www.catalyzex.com/paper/planning-with-large-language-models-for-code/code)|https://openreview.net/pdf/df528a2091311eb1b37a22a38af8732c0466b36a.pdf|
|930|StructGPT: A General Framework for Large Language Model to Reason over Structured Data|Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, Ji-Rong Wen|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/RUCAIBox/StructGPT|https://doi.org/10.18653/v1/2023.emnlp-main.574|
|931|Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models|Fobo Shi, Peijun Qing, Dong Yang, Nan Wang, Youbo Lei, Haonan Lu, Xiaodong Lin, Duantengchuan Li|<li><span style="color:#FF5733;">2023-01-01</span></li>|Findings of the Association for Computational Linguistics: NAACL 2022|https://github.com/YouBLEI/Prompt-Space|https://doi.org/10.18653/v1/2024.findings-naacl.119|
|932|Towards Open-Ended Visual Recognition with Large Language Models|Qihang Yu, Xiaohui Shen, Liang-Chieh Chen|<li><span style="color:#FF5733;">2023-01-01</span></li>|Lecture notes in computer science|https://github.com/bytedance/OmniScient-Model.|https://doi.org/10.1007/978-3-031-72630-9_21|
|933|TurkishBERTweet: Fast and Reliable Large Language Model for Social Media Analysis|Ali Najafi, Onur Varol|<li><span style="color:#FF5733;">2023-01-01</span></li>|Expert Systems with Applications|https://github.com/ViralLab/TurkishBERTweet|https://doi.org/10.1016/j.eswa.2024.124737|
|934|Tuna: Instruction Tuning using Feedback from Large Language Models|Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, Furu Wei|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/microsoft/LMOps.|https://doi.org/10.18653/v1/2023.findings-emnlp.1011|
|935|TrojanedCM: A Repository of Trojaned Large Language Models of Code|Aftab Hussain, Md Rafiqul Islam Rabin, Mohammad Amin Alipour|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv (Cornell University)|https://github.com/UH-SERG/TrojanedCM.|https://arxiv.org/abs/2311.14850|
|936|TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models|Jiaqi Xue, Mengxin Zheng, Ting Hua, Yilin Shen, Yepeng Liu, Ladislau Bölöni, Qian Lou|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/UCF-ML-Research/TrojLLM.|http://papers.nips.cc/paper_files/paper/2023/hash/cf04d01a0e76f8b13095349d9caca033-Abstract-Conference.html|
|937|Tree of Thoughts: Deliberate Problem Solving with Large Language Models|Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, Karthik Narasimhan|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/princeton-nlp/tree-of-thought-llm.|http://papers.nips.cc/paper_files/paper/2023/hash/271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html|
|938|Tree of Clarifications: Answering Ambiguous Questions with Retrieval-Augmented Large Language Models|Gangwoo Kim, Sungdong Kim, Byeongguk Jeon, Joonsuk Park, Jaewoo Kang|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/gankim/tree-of-clarifications.|https://doi.org/10.18653/v1/2023.emnlp-main.63|
|939|TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer|Zhen Qin, Li Dong, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, Yu Q...|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv (Cornell University)|https://github.com/OpenNLPLab/TransnormerLLM.|https://arxiv.org/abs/2307.14995|
|940|TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety|Ou Zheng, Mohamed A. Abdel-Aty, Dongdong Wang, Chenzhu Wang, Shengxuan Ding|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/ozheng1993/TrafficSafetyGPT.|https://doi.org/10.48550/arXiv.2307.15311|
|941|Towards autonomous system: flexible modular production system enhanced with large language model agents|Yuchen Xia, Manthan Shenoy, Nasser Jazdi, Michael Weyrich|<li><span style="color:#FF5733;">2023-01-01</span></li>|2022 IEEE 27th International Conference on Emerging Technologies and Factory Automation (ETFA)|https://github.com/YuchenXia/GPT4IndustrialAutomation|https://doi.org/10.1109/ETFA54631.2023.10275362|
|942|Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond|Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Tianyu Liu, Baobao Chang|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/pkunlp-icler/PCA-EVAL|https://doi.org/10.48550/arXiv.2310.02071|
|943|Unveiling the Pitfalls of Knowledge Editing for Large Language Models|Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/zjunlp/PitfallsKnowledgeEditing.|https://openreview.net/forum?id=fNktD3ib16|
|944|Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models|Qingyu Tan, Hwee Tou Ng, Lidong Bing|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/DAMO-NLP-SG/TempReason.|https://doi.org/10.18653/v1/2023.acl-long.828|
|945|Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models|Ziqiao Ma, Jacob Sansom, Run Peng, Joyce Chai|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/Mars-tin/awesome-theory-of-mind|https://doi.org/10.18653/v1/2023.findings-emnlp.72|
|946|TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones|Zhengqing Yuan, Zhaoxu Li, Lichao Sun|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/DLYuanGod/TinyGPT-V|https://doi.org/10.48550/arXiv.2312.16862|
|947|TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models|Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Haotian Wang, Ming Liu, Bing Qin|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/zchuz/TimeBench|https://doi.org/10.18653/v1/2024.acl-long.66|
|948|The Rise and Potential of Large Language Model Based Agents: A Survey|Zhiheng Xi, Wen-Xiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Ru...|<li><span style="color:#FF5733;">2023-01-01</span></li>|Science China Information Sciences|https://github.com/WooooDyy/LLM-Agent-Paper-List.|https://doi.org/10.1007/s11432-024-4222-0|
|949|The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code|Xiao Liu, Da Yin, Chen Zhang, Yansong Feng, Dongyan Zhao|<li><span style="color:#FF5733;">2023-01-01</span></li>|Findings of the Association for Computational Linguistics: ACL 2022|https://github.com/xxxiaol/magic-if.|https://doi.org/10.18653/v1/2023.findings-acl.574|
|950|The Efficiency Spectrum of Large Language Models: An Algorithmic Survey|Tianyu Ding, Tianyi Chen, Haidong Zhu, J. Z. Jiang, Yiqi Zhong, Jin‐Xin Zhou, Guangzhi Wang, Zhihui Zhu, Ilya Zharkov, L...|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/tding1/Efficient-LLM-Survey|https://doi.org/10.48550/arXiv.2312.00678|
|951|TagGPT: Large Language Models are Zero-shot Multimodal Taggers|Chen Li, Yixiao Ge, Jiayong Mao, Dian Li, Ying Shan|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/TencentARC/TagGPT.|https://doi.org/10.48550/arXiv.2304.03022|
|952|TIM: Teaching Large Language Models to Translate with Comparison|Jiali Zeng, Fandong Meng, Yongjing Yin, Jie Zhou|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/lemon0830/TIM.|https://doi.org/10.48550/arXiv.2307.04408|
|953|Unveiling the Implicit Toxicity in Large Language Models|Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, Minlie Huang|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/thu-coai/Implicit-Toxicity.|https://doi.org/10.18653/v1/2023.emnlp-main.84|
|954|Use of Large Language Models for Stance Classification|Iain J. Cruickshank, Lynnette Hui Xian Ng|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/ijcruic/LLM-Stance-Labeling|https://doi.org/10.48550/arXiv.2309.13734|
|955|PromptBench: A Unified Library for Evaluation of Large Language Models|Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie|<li><span style="color:#FF5733;">2023-01-01</span></li>|J. Mach. Learn. Res.|https://github.com/microsoft/promptbench|https://doi.org/10.48550/arXiv.2312.07910|
|956|When Large Language Model based Agent Meets User Behavior Analysis: A Novel User Simulation Paradigm|Lei Wang, Jingsen Zhang, Xu Chen, Yankai Lin, Ruihua Song, Wayne Xin Zhao, Ji-Rong Wen|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv (Cornell University)|https://github.com/RUC-GSAI/YuLan-Rec|https://arxiv.org/abs/2306.02552|
|957|mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model|Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, Fei Huang|<li><span style="color:#FF5733;">2023-01-01</span></li>|ACM Multimedia|https://github.com/X-PLUG/mPLUG-DocOwl|https://doi.org/10.48550/arXiv.2311.18248|
|958|mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality|Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Che...|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/X-PLUG/mPLUG-Owl.|https://doi.org/10.48550/arXiv.2304.14178|
|959|mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding|Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qi...|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/X-PLUG/mPLUG-DocOwl.|https://doi.org/10.48550/arXiv.2307.02499|
|960|Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue|Songhua Yang, Hanjia Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, Hongying Zan|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/SupritYoung/Zhongjing.|https://doi.org/10.48550/arXiv.2308.03549|
|961|YaRN: Efficient Context Window Extension of Large Language Models|Bowen Peng, Jeffrey Quesnelle, Honglu Fan, Enrico Shippole|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/jquesnelle/yarn|https://openreview.net/forum?id=wHBfxhZu1u|
|962|X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models|Yixiong Chen, Li Liu, Chris Ding|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/Schuture/Benchmarking-Awesome-Diffusion-Models|https://doi.org/10.48550/arXiv.2305.10843|
|963|WizardLM: Empowering Large Language Models to Follow Complex Instructions|Can Xu, Qing‐Feng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Daxin Jiang|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/nlpxucan/WizardLM|https://doi.org/10.48550/arXiv.2304.12244|
|964|WizardCoder: Empowering Code Large Language Models with Evol-Instruct|Ziyang Luo, Can Xu, Pu Zhao, Qing‐Feng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/nlpxucan/WizardLM|https://openreview.net/forum?id=UnUwSIgK5W|
|965|Where Would I Go Next? Large Language Models as Human Mobility Predictors|Xinglei Wang, Meng Fang, Zichao Zeng, Tao Cheng|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/xlwang233/LLM-Mob.|https://doi.org/10.48550/arXiv.2308.15197|
|966|What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks|Taicheng Guo, Kehan Guo, Bozhao Nan, Zhengwen Liang, Zhichun Guo, Nitesh V. Chawla, Olaf Wiest, Xiangliang Zhang|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/ChemFoundationModels/ChemLLMBench.|http://papers.nips.cc/paper_files/paper/2023/hash/bbb330189ce02be00cf7346167028ab1-Abstract-Datasets_and_Benchmarks.html|
|967|User Modeling in the Era of Large Language Models: Current Research and Future Directions|Zhaoxuan Tan, Meng Jiang|<li><span style="color:#FF5733;">2023-01-01</span></li>|IEEE Data Eng. Bull.|https://github.com/TamSiuhin/LLM-UM-Reading|http://sites.computer.org/debull/A23dec/p57.pdf|
|968|WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models|Shangqing Tu, Yuliang Sun, Yushi Bai, Jifan Yu, Lei Hou, Juanzi Li|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/THU-KEG/WaterBench|https://doi.org/10.18653/v1/2024.acl-long.83|
|969|War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars|Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, Yongfeng Zhang|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/agiresearch/WarAgent|https://doi.org/10.48550/arXiv.2311.17227|
|970|Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models|Geewook Kim, Hodong Lee, Daehee Kim, Haeji Jung, Sanghee Park, Yoonsik Kim, Sangdoo Yun, Taeho Kil, Bado Lee, Seunghyun ...|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/naver-ai/cream|https://doi.org/10.18653/v1/2023.emnlp-main.735|
|971|VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks|Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Da...|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/OpenGVLab/InternGPT.|http://papers.nips.cc/paper_files/paper/2023/hash/c1f7b1ed763e9c75e4db74b49b76db5f-Abstract-Conference.html|
|972|VideoLLM: Modeling Video Sequence with Large Language Models|Chen Guo, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong Lü, Limin W...|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/cg1177/VideoLLM.|https://doi.org/10.48550/arXiv.2305.13292|
|973|Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models|Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, Li Yuan|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/PKU-YuanGroup/Video-Bench|https://doi.org/10.48550/arXiv.2311.16103|
|974|Video Understanding with Large Language Models: A Survey|Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, A...|<li><span style="color:#FF5733;">2023-01-01</span></li>|IEEE Transactions on Circuits and Systems for Video Technology|https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.|https://doi.org/10.1109/tcsvt.2025.3566695|
|975|VCoder: Versatile Vision Encoders for Multimodal Large Language Models|Jitesh Jain, Jianwei Yang, Humphrey Shi|<li><span style="color:#FF5733;">2023-01-01</span></li>|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/SHI-Labs/VCoder|https://doi.org/10.1109/CVPR52733.2024.02644|
|976|Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata|Bohui Zhang, Ioannis Reklos, Nitisha Jain, Albert Meroño-Peñuela, Elena Simperl|<li><span style="color:#FF5733;">2023-01-01</span></li>|KBC-LM/LM-KBC@ISWC|https://github.com/bohuizhang/LLMKE.|https://doi.org/10.48550/arXiv.2309.08491|
|977|TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation|Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/SAI990323/TALLRec.|https://doi.org/10.48550/arXiv.2305.00447|
|978|T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Mixed Large Language Model Signals for Science Question Answering|Lei Wang, Yi Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, Heng Tao Shen|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/T-SciQ/T-SciQ.|https://doi.org/10.48550/arXiv.2305.03453|
|979|SwitchGPT: Adapting Large Language Models for Non-Text Outputs|Xinyu Wang, Bohan Zhuang, Qi Wu|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/xinke-wang/SwitchGPT.|https://doi.org/10.48550/arXiv.2309.07623|
|980|RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models|Ronak Pradeep, Sahel Sharifymoghaddam, Jimmy Lin|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/castorini/rank_llm.|https://doi.org/10.48550/arXiv.2309.15088|
|981|RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting|Lei Shu, Liangchen Luo, Jayakumar Hoskere, Yun Zhu, Yinxiao Liu, Simon Tong, Jindong Chen, Lei Meng|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/google-research/google-research|https://doi.org/10.48550/arXiv.2305.15685|
|982|Revisiting the Reliability of Psychological Scales on Large Language Models|Jen-tse Huang, Wenxuan Wang, Man Ho Lam, Eric John Li, Wenxiang Jiao, Michael R. Lyu|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv (Cornell University)|https://github.com/CUHK-ARISE/LLMPersonality.|https://arxiv.org/abs/2305.19926|
|983|Retrieve Anything To Augment Large Language Models|Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, Jian-Yun Nie|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/FlagOpen/FlagEmbedding.|https://doi.org/10.48550/arXiv.2310.07554|
|984|Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models|Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, Ji-Rong Wen|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/RUCAIBox/iEvaLM-CRS.|https://doi.org/10.18653/v1/2023.emnlp-main.621|
|985|Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models|Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/johnheo/adadim-llm|https://openreview.net/forum?id=JzG7kSpjJk|
|986|Representation Learning with Large Language Models for Recommendation|Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the ACM Web Conference 2022|https://github.com/HKUDS/RLMRec.|https://doi.org/10.48550/arXiv.2310.15950|
|987|Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves|Yihe Deng, Weitong Zhang, Zixiang Chen, Quanquan Gu|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/uclaml/Rephrase-and-Respond.|https://doi.org/10.48550/arXiv.2311.04205|
|988|RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability|Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, Xing Xie|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/microsoft/RecAI.|https://doi.org/10.48550/arXiv.2311.10947|
|989|ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support Lateral Reading|Dake Zhang, Ronak Pradeep|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/DakeZhang1998/ReadProbe.|https://doi.org/10.48550/arXiv.2306.07875|
|990|RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models|Yasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu Torii, Jun Deguchi|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/yhoshi3/RaLLe.|https://doi.org/10.18653/v1/2023.emnlp-demo.4|
|991|Supervised Learning and Large Language Model Benchmarks on Mental Health Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media|Hongzhi Qi, Qing Zhao, Changwei Song, Wei Zhai, Dan Luo, Shuo Liu, Yi Jing Yu, Fan Wang, Huijing Zou, Bing Xiang Yang, J...|<li><span style="color:#FF5733;">2023-01-01</span></li>|Research Square (Research Square)|https://github.com/HongzhiQ/SupervisedVsLLM-EfficacyEval|https://doi.org/10.21203/rs.3.rs-3523508/v1|
|992|RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit|Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng Dou, Ji-Rong Wen|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/RUC-GSAI/YuLan-IR|https://doi.org/10.48550/arXiv.2306.05212|
|993|R-Tuning: Teaching Large Language Models to Refuse Unknown Questions|Hanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, Tong Zhang|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/shizhediao/R-Tuning.|https://doi.org/10.48550/arXiv.2311.09677|
|994|QuIP: 2-Bit Quantization of Large Language Models With Guarantees|Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa|<li><span style="color:#FF5733;">2023-01-01</span></li>|NeurIPS|https://github.com/Cornell-RelaxML/QuIP.|http://papers.nips.cc/paper_files/paper/2023/hash/0df38cd13520747e1e64e5b123a78ef8-Abstract-Conference.html|
|995|QIGen: Generating Efficient Kernels for Quantized Inference on Large Language Models|Tommaso Pegolotti, Elias Frantar, Dan Alistarh, Markus Püschel|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/IST-DASLab/QIGen.|https://doi.org/10.48550/arXiv.2307.03738|
|996|QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models|Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhengsu Chen, Xiaopeng Zhang, Qi Tian|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/yuhuixu1993/qa-lora.|https://openreview.net/forum?id=WvFoJccpo8|
|997|Proto-lm: A Prototypical Network-Based Framework for Built-in Interpretability in Large Language Models|Sean Xie, Soroush Vosoughi, Saeed Hassanpour|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/yx131/proto-lm.|https://doi.org/10.18653/v1/2023.findings-emnlp.261|
|998|Prompting Frameworks for Large Language Models: A Survey|Xiaoxia Liu, Jingyi Wang, Jun Sun, Xiaohan Yuan, Guoliang Dong, Peng Di, Wenhai Wang, Dongxia Wang|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/lxx0628/Prompting-Framework-Survey|https://doi.org/10.48550/arXiv.2311.12785|
|999|PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation|Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, Issam H. Laradji|<li><span style="color:#FF5733;">2023-01-01</span></li>|Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing|https://github.com/ServiceNow/PromptMix-EMNLP-2023.|https://openreview.nethttps://arxiv.org/pdf/2310.14192.pdf|
|1000|PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts|Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue...|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/microsoft/promptbench.|https://doi.org/10.48550/arXiv.2306.04528|
|1001|RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models|Zijun Long, George Killick, Richard McCreadie, Gerardo Aragon-Camarasa|<li><span style="color:#FF5733;">2023-01-01</span></li>|OpenAlex|https://github.com/longkukuhi/armbench.|https://doi.org/10.1109/ICRA57147.2024.10610797|
|1002|Robot Task Planning Based on Large Language Model Representing Knowledge with Directed Graph Structures|Zhen Yue, Sheng Bi, Lu Xing-tong, Pan Wei-qin, Shi Hai-peng, Chen Zi-rui, Fang Yi-shu|<li><span style="color:#FF5733;">2023-01-01</span></li>|arXiv|https://github.com/NOMIzy/Think_Net_Prompt|https://doi.org/10.48550/arXiv.2306.05171|
