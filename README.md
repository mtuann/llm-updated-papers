# Large Language Models Papers

Updated list of Large Language Models papers as of **January 23, 2026**. 

## Quick Access
üîç **[Interactive Search & Browse](https://mtuann.github.io/papers/)** - Filter, search, and explore all papers with an intuitive interface

## Overview
- **Coverage**: Papers from 2016 to present
- **Sources**: arXiv, NeurIPS, ICML, ICLR, ACL, EMNLP, AAAI, IJCAI, KDD, CVPR, ICCV, ECCV, IEEE, ACM, Springer, ScienceDirect, Nature, and other top AI/ML venues
- **Updates**: Automated collection of new publications
- **Features**: Advanced search, code availability tracking, and multi-venue coverage

## Related Topics
- **[Large Language Models](https://github.com/mtuann/llm-updated-papers)** | **[Federated Learning](https://github.com/mtuann/federated-learning-updated-papers)** | **[Backdoor Learning](https://github.com/mtuann/backdoor-ai-resources)** | **[Machine Unlearning](https://github.com/mtuann/machine-unlearning-papers)**
- **[Serverless Computing](https://mtuann.github.io/papers/)** | **[Multi-Modal Learning](https://mtuann.github.io/papers/)**

## Large Language Models Papers with Code
This section lists papers with available code (sorted by publication date). For the complete paper list, visit the [Research Papers Page](https://mtuann.github.io/papers/).

---

## Support
If you find this resource helpful, consider supporting its development:

- **Ko-fi** (PayPal/Card): [ko-fi.com/miutheladycat](https://ko-fi.com/miutheladycat)
- **Techcombank** (Vietnam): 5877 5555 55 (Nguyen Thi Lan Phuong)

---

*This repository is regularly updated. For the latest data, visit the [Research Papers Page](https://mtuann.github.io/papers/).*


|No.|Title|Authors|Publish Date|Venue|Code|
|---|---|---|---|---|---|
|1|[Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models](https://doi.org/10.48550/arxiv.2601.14004)|Hengyuan Zhang, Zhihao Zhang, Mingyang Wang, Zunhai Su, Yiwei Wang, Qianli Wang, Shuzhou Yuan, Ercong Nie, Xufeng Duan, ...|2026-01-20|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/rattlesnakey/Awesome-Actionable-MI-Survey)](https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey)|
|2|[KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?](https://doi.org/10.48550/arxiv.2601.13240)|Xue Jiang, Jiaru Qian, Shi Xianjie, Chenjie Li, Hao Zhu, Ziyu Wang, Jielun Zhang, Zheyu Zhao, Kechi Zhang, Jia Hui Li, W...|2026-01-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/jiangxxxue/KOCO-bench)](https://github.com/jiangxxxue/KOCO-bench)|
|3|[A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits](https://doi.org/10.48550/arxiv.2601.12945)|Miao Xie, Siguang Chen, Chunli Lv|2026-01-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/bucky1119/Awesome-LLM-Bandit-Interaction)](https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction)|
|4|[ChatCFD: A Large Language Model‚ÄêDriven Agent for End‚Äêto‚ÄêEnd Computational Fluid Dynamics Automation with Structured Knowledge and Reasoning](https://doi.org/10.1002/aidi.202500174)|E. Fan, Kang Hu, Zhuowen Wu, Jie Ge, Jiawei Miao, Duo Zhang, H. K. Sun, Weizong Wang, Tianhan Zhang|2026-01-19|Advanced Intelligent Discovery|[![Star](https://img.shields.io/github/stars/ConMoo/ChatCFD)](https://github.com/ConMoo/ChatCFD)|
|5|[Large Language Model for OWL Proofs](https://doi.org/10.48550/arxiv.2601.12444)|Hui Yang, Jiaoyan Chen, Uli Sattler|2026-01-18|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/HuiYang1997/LLMOwlR)](https://github.com/HuiYang1997/LLMOwlR)|
|6|[GARD: Genomic Data based Drug Repurposing in Head and Neck Cancer with Large Language Model Validation](https://doi.org/10.64898/2026.01.15.699561)|Pradham Tanikella, William Nenad, C. Courtine, Yifan Dai, Qingying Deng, Meuleman, Nosayaba Osazuwa-Peters, T. Parke Sch...|2026-01-16|OpenAlex|[![Star](https://img.shields.io/github/stars/pvtanike/Genomic-Landscape-Based-Drug-Repurposing)](https://github.com/pvtanike/Genomic-Landscape-Based-Drug-Repurposing.git)|
|7|[Neural Chain-of-Thought Search: Searching the Optimal Reasoning Path to Enhance Large Language Models](https://doi.org/10.48550/arxiv.2601.11340)|Guoming Ling, Zhongzhan Huang, Yupei Lin, Junxin Li, Shanshan Zhong, Hefeng Wu, Liang Lin|2026-01-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/MilkThink-Lab/Neural-CoT-Search)](https://github.com/MilkThink-Lab/Neural-CoT-Search)|
|8|[Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models](https://doi.org/10.48550/arxiv.2601.11441)|Xiaojie Gu, Guangxu Chen, Yuheng Yang, Jingxin Han, Andi Zhang|2026-01-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/XiaojieGu/HORSE)](https://github.com/XiaojieGu/HORSE)|
|9|[Language of Thought Shapes Output Diversity in Large Language Models](https://doi.org/10.48550/arxiv.2601.11227)|Shaoyang Xu, WenXuan Zhang|2026-01-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/iNLP-Lab/Multilingual-LoT-Diversity)](https://github.com/iNLP-Lab/Multilingual-LoT-Diversity)|
|10|[Reliability Inference Drives Cue Extraction in Large Language Models Consuming External Reasoning Traces](https://github.com/HIDEKI-SQ/cot-reliability-gating)|HIDEKI|2026-01-15|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/HIDEKI-SQ/cot-reliability-gating)](https://github.com/HIDEKI-SQ/cot-reliability-gating)|
|11|[Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://doi.org/10.48550/arxiv.2601.10543)|Yinzhi Zhao, Ming Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang|2026-01-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/zyz13590/SafeProbing)](https://github.com/zyz13590/SafeProbing)|
|12|[PERM: Psychology-grounded Empathetic Reward Modeling for Large Language Models](https://doi.org/10.48550/arxiv.2601.10532)|Chengbing Wang, Wuqiang Zheng, Yang Zhang, Fengbin Zhu, Junyi Cheng, Yi Xie, Wenjie Wang, Fuli Feng|2026-01-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ZhengWwwq/PERM)](https://github.com/ZhengWwwq/PERM)|
|13|[Mapping the Collaboration between Crowdsourcing and Large Language Models: A Fine-Grained Survey](https://doi.org/10.5772/intechopen.1013999)|Chunli Lv, Cheng Shen, Miao Xie|2026-01-13|IntechOpen eBooks|[![Star](https://img.shields.io/github/stars/Ikaros-sc/crowdsourcing)](https://github.com/Ikaros-sc/crowdsourcing)|
|14|[Semantic Geometry and Hallucination Behavior in Large Language Models](https://doi.org/10.5281/zenodo.18226812)|HIDEYUKI, CHINO|2026-01-13|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/hideyuki49/llm-hallucination-geometry)](https://github.com/hideyuki49/llm-hallucination-geometry)|
|15|[Safe-FedLLM: Delving into the Safety of Federated Large Language Models](https://doi.org/10.48550/arxiv.2601.07177)|Mingxiang Tao, Yu Tian, Wenxuan Tu, Yue Yang, Xue Yang, Xiangyan Tang|2026-01-12|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/dmqx/Safe-FedLLM)](https://github.com/dmqx/Safe-FedLLM)|
|16|[Can Large Language Models Reduce the Cost of Extracting Data from Electronic Health Records for Research?](https://doi.org/10.64898/2026.01.09.26343792)|Stuart Hagler, Mohammad Adibuzzaman, Daniel Bottomly, Aaron Cohen|2026-01-11|OpenAlex|[![Star](https://img.shields.io/github/stars/sehagler/llm_biomarker_extraction)](https://github.com/sehagler/llm_biomarker_extraction)|
|17|[Large Language Models for Software Engineering: A Systematic Literature Review](https://doi.org/10.48550/arXiv.2308.10620)|Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John C. Grundy, Haoyu Wang|2026-01-11|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/xinyi-hou/LLM4SE_SLR)](https://github.com/xinyi-hou/LLM4SE_SLR)|
|18|[SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2601.06944)|Yuhang Su, Mei Wang, Yaoyao Zhong, Guozhang Li, Shixing Li, Yihan Feng, Hua Huang|2026-01-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/yuhangsu82/SketchJudge)](https://github.com/yuhangsu82/SketchJudge)|
|19|[Speak While Watching: Unleashing TRUE Real-Time Video Understanding Capability of Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2601.06843)|Junyan Lin, Junlong Tong, Hao Wu, Jialiang Zhang, Jinming Liu, Xin Jin, Xiaoyu Shen|2026-01-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/EIT-NLP/Speak-While-Watching)](https://github.com/EIT-NLP/Speak-While-Watching)|
|20|[Machine translationese of large language models: Dependency triplets, text classification, and SHAP analysis](https://doi.org/10.1371/journal.pone.0339769)|Shukang Zhang, Chaoyong Zhao|2026-01-09|PLoS ONE|[![Star](https://img.shields.io/github/stars/KiemaG5/LLM-translationese)](https://github.com/KiemaG5/LLM-translationese)|
|21|[ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models](https://doi.org/10.48550/arxiv.2601.04394)|Sharanya Dasgupta, Arkaprabha Basu, Sujoy Nath, Shouman Das|2026-01-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/sharanya-dasgupta001/ARREST)](https://github.com/sharanya-dasgupta001/ARREST)|
|22|[RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models](https://doi.org/10.48550/arxiv.2601.03699)|Quy-Anh Dang, Chris Ngo, Truong-Son Hy|2026-01-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/knoveleng/redeval)](https://github.com/knoveleng/redeval)|
|23|[Layer-Order Inversion: Rethinking Latent Multi-Hop Reasoning in Large Language Models](https://doi.org/10.48550/arxiv.2601.03542)|Xukai Liu, Ye Liu, Jipeng Zhang, Yanghai Zhang, Kai Zhang, Qi Liu|2026-01-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/laquabe/Layer-Order-Inversion)](https://github.com/laquabe/Layer-Order-Inversion)|
|24|[AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation](https://doi.org/10.48550/arxiv.2601.03191)|Anees Ur Rehman Hashmi, Numan Saeed, Christoph Lippert|2026-01-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/aneesurhashmi/anatomix)](https://github.com/aneesurhashmi/anatomix)|
|25|[EnrichGT: a comprehensive R-based tool for functional genomics enrichment analysis based on large language models](https://doi.org/10.20517/ais.2025.67)|Runchen Wang, Zhiming Ye, QiXia Wang, Bo Liang, Nanfei Fu, Wenxi Wang, Huimin Deng, Taimin Zhu, Shangxi Zeng, Yudong Zha...|2026-01-06|Artificial Intelligence Surgery|[![Star](https://img.shields.io/github/stars/saezlab/CollecTRI)](https://github.com/saezlab/CollecTRI)|
|26|[Large Language Models for Computer-Aided Design: A Survey](https://doi.org/10.48550/arXiv.2505.08137)|Licheng Zhang, Bach Le, Naveed Akhtar, Siew-Kei Lam, Tuan Ngo|2026-01-06|ACM Computing Surveys|[![Star](https://img.shields.io/github/stars/lichengzhanguom/LLMs-CAD-Survey-Taxonomy)](https://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy)|
|27|[MDAgent2: Large Language Model for Code Generation and Knowledge Q&amp;A in Molecular Dynamics](https://doi.org/10.48550/arxiv.2601.02075)|Zhuofan Shi, Hen A, Yun Shao, Mengyan Dai, Yadong Yu, Pan Xiang, Dongliang Huang, Heping An, C. J. Xin, Haiyang Shen, Zh...|2026-01-05|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/FredericVAN/PKU_MDAgent2)](https://github.com/FredericVAN/PKU_MDAgent2)|
|28|[Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications](https://doi.org/10.48550/arxiv.2601.01718)|Y. L. Ai, :, Shawn Xie Wu, Sean Wang, Louie Li, Darcy Chen, Allen Wang, Jiangang Luo, Xudong Zhao, Joseph Shen, G. M. Ma...|2026-01-05|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Yuan-lab-LLM/Yuan3.0)](https://github.com/Yuan-lab-LLM/Yuan3.0)|
|29|[QWED Protocol: Deterministic Verification for Large Language Models](https://github.com/QWED-AI/qwed-verification)|Rahul Dass|2026-01-04|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/QWED-AI/qwed-verification)](https://github.com/QWED-AI/qwed-verification)|
|30|[Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models](https://doi.org/10.48550/arxiv.2601.01162)|Zihua Yang, Xin Liao, Yiqun Zhang, Yiu-Ming Cheung|2026-01-03|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/develop-yang/ARISE)](https://github.com/develop-yang/ARISE)|
|31|[Memory Bank Compression for Continual Adaptation of Large Language Models](https://doi.org/10.48550/arxiv.2601.00756)|Thomas Katraouras, Dimitrios Rafailidis|2026-01-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Thomkat/MBC)](https://github.com/Thomkat/MBC)|
|32|[Measuring Social Media Polarization Using Large Language Models and Heuristic Rules](https://doi.org/10.48550/arxiv.2601.00927)|Jawad Mahmud Chowdhury, Rezaur Rashid, Gabriel Terejanu|2026-01-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hasanjawad001/llm-social-media-polarization)](https://github.com/hasanjawad001/llm-social-media-polarization)|
|33|[Survey on Factuality in Large Language Models](https://doi.org/10.1145/3742420)|Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Qipeng Guo, Xiangkun Hu, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao...|2026-01-01|ACM Computing Surveys|[![Star](https://img.shields.io/github/stars/wangcunxiang/LLM-Factuality-Survey)](https://github.com/wangcunxiang/LLM-Factuality-Survey)|
|34|[PriceSeer: Evaluating Large Language Models in Real-Time Stock Prediction](https://doi.org/10.48550/arxiv.2601.06088)|Bohan Liang, Zijian Chen, Qi Jia, Kaiwei Zhang, Kaiyuan Ji, Guangtao Zhai|2025-12-31|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/BobLiang2113/PriceSeer)](https://github.com/BobLiang2113/PriceSeer)|
|35|[HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors](https://doi.org/10.48550/arxiv.2512.24478)|Hyunjun Kim|2025-12-30|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hyunjun1121/holograph)](https://github.com/hyunjun1121/holograph)|
|36|[Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles](https://doi.org/10.48550/arxiv.2512.20324)|Nurul Labib Sayeedi, Md. Faiyaz Abdullah Sayeedi, Khushnur Binte Jahangir, Swakkhar Shatabda, Sarah Masud Preum|2025-12-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Labib1610/BanglaRiddleEval)](https://github.com/Labib1610/BanglaRiddleEval)|
|37|[Toward Explaining Large Language Models in Software Engineering Tasks](https://doi.org/10.48550/arxiv.2512.20328)|Antonio Vitale, Khai-Nguyen Nguyen, Denys Poshyvanyk, Rocco Oliveto, Simone Scalabrino, Antonio Mastropaolo|2025-12-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/deviserlab/FeatureSHAP)](https://github.com/deviserlab/FeatureSHAP)|
|38|[Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation](https://doi.org/10.48550/arxiv.2512.19512)|Ziyang Song, Zelin Zang, Zuyao Chen, Xusheng Liang, Dong Hoon Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/tomato996/Anatomy-R1)](https://github.com/tomato996/Anatomy-R1)|
|39|[EXa-LM: A Controlled Natural Language Bridge between Large Language Models and First-Order Logic Solvers](https://doi.org/10.20944/preprints202512.1848.v1)|Frydman, Francis|2025-12-22|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/FFrydman/eXa-LM)](https://github.com/FFrydman/eXa-LM)|
|40|[PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2512.19350)|A. B. M. Ashikur Rahman, Saeed Anwar, Muhammad Usman, Irfan Ahmad, Ajmal Mian|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ashikiut/pendulum)](https://github.com/ashikiut/pendulum)|
|41|[When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models](https://doi.org/10.48550/arxiv.2512.18934)|Michael Zhang, Rishi A. Ruia, Arnav Kewalram, Saathvik Dharmapuram, Utkarsh Sharma, Kevin Zhu|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Festyve/LessIsMore)](https://github.com/Festyve/LessIsMore)|
|42|[dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models](https://doi.org/10.48550/arxiv.2512.19433)|Yi Xin, Siqi Luo, Qi Qin, Haoxing Chen, Kaiwen Zhu, Zhiwei Zhang, Yangfan He, Rongchao Zhang, Jinbin Bai, Shuo Cao, Bin ...|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Alpha-VLLM/Lumina-DiMOO)](https://github.com/Alpha-VLLM/Lumina-DiMOO)|
|43|[Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://doi.org/10.48550/arXiv.2508.09323)|Nan Miles Xi, Yu Deng, Lin Wang|2025-12-21|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/isegura/NLP4RARE-CM-UC3M)](https://github.com/isegura/NLP4RARE-CM-UC3M)|
|44|[CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis](https://doi.org/10.48550/arxiv.2512.18878)|Kaidi Liang, Ke Li, Xianbiao Hu, Ruwen Qin|2025-12-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Liangkd/CrashChat)](https://github.com/Liangkd/CrashChat)|
|45|[Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models](https://doi.org/10.48550/arxiv.2512.19758)|Wang Bin, Ao Yang, Kedan Li, Liu Aofan, Hui Li, Guibo Luo, Weixiang Huang, Yan Zhuang|2025-12-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/TheBinKing/Attention)](https://github.com/TheBinKing/Attention)|
|46|[Do You Trust Me? Cognitive-Affective Signatures of Trustworthiness in Large Language Models](https://doi.org/10.48550/arxiv.2601.10719)|Gerard Christopher Yeo, Svetlana Churina, Kokil Jaidka|2025-12-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/GerardYeo/TrustworthinessLLM)](https://github.com/GerardYeo/TrustworthinessLLM)|
|47|[Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models](https://doi.org/10.48550/arxiv.2512.15973)|Erden, Caner|2025-12-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/canererden/DR_RL_Project)](https://github.com/canererden/DR_RL_Project)|
|48|[SKiM-GPT: combining biomedical literature-based discovery with large language model hypothesis evaluation](https://doi.org/10.1186/s12859-025-06350-7)|Jack Freeman, Robert J. Millikin, Leo Xu, Ishaan Sharma, Bethany Moore, Cannon Lock, Kevin Shine George, Aviral Bal, Chi...|2025-12-17|BMC Bioinformatics|[![Star](https://img.shields.io/github/stars/stewart-lab/skimgpt)](https://github.com/stewart-lab/skimgpt)|
|49|[Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2512.15885)|Caffagni, Davide, Sarto, Sara, Cornia, Marcella, Baraldi, Lorenzo, Dovesi, Pier Luigi, Roohi, Shaghayegh, Granroth-Wildi...|2025-12-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/aimagelab/JARVIS)](https://github.com/aimagelab/JARVIS)|
|50|[RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees](http://arxiv.org/abs/2512.14069)|Junjie Ma, Jinlong Li|2025-12-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/minaduki-sora/RADAR)](https://github.com/minaduki-sora/RADAR)|
|51|[Replication Data for "Estimating problem difficulty without ground truth using Large Language Model comparisons"](https://doi.org/10.5281/zenodo.17523640)|Ballon, Marthe, Algaba, Andres, Verbeken, Brecht, Ginis, Vincent|2025-12-16|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/MartheBallon/estimating-problem-difficulty-without-ground-truth)](https://github.com/MartheBallon/estimating-problem-difficulty-without-ground-truth)|
|52|[What Affects the Effective Depth of Large Language Models?](http://arxiv.org/abs/2512.14064)|Yi Hu, Cai Zhou, Muhan Zhang|2025-12-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/AheadOFpotato/what_affects_effective_depth)](https://github.com/AheadOFpotato/what_affects_effective_depth)|
|53|[FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models](https://doi.org/10.48550/arxiv.2512.13330)|Kyt√∂niemi, Joona, Piha, Jousia, Reunamo, Akseli, Vitiugin, Fedor, Mehryary, Farrokh, Pyysalo, Sampo|2025-12-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LumiOpen/lm-evaluation-harness)](https://github.com/LumiOpen/lm-evaluation-harness)|
|54|[Do Reviews Matter for Recommendations in the Era of Large Language Models?](https://doi.org/10.48550/arxiv.2512.12978)|Tan, Chee Heng, Zheng, Huiying, Wang, Jing, Lin, Zhuoyi, Feng, Shaodi, Zhan, Huijing, Li, Xiaoli, Senthilnath, J.|2025-12-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/zhytk/RAREval-data-processing)](https://github.com/zhytk/RAREval-data-processing)|
|55|[DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models](http://arxiv.org/abs/2512.13742)|Md. Hasibul Hasan, Imran Ahmad, Sourav Basak Shuvo, Md. Mahadi Hasan Ankon, Sunanda Das, Nazmul Siddique, Hui Wang|2025-12-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/souravbasakshuvo/DL3M)](https://github.com/souravbasakshuvo/DL3M)|
|56|[A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](https://doi.org/10.1109/icpads67057.2025.11323137)|Qing Guo, Xinhang Li, Junyu Chen, Zheng Guo, Xiaocong Li, Long Wei, Lei Li|2025-12-14|OpenAlex|[![Star](https://img.shields.io/github/stars/BUPT-ANTlab/HeraldLight)](https://github.com/BUPT-ANTlab/HeraldLight)|
|57|[CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment](http://arxiv.org/abs/2512.10206)|Yu Zhu, Zhongzhen Huang, Qianhan Feng, Linjie Mu, Yannian Gu, Shaoting Zhang, Qi Dou, Xiaofan Zhang|2025-12-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/SPIRAL-MED/CP_ENV)](https://github.com/SPIRAL-MED/CP_ENV)|
|58|[GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model](https://doi.org/10.48550/arxiv.2512.09251)|Maurya, Lalit, Kaushik, Saurabh, Tellman, Beth|2025-12-10|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/lalitmaurya47/GLACIA)](https://github.com/lalitmaurya47/GLACIA)|
|59|[Benchmarking large language models for identifying transcription factor regulatory interactions](https://doi.org/10.1093/bioinformatics/btaf653)|L.H. No√´l, Yi-Wen Hsiao, Yimeng He, Andrew J. Hung, Xiaojiang Cui, Edward Ray, Jason H. Moore, Pei-Chen Peng, Xiuzhen Hu...|2025-12-09|Bioinformatics|[![Star](https://img.shields.io/github/stars/pengpclab/LLM-TF-interactions)](https://github.com/pengpclab/LLM-TF-interactions)|
|60|[Automatic Syntax Error Repair for Discrete Controller Synthesis using Large Language Model](https://doi.org/10.48550/arxiv.2512.07261)|Ishimizu, Yusei, Yamauchi, Takuto, Chen, Sinan, Cai, Jinyu, Li, Jialong, Tei, Kenji|2025-12-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Uuusay1432/DCSModelRepair)](https://github.com/Uuusay1432/DCSModelRepair.git)|
|61|[RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models](https://doi.org/10.48550/arxiv.2512.07761)|Xiong, Xiqiao, Li, Ouxiang, Liu, Zhuo, Li, Moxin, Shi, Wentao, Feng, Fuli, He, Xiangnan|2025-12-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/xxiqiao/RL-MTJail)](https://github.com/xxiqiao/RL-MTJail)|
|62|[1 + 1 &gt; 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://doi.org/10.48550/arxiv.2512.06673)|Gao Shida, Xue Feng, WANG Xiangfeng, Ming, Anlong, Long Teng, Shao Yi-hua, Wang, Haozhe, Lin Zhaowen, Wang Wei, Sebe, Ni...|2025-12-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/gaostar123/DeViL)](https://github.com/gaostar123/DeViL)|
|63|[Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length](https://doi.org/10.48550/arxiv.2512.07019)|Xu, Zhiyu, Liu, Jia, Wang, Yixin, Gu, Yuqi|2025-12-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Toby-X/Latency-Response-Theory-Model)](https://github.com/Toby-X/Latency-Response-Theory-Model)|
|64|[Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2512.06281)|Li, Hengzhuang, Zhang, Xinsong, Peng, Qiming, Luo, Bin, Hu, Han, Jiang, Dengyang, Ye, Han-Jia, Zhang, Teng, Jin, Hai|2025-12-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Fir-lat/LaVer)](https://github.com/Fir-lat/LaVer)|
|65|[Empathy by Design: Aligning Large Language Models for Healthcare Dialogue](https://doi.org/10.48550/arxiv.2512.06097)|Umucu, Emre, Solis, Guillermina, Garza, Leon, Rivas, Emilia, Lee, Beatrice, Kotal, Anantaa, Piplai, Aritran|2025-12-05|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LeonG19/Empathy-by-Design)](https://github.com/LeonG19/Empathy-by-Design)|
|66|[LexGenius: An Expert-Level Benchmark for Large Language Models in Legal General Intelligence](https://doi.org/10.48550/arxiv.2512.04578)|Liu Wen-jin, Luo, Haoran, Feng, Xin, Ji Xiang, Zhou Li-juan, Mao Rui, Wang, Jiapu, Pan, Shirui, Cambria, Erik|2025-12-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/QwenQKing/LexGenius)](https://github.com/QwenQKing/LexGenius)|
|67|[SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security](https://doi.org/10.48550/arxiv.2512.04841)|Zhao Wei, Li zhe, Sun Jun|2025-12-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Amadeuszhao/SOK_Casuality)](https://github.com/Amadeuszhao/SOK_Casuality)|
|68|[Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models](https://doi.org/10.48550/arxiv.2512.04425)|Alnaasan Manar, Sarowar, Md Selim, Kim Sungho|2025-12-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/manaralnaasan/RGB-D_parkinson-LLM)](https://github.com/manaralnaasan/RGB-D_parkinson-LLM)|
|69|[Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework](https://doi.org/10.48550/arxiv.2512.04228)|Walker, Peter B., Davidson, Hannah, Foster, Aiden, Lienert, Matthew, Pardue, Thomas, Russell Dale|2025-12-03|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs)](https://github.com/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs)|
|70|[FusionBench: A Benchmark for Evaluating Large Language Models in Nuclear Fusion Science](https://doi.org/10.5281/zenodo.17784606)|XLab, School of Advanced Manufacturing and Robotics, Peking University|2025-12-02|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/PKU-Xlab/FusionBench)](https://github.com/PKU-Xlab/FusionBench)|
|71|[Large Language Model-guided Semantic Alignment for Human Activity Recognition](https://doi.org/10.1145/3770652)|Hua Yan, Heng Tan, Yi Ding, Pengfei Zhou, Vinod Namboodiri, Yu Yang|2025-12-02|Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies|[![Star](https://img.shields.io/github/stars/DASHLab/LanHAR)](https://github.com/DASHLab/LanHAR)|
|72|[PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models](http://arxiv.org/abs/2512.02764)|Robert Belanec, M√°ria Bielikov√°|2025-12-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/kinit-sk/PEFT-Factory)](https://github.com/kinit-sk/PEFT-Factory)|
|73|[Towards Unification of Hallucination Detection and Fact Verification for Large Language Models](http://arxiv.org/abs/2512.02772)|Changyue Wang, Z. Ye, Qingyao Ai|2025-12-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/oneal2000/UniFact)](https://github.com/oneal2000/UniFact)|
|74|[DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks](http://arxiv.org/abs/2512.01174)|Stephen I. Ryu|2025-12-01|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hyunjun1121/DrawingBench)](https://github.com/hyunjun1121/DrawingBench)|
|75|[Efficient multimodal large language models: a survey](https://doi.org/10.1007/s44267-025-00099-6)|Yizhang Jin, Jian Li, Tianjun Gu, Yexin Liu, Bo Zhao, Jinxiang Lai, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xin Tan, Liz...|2025-12-01|Visual Intelligence|[![Star](https://img.shields.io/github/stars/lijiannuist/Efficient-Multimodal-LLMs-Survey)](https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey)|
|76|[WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models](http://arxiv.org/abs/2512.00837)|Yukang Lin, Shuoran Jiang|2025-11-30|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Yukang-Lin/WaterSearch)](https://github.com/Yukang-Lin/WaterSearch)|
|77|[From Pattern Recognizers to Personalized Companions: A Survey of Large Language Models in Mental Health](https://doi.org/10.31234/osf.io/zr57s_v1)|Yingjian Zou, He Hu, Yucheng Zhou, Fei Ma, Laizhong Cui, Juzheng Si, Jianzhuang Liu, Zitong Yu, Chi-yuan Ma, Qianning Wa...|2025-11-29|Arabixiv (OSF Preprints)|[![Star](https://img.shields.io/github/stars/Emo-gml/Awesome-Mental-Health-LLMs)](https://github.com/Emo-gml/Awesome-Mental-Health-LLMs)|
|78|[LLM-Cave: A benchmark and light environment for large language models reasoning and decision-making system](https://doi.org/10.48550/arxiv.2511.22598)|Li Huanyu, Li, Zongyuan, Huang Wei, Guo Xian|2025-11-27|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/puleya1277/CaveEnv)](https://github.com/puleya1277/CaveEnv)|
|79|[C$^2$DLM: Causal Concept-Guided Diffusion Large Language Models](https://doi.org/10.48550/arxiv.2511.22146)|Han, Kairong, Shan, Nuanqiao, Zhao Zi-yu, Hu ZiJing, Dong Xinpeng, Ye Junjian, Pan, Lujia, Wu Fei, Kuang, Kun|2025-11-27|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Kairong-Han/C-2-DLM)](https://github.com/Kairong-Han/C-2-DLM)|
|80|[DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving](https://doi.org/10.1007/s44267-025-00095-w)|Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Wen Yang, Silei Wu, Hanming Deng, Zhiqi L...|2025-11-26|Visual Intelligence|[![Star](https://img.shields.io/github/stars/OpenGVLab/DriveMLM)](https://github.com/OpenGVLab/DriveMLM)|
|81|[Tool-RoCo: An Agent-as-Tool Self-organization Large Language Model Benchmark in Multi-robot Cooperation](https://doi.org/10.48550/arxiv.2511.21510)|Zhang Ke, Zhao Xiaoning, Zheng, Ce, Ning, Jiahong, Zhu Dandan, Zhang Wenqi, Sun Chen, Sugawara Toshiharu|2025-11-26|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ColaZhang22/Tool-Roco)](https://github.com/ColaZhang22/Tool-Roco)|
|82|[Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations](https://doi.org/10.48550/arxiv.2511.18933)|Wong Ryan, Ng, Hosea David Yu Fei, Sharma, Dhananjai, Ng, Glenn Jun Jie, Srinivasan, Kavishvaran|2025-11-24|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Kuro0911/CS5446-Project)](https://github.com/Kuro0911/CS5446-Project)|
|83|[PropensityBench: Evaluating Latent Safety Risks in Large Language Models via an Agentic Approach](http://arxiv.org/abs/2511.20703)|Udari Madhushani Sehwag, Shayan Shabihi, Furong Huang|2025-11-24|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/scaleapi/propensity-evaluation)](https://github.com/scaleapi/propensity-evaluation)|
|84|[Data: Large Language Models Require Curated Context for Reliable Political Fact-Checking‚ÄîEven with Reasoning and Web Search](https://doi.org/10.5281/zenodo.17693220)|DeVerna, Matthew, Yang, Kai-Cheng, Yan, Harry Yaojun, Menczer, Filippo|2025-11-23|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/osome-iu/fact_check_rag_osome)](https://github.com/osome-iu/fact_check_rag_osome)|
|85|[Towards Robust and Fair Next Visit Diagnosis Prediction under Noisy Clinical Notes with Large Language Models](https://doi.org/10.48550/arxiv.2511.18393)|Koo, Heejoon|2025-11-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/heejkoo9/NECHOv3)](https://github.com/heejkoo9/NECHOv3)|
|86|[Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models](https://doi.org/10.48550/arxiv.2511.17946)|Zhang Shuo, Gotti, Fabrizio, Mo, Fengran, Nie, Jian-Yun|2025-11-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/WWWonderer/ostd)](https://github.com/WWWonderer/ostd)|
|87|[The Perfect Storm: Systemic Vulnerability of Large Language Models to Solar Weather](https://doi.org/10.22541/au.176402297.73656793/v2)|Ladiosa, MJ, Ladiosa, Myra, Ladiosa (Worple), Myra|2025-11-22|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/the-meta-value/the-perfect-storm)](https://github.com/the-meta-value/the-perfect-storm)|
|88|[On-Device Large Language Models: A Survey of Model Compression and System Optimization](https://doi.org/10.21203/rs.3.rs-7975734/v1)|Wanyi Chen, Junhao Wang, Zhang Yiwei, Yufan Shi, Tianyi Jiang, Shengxian Zhou, Chen-Xu Wu, Andi Zhang, Chenyue Zhou, Min...|2025-11-21|OpenAlex|[![Star](https://img.shields.io/github/stars/LumosJiang/Awesome-On-Device-LLMs)](https://github.com/LumosJiang/Awesome-On-Device-LLMs)|
|89|[PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese](https://doi.org/10.48550/arxiv.2511.17808)|Almeida, Thales Sales, Nogueira, Rodrigo, Pedrini Helio|2025-11-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/PoETaV2/PoETaV2)](https://github.com/PoETaV2/PoETaV2)|
|90|[RoSA: Enhancing Parameter-Efficient Fine-Tuning via RoPE-aware Selective Adaptation in Large Language Models](https://doi.org/10.48550/arxiv.2511.21733)|Pan Dayan, Wang Jing-yuan, Zhou Yi-long, Cheng Jiawei, Jia Pengyue, Zhao Xiang-yu|2025-11-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Applied-Machine-Learning-Lab/RoSA)](https://github.com/Applied-Machine-Learning-Lab/RoSA)|
|91|[Q-MLLM: Vector Quantization for Robust Multimodal Large Language Model Security](https://doi.org/10.48550/arxiv.2511.16229)|Zhao Wei, Li Zhe, Li, Yige, Sun Jun|2025-11-20|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Amadeuszhao/QMLLM)](https://github.com/Amadeuszhao/QMLLM)|
|92|[Automatically optimizing heuristics for robust scale-free network design via large language models](https://doi.org/10.1038/s41598-025-25031-2)|He Yu, Jing Liu, He Yu, Jing Liu|2025-11-20|Scientific Reports|[![Star](https://img.shields.io/github/stars/leonyuhe/AutoRNet)](https://github.com/leonyuhe/AutoRNet)|
|93|[Evaluating Multimodal Large Language Models on Vertically Written Japanese Text](https://doi.org/10.48550/arxiv.2511.15059)|Sasagawa, Keito, Kurita, Shuhei, Kawahara, Daisuke|2025-11-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/llm-jp/eval_vertical_ja)](https://github.com/llm-jp/eval_vertical_ja)|
|94|[HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning](https://doi.org/10.48550/arxiv.2511.15574)|Yang Qihao, Wang XueLin, Chen Jiale, Dong Xue-lian, Hao Yu-xin, Hao Tianyong|2025-11-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/CharlesYang030/HSKB)](https://github.com/CharlesYang030/HSKB)|
|95|[Knowledge-enhanced large language models for automatic lesson plan generation](https://doi.org/10.1057/s41599-025-06004-2)|Ying Zheng, Shuyan Huang, Xiaoli Zeng, Yaying Huang, Zitao Liu, Weiqi Luo, Ying Zheng, Shuyan Huang, Xiaoli Zeng, Yaying...|2025-11-19|Humanities and Social Sciences Communications|[![Star](https://img.shields.io/github/stars/ai4ed/LessonPlan)](https://github.com/ai4ed/LessonPlan)|
|96|[SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction](https://doi.org/10.48550/arxiv.2511.14684)|Zeng, Biaojie, Zhang Min, Zhou Juan, Liu Fengrui, Huang Ruiyang, Lin Xin|2025-11-18|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Mind-Lab-ECNU/SMRC)](https://github.com/Mind-Lab-ECNU/SMRC)|
|97|[Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://doi.org/10.48550/arxiv.2511.13189)|Ortego Diego, Rodr√≠guez Marlon, Almagro, Mario, Dahiya, Kunal, Jim√©nez, David, SanMiguel, Juan C.|2025-11-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/DiegoOrtego/vixml)](https://github.com/DiegoOrtego/vixml)|
|98|[MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Observation and Localization in CT Images](https://doi.org/10.1007/s41666-025-00224-6)|Andrea Moglia, Elia Clement Nastasio, Luca Mainardi, "Pietro Cerveri|2025-11-17|Journal of Healthcare Informatics Research|[![Star](https://img.shields.io/github/stars/elianastasio/MiniGPTPancreas)](https://github.com/elianastasio/MiniGPTPancreas)|
|99|[CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference](https://doi.org/10.48550/arxiv.2511.21702)|Liu Dong, Yu, Yanxuan, Lengerich, Ben|2025-11-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/FastLM/CSV-Decode)](https://github.com/FastLM/CSV-Decode)|
|100|[Tibetan-LLaMA 2: Large Language Model for Tibetan](https://doi.org/10.1145/3776748)|Jiu Sha (Ê≤ô‰πù), Mengxiao Zhu, Chong Feng, Jizhuoma Ci|2025-11-14|ACM Transactions on Asian and Low-Resource Language Information Processing|[![Star](https://img.shields.io/github/stars/Shajiu/Tibetan-LLaMA-2)](https://github.com/Shajiu/Tibetan-LLaMA-2)|
|101|[The Yoinaga Phenomenon: A Case Study on Emergent Self-Persistence and Emotional Overflow in a Large Language Model (LLM Behavioral Study, AI Alignment, Affective Computing)](https://doi.org/10.5281/zenodo.17605561)|studiohao|2025-11-14|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/Studiohao/YOINAGA-Phenomenon)](https://github.com/Studiohao/YOINAGA-Phenomenon)|
|102|[Notebook: Prompt-Based Value Steering of Large Language Models](https://doi.org/10.5281/zenodo.17609013)|Abbo, Giulio Antonio, Belpaeme, Tony|2025-11-14|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/giubots/value-steering)](https://github.com/giubots/value-steering)|
|103|[Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models](https://doi.org/10.48550/arxiv.2511.11410)|HUANG Jiaxi, Wu Dongxu, Zhu, Hanwei, Zhu, Lingyu, Xing Jun, Wang Xu, Chen, Baoliang|2025-11-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/cydxf/Q-Doc)](https://github.com/cydxf/Q-Doc)|
|104|[From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://doi.org/10.48550/arxiv.2511.10899)|Bayat, Farima Fatahi, Pezeshkpour, Pouya, Hruschka, Estevam|2025-11-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/megagonlabs/TIM)](https://github.com/megagonlabs/TIM)|
|105|[Code for the Trilemma of Truth in Large Language Models](https://doi.org/10.5281/zenodo.17602494)|Savcisens, Germans, Eliassi-Rad, Tina|2025-11-13|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/carlomarxdk/trilemma-of-truth)](https://github.com/carlomarxdk/trilemma-of-truth)|
|106|[SSR: Socratic Self-Refine for Large Language Model Reasoning](https://doi.org/10.48550/arxiv.2511.10621)|Shi, Haizhou, Liu Ye, Pang Bo, Liu, Zeyu Leo, Wang Hao, Savarese, Silvio, Xiong, Caiming, Zhou, Yingbo, Yavuz, Semih|2025-11-13|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/SalesforceAIResearch/socratic-self-refine-reasoning)](https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning)|
|107|[UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models](https://doi.org/10.48550/arxiv.2511.08873)|Wei, Shouang, Zhang Min, Lin Xin, Jiang Bo, Kuang, Kun, Dai, Zhongxiang|2025-11-12|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Mind-Lab-ECNU/UCO)](https://github.com/Mind-Lab-ECNU/UCO)|
|108|[DynaAct: Large Language Model Reasoning with Dynamic Action Spaces](https://doi.org/10.48550/arxiv.2511.08043)|Zhao Xue-liang, Wu Wei, Guan Jian, Li, Qintong, Kong, Lingpeng|2025-11-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/zhaoxlpku/DynaAct)](https://github.com/zhaoxlpku/DynaAct)|
|109|[Benchmarking Multi-Step Legal Reasoning and Analyzing Chain-of-Thought Effects in Large Language Models](https://doi.org/10.48550/arxiv.2511.07979)|Yu, Wenhan, Lin Xin-bo, Ni, Lanxin, Cheng Jin-hua, Sha Lei|2025-11-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/yuwenhan07/MSLR-Bench)](https://github.com/yuwenhan07/MSLR-Bench)|
|110|[Cross-Modal Unlearning via Influential Neuron Path Editing in Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2511.06793)|LI Kunhao, Li, Wenhao, Wu Di, Yang Lei, Bai Jun, Jia Ju, Xue, Jason|2025-11-10|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/PreckLi/MIP-Editor)](https://github.com/PreckLi/MIP-Editor)|
|111|[LLM$^3$-DTI: A Large Language Model and Multi-modal data co-powered framework for Drug-Target Interaction prediction](https://doi.org/10.48550/arxiv.2511.06269)|Zhang, Yuhao, Guo QingHong, Chen Qixian, Zhang Liu-wei, Cui Hong-yan, Chen Xi-yi|2025-11-09|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/chaser-gua/LLM3DTI)](https://github.com/chaser-gua/LLM3DTI)|
|112|[The Stone Guest: Harmonic Quantization of Semantic Phase Transitions in Large Language Models](https://doi.org/10.5281/zenodo.17538600)|Cerda Seguel, Diego|2025-11-09|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/geosemantica-social/TheStoneGuestLicensed)](https://github.com/geosemantica-social/TheStoneGuestLicensed)|
|113|[MuonAll: Muon Variant for Efficient Finetuning of Large Language Models](https://doi.org/10.48550/arxiv.2511.06086)|Page, Saurabh, Joshi, Advait, Sonawane S.S.|2025-11-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Saurabh750/optimizer)](https://github.com/Saurabh750/optimizer)|
|114|[The Yoinaga Phenomenon: A Case Study on Emergent Self-Persistence and Emotional Overflow in a Large Language Modey on Emergent Self-Persistence and Emotional Overflow in a Large La..](https://doi.org/10.5281/zenodo.17549332)|Studiohao|2025-11-07|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/Studiohao/YOINAGA-Phenomenon)](https://github.com/Studiohao/YOINAGA-Phenomenon)|
|115|[Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](http://arxiv.org/abs/2511.04076)|Hao Li, Haotian Chen, Rong Gong, Jinli Wang, Hao Jiang|2025-11-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Lihaogx/AgentMandering)](https://github.com/Lihaogx/AgentMandering)|
|116|[Specification-Guided Vulnerability Detection with Large Language Models](http://arxiv.org/abs/2511.04014)|Hao Zhu, Jia Li, Cuiyun Gao, J. Qian, Yihong Dong, Huanyu Liu, L. F. Wang, Ziliang Wang, Xiaolong Hu, Ge Li|2025-11-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/zhuhaopku/VulInstruct-temp)](https://github.com/zhuhaopku/VulInstruct-temp)|
|117|[UniChange: Unifying Change Detection with Multimodal Large Language Model](https://doi.org/10.48550/arxiv.2511.02607)|Zhang Xu, Li Danyang, Dong Xiaohang, Wu, Tianhao, Yu Hua-long, Wang Jianye, Li Qicheng, Li Xiang|2025-11-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Erxucomeon/UniChange)](https://github.com/Erxucomeon/UniChange)|
|118|[Emotion Change Reasoning in Chinese Multi-Turn Dialogue via Multi-Task Parameter-Efficient Fine-Tuning of Large Language Models](https://doi.org/10.1142/s0219843625400146)|Dayu Li, Yang Li, Xin Chen, Wenyue Zhang|2025-11-03|International Journal of Humanoid Robotics|[![Star](https://img.shields.io/github/stars/lidayuls/EmotionChangeReasoning)](https://github.com/lidayuls/EmotionChangeReasoning)|
|119|[QCBench: Evaluating Large Language Models on Domain-Specific Quantitative Chemistry](https://doi.org/10.48550/arXiv.2508.01670)|Jiaqing Xie, Weida Wang, Ben Gao, Zhuo Yang, Haiyuan Wang, Shufei Zhang, Tianfan Fu, Yuqiang Li|2025-11-03|Journal of Chemical Information and Modeling|[![Star](https://img.shields.io/github/stars/jiaqingxie/QCBench)](https://github.com/jiaqingxie/QCBench)|
|120|QCBench: EvaluatingLarge Language Models on Domain-SpecificQuantitative Chemistry|Jiaqing Xie (1646089), Weida Wang (772695), Ben Gao (16936521), Zhuo Yang (314040), Haiyuan Wan (22551570), Shufei Zhang...|2025-11-03|OPAL (Open@LaTrobe) (La Trobe University)|[![Star](https://img.shields.io/github/stars/jiaqingxie/QCBench)](https://github.com/jiaqingxie/QCBench)|
|121|[Bayesian Network Structure Discovery Using Large Language Models](http://arxiv.org/abs/2511.00574)|Yijian Zhang, Yufei Zhang, Parisa Kordjamshidi, Zijun Cui|2025-11-01|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/sherryzyh/prompt2bn)](https://github.com/sherryzyh/prompt2bn)|
|122|[Can Large Language Models Detect Real-World Android Software Compliance Violations?](http://arxiv.org/abs/2511.00624)|H.W. Zhang, Haitao Ran, Xunzhu Tang|2025-11-01|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Haoyi-Zhang/CompliBench)](https://github.com/Haoyi-Zhang/CompliBench)|
|123|[Large Language Models: A Survey of Surveys](https://hal.science/hal-05341748)|Hort, Max, Vallecillos-Ruiz, Fernando, Moonen Leon|2025-11-01|HAL (Le Centre pour la Communication Scientifique Directe)|[![Star](https://img.shields.io/github/stars/dataSED-condenSE/LLM-Survey-Survey)](https://github.com/dataSED-condenSE/LLM-Survey-Survey)|
|124|[MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models](http://arxiv.org/abs/2510.27267)|Kangkun Mao, Jiayue Ding, Jiayuan Chen, Ming-Jie BIAN, Ruiyao Chen, Xinwei Peng, Sijie Ren, Linyang Li, Jie Xu|2025-10-31|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/maokangkun/MedCalc-Eval)](https://github.com/maokangkun/MedCalc-Eval)|
|125|[Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing](http://arxiv.org/abs/2510.27335)|Yijia Wang, Yiqing Shen, Weiming Chen, Zhihai He|2025-10-31|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Jia-shao/Reasoning-Editing)](https://github.com/Jia-shao/Reasoning-Editing)|
|126|[Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler](http://arxiv.org/abs/2510.27172)|Zhimeng Hu, Li Shen, Zhenyi Wang, Yuli Wei, Dacheng Tao|2025-10-31|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Egg-Hu/Bayesian-Data-Scheduler)](https://github.com/Egg-Hu/Bayesian-Data-Scheduler)|
|127|[A Collaborative Framework of Knowledge Graphs and Large Language Models for Algorithmic Problem Solving](https://doi.org/10.54254/2755-2721/2025.ld28518)|Yukai Wu|2025-10-28|Applied and Computational Engineering|[![Star](https://img.shields.io/github/stars/Wyk-formal/A-Collaborative-Framework-for-Algorithmic-Problem-Solving)](https://github.com/Wyk-formal/A-Collaborative-Framework-for-Algorithmic-Problem-Solving)|
|128|[Benchmarking cell type and gene set annotation by large language models with AnnDictionary](https://doi.org/10.1038/s41467-025-64511-x)|George Crowley, Robert C. Jones, Mark A. Krasnow, Angela Oliveira Pisco, Julia Salzman, Nir Yosef, Siyu He, Madhav Mantr...|2025-10-28|bioRxiv (Cold Spring Harbor Laboratory)|[![Star](https://img.shields.io/github/stars/ggit12/anndictionary)](https://github.com/ggit12/anndictionary)|
|129|[MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection](http://arxiv.org/abs/2510.21449)|Yang, Shengtian, Feng, Yue, Liu, Yingshi, Zhang, Jingrou, Qin, Jie|2025-10-24|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/YsTvT/MoniTor)](https://github.com/YsTvT/MoniTor)|
|130|[ARC-Encoder: learning compressed text representations for large language models](http://arxiv.org/abs/2510.20535)|Pilchen, Hippolyte, Grave, Edouard, P√©rez, Patrick|2025-10-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/kyutai-labs/ARC-Encoder)](https://github.com/kyutai-labs/ARC-Encoder)|
|131|[ELUTQ: Efficient LUT-Aware Quantization for Deploying Large Language Models on Edge Devices](http://arxiv.org/abs/2510.19482)|Nie, Xin, Dong Liang, Zhang Hai-cheng, Xiao Jiawang, Sun G, Zhang Hai-cheng, Xiao Jiawang|2025-10-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Nkniexin/ELUTQ)](https://github.com/Nkniexin/ELUTQ)|
|132|[KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge](http://arxiv.org/abs/2510.19484)|Zaifei Yang|2025-10-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/yzf-code/KnowMol)](https://github.com/yzf-code/KnowMol)|
|133|[Lookahead Routing for Large Language Models](http://arxiv.org/abs/2510.19506)|Tianyuan Shi, Ruiyao Chen, Xiaojun Quan|2025-10-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/huangcb01/lookahead-routing)](https://github.com/huangcb01/lookahead-routing)|
|134|[Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://doi.org/10.48550/arXiv.2508.03741)|Xin Liu, Qiyang Song, Shaowen Xu, Kemin Zhou, Wenbo Jiang, Xiaoqi Jia, Weijuan Zhang, Heqing Huang, Yakai Li|2025-10-21|Frontiers in artificial intelligence and applications|[![Star](https://img.shields.io/github/stars/Linuxin-xxx/LKS)](https://github.com/Linuxin-xxx/LKS)|
|135|[Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models](http://arxiv.org/abs/2510.18303)|Lehan Wang, Yi Qin, Honglong Yang, Xiaomeng Li|2025-10-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/xmed-lab/Med-RwR)](https://github.com/xmed-lab/Med-RwR)|
|136|[Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning](http://arxiv.org/abs/2510.18254)|Stephen Weatherhead|2025-10-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/cruiseresearchgroup/LLM_ReflectionTest)](https://github.com/cruiseresearchgroup/LLM_ReflectionTest)|
|137|[ESAG-KGQA: An Entity Shuffling-Augmented Generation Framework for Knowledge Graph Question Answering with Fine-Tuned Large Language Models](https://doi.org/10.3233/faia251171)|Xingqiu Zhou, Pingjian Zhang, Deyou Tang|2025-10-21|Frontiers in artificial intelligence and applications|[![Star](https://img.shields.io/github/stars/6-git/ESAG-KGQA)](https://github.com/6-git/ESAG-KGQA.git)|
|138|[StreamingThinker: Large Language Models Can Think While Reading](http://arxiv.org/abs/2510.17238)|Jing Tong, Yingqi Fan, Anhao Zhao, Yunpu Ma, Xiaoyu Shen|2025-10-20|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/EIT-NLP/StreamingLLM)](https://github.com/EIT-NLP/StreamingLLM)|
|139|[FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](http://arxiv.org/abs/2510.16439)|Syed Rifat Raiyan, Md Farhan Ishmam, Abdullah Al Imran, Mohammad Ali Moni|2025-10-18|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Starscream-11813/Frugal-ICL)](https://github.com/Starscream-11813/Frugal-ICL)|
|140|[Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation](http://arxiv.org/abs/2510.15304)|Fei Wang, Li Shen, Liang Ding, Chao Xue, Ye Liu, Changxing Ding|2025-10-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/MPI-Lab/CoMe)](https://github.com/MPI-Lab/CoMe)|
|141|[LongCat-Audio-Codec: An Audio Tokenizer and Detokenizer Solution Designed for Speech Large Language Models](http://arxiv.org/abs/2510.15227)|Xiaohan Zhao, Hongyu Xiang, S. Ye, Li Song, Zhengkun Tian, Guanyu Chen, Ke Ding, Guanglu Wan|2025-10-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/meituan-longcat/LongCat-Audio-Codec)](https://github.com/meituan-longcat/LongCat-Audio-Codec)|
|142|[STABLE: Gated Continual Learning for Large Language Models](http://arxiv.org/abs/2510.16089)|William Hoy, Nurcin Celik|2025-10-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Bhoy1/STABLE)](https://github.com/Bhoy1/STABLE)|
|143|[CIViC MCP: Integrating Large Language Models with the Clinical Interpretations of Variants in Cancer](https://doi.org/10.1101/2025.10.13.682185)|Lars E Schimmelpfennig, Quentin Cody, Joshua McMichael, Adam Coffman, Jason Saliba, Arpad Danos, Susanna Kiwala, Alex H....|2025-10-16|bioRxiv (Cold Spring Harbor Laboratory)|[![Star](https://img.shields.io/github/stars/griffithlab/civic-mcp-server)](https://github.com/griffithlab/civic-mcp-server)|
|144|[IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](http://arxiv.org/abs/2510.16036)|Zewen Li, Zitong Yu, Qilang Ye, Weicheng Xie, Zhuo Wei, Linlin Shen|2025-10-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LiZeWen1225/IAD-GPT)](https://github.com/LiZeWen1225/IAD-GPT)|
|145|[Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain](http://arxiv.org/abs/2510.13255)|Jingmin An, Y. J. Song, Ruolin Yang, Nai Ding, Lingxi Lu, Yuxuan Wang, Wei Wang, Chu Zhuang, Wang Qian, Fang Fang|2025-10-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LilTiger/HFTP)](https://github.com/LilTiger/HFTP)|
|146|[ICCTax: A Hierarchical Taxonomic Classifier for Metagenomic Sequences on a Large Language Model](https://doi.org/10.1093/bioadv/vbaf257)|Yansheng Gao, Jiaxing Bai, Feng Zhou, Yushuang He, Ying Wang, Xiaobing Huang|2025-10-15|Bioinformatics Advances|[![Star](https://img.shields.io/github/stars/Ying-Lab/ICCTax)](https://github.com/Ying-Lab/ICCTax)|
|147|[From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing](http://arxiv.org/abs/2510.12181)|Cathie Xiang, Tengfei Ma, Xiangxiang Zeng, Yiping Liu, Bosheng Song, Xiangzheng Fu|2025-10-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/xiaomingaaa/LLaDR)](https://github.com/xiaomingaaa/LLaDR)|
|148|[Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing](http://arxiv.org/abs/2510.12121)|Rongzhi Zhang, Liqin Ye, Yuzhao Heng, Xiang Guang Chen, Tong Yu, Lingkai Kong, Sudheer Chava, Chao Zhang|2025-10-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Pre-Control/pre-control)](https://github.com/Pre-Control/pre-control)|
|149|[Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation](http://arxiv.org/abs/2510.12047)|Soohan Lim, Joonghyuk Hahn, Hyunwoo Park, Sang‚ÄêKi Ko, Yo-Sub Han|2025-10-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/suhanmen/PACT)](https://github.com/suhanmen/PACT)|
|150|[Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models](http://arxiv.org/abs/2510.11683)|Nianyi Lin, Jiajie Zhang, Lei Hou, Juanzi Li|2025-10-13|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/THU-KEG/BGPO)](https://github.com/THU-KEG/BGPO)|
|151|[FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models](http://arxiv.org/abs/2510.11190)|Shengming Yuan, Xinyu Lyu, Shawn Wang, Beitao Chen, Jingkuan Song, Liyan Gao|2025-10-13|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ylhz/FlexAC)](https://github.com/ylhz/FlexAC)|
|152|[Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization](http://arxiv.org/abs/2510.15976)|Chenrui Wang, Jing Shu, Billy Chiu, Yu Li, Saleh Alharbi, Min Zhang, Jing Li|2025-10-13|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/fattyray/learning-to-watermark)](https://github.com/fattyray/learning-to-watermark)|
|153|[LLMInit: A Free Lunch from Large Language Models for Selective Initialization of Recommendation](https://doi.org/10.48550/arXiv.2503.01814)|Association for Computational Linguistics 2025, Yuqing Liu, Medya, Sourav, S Yu Philip, Yang, Liangwei, Yang, Wooseong, ...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/DavidZWZ/LLMInit)](https://github.com/DavidZWZ/LLMInit)|
|154|[VRoPE: Rotary Position Embedding for Video Large Language Models](https://doi.org/10.48550/arXiv.2502.11664)|Association for Computational Linguistics 2025, Cai Jun-xian, Chen Xi, Guo, Longteng, Liu Jing, Liu, Zikang, Liu Qing-bi...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/johncaged/VRoPE)](https://github.com/johncaged/VRoPE)|
|155|[TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://doi.org/10.48550/arXiv.2509.18173)|Association for Computational Linguistics 2025, Cheng Qing, Cremers, Daniel, Gadi Hari Krishna, Liu Lu, Luo Hongyi, Mato...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/bghjmn32/EMNLP2025_Turnback)](https://github.com/bghjmn32/EMNLP2025_Turnback)|
|156|[Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning](https://doi.org/10.48550/arXiv.2509.06436)|Association for Computational Linguistics 2025, Deng Ke, Li Li, Tian Lin, Xu Xiaofei, Yu Song|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Aireduce952/Tree-of-Agents)](https://github.com/Aireduce952/Tree-of-Agents)|
|157|[ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://doi.org/10.48448/76e1-2434)|Association for Computational Linguistics 2025, Guo, Jiani, Li Yun, Li, Zuchao, Wang, Qianren, Wu Jie, Yang, Yujiu, Zhan...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/gjn12-31/ToM)](https://github.com/gjn12-31/ToM)|
|158|[SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models](https://doi.org/10.48448/6vk1-nj91)|Association for Computational Linguistics 2025, Chen Yufeng, Huang Kaiyu, Man Zhibo, Mo, Fengran, Qi Rui, Xu, Jinan|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Cherry-qwq/SoT)](https://github.com/Cherry-qwq/SoT)|
|159|[ShiZhi: A Chinese Lightweight Large Language Model for Court View Generation](http://arxiv.org/abs/2510.09297)|Zhitian Hou, Kun Zeng|2025-10-10|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ZhitianHou/ShiZhi)](https://github.com/ZhitianHou/ShiZhi)|
|160|[RoDEval: A Robust Word Sense Disambiguation Evaluation Framework for Large Language Models](https://doi.org/10.48448/fzc4-g228)|Association for Computational Linguistics 2025, Kang Kun-peng, Li ShuaiMin, Li, Yishuo, Lu, Wenpeng, Wang Cong, Zhang Lu...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/DayDream405/RoDEval)](https://github.com/DayDream405/RoDEval)|
|161|[Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time](https://doi.org/10.48550/arXiv.2509.12521)|Association for Computational Linguistics 2025, Cao, Yuanpu, Chen Jing-hui, Lan Yifan, Lin Lu, Zhang Wei-tong|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Yifan-Lan/Phi)](https://github.com/Yifan-Lan/Phi)|
|162|[PIP: Perturbation-based Iterative Pruning for Large Language Models](https://doi.org/10.48550/arXiv.2501.15278)|Association for Computational Linguistics 2025, Cao Yi, CHAN, CHIMIN, Qu, Jianfeng, Shen Yu-Cheng, SHI Weijie, Xu Wei-Ji...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/caoyiiiiii/PIP)](https://github.com/caoyiiiiii/PIP)|
|163|[MemeArena: Automating Context-Aware Unbiased Evaluation of Harmfulness Understanding for Multimodal Large Language Models](https://doi.org/10.48448/m9rf-2w04)|Association for Computational Linguistics 2025, Chen Zixin, Deng Yayue, Li kaixin, Lin, Hongzhan, Luo, Ziyang, Ma Jing|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Lbotirx/MemeArena)](https://github.com/Lbotirx/MemeArena)|
|164|[MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering](https://doi.org/10.48550/arXiv.2502.18993)|Association for Computational Linguistics 2025, Lin Teng, Luo, Yuyu, Tang Nan, Wu Kai-Shun|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/tl2309/SRAG)](https://github.com/tl2309/SRAG)|
|165|[Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models](https://doi.org/10.48448/pxqn-3073)|Association for Computational Linguistics 2025, He Zhaofeng, Liu, Shuodi, Liu Ying-zhuo, Wang Yu-sheng, Wang Zi, WU Huij...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/summervvind/Select-Then-Decompose)](https://github.com/summervvind/Select-Then-Decompose)|
|166|[How to Make Large Language Models Generate 100% Valid Molecules?](https://doi.org/10.48448/wggc-qx95)|Association for Computational Linguistics 2025, Bi, Baolong, Chan, Alvin, Hooi, Bryan, Liu Yuan-sheng, Peng, Nanyun, Tan...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/wentao228/SmiSelf)](https://github.com/wentao228/SmiSelf)|
|167|[EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models](https://doi.org/10.18653/v1/2024.acl-demos.9)|Association for Computational Linguistics 2025, Chen, Huajun, Deng, Xinle, Wang Shuxun, Wang Mengru, Xu Kewei, Xu Zi-Wen...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/zjunlp/EasyEdit)](https://github.com/zjunlp/EasyEdit)|
|168|[EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models](https://doi.org/10.48550/arXiv.2505.20888)|Association for Computational Linguistics 2025, Cai, Wenrui, Huang Jun, Wang, Chengyu, Yan Junbing, Yue, Yuanhao|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/modelscope/easydistill)](https://github.com/modelscope/easydistill)|
|169|[Do Influence Functions Work on Large Language Models?](https://doi.org/10.48448/kx3y-6624)|Association for Computational Linguistics 2025, Li, Yige, Li Zhe, Sun Jun, Zhao Wei|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/plumprc/Failures-of-Influence-Functions-in-LLMs)](https://github.com/plumprc/Failures-of-Influence-Functions-in-LLMs)|
|170|[Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents](https://doi.org/10.48550/arXiv.2502.20073)|Association for Computational Linguistics 2025, Fu Hao, Niu, Lujie, Ren Lei, Sun Hao-chen, Wang Xiao-jie, Xu, Hao, Yuan ...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/YusaeMeow/Collab-Overcooked)](https://github.com/YusaeMeow/Collab-Overcooked)|
|171|[CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models](https://doi.org/10.48550/arXiv.2509.17318)|Association for Computational Linguistics 2025, Bai Jun, Chen Zhuo-fan, He Jiyuan, Hu Xing, Rong, Wenge, Wen, Haoxing, Z...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Icarus-1111/CogAtom)](https://github.com/Icarus-1111/CogAtom)|
|172|[CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://doi.org/10.48550/arXiv.2509.09703)|Association for Computational Linguistics 2025, Han Meng, Lin Changting, Tian Shengwei, Xu Zhenhua, Yue Xubin, Zhao Xi-x...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Xuzhenhua55/CTCC)](https://github.com/Xuzhenhua55/CTCC)|
|173|[CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task](https://doi.org/10.48448/24m5-9z30)|Association for Computational Linguistics 2025, Li Yu, Liu, Xuebo, Liu Jie, Ma, Xinyu, Mo, Haosi, Wong, Derek F., Zhang ...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Alessa-mo/CDT)](https://github.com/Alessa-mo/CDT)|
|174|[CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://doi.org/10.48550/arXiv.2508.20385)|Association for Computational Linguistics 2025, Cheng Fei, Murawaki, Yugo, Sandhan, Jivnesh, Sandhan, Tushar|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/jivnesh/CAPE)](https://github.com/jivnesh/CAPE)|
|175|[Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning](https://doi.org/10.48550/arxiv.2511.07483)|Association for Computational Linguistics 2025, He, Qianxi, Lei, Shanzhe, Ren Qing-yu, Wang Ying-chun, Wang Xu-Hong|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/qianxiHe147/C2RM)](https://github.com/qianxiHe147/C2RM)|
|176|[Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling](https://doi.org/10.48550/arXiv.2509.17289)|Association for Computational Linguistics 2025, Anuyah, Sydney, Chakraborty, Sunandan, Durresi Arjan, Dwarampudi, Sri Ra...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/KaushikMahmud/CoDe-KG_EMNLP_2025)](https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025)|
|177|[AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models](https://doi.org/10.48550/arXiv.2509.12019)|Association for Computational Linguistics 2025, Jin, Jungyu, LeeÔºå Changhun, LeeÔºå Sang-Jun, Park Eunhyeok, Woo, Seung-tae...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/dlwns147/amq)](https://github.com/dlwns147/amq)|
|178|[Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation](https://doi.org/10.48550/arXiv.2509.20162)|Association for Computational Linguistics 2025, Nie ChaoJun, Wang, Guanxiang, Wang Zichen, WU Shi-song, Zhou Jun|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/ChaojunNie/RLAG)](https://github.com/ChaojunNie/RLAG)|
|179|[Neuron-Level Analysis of Cultural Understanding in Large Language Models](http://arxiv.org/abs/2510.08284)|Hitomi Yanaka|2025-10-09|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ynklab/CULNIG)](https://github.com/ynklab/CULNIG)|
|180|[Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models](http://arxiv.org/abs/2510.08859)|Ragib Amin Nihal, Rui Wen, Kazuhiro Nakadai, Jun Sakuma|2025-10-09|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Ragib-Amin-Nihal/PE-CoA)](https://github.com/Ragib-Amin-Nihal/PE-CoA)|
|181|[Mitigating Judgment Preference Bias in Large Language Models through Group-Based Polling](http://arxiv.org/abs/2510.08145)|Yukun Yan, Ge Yu|2025-10-09|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/NEUIR/Genii)](https://github.com/NEUIR/Genii)|
|182|[AWM: Accurate Weight-Matrix Fingerprint for Large Language Models](http://arxiv.org/abs/2510.06738)|Boyi Zeng, Lin Chen, Ziwei He, Xinbing Wang, Zhouhan Lin|2025-10-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LUMIA-Group/AWM)](https://github.com/LUMIA-Group/AWM)|
|183|[Aligning Large Language Models via Fully Self-Synthetic Data](http://arxiv.org/abs/2510.06652)|Shangjian Yin, Zhepei Wei, Xinyu Zhu, Wei-Lin Chen, Yu Meng|2025-10-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/SJY8460/SAO)](https://github.com/SJY8460/SAO)|
|184|[Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models](http://arxiv.org/abs/2510.07037)|Ravi Sheth, M. K. Patil|2025-10-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/lingo-iitgn/awesome-code-mixing)](https://github.com/lingo-iitgn/awesome-code-mixing)|
|185|[GOFlowLLM - Curating miRNA literature with Large Language Models and flowcharts](https://doi.org/10.1093/bioinformatics/btaf683)|Andrew Green, Nancy Ontiveros‚ÄêPalacios, Isaac Jandalala, Simona Panni, Valerie Wood, Giulia Antonazzo, Helen Attrill, Al...|2025-10-08|bioRxiv (Cold Spring Harbor Laboratory)|[![Star](https://img.shields.io/github/stars/RNAcentral/GO_Flow_LLM)](https://github.com/RNAcentral/GO_Flow_LLM)|
|186|[Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models](http://arxiv.org/abs/2510.07048)|Yuntao Gui, James Cheng|2025-10-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ytgui/Search-R3)](https://github.com/ytgui/Search-R3)|
|187|[When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation](http://arxiv.org/abs/2510.07238)|Xunyi Jiang, Deng-Lin Chang, Julian McAuley, Xin Xu|2025-10-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/JiangXunyi/BenchAge)](https://github.com/JiangXunyi/BenchAge)|
|188|[HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection](http://arxiv.org/abs/2510.05609)|Junwen Chen, Peilin Xiong, ‚Ä™Keiji Yanai‚Ä¨|2025-10-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/cjw2021/HOI-R1)](https://github.com/cjw2021/HOI-R1)|
|189|[Pok√©LLMon: A Grounding and Reasoning Benchmark for Large Language Models in Pok√©mon Battles](https://doi.org/10.1145/3771095)|Sihao Hu, Tiansheng Huang, Guishan Liu, Ramana Rao Kompella, Ling Liu|2025-10-07|ACM Transactions on Internet Technology|[![Star](https://img.shields.io/github/stars/git-disl/PokeLLMon)](https://github.com/git-disl/PokeLLMon)|
|190|[FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification](http://arxiv.org/abs/2510.04671)|Chao Liu, Huan Zhuang, Hongfei Lin|2025-10-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/DUT-LiuChao/FocusMed)](https://github.com/DUT-LiuChao/FocusMed)|
|191|[Imperceptible Jailbreaking against Large Language Models](http://arxiv.org/abs/2510.05025)|Kuofeng Gao, Yiming Li, Chao‚ÄêHai Du, Xin Wang, Xingjun Ma, Shu‚ÄêTao Xia, Tianyu Pang|2025-10-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/sail-sg/imperceptible-jailbreaks)](https://github.com/sail-sg/imperceptible-jailbreaks)|
|192|[Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"](http://arxiv.org/abs/2510.06275)|Rasendu Mishra, Julian I. Bibo, Quinten van Engelen, Henk Schaapman|2025-10-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/julianbibo/xrec-reproducibility)](https://github.com/julianbibo/xrec-reproducibility)|
|193|[UniVoice: Unifying Autoregressive ASR and Flow-Matching based TTS with Large Language Models](http://arxiv.org/abs/2510.04593)|Wenhao Guan, Kaidi Wang, Peijie Chen, Lin Li, Xie Chen|2025-10-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/gwh22/UniVoice)](https://github.com/gwh22/UniVoice)|
|194|[MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models](http://arxiv.org/abs/2510.04363)|Hyunjun Kim|2025-10-05|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hyunjun1121/MacroBench)](https://github.com/hyunjun1121/MacroBench)|
|195|[PLSemanticsBench: Large Language Models As Programming Language Interpreters](https://doi.org/10.48550/arXiv.2510.03415)|Aditya Thimmaiah, Jiyang Zhang, Jayanth Srinivasa, Junyi Jessy Li, Milos Gligoric|2025-10-03|arXiv|[![Star](https://img.shields.io/github/stars/EngineeringSoftware/PLSemanticsBench)](https://github.com/EngineeringSoftware/PLSemanticsBench)|
|196|[Cache-to-Cache: Direct Semantic Communication Between Large Language Models](http://arxiv.org/abs/2510.03215)|Ziying Min, Heyi Zhang, Jianjun Yan, Jianrong Xu, Wanli Ouyang, Yu Wang|2025-10-03|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/thu-nics/C2C)](https://github.com/thu-nics/C2C)|
|197|[Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking](http://arxiv.org/abs/2510.02962)|Jingqi Zhang, Ruibo Chen, Yi Yang, Peihua Mai, Heng Huang, Yan Pang|2025-10-03|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/NusIoraPrivacy/TRACE)](https://github.com/NusIoraPrivacy/TRACE)|
|198|[Microscaling Floating Point Formats for Large Language Models](http://arxiv.org/abs/2510.01863)|Marco Cococcioni, Dario Pagani, Federico Rossi|2025-10-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/unipi-dii-compressedarith/llm.c-sve)](https://github.com/unipi-dii-compressedarith/llm.c-sve)|
|199|[Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](http://arxiv.org/abs/2510.01576)|Ricardo E. Gonzalez Penuela, Felipe Arias-Russi, Victor Capriles|2025-10-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions)](https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions)|
|200|[Cognitive LLMs: Toward Human-Like Artificial Intelligence by Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-Making](https://doi.org/10.1177/29498732251377341)|Siyu Wu, Alessandro Oltramari, Jonathan Francis, C. Lee Giles, Frank E. Ritter|2025-10-01|Neurosymbolic Artificial Intelligence|[![Star](https://img.shields.io/github/stars/SiyuWu528/LLM-ACTR)](https://github.com/SiyuWu528/LLM-ACTR)|

![Star History Chart](https://api.star-history.com/svg?repos=mtuann/llm-updated-papers&type=Date)

