# Table of Contents
1. [Large Language Models Papers](#large-language-models-papers)
2. [Other topics](#other-topics)
3. [Large Language Models Papers with Code](#large-language-models-papers-with-code)


## Large Language Models Papers
This GitHub repository contains an updated list of Federated Learning papers as of **July 16, 2025**. 

- The resources are collected from various sources, including arXiv, NeurIPS, ICML, ICLR, ACL, EMNLP, AAAI, IJCAI, KDD, CVPR, ICCV, ECCV, NIPS, IEEE, ACM, Springer, ScienceDirect, Wiley, Nature, Science, and other top AI/ML conferences and journals.
- For a better reading experience, visit the [Shinyapps website](https://mtuann.shinyapps.io/research-papers/).

---
# Other Topics
Explore additional research papers on the following topics:
- For **Large Language Models** papers, please visit the [**LLM Repository**](https://github.com/mtuann/llm-updated-papers).
- For **Backdoor Learning** papers, please visit the [**Backdoor Learning Repository**](https://github.com/mtuann/backdoor-ai-resources).
- For **Federated Learning** papers, please visit the [**Federated Learning Repository**](https://github.com/mtuann/federated-learning-updated-papers).
- For **Machine Unlearning** papers, please visit the [**Machine Unlearning Repository**](https://github.com/mtuann/machine-unlearning-papers).

---

For contributions, inquiries, or suggestions, feel free to reach out via [email](mailto:tuannm0312@gmail.com).

---

If you find this application helpful and would like to support its development, you can buy me a coffee using one of the following methods:
- **Techcombank (Vietnam):** 5877 5555 55 (Nguyen Thi Lan Phuong)
- **PayPal or Credit/Debit Card:** [https://ko-fi.com/miutheladycat](https://ko-fi.com/miutheladycat)

---

## Large Language Models Papers with Code
Due to GitHub repository limitations, this section includes only those papers that provide accompanying code, sorted by publish date. For access to the full list of papers, please visit the [Shinyapps website](https://mtuann.shinyapps.io/research-papers/).

---


|No.|Title|Authors|Publish Date|Venue|Code|URL|
|---|---|---|---|---|---|---|
|1|DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering|Yinsheng Li, Zhen Dong, Yi Shao|2025-07-15|arXiv|https://github.com/Eason-Li-AIS/DrafterBench|http://arxiv.org/abs/2507.11527v1|
|2|The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs|Zichen Wen, Jiashu Qu, Dongrui Liu, Zhiyuan Liu, Ruixi Wu, Yicun Yang, Xiangqi Jin, Haoyun Xu, Xuyang Liu, Weijia Li, Chaochao Lu, Jing Shao, Conghui He, Linfeng Zhang|2025-07-15|arXiv|https://github.com/ZichenWen1/DIJA|http://arxiv.org/abs/2507.11097v1|
|3|First-Order Error Matters: Accurate Compensation for Quantized Large Language Models|Xingyu Zheng, Haotong Qin, Yuye Li, Jiakai Wang, Jinyang Guo, Michele Magno, Xianglong Liu|2025-07-15|arXiv|https://github.com/Xingyu-Zheng/FOEM|http://arxiv.org/abs/2507.11017v1|
|4|Internal Value Alignment in Large Language Models through Controlled Value Vector Activation|Haoran Jin, Meng Li, Xiting Wang, Zhihao Xu, Minlie Huang, Yantao Jia, Defu Lian|2025-07-15|arXiv|https://github.com/hr-jin/ConVA|http://arxiv.org/abs/2507.11316v1|
|5|Past-Future Scheduler for LLM Serving under SLA Guarantees|Ruihao Gong, Shihao Bai, Siyu Wu, Yunqian Fan, Zaijun Wang, Xiuhong Li, Hailong Yang, Xianglong Liu|2025-07-14|arXiv|https://github.com/ModelTC/lightllm|http://arxiv.org/abs/2507.10150v1|
|6|Warehouse Spatial Question Answering with LLM Agent|Hsiang-Wei Huang, Jen-Hao Cheng, Kuang-Ming Chen, Cheng-Yen Yang, Bahaa Alattar, Yi-Ru Lin, Pyongkun Kim, Sangwon Kim, Kwangju Kim, Chung-I Huang, Jenq-Neng Hwang|2025-07-14|arXiv|https://github.com/hsiangwei0903/SpatialAgent|http://arxiv.org/abs/2507.10778v1|
|7|Memorization Sinks: Isolating Memorization during LLM Training|Gaurav R. Ghosal, Pratyush Maini, Aditi Raghunathan|2025-07-14|arXiv|http://github.com/grghosal/MemSinks|http://arxiv.org/abs/2507.09937v1|
|8|A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study|Taniv Ashraf|2025-07-13|arXiv|https://github.com/TanivAshraf/ai-stock-analyzer|http://arxiv.org/abs/2507.09583v1|
|9|AICrypto: A Comprehensive Benchmark For Evaluating Cryptography Capabilities of Large Language Models|Yu Wang, Yijian Liu, Liheng Ji, Han Luo, Wenjie Li, Xiaofei Zhou, Chiyun Feng, Puji Wang, Yuhan Cao, Geyuan Zhang, Xiaojian Li, Rongwu Xu, Yilei Chen, Tianxing He|2025-07-13|arXiv|https://aicryptobench.github.io|http://arxiv.org/abs/2507.09580v1|
|10|TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit|Paulo Salem, Robert Sim, Christopher Olsen, Prerit Saxena, Rafael Barcelos, Yi Ding|2025-07-13|arXiv|https://github.com/microsoft/tinytroupe|http://arxiv.org/abs/2507.09788v1|
|11|Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs|Yangning Li, Weizhi Zhang, Yuyao Yang, Wei-Chieh Huang, Yaozu Wu, Junyu Luo, Yuanchen Bei, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Chunkit Chan, Yankai Chen, Zhongfen Deng, Yinghui Li, Hai-Tao Zheng, Dongyuan Li, Renhe Jiang, Ming Zhang, Yangqiu Song, Philip S. Yu|2025-07-13|arXiv|https://github.com/DavidZWZ/Awesome-RAG-Reasoning|http://arxiv.org/abs/2507.09477v1|
|12|Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models|Anita Kriz, Elizabeth Laura Janes, Xing Shen, Tal Arbel|2025-07-12|arXiv|https://github.com/xingbpshen/prompt4trust|http://arxiv.org/abs/2507.09279v2|
|13|Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding|Wencan Huang, Daizong Liu, Wei Hu|2025-07-12|arXiv|https://github.com/wencan25/Fast3D|http://arxiv.org/abs/2507.09334v1|
|14|DS@GT at Touché: Large Language Models for Retrieval-Augmented Debate|Anthony Miyaguchi, Conor Johnston, Aaryan Potdar|2025-07-12|arXiv|https://github.com/dsgt-arc/touche-2025-rad|http://arxiv.org/abs/2507.09090v1|
|15|A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench|David Schlangen, Sherzod Hakimov, Jonathan Jordan, Philipp Sadler|2025-07-11|arXiv|https://github.com/clembench/clembench|http://arxiv.org/abs/2507.08491v1|
|16|Exploring Design of Multi-Agent LLM Dialogues for Research Ideation|Keisuke Ueda, Wataru Hirota, Takuto Asakura, Takahiro Omi, Kosuke Takahashi, Kosuke Arima, Tatsuya Ishigaki|2025-07-11|arXiv|https://github.com/g6000/MultiAgent-Research-Ideator|http://arxiv.org/abs/2507.08350v1|
|17|ToolRegistry: A Protocol-Agnostic Tool Management Library for Function-Calling LLMs|Peng Ding|2025-07-11|arXiv|https://github.com/Oaklight/ToolRegistry|http://arxiv.org/abs/2507.10593v1|
|18|A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning|Hiroshi Yoshihara, Taiki Yamaguchi, Yuichi Inoue|2025-07-11|arXiv|https://github.com/analokmaus/kaggle-aimo2-fast-math-r1|http://arxiv.org/abs/2507.08267v1|
|19|Corvid: Improving Multimodal Large Language Models Towards Chain-of-Thought Reasoning|Jingjing Jiang, Chao Ma, Xurui Song, Hanwang Zhang, Jun Luo|2025-07-10|arXiv|https://mm-vl.github.io/corvid|http://arxiv.org/abs/2507.07424v1|
|20|Machine Bullshit: Characterizing the Emergent Disregard for Truth in Large Language Models|Kaiqu Liang, Haimin Hu, Xuandong Zhao, Dawn Song, Thomas L. Griffiths, Jaime Fernández Fisac|2025-07-10|arXiv|https://machine-bullshit.github.io|http://arxiv.org/abs/2507.07484v1|
|21|PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations|Fedor Rodionov, Abdelrahman Eldesokey, Michael Birsak, John Femiani, Bernard Ghanem, Peter Wonka|2025-07-10|arXiv|https://OldDelorean.github.io/PlanQA/|http://arxiv.org/abs/2507.07644v1|
|22|RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning|Hongzhi Zhang, Jia Fu, Jingyuan Zhang, Kai Fu, Qi Wang, Fuzheng Zhang, Guorui Zhou|2025-07-10|arXiv|https://github.com/Kwai-Klear/RLEP|http://arxiv.org/abs/2507.07451v1|
|23|StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley|Weihao Tan, Changjiu Jiang, Yu Duan, Mingcong Lei, Jiageng Li, Yitian Hong, Xinrun Wang, Bo An|2025-07-10|arXiv|https://weihaotan.github.io/StarDojo|http://arxiv.org/abs/2507.07445v2|
|24|TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs|Duygu Nur Yaldiz, Yavuz Faruk Bakman, Sungmin Kang, Alperen Öziş, Hayrettin Eren Yildiz, Mitash Ashish Shah, Zhiqi Huang, Anoop Kumar, Alfy Samuel, Daben Liu, Sai Praneeth Karimireddy, Salman Avestimehr|2025-07-10|arXiv|https://github.com/Ybakman/TruthTorchLM|http://arxiv.org/abs/2507.08203v1|
|25|InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior|Huisheng Wang, Zhuoshi Pan, Hangjing Zhang, Mingxiao Liu, Hanqing Gao, H. Vicky Zhao|2025-07-09|arXiv|https://github.com/thu-social-network-research-group/InvestAlign|http://arxiv.org/abs/2507.06528v1|
|26|Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model|Jing Liang, Hongyao Tang, Yi Ma, Jinyi Liu, Yan Zheng, Shuyue Hu, Lei Bai, Jianye Hao|2025-07-09|arXiv|https://anitaleungxx.github.io/ReMix|http://arxiv.org/abs/2507.06892v2|
|27|CAVGAN: Unifying Jailbreak and Defense of LLMs via Generative Adversarial Attacks on their Internal Representations|Xiaohu Li, Yunfeng Ning, Zepeng Bao, Mayi Xu, Jianhao Chen, Tieyun Qian|2025-07-08|arXiv|https://github.com/NLPGM/CAVGAN|http://arxiv.org/abs/2507.06043v1|
|28|UQLM: A Python Package for Uncertainty Quantification in Large Language Models|Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Ho-Kyeong Ra, Viren Bajaj, Zeya Ahmad|2025-07-08|arXiv|https://github.com/cvs-health/uqlm|http://arxiv.org/abs/2507.06196v1|
|29|ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation|Chenchen Zhang, Yuhang Li, Can Xu, Jiaheng Liu, Ao Liu, Shihui Hu, Dengpeng Wu, Guanhua Huang, Kejiao Li, Qi Yi, Ruibin Xiong, Haotian Zhu, Yuanxing Zhang, Yuhao Jiang, Yue Zhang, Zenan Xu, Bohui Zhai, Guoxiang He, Hebin Li, Jie Zhao, Le Zhang, Lingyun Tan, Pengyu Guo, Xianshu Pang, Yang Ruan, Zhifeng Zhang, Zhonghu Wang, Ziyan Xu, Zuopu Yin, Wiggin Zhou, Chayse Zhou, Fengzong Lian|2025-07-07|arXiv|https://artifactsbenchmark.github.io/|http://arxiv.org/abs/2507.04952v1|
|30|Hierarchical Intent-guided Optimization with Pluggable LLM-Driven Semantics for Session-based Recommendation|Jinpeng Chen, Jianxiang He, Huan Li, Senzhang Wang, Yuan Cao, Kaimin Wei, Zhenye Yang, Ye Ji|2025-07-07|arXiv|https://github.com/hjx159/HIPHOP|http://arxiv.org/abs/2507.04623v1|
|31|Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models|Ziqi Miao, Lijun Li, Yuan Xiong, Zhenhua Liu, Pengyu Zhu, Jing Shao|2025-07-07|arXiv|https://github.com/Dtc7w3PQ/Response-Attack|http://arxiv.org/abs/2507.05248v1|
|32|Spatio-Temporal LLM: Reasoning about Environments and Actions|Haozhen Zheng, Beitong Tian, Mingyuan Wu, Zhenggang Tang, Klara Nahrstedt, Alex Schwing|2025-07-07|arXiv|https://zoezheng126.github.io/STLLM-website/|http://arxiv.org/abs/2507.05258v1|
|33|VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots|Danil S. Grigorev, Alexey K. Kovalev, Aleksandr I. Panov|2025-07-07|arXiv|https://verifyllm.github.io|http://arxiv.org/abs/2507.05118v1|
|34|any4: Learned 4-bit Numeric Representation for LLMs|Mostafa Elhoushi, Jeff Johnson|2025-07-07|arXiv|https://github.com/facebookresearch/any4|http://arxiv.org/abs/2507.04610v1|
|35|DP-Fusion: Token-Level Differentially Private Inference for Large Language Models|Rushil Thareja, Preslav Nakov, Praneeth Vepakomma, Nils Lukas|2025-07-06|arXiv|https://github.com/MBZUAI-Trustworthy-ML/DP-Fusion-DPI|http://arxiv.org/abs/2507.04531v1|
|36|FIXME: Towards End-to-End Benchmarking of LLM-Aided Design Verification|Gwok-Waa Wan, Shengchu Su, Ruihu Wang, Qixiang Chen, Sam-Zaak Wong, Mengnv Xing, Hefei Feng, Yubo Wang, Yinan Zhu, Jingyi Zhang, Jianmin Ye, Xinlai Wan, Tao Ni, Qiang Xu, Nan Guan, Zhe Jiang, Xi Wang, Yang Jun|2025-07-06|arXiv|https://github.com/ChatDesignVerification/FIXME|http://arxiv.org/abs/2507.04276v1|
|37|Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents|Ziyang Miao, Qiyu Sun, Jingyuan Wang, Yuchen Gong, Yaowei Zheng, Shiqi Li, Richong Zhang|2025-07-05|arXiv|https://github.com/ConardLi/easy-dataset|http://arxiv.org/abs/2507.04009v1|
|38|Chat2SPaT: A Large Language Model Based Tool for Automating Traffic Signal Control Plan Management|Yue Wang, Miao Zhou, Guijing Huang, Rui Zhuo, Chao Yi, Zhenliang Ma|2025-07-04|arXiv|https://github.com/yuewangits/Chat2SPaT|http://arxiv.org/abs/2507.05283v1|
|39|Continual Gradient Low-Rank Projection Fine-Tuning for LLMs|Chenxu Wang, Yilin Lyu, Zicheng Sun, Liping Jing|2025-07-03|arXiv|https://github.com/Wcxwcxw/GORP|http://arxiv.org/abs/2507.02503v1|
|40|DistZO2: High-Throughput and Memory-Efficient Zeroth-Order Fine-tuning LLMs with Distributed Parallel Computing|Liangyu Wang, Huanyi Xie, Di Wang|2025-07-03|arXiv|https://github.com/liangyuwang/zo2|http://arxiv.org/abs/2507.03211v1|
|41|FlowSpec: Continuous Pipelined Speculative Decoding for Efficient Distributed LLM Inference|Xing Liu, Lizhuo Luo, Ming Tang, Chao Huang|2025-07-03|arXiv|https://github.com/Leosang-lx/FlowSpec#|http://arxiv.org/abs/2507.02620v1|
|42|JoyTTS: LLM-based Spoken Chatbot With Voice Cloning|Fangru Zhou, Jun Zhao, Guoxin Wang|2025-07-03|arXiv|https://github.com/jdh-algo/JoyTTS|http://arxiv.org/abs/2507.02380v1|
|43|MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs|Purbesh Mitra, Sennur Ulukus|2025-07-03|arXiv|https://github.com/purbeshmitra/MOTIF|http://arxiv.org/abs/2507.02851v1|
|44|AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness|Zixin Chen, Hongzhan Lin, Kaixin Li, Ziyang Luo, Zhen Ye, Guang Chen, Zhiyong Huang, Jing Ma|2025-07-02|arXiv|https://github.com/Lbotirx/AdamMeme|http://arxiv.org/abs/2507.01702v1|
|45|Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages|Samridhi Raj Sinha, Rajvee Sheth, Abhishek Upperwal, Mayank Singh|2025-07-02|arXiv|https://github.com/lingo-iitgn/eka-eval|http://arxiv.org/abs/2507.01853v2|
|46|Is External Information Useful for Stance Detection with LLMs?|Quang Minh Nguyen, Taegyoon Kim|2025-07-02|arXiv|https://github.com/ngqm/acl2025-stance-detection|http://arxiv.org/abs/2507.01543v1|
|47|GAF-Guard: An Agentic Framework for Risk Management and Governance in Large Language Models|Seshu Tirupathi, Dhaval Salwala, Elizabeth Daly, Inge Vejsbjerg|2025-07-01|arXiv|https://github.com/IBM/risk-atlas-nexus-demos/tree/main/gaf-guard|http://arxiv.org/abs/2507.02986v2|
|48|Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications|Jindong Han, Yansong Ning, Zirui Yuan, Hang Ni, Fan Liu, Tengfei Lyu, Hao Liu|2025-07-01|arXiv|https://github.com/usail-hkust/Awesome-Urban-LLM-Agents|http://arxiv.org/abs/2507.00914v1|
|49|MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models|Jianghao Lin, Xinyuan Wang, Xinyi Dai, Menghui Zhu, Bo Chen, Ruiming Tang, Yong Yu, Weinan Zhang|2025-07-01|arXiv|https://github.com/wxydada/MassTool|http://arxiv.org/abs/2507.00487v2|
|50|FlashDP: Private Training Large Language Models with Efficient DP-SGD|Liangyu Wang, Junxiao Wang, Jie Ren, Zihang Xiang, David E. Keyes, Di Wang|2025-07-01|arXiv|https://github.com/kaustpradalab/flashdp|http://arxiv.org/abs/2507.01154v1|
|51|Causal Prompting for Implicit Sentiment Analysis with Large Language Models|Jing Ren, Wenhao Zhou, Bowen Li, Mujie Liu, Nguyen Linh Dan Le, Jiade Cen, Liping Chen, Ziqi Xu, Xiwei Xu, Xiaodong Li|2025-07-01|arXiv|https://github.com/whZ62/CAPITAL|http://arxiv.org/abs/2507.00389v1|
|52|Enhancing LLM Agent Safety via Causal Influence Prompting|Dongyoon Hahm, Woogyeol Jin, June Suk Choi, Sungsoo Ahn, Kimin Lee|2025-07-01|arXiv|https://github.com/HahmDY/causal_influence_prompting|http://arxiv.org/abs/2507.00979v1|
|53|Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks|Xian Zhang, Xiang Cheng|2025-06-30|arXiv|https://github.com/zxyl1003/MLLM-Geolocation-Evaluation|https://doi.org/10.48550/arXiv.2506.23481|
|54|AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data|JiaRu Wu, Mingwei Liu|2025-06-30|arXiv|https://github.com/SYSUSELab/AutoEvoEval|http://arxiv.org/abs/2506.23735v1|
|55|ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data|Yu Zhang, Ruijie Yu, Jidong Tian, Feng Zhu, Jiapeng Liu, Xiaokang Yang, Yaohui Jin, Yanyan Xu|2025-06-30|arXiv|https://github.com/Zhanghahah/ChemActor|http://arxiv.org/abs/2506.23520v2|
|56|Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent|Haocheng Yu, Yaxiong Wu, Hao Wang, Wei Guo, Yong Liu, Yawen Li, Yuyang Ye, Junping Du, Enhong Chen|2025-06-30|arXiv|https://github.com/Alcein/TAIRA|http://arxiv.org/abs/2506.23485v1|
|57|Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons|Chi Chiu So, Yueyue Sun, Jun-Min Wang, Siu Pang Yung, Anthony Wai Keung Loh, Chun Pong Chau|2025-06-29|arXiv|https://github.com/kelvinhkcs/Deep-Relational-Reasoning|https://doi.org/10.48550/arXiv.2506.23128|
|58|UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding|Jie Feng, Shengyuan Wang, Tianhui Liu, Yanxin Xi, Yong Li|2025-06-29|arXiv|https://github.com/tsinghua-fib-lab/UrbanLLaVA|https://doi.org/10.48550/arXiv.2506.23219|
|59|Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning|Xiang Zhuang, Bin Wu, Jiyu Cui, Kehua Feng, Xiaotong Li, Huabin Xing, Keyan Ding, Qiang Zhang, Huajun Chen|2025-06-29|arXiv|https://github.com/HICAI-ZJU/K-MSE|http://arxiv.org/abs/2506.23056v1|
|60|Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models|Younwoo Choi, Changling Li, Yongjin Yang, Zhijing Jin|2025-06-28|arXiv|https://github.com/younwoochoi/InterlocutorAwarenessLLM|https://doi.org/10.48550/arXiv.2506.22957|
|61|MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs|Jianhui Wei, Zijie Meng, Zikai Xiao, Tianxiang Hu, Yang Feng, Zhijie Zhou, Jian Wu, Zuozhu Liu|2025-06-28|arXiv|https://github.com/JianhuiWei7/MedEthicsQA|http://arxiv.org/abs/2506.22808v1|
|62|LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs|Boyuan Sun, Jiaxing Zhao, Xihan Wei, Qibin Hou|2025-06-27|arXiv|https://github.com/HumanMLLM/LLaVA-Scissor|http://arxiv.org/abs/2506.21862v1|
|63|LMPVC and Policy Bank: Adaptive voice control for industrial robots with code generating LLMs and reusable Pythonic policies|Ossi Parikka, Roel Pieters|2025-06-27|arXiv|https://github.com/ozzyuni/LMPVC|http://arxiv.org/abs/2506.22028v1|
|64|GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling|Tianhao Chen, Xin Xu, Zijing Liu, Pengxiang Li, Xinyuan Song, Ajay Kumar Jaiswal, Fan Zhang, Jishan Hu, Yang Wang, Hao Chen, Shizhe Diao, Shiwei Liu, Yu Li, Lu Yin, Can Yang|2025-06-27|arXiv|https://github.com/dandingsky/GPAS|http://arxiv.org/abs/2506.22049v2|
|65|PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory|Junho Myung, Yeon Su Park, Sunwoo Kim, Shin Yoo, Alice Oh|2025-06-27|arXiv|https://github.com/yeonsuuuu28/papers-please|https://doi.org/10.48550/arXiv.2506.21961|
|66|Evaluating List Construction and Temporal Understanding capabilities of Large Language Models|Alexandru Dumitru, Venktesh V, Adam Jatowt, Avishek Anand|2025-06-26|arXiv|https://github.com/elixir-research-group/TLQA|https://doi.org/10.48550/arXiv.2506.21783|
|67|MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models|Yifan Liu, Xishun Liao, Haoxuan Ma, Jonathan Liu, Rohan Jadhav, Jiaqi Ma|2025-06-26|arXiv|https://github.com/ucla-mobility/MobiVerse|https://doi.org/10.48550/arXiv.2506.21784|
|68|ThinkSound: Chain-of-Thought Reasoning in Multimodal Large Language Models for Audio Generation and Editing|Huadai Liu, Jialei Wang, Kaicheng Luo, Wen Wang, Qian Chen, Zhou Zhao, Wei Xue|2025-06-26|arXiv|https://ThinkSound-Project.github.io|https://doi.org/10.48550/arXiv.2506.21448|
|69|Double-Checker: Enhancing Reasoning of Slow-Thinking LLMs via Self-Critical Fine-Tuning|Xin Xu, Tianhao Chen, Fan Zhang, Wanlong Liu, Pengxiang Li, Ajay Kumar Jaiswal, Yuchen Yan, Jishan Hu, Yang Wang, Hao Chen, Shiwei Liu, Shizhe Diao, Can Yang, Lu Yin|2025-06-26|arXiv|https://github.com/XinXU-USTC/DoubleChecker|http://arxiv.org/abs/2506.21285v1|
|70|A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection|Songsoo Kim, Seungtae Lee, See Young Lee, Joonho Kim, Keechan Kan, Dukyong Yoon|2025-06-25|arXiv|https://github.com/radssk/mp-rred|https://doi.org/10.48550/arXiv.2506.20112|
|71|GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching|Guinan Su, Li Shen, Lu Yin, Shiwei Liu, Yanwu Yang, Jonas Geiping|2025-06-25|arXiv|https://github.com/Guinan-Su/auto-merge-llm|https://doi.org/10.48550/arXiv.2506.20480|
|72|Q-resafe: Assessing Safety Risks and Quantization-aware Safety Patching for Quantized Large Language Models|Kejia Chen, Jiawen Zhang, Jiacong Hu, Yu Wang, Jian Lou, Zunlei Feng, Mingli Song|2025-06-25|arXiv|https://github.com/Thecommonirin/Qresafe|https://doi.org/10.48550/arXiv.2506.20251|
|73|Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models|Weiyi Zhao, Xiaoyu Tan, Liang Liu, Sijia Li, Youwei Song, Xihe Qiu|2025-06-25|arXiv|https://github.com/zgg2577/VS-KC|https://doi.org/10.48550/arXiv.2506.22500|
|74|LLM-Driven Medical Document Analysis: Enhancing Trustworthy Pathology and Differential Diagnosis|Lei Kang, Xuanshuo Fu, Oriol Ramos Terrades, Javier Vazquez-Corral, Ernest Valveny, Dimosthenis Karatzas|2025-06-24|arXiv|https://github.com/leitro/Differential-Diagnosis-LoRA|http://arxiv.org/abs/2506.19702v1|
|75|CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems|Haochen Zhang, Tianyi Zhang, Junze Yin, Oren Gal, Anshumali Shrivastava, Vladimir Braverman|2025-06-24|arXiv|https://github.com/HaochenZhang717/CoVE-official-Repo|http://arxiv.org/abs/2506.19993v1|
|76|Surgery-R1: Advancing Surgical-VQLA with Reasoning Multimodal Large Language Model via Reinforcement Learning|Pengfei Hao, Shuaibo Li, Hongqiu Wang, Zhizhuo Kou, Junhang Zhang, Guang Yang, Lei Zhu|2025-06-24|arXiv|https://github.com/FiFi-HAO467/Surgery-R1|https://doi.org/10.48550/arXiv.2506.19469|
|77|Outlier-Safe Pre-Training for Robust 4-Bit Quantization of Large Language Models|Jungwoo Park, Taewhoo Lee, Chanwoong Yoon, Hyeon Hwang, Jaewoo Kang|2025-06-24|arXiv|https://github.com/dmis-lab/Outlier-Safe-Pre-Training|https://doi.org/10.48550/arXiv.2506.19697|
|78|ECCoT: A Framework for Enhancing Effective Cognition via Chain of Thought in Large Language Model|Zhenke Duan, Jiqun Pan, Jiani Tu, Xiaoyi Wang, Yanqing Wang|2025-06-24|arXiv|https://github.com/erwinmsmith/ECCoT|https://doi.org/10.48550/arXiv.2506.19599|
|79|Can Large Language Models Capture Human Annotator Disagreements?|Jingwei Ni, Yu Fan, Vilém Zouhar, Donya Rooein, Alexander Miserlis Hoyle, Mrinmaya Sachan, Markus Leippold, Dirk Hovy, Elliott Ash|2025-06-24|arXiv|https://github.com/EdisonNi-hku/Disagreement_Prediction|https://doi.org/10.48550/arXiv.2506.19467|
|80|What You Think Is What You Get: Bridge User Intent and Transfer Function Design through Multimodal Large Language Models|Yiyao Wang, Bo Pan, Ke Wang, Han Liu, Jinyuan Mao, Yuxin Liu, Minfeng Zhu, Bo Zhang, Weifeng Chen, Xiuqi Huang, Wei Chen|2025-06-23|arXiv|https://github.com/wyysteelhead/TFevolve|https://doi.org/10.48550/arXiv.2506.18407|
|81|SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in Real-World Applications|Jinyang Li, Xiaolong Li, Ge Qu, Per Jacobsson, Bowen Qin, Binyuan Hui, Shuzheng Si, Nan Huo, Xiaohan Xu, Yue Zhang, Ziwei Tang, Yuanshuai Li, Florensia Widjaja, Xintong Zhu, Feige Zhou, Yongfeng Huang, Yannis Papakonstantinou, Fatma Ozcan, Chenhao Ma, Reynold Cheng|2025-06-23|arXiv|https://bird-critic.github.io/|http://arxiv.org/abs/2506.18951v1|
|82|WiLLM: an Open Framework for LLM Services over Wireless Systems|Boyi Liu, Yongguang Lu, Jianguo Zhao, Qiang Yang, Wen Wu, Lin Chen, Jagmohan Chauhan, Jun Zhang|2025-06-23|arXiv|https://openwillm.github.io|http://arxiv.org/abs/2506.19030v2|
|83|Co-persona: Leveraging LLMs and Expert Collaboration to Understand User Personas through Social Media Data Analysis|Min Yin, Haoyu Liu, Boyi Lian, Chunlei Chai|2025-06-23|arXiv|https://github.com/INFPa/LLMwithPersona|http://arxiv.org/abs/2506.18269v2|
|84|Command-V: Pasting LLM Behaviors via Activation Profiles|Barry Wang, Avi Schwarzschild, Alexander Robey, Ali Payani, Charles Fleming, Mingjie Sun, Daphne Ippolito|2025-06-23|arXiv|https://github.com/GithuBarry/Command-V/|http://arxiv.org/abs/2506.19140v1|
|85|Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning|Lixin Wu, Na Cai, Qiao Cheng, Jiachen Wang, Yitao Duan|2025-06-23|arXiv|https://github.com/netease-youdao/Confucius3-Math|http://arxiv.org/abs/2506.18330v2|
|86|Existing LLMs Are Not Self-Consistent For Simple Tasks|Zhenru Lin, Jiawen Tao, Yang Yuan, Andrew Chi-Chih Yao|2025-06-23|arXiv|https://github.com/scorpio-nova/llm-self-consistency|http://arxiv.org/abs/2506.18781v1|
|87|MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis|Yuting Zhang, Kaishen Yuan, Hao Lu, Yutao Yue, Jintai Chen, Kaishun Wu|2025-06-23|arXiv|https://github.com/keke-nice/MedTVT-R1|http://arxiv.org/abs/2506.18512v1|
|88|ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs|Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang|2025-06-23|arXiv|https://github.com/Gen-Verse/ReasonFlux|http://arxiv.org/abs/2506.18896v1|
|89|SegChange-R1: LLM-Augmented Remote Sensing Change Detection|Fei Zhou|2025-06-22|arXiv|https://github.com/Yu-Zhouz/SegChange-R1|http://arxiv.org/abs/2506.17944v2|
|90|Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster|Fenghe Tang, Wenxin Ma, Zhiyang He, Xiaodong Tao, Zihang Jiang, S. Kevin Zhou|2025-06-22|arXiv|https://github.com/FengheTan9/LLM4Seg|http://arxiv.org/abs/2506.18034v1|
|91|Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models|Jihyun Kim, Junho Park, Kyeongbo Kong, Suk-Ju Kang|2025-06-21|arXiv|https://jihyun0510.github.io/Programmable_Room_Page/|https://doi.org/10.48550/arXiv.2506.17707|
|92|Do LLMs Know When to Flip a Coin? Strategic Randomization through Reasoning and Experience|Lingyu Yang|2025-06-21|arXiv|https://github.com/ocelopus/llm-when-to-throw-coin|http://arxiv.org/abs/2506.18928v1|
|93|May the Feedback Be with You! Unlocking the Power of Feedback-Driven Deep Learning Framework Fuzzing via LLMs|Shaoyu Yang, Chunrong Fang, Haifeng Lin, Xiang Chen, Zhenyu Chen|2025-06-21|arXiv|https://github.com/NJU-iSE/FUEL|http://arxiv.org/abs/2506.17642v1|
|94|Safe Pruning LoRA: Robust Distance-Guided Pruning for Safety Alignment in Adaptation of LLMs|Shuang Ao, Yi Dong, Jinwei Hu, Sarvapali Ramchurn|2025-06-21|arXiv|https://github.com/AoShuang92/SPLoRA|http://arxiv.org/abs/2506.18931v1|
|95|Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation|Yang Wu, Yifan Zhang, Yurong Wu, Yuran Wang, Junkai Zhang, Jian Cheng|2025-06-21|arXiv|https://github.com/samwu-learn/Step|http://arxiv.org/abs/2506.17637v1|
|96|Large Language Model-Driven Surrogate-Assisted Evolutionary Algorithm for Expensive Optimization|Lindong Xie, Genghui Li, Zhenkun Wang, Edward Chung, Maoguo Gong|2025-06-20|arXiv|https://github.com/ForrestXie9/LLM-SAEA|http://arxiv.org/abs/2507.02892v1|
|97|A Large-Scale Real-World Evaluation of LLM-Based Virtual Teaching Assistant|Sunjun Kweon, Sooyohn Nam, Hyunseung Lim, Hwajung Hong, Edward Choi|2025-06-20|arXiv|https://github.com/sean0042/VTA|http://arxiv.org/abs/2506.17363v1|
|98|UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making|Jinhao Duan, James Diffenderfer, Sandeep Madireddy, Tianlong Chen, Bhavya Kailkhura, Kaidi Xu|2025-06-20|arXiv|https://github.com/jinhaoduan/UProp|http://arxiv.org/abs/2506.17419v1|
|99|TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs|Sahil Kale, Vijaykant Nadadur|2025-06-20|arXiv|https://github.com/knowledge-verse-ai/TeXpert|http://arxiv.org/abs/2506.16990v1|
|100|Drag-and-Drop LLMs: Zero-Shot Prompt-to-Weights|Zhiyuan Liang, Dongwen Tang, Yuhao Zhou, Xuanlei Zhao, Mingjia Shi, Wangbo Zhao, Zekai Li, Peihao Wang, Konstantin Schürholt, Damian Borth, Michael M. Bronstein, Yang You, Zhangyang Wang, Kai Wang|2025-06-19|arXiv|https://jerryliang24.github.io/DnD|http://arxiv.org/abs/2506.16406v1|
|101|From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling|Yao Lu, Zhaiyuan Ji, Jiawei Du, Yu Shanqing, Qi Xuan, Tianyi Zhou|2025-06-19|arXiv|https://github.com/Zhaiyuan-Ji/AutoAnnotator|http://arxiv.org/abs/2506.16393v1|
|102|AgentGroupChat-V2: Divide-and-Conquer Is What LLM-Based Multi-Agent System Need|Zhouhong Gu, Xiaoxuan Zhu, Yin Cai, Hao Shen, Xingzhou Chen, Qingyi Wang, Jialin Li, Xiaoran Shi, Haoran Guo, Wenxuan Huang, Hongwei Feng, Yanghua Xiao, Zheyu Ye, Yao Hu, Shaosheng Cao|2025-06-18|arXiv|https://github.com/MikeGu721/AgentGroupChat-V2|http://arxiv.org/abs/2506.15451v1|
|103|RAS-Eval: A Comprehensive Benchmark for Security Evaluation of LLM Agents in Real-World Environments|Yuchuan Fu, Xiaohan Yuan, Dongxia Wang|2025-06-18|arXiv|https://github.com/lanzer-tree/RAS-Eval|http://arxiv.org/abs/2506.15253v1|
|104|HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial Optimization Challenges|Xianliang Yang, Ling Zhang, Haolong Qian, Lei Song, Jiang Bian|2025-06-18|arXiv|https://github.com/microsoft/HeurAgenix|http://arxiv.org/abs/2506.15196v1|
|105|video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models|Changli Tang, Yixuan Li, Yudong Yang, Jimin Zhuang, Guangzhi Sun, Wei Li, Zejun Ma, Chao Zhang|2025-06-18|arXiv|https://github.com/bytedance/video-SALMONN-2|https://doi.org/10.48550/arXiv.2506.15220|
|106|From Black Boxes to Transparent Minds: Evaluating and Enhancing the Theory of Mind in Multimodal Large Language Models|Xinyang Li, Siqi Liu, Bochao Zou, Jiansheng Chen, Huimin Ma|2025-06-17|arXiv|https://annaisavailable.github.io/GridToM/|https://doi.org/10.48550/arXiv.2506.14224|
|107|AlphaDecay: Module-wise Weight Decay for Heavy-Tailed Balancing in LLMs|Di He, Ajay Jaiswal, Songjun Tu, Li Shen, Ganzhao Yuan, Shiwei Liu, Lu Yin|2025-06-17|arXiv|https://github.com/hed-ucas/AlphaDecay|http://arxiv.org/abs/2506.14562v2|
|108|LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs|Xiaoran Liu, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu|2025-06-17|arXiv|https://github.com/OpenMOSS/LongLLaDA|http://arxiv.org/abs/2506.14429v1|
|109|Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain Perspective|Zhoujun Cheng, Shibo Hao, Tianyang Liu, Fan Zhou, Yutao Xie, Feng Yao, Yuexin Bian, Yonghao Zhuang, Nilabjo Dey, Yuheng Zha, Yi Gu, Kun Zhou, Yuqi Wang, Yuan Li, Richard Fan, Jianshu She, Chengqian Gao, Abulhair Saparov, Haonan Li, Taylor W. Killian, Mikhail Yurochkin, Zhengzhong Liu, Eric P. Xing, Zhiting Hu|2025-06-17|arXiv|https://github.com/LLM360/Reasoning360|http://arxiv.org/abs/2506.14965v1|
|110|SceneAware: Scene-Constrained Pedestrian Trajectory Prediction with LLM-Guided Walkability|Juho Bai, Inwook Shim|2025-06-17|arXiv|https://github.com/juho127/SceneAware|http://arxiv.org/abs/2506.14144v1|
|111|VIS-Shepherd: Constructing Critic for LLM-based Data Visualization Generation|Bo Pan, Yixiao Fu, Ke Wang, Junyu Lu, Lunke Pan, Ziyang Qian, Yuhan Chen, Guoliang Wang, Yitao Zhou, Li Zheng, Yinghao Tang, Zhen Wen, Yuchen Wu, Junhua Lu, Biao Zhu, Minfeng Zhu, Bo Zhang, Wei Chen|2025-06-16|arXiv|https://github.com/bopan3/VIS-Shepherd|http://arxiv.org/abs/2506.13326v1|
|112|Steering LLM Thinking with Budget Guidance|Junyan Li, Wenshuo Zhao, Yang Zhang, Chuang Gan|2025-06-16|arXiv|https://github.com/UMass-Embodied-AGI/BudgetGuidance|http://arxiv.org/abs/2506.13752v1|
|113|Safe-Child-LLM: A Developmental Benchmark for Evaluating LLM Safety in Child-LLM Interactions|Junfeng Jiao, Saleh Afroogh, Kevin Chen, Abhejay Murali, David Atkinson, Amit Dhurandhar|2025-06-16|arXiv|https://github.com/The-Responsible-AI-Initiative/Safe_Child_LLM_Benchmark|http://arxiv.org/abs/2506.13510v2|
|114|RealHiTBench: A Comprehensive Realistic Hierarchical Table Benchmark for Evaluating LLM-Based Table Analysis|Pengzuo Wu, Yuhang Yang, Guangcheng Zhu, Chao Ye, Hong Gu, Xu Lu, Ruixuan Xiao, Bowen Bao, Yijing He, Liangyu Zha, Wentao Ye, Junbo Zhao, Haobo Wang|2025-06-16|arXiv|https://github.com/cspzyy/RealHiTBench|http://arxiv.org/abs/2506.13405v1|
|115|LocationReasoner: Evaluating LLMs on Real-World Site Selection Reasoning|Miho Koda, Yu Zheng, Ruixian Ma, Mingyang Sun, Devesh Pansare, Fabio Duarte, Paolo Santi|2025-06-16|arXiv|https://github.com/miho-koda/LocationReasoner|http://arxiv.org/abs/2506.13841v1|
|116|Attribution-guided Pruning for Compression, Circuit Discovery, and Targeted Correction in LLMs|Sayed Mohammad Vakilzadeh Hatefi, Maximilian Dreyer, Reduan Achtibat, Patrick Kahardipraja, Thomas Wiegand, Wojciech Samek, Sebastian Lapuschkin|2025-06-16|arXiv|https://github.com/erfanhatefi/SparC3|http://arxiv.org/abs/2506.13727v1|
|117|Align-then-Unlearn: Embedding Alignment for LLM Unlearning|Philipp Spohn, Leander Girrbach, Jessica Bader, Zeynep Akata|2025-06-16|arXiv|https://github.com/ExplainableML/align-then-unlearn|http://arxiv.org/abs/2506.13181v1|
|118|Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs|Yiwei Chen, Soumyadeep Pal, Yimeng Zhang, Qing Qu, Sijia Liu|2025-06-16|arXiv|https://github.com/OPTML-Group/Unlearn-Trace|http://arxiv.org/abs/2506.14003v1|
|119|Stream-Omni: Simultaneous Multimodal Interactions with Large Language-Vision-Speech Model|Shaolei Zhang, Shoutao Guo, Qingkai Fang, Yan Zhou, Yang Feng|2025-06-16|arXiv|https://github.com/ictnlp/Stream-Omni|https://doi.org/10.48550/arXiv.2506.13642|
|120|CFBenchmark-MM: Chinese Financial Assistant Benchmark for Multimodal Large Language Model|Yang Lei, Jiangtong Li, Ming Jiang, Junjie Hu, Dawei Cheng, Zhijun Ding, Changjun Jiang|2025-06-16|arXiv|https://github.com/TongjiFinLab/CFBenchmark|https://doi.org/10.48550/arXiv.2506.13055|
|121|LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation|Yingzhi He, Xiaohao Liu, An Zhang, Yunshan Ma, Tat-Seng Chua|2025-06-16|arXiv|https://github.com/HappyPointer/LLM2Rec|https://doi.org/10.48550/arXiv.2506.21579|
|122|Democratic or Authoritarian? Probing a New Dimension of Political Biases in Large Language Models|David Guzman Piedrahita, Irene Strauss, Bernhard Schölkopf, Rada Mihalcea, Zhijing Jin|2025-06-15|arXiv|https://github.com/irenestrauss/Democratic-Authoritarian-Bias-LLMs|https://doi.org/10.48550/arXiv.2506.12758|
|123|MaskPro: Linear-Space Probabilistic Learning for Strict (N:M)-Sparsity on Large Language Models|Yan Sun, Qixin Zhang, Zhiyuan Yu, Xikun Zhang, Li Shen, Dacheng Tao|2025-06-15|arXiv|https://github.com/woodenchild95/Maskpro|https://doi.org/10.48550/arXiv.2506.12876|
|124|SmartHome-Bench: A Comprehensive Benchmark for Video Anomaly Detection in Smart Homes Using Multi-Modal Large Language Models|Xinyi Zhao, Congjing Zhang, Pei Guo, Wei Li, Lin Chen, Chaoyue Zhao, Shuai Huang|2025-06-15|arXiv|https://github.com/Xinyi-0724/SmartHome-Bench-LLM|https://doi.org/10.48550/arXiv.2506.12992|
|125|Humanity's Last Code Exam: Can Advanced LLMs Conquer Human's Hardest Code Competition?|Xiangyang Li, Xiaopeng Li, Kuicai Dong, Quanhu Zhang, Rongju Ruan, Xinyi Dai, Xiaoshuang Liu, Shengchun Xu, Yasheng Wang, Ruiming Tang|2025-06-15|arXiv|https://github.com/Humanity-s-Last-Code-Exam/HLCE|http://arxiv.org/abs/2506.12713v1|
|126|Training-free LLM Merging for Multi-task Learning|Zichuan Fu, Xian Wu, Yejing Wang, Wanyu Wang, Shanshan Ye, Hongzhi Yin, Yi Chang, Yefeng Zheng, Xiangyu Zhao|2025-06-14|ACL 2025 Main|https://github.com/Applied-Machine-Learning-Lab/Hi-Merging|http://arxiv.org/abs/2506.12379v1|
|127|Profiling News Media for Factuality and Bias Using LLMs and the Fact-Checking Methodology of Human Experts|Zain Muhammad Mujahid, Dilshod Azizov, Maha Tufail Agro, Preslav Nakov|2025-06-14|arXiv|https://github.com/mbzuai-nlp/llm-media-profiling|http://arxiv.org/abs/2506.12552v1|
|128|Graph of Verification: Structured Verification of LLM Reasoning with Directed Acyclic Graphs|Jiwei Fang, Bin Zhang, Changwei Wang, Jin Wan, Zhiwei Xu|2025-06-14|arXiv|https://github.com/Frevor/Graph-of-Verification|http://arxiv.org/abs/2506.12509v1|
|129|RealFactBench: A Benchmark for Evaluating Large Language Models in Real-World Fact-Checking|Shuo Yang, Yuqin Dai, Guoqing Wang, Xinran Zheng, Jinfeng Xu, Jinze Li, Zhenzhe Ying, Weiqiang Wang, Edith C. H. Ngai|2025-06-14|arXiv|https://github.com/kalendsyang/RealFactBench|https://doi.org/10.48550/arXiv.2506.12538|
|130|ConsistencyChecker: Tree-based Evaluation of LLM Generalization Capabilities|Zhaochen Hong, Haofei Yu, Jiaxuan You|2025-06-14|arXiv|https://github.com/ulab-uiuc/consistencychecker|http://arxiv.org/abs/2506.12376v2|
|131|Improving Large Language Model Safety with Contrastive Representation Learning|Samuel Simko, Mrinmaya Sachan, Bernhard Schölkopf, Zhijing Jin|2025-06-13|arXiv|https://github.com/samuelsimko/crl-llm-defense|https://doi.org/10.48550/arXiv.2506.11938|
|132|Persona-driven Simulation of Voting Behavior in the European Parliament with Large Language Models|Maximilian Kreutner, Marlene Lutz, Markus Strohmaier|2025-06-13|arXiv|https://github.com/dess-mannheim/european_parliament_simulation|https://doi.org/10.48550/arXiv.2506.11798|
|133|code_transformed: The Influence of Large Language Models on Code|Yuliang Xu, Siming Huang, Mingmeng Geng, Yao Wan, Xuanhua Shi, Dongping Chen|2025-06-13|arXiv|https://github.com/ignorancex/LLM_code|https://doi.org/10.48550/arXiv.2506.12014|
|134|Long-Short Alignment for Effective Long-Context Modeling in LLMs|Tianqi Du, Haotian Huang, Yifei Wang, Yisen Wang|2025-06-13|arXiv|https://github.com/PKU-ML/LongShortAlignment|http://arxiv.org/abs/2506.11769v1|
|135|Semantic Scheduling for LLM Inference|Wenyue Hua, Dujian Ding, Yile Gu, Yujie Ren, Kai Mei, Minghua Ma, William Yang Wang|2025-06-13|arXiv|https://github.com/Wenyueh/latency_optimization_with_priority_constraints|http://arxiv.org/abs/2506.12204v1|
|136|OPT-BENCH: Evaluating LLM Agent on Large-Scale Search Spaces Optimization Problems|Xiaozhe Li, Jixuan Chen, Xinyu Fang, Shengyuan Ding, Haodong Duan, Qingwen Liu, Kai Chen|2025-06-12|arXiv|https://github.com/OliverLeeXZ/OPT-BENCH|http://arxiv.org/abs/2506.10764v1|
|137|TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to Evolving Research Corpora|Priyanka Kargupta, Nan Zhang, Yunyi Zhang, Rui Zhang, Prasenjit Mitra, Jiawei Han|2025-06-12|arXiv|https://github.com/pkargupta/taxoadapt|http://arxiv.org/abs/2506.10737v1|
|138|SOFT: Selective Data Obfuscation for Protecting LLM Fine-tuning against Membership Inference Attacks|Kaiyuan Zhang, Siyuan Cheng, Hanxi Guo, Yuetian Chen, Zian Su, Shengwei An, Yuntao Du, Charles Fleming, Ashish Kundu, Xiangyu Zhang, Ninghui Li|2025-06-12|arXiv|https://github.com/KaiyuanZh/SOFT|http://arxiv.org/abs/2506.10424v1|
|139|ReCUT: Balancing Reasoning Length and Accuracy in LLMs via Stepwise Trails and Preference Optimization|Zhensheng Jin, Xinze Li, Yifan Ji, Chunyi Peng, Zhenghao Liu, Qi Shi, Yukun Yan, Shuo Wang, Furong Peng, Ge Yu|2025-06-12|arXiv|https://github.com/NEUIR/ReCUT|http://arxiv.org/abs/2506.10822v1|
|140|PAL: Probing Audio Encoders via LLMs -- A Study of Information Transfer from Audio Encoders to LLMs|Tony Alex, Wish Suharitdamrong, Sara Atito, Armin Mustafa, Philip J. B. Jackson, Imran Razzak, Muhammad Awais|2025-06-12|arXiv|https://ta012.github.io/PAL/|http://arxiv.org/abs/2506.10423v1|
|141|AdaptiveLLM: A Framework for Selecting Optimal Cost-Efficient LLM for Code-Generation Based on CoT Length|Junhang Cheng, Fang Liu, Chengru Wu, Li Zhang|2025-06-12|arXiv|https://github.com/cjhCoder7/AdaptiveLLM|http://arxiv.org/abs/2506.10525v1|
|142|SoK: Evaluating Jailbreak Guardrails for Large Language Models|Xunguang Wang, Zhenlan Ji, Wenxuan Wang, Zongjie Li, Daoyuan Wu, Shuai Wang|2025-06-12|arXiv|https://github.com/xunguangwang/SoK4JailbreakGuardrails|https://doi.org/10.48550/arXiv.2506.10597|
|143|Farseer: A Refined Scaling Law in Large Language Models|Houyi Li, Wenzhen Zheng, Qiufeng Wang, Zhenyu Ding, Haoying Wang, Zili Wang, Shijie Xuyang, Ning Ding, Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang|2025-06-12|arXiv|https://github.com/Farseer-Scaling-Law/Farseer|https://doi.org/10.48550/arXiv.2506.10972|
|144|DanceChat: Large Language Model-Guided Music-to-Dance Generation|Qing Wang, Xiaohang Yang, Yilan Dong, Naveen Raj Govindaraj, Gregory G. Slabaugh, Shanxin Yuan|2025-06-12|arXiv|https://dancechat.github.io/anon/|https://doi.org/10.48550/arXiv.2506.10574|
|145|Burn After Reading: Do Multimodal Large Language Models Truly Capture Order of Events in Image Sequences?|Yingjin Song, Yupei Du, Denis Paperno, Albert Gatt|2025-06-12|arXiv|https://github.com/yjsong22/TempVS|https://doi.org/10.48550/arXiv.2506.10415|
|146|CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language Models in Tool-Calling Error Scenarios|Shiting Huang, Zhen Fang, Zehui Chen, Siyu Yuan, Junjie Ye, Yu Zeng, Lin Chen, Qi Mao, Feng Zhao|2025-06-11|arXiv|https://github.com/Shellorley0513/CriticTool|https://doi.org/10.48550/arXiv.2506.13977|
|147|Evaluating Multimodal Large Language Models on Video Captioning via Monte Carlo Tree Search|Linhao Yu, Xinguang Ji, Yahui Liu, Fanheng Kong, Chenxi Sun, Jingyuan Zhang, Hongzhi Zhang, V. W., Fuzheng Zhang, Deyi Xiong|2025-06-11|arXiv|https://github.com/tjunlp-lab/MCTS-VCB|https://doi.org/10.48550/arXiv.2506.11155|
|148|The NordDRG AI Benchmark for Large Language Models|Tapio Pitkäranta|2025-06-11|arXiv|https://github.com/longshoreforrest/norddrg-ai-benchmark|https://doi.org/10.48550/arXiv.2506.13790|
|149|LLMs Cannot Reliably Judge (Yet?): A Comprehensive Assessment on the Robustness of LLM-as-a-Judge|Songze Li, Chuokun Xu, Jiaying Wang, Xueluan Gong, Chen Chen, Jirui Zhang, Jun Wang, Kwok-Yan Lam, Shouling Ji|2025-06-11|arXiv|https://github.com/S3IC-Lab/RobustJudge|http://arxiv.org/abs/2506.09443v1|
|150|UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs|Prameshwar Thiyagarajan, Vaishnavi Parimi, Shamant Sai, Soumil Garg, Zhangir Meirbek, Nitin Yarlagadda, Kevin Zhu, Chris Kim|2025-06-11|arXiv|https://github.com/Shamant/unifiedtombenchmark|http://arxiv.org/abs/2506.09450v1|
|151|Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via Reinforcement Learning|Haozhen Zhang, Tao Feng, Jiaxuan You|2025-06-10|arXiv|https://github.com/ulab-uiuc/Router-R1|http://arxiv.org/abs/2506.09033v2|
|152|Draft-based Approximate Inference for LLMs|Kevin Galim, Ethan Ewer, Wonjun Kang, Minjae Lee, Hyung Il Koo, Kangwook Lee|2025-06-10|arXiv|https://github.com/furiosa-ai/draft-based-approx-llm|http://arxiv.org/abs/2506.08373v1|
|153|LLM-as-a-qualitative-judge: automating error analysis in natural language generation|Nadezhda Chirkova, Tunde Oluwaseyi Ajayi, Seth Aycock, Zain Muhammad Mujahid, Vladana Perlić, Ekaterina Borisova, Markarit Vartampetian|2025-06-10|arXiv|https://github.com/tunde-ajayi/llm-as-a-qualitative-judge|http://arxiv.org/abs/2506.09147v1|
|154|AsFT: Anchoring Safety During LLM Fine-Tuning Within Narrow Safety Basin|Shuo Yang, Qihui Zhang, Yuyang Liu, Yue Huang, Xiaojun Jia, Kunpeng Ning, Jiayu Yao, Jigang Wang, Hailiang Dai, Yibing Song, Li Yuan|2025-06-10|arXiv|https://github.com/PKU-YuanGroup/AsFT|http://arxiv.org/abs/2506.08473v2|
|155|Consistent Paths Lead to Truth: Self-Rewarding Reinforcement Learning for LLM Reasoning|Kongcheng Zhang, Qi Yao, Shunyu Liu, Yingjie Wang, Baisheng Lai, Jieping Ye, Mingli Song, Dacheng Tao|2025-06-10|arXiv|https://github.com/sastpg/CoVo|http://arxiv.org/abs/2506.08745v1|
|156|Reinforcement Fine-Tuning for Reasoning towards Multi-Step Multi-Source Search in Large Language Models|Wentao Shi, Yiqing Shen|2025-06-10|arXiv|https://github.com/wentao0429/Reasoning-search|https://doi.org/10.48550/arXiv.2506.08352|
|157|Know-MRI: A Knowledge Mechanisms Revealer&amp;Interpreter for Large Language Models|Jiaxiang Liu, Boxuan Xing, Chenhao Yuan, Chenxiang Zhang, Di Wu, Xiusheng Huang, Haida Yu, Chuhan Lang, Pengfei Cao, Jun Zhao, Kang Liu|2025-06-10|arXiv|https://github.com/nlpkeg/Know-MRI|https://doi.org/10.48550/arXiv.2506.08427|
|158|Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data|Xin-Cheng Wen, Yijun Yang, Cuiyun Gao, Yang Xiao, Deheng Ye|2025-06-09|arXiv|https://github.com/Xin-Cheng-Wen/PO4Vul|http://arxiv.org/abs/2506.07390v1|
|159|Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation|Roman Kyslyi, Yuliia Maksymiuk, Ihor Pysmennyi|2025-06-09|arXiv|https://github.com/woters/vuyko-hutsul|http://arxiv.org/abs/2506.07617v1|
|160|TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review|Yuan Chang, Ziyue Li, Hengyuan Zhang, Yuanbo Kong, Yanru Wu, Zhijiang Guo, Ngai Wong|2025-06-09|arXiv|https://github.com/YuanChang98/tree-review|http://arxiv.org/abs/2506.07642v1|
|161|SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition|Mengsong Wu, Di Zhang, Yuqiang Li, Dongzhan Zhou, Wenliang Chen|2025-06-09|arXiv|https://github.com/fairyshine/SELT|http://arxiv.org/abs/2506.07557v1|
|162|LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking|Vahid Azizi, Fatemeh Koochaki|2025-06-09|arXiv|https://github.com/VahidAz/LlamaRec-LKG-RAG|http://arxiv.org/abs/2506.07449v1|
|163|CheMatAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning|Mengsong Wu, YaFei Wang, Yidong Ming, Yuqi An, Yuwei Wan, Wenliang Chen, Binbin Lin, Yuqiang Li, Tong Xie, Dongzhan Zhou|2025-06-09|arXiv|https://github.com/AI4Chem/ChemistryAgent|http://arxiv.org/abs/2506.07551v2|
|164|From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium|Xie Yi, Zhanke Zhou, Chentao Cao, Qiyu Niu, Tongliang Liu, Bo Han|2025-06-09|arXiv|https://github.com/tmlr-group/ECON|http://arxiv.org/abs/2506.08292v1|
|165|BLUR: A Bi-Level Optimization Approach for LLM Unlearning|Hadi Reisizadeh, Jinghan Jia, Zhiqi Bu, Bhanukiran Vinzamuri, Anil Ramakrishna, Kai-Wei Chang, Volkan Cevher, Sijia Liu, Mingyi Hong|2025-06-09|arXiv|https://github.com/OptimAI-Lab/BLURLLMUnlearning|http://arxiv.org/abs/2506.08164v1|
|166|ARGUS: Hallucination and Omission Evaluation in Video-LLMs|Ruchit Rawal, Reza Shirkavand, Heng Huang, Gowthami Somepalli, Tom Goldstein|2025-06-09|arXiv|https://ruchitrawal.github.io/argus|http://arxiv.org/abs/2506.07371v2|
|167|SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence|Ziyang Gong, Wenhao Li, Oliver Ma, Songyuan Li, Jiayi Ji, Xue Yang, Gen Luo, Junchi Yan, Rongrong Ji|2025-06-09|arXiv|https://github.com/Cuzyoung/SpaCE-10|https://doi.org/10.48550/arXiv.2506.07966|
|168|Solving Inequality Proofs with Large Language Models|Jiayi Sheng, Luna Lyu, Jikai Jin, Tony Xia, Alex Gu, James Zou, Pan Lu|2025-06-09|arXiv|https://ineqmath.github.io/|https://doi.org/10.48550/arXiv.2506.07927|
|169|MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models|Philip R. Liu, Sparsh Bansal, Jimmy Dinh, Aditya Pawar, Ramani Satishkumar, Shail Desai, Neeraj Gupta, Xin Wang, Shu Hu|2025-06-09|arXiv|https://github.com/Purdue-M2/MedChat|https://doi.org/10.48550/arXiv.2506.07400|
|170|Improving Large Language Models with Concept-Aware Fine-Tuning|Michael K. Chen, Xikun Zhang, Jiaxing Huang, Dacheng Tao|2025-06-09|arXiv|https://github.com/michaelchen-lab/caft-llm|https://doi.org/10.48550/arXiv.2506.07833|
|171|From Passive to Active Reasoning: Can Large Language Models Ask the Right Questions under Incomplete Information?|Zhanke Zhou, Xiao Feng, Zhaocheng Zhu, Jiangchao Yao, Sanmi Koyejo, Bo Han|2025-06-09|arXiv|https://github.com/tmlr-group/AR-Bench|https://doi.org/10.48550/arXiv.2506.08295|
|172|AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language Models|Jaeho Lee, Atharv Chowdhary|2025-06-08|arXiv|https://github.com/achowd32/assert-bench|https://doi.org/10.48550/arXiv.2506.11110|
|173|Com2: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models|Kai Xiong, Xiao Ding, Yixin Cao, Yuxiong Yan, Li Du, Yufei Zhang, Jinglong Gao, Jiaqian Liu, Bing Qin, Ting Liu|2025-06-08|arXiv|https://github.com/Waste-Wood/Com2|https://doi.org/10.48550/arXiv.2506.07064|
|174|Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions|Kun Zhang, Le Wu, Kui Yu, Guangyi Lv, Dacao Zhang|2025-06-08|arXiv|https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers|https://doi.org/10.48550/arXiv.2506.11111|
|175|Prime the search: Using large language models for guiding geometric task and motion planning by warm-starting tree search|Dongryung Lee, Se June Joo, Kimin Lee, Beomjoon Kim|2025-06-08|arXiv|https://github.com/iMSquared/prime-the-search|https://doi.org/10.48550/arXiv.2506.07062|
|176|Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages|Olga Kellert, Nemika Tyagi, Muhammad Imran, Nelvin Licona-Guevara, Carlos Gómez-Rodríguez|2025-06-08|arXiv|https://github.com/N3mika/ParsingProject|http://arxiv.org/abs/2506.07274v1|
|177|How Important are Videos for Training Video LLMs?|George Lydakis, Alexander Hermans, Ali Athar, Daan de Geus, Bastian Leibe|2025-06-07|arXiv|https://visualcomputinginstitute.github.io/videollm-pseudovideo-training/|http://arxiv.org/abs/2506.06928v1|
|178|They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse|Walter Paci, Alessandro Panunzi, Sandro Pezzelle|2025-06-07|arXiv|https://github.com/WalterPaci/IMPAQTS-PID|http://arxiv.org/abs/2506.06775v1|
|179|Quantile Regression with Large Language Models for Price Prediction|Nikhita Vedula, Dushyanta Dhyani, Laleh Jalali, Boris Oreshkin, Mohsen Bayati, Shervin Malmasi|2025-06-07|arXiv|https://github.com/vnik18/llm-price-quantile-reg/|https://doi.org/10.48550/arXiv.2506.06657|
|180|On the Adaptive Psychological Persuasion of Large Language Models|Tianjie Ju, Yujia Chen, Hao Fei, Mong-Li Lee, Wynne Hsu, Pengzhou Cheng, Zongru Wu, Zhuosheng Zhang, Gongshen Liu|2025-06-07|arXiv|https://github.com/KalinaEine/PsychologicalPersuasion|https://doi.org/10.48550/arXiv.2506.06800|
|181|DAM: Dynamic Attention Mask for Long-Context Large Language Model Inference Acceleration|Hanzhi Zhang, Heng Fan, Kewei Sha, Yan Huang, Yunhe Feng|2025-06-06|arXiv|https://github.com/HanzhiZhang-Ulrica/DAM|https://doi.org/10.48550/arXiv.2506.11104|
|182|FedShield-LLM: A Secure and Scalable Federated Fine-Tuned Large Language Model|Md. Jueal Mia, M. Hadi Amini|2025-06-06|arXiv|https://github.com/solidlabnetwork/fedshield-llm|https://doi.org/10.48550/arXiv.2506.05640|
|183|Large Language Models are Good Relational Learners|Fang Wu, Vijay Prakash Dwivedi, Jure Leskovec|2025-06-06|arXiv|https://github.com/smiles724/Rel-LLM|https://doi.org/10.48550/arXiv.2506.05725|
|184|MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient Fine-Tuning of Large Language Models|Jie Cao, Tianwei Lin, Hongyang He, Rolan Yan, Wenqiao Zhang, Juncheng Li, Dongping Zhang, Siliang Tang, Yueting Zhuang|2025-06-06|arXiv|https://github.com/DCDmllm/MoA|https://doi.org/10.48550/arXiv.2506.05928|
|185|AgentSwift: Efficient LLM Agent Design via Value-guided Hierarchical Search|Yu Li, Lehui Li, Zhihao Wu, Qingmin Liao, Jianye Hao, Kun Shao, Fengli Xu, Yong Li|2025-06-06|arXiv|https://github.com/Ericccc02/AgentSwift|http://arxiv.org/abs/2506.06017v1|
|186|Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness|Rongzhe Wei, Peizhi Niu, Hans Hao-Hsun Hsu, Ruihan Wu, Haoteng Yin, Mohsen Ghassemi, Yifan Li, Vamsi K. Potluru, Eli Chien, Kamalika Chaudhuri, Olgica Milenkovic, Pan Li|2025-06-06|arXiv|https://github.com/Graph-COM/Knowledge_Unlearning|http://arxiv.org/abs/2506.05735v1|
|187|Zero-Shot Detection of LLM-Generated Code via Approximated Task Conditioning|Maor Ashkenazi, Ofir Brenner, Tal Furman Shohet, Eran Treister|2025-06-06|arXiv|https://github.com/maorash/ATC|http://arxiv.org/abs/2506.06069v1|
|188|BAQ: Efficient Bit Allocation Quantization for Large Language Models|Chao Zhang, Li Wang, Samson Lasaulce, Mérouane Debbah|2025-06-06|arXiv|https://github.com/CSU-ModelCompression/BAQ|https://doi.org/10.48550/arXiv.2506.05664|
|189|LeanPO: Lean Preference Optimization for Likelihood Alignment in Video-LLMs|Xiaodong Wang, Jinfa Huang, Li Yuan, Peixi Peng|2025-06-05|arXiv|https://github.com/Wang-Xiaodong1899/LeanPO|http://arxiv.org/abs/2506.05260v1|
|190|Towards Holistic Visual Quality Assessment of AI-Generated Videos: A LLM-Based Multi-Dimensional Evaluation Model|Zelu Qi, Ping Shi, Chaoyang Zhang, Shuqi Wang, Fei Zhao, Da Pan, Zefeng Ying|2025-06-05|arXiv|https://github.com/QiZelu/AIGVEval|http://arxiv.org/abs/2506.04715v2|
|191|Search Arena: Analyzing Search-Augmented LLMs|Mihran Miroyan, Tsung-Han Wu, Logan King, Tianle Li, Jiayi Pan, Xinyan Hu, Wei-Lin Chiang, Anastasios N. Angelopoulos, Trevor Darrell, Narges Norouzi, Joseph E. Gonzalez|2025-06-05|arXiv|https://github.com/lmarena/search-arena|http://arxiv.org/abs/2506.05334v1|
|192|On the Comprehensibility of Multi-structured Financial Documents using LLMs and Pre-processing Tools|Shivani Upadhyay, Messiah Ataey, Shariyar Murtuza, Yifan Nie, Jimmy Lin|2025-06-05|arXiv|https://github.com/OGCDS/FinancialQA|http://arxiv.org/abs/2506.05182v1|
|193|Are LLMs Reliable Translators of Logical Reasoning Across Lexically Diversified Contexts?|Qingchuan Li, Jiatong Li, Zirui Liu, Mingyue Cheng, Yuting Zeng, Qi Liu, Tongxuan Liu|2025-06-05|arXiv|https://github.com/wufeiwuwoshihua/LexicalDiver|http://arxiv.org/abs/2506.04575v1|
|194|LLMs for sensory-motor control: Combining in-context and iterative learning|Jônata Tyska Carvalho, Stefano Nolfi|2025-06-05|arXiv|https://github.com/jtyska/llm-robotics-article/|http://arxiv.org/abs/2506.04867v1|
|195|LLM-First Search: Self-Guided Exploration of the Solution Space|Nathan Herr, Tim Rocktäschel, Roberta Raileanu|2025-06-05|arXiv|https://github.com/NathanHerr/LLM-First-Search|http://arxiv.org/abs/2506.05213v1|
|196|ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests|Shiyi Xu, Yiwen Hu, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Ji-Rong Wen|2025-06-05|arXiv|https://github.com/RUCAIBox/Slow_Thinking_with_LLMs|http://arxiv.org/abs/2506.04894v1|
|197|MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal Large Language Models|Gio Paik, Geewook Kim, Jinbae Im|2025-06-05|arXiv|https://github.com/naver-ai/MMRefine|https://doi.org/10.48550/arXiv.2506.04688|
|198|Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on Diverse Video Games|Dongmin Park, Minkyu Kim, Beongjun Choi, Junhyuck Kim, Keon Lee, Jonghyun Lee, Inkyu Park, Byeong-Uk Lee, Jaeyoung Hwang, Jaewoo Ahn, Ameya S. Mahabaleshwarkar, Bilal Kartal, Pritam Biswas, Yoshi Suhara, Kangwook Lee, Jaewoong Cho|2025-06-04|arXiv|https://github.com/krafton-ai/Orak|http://arxiv.org/abs/2506.03610v1|
|199|R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning|Qingfei Zhao, Ruobing Wang, Dingling Xu, Daren Zha, Limin Liu|2025-06-04|arXiv|https://github.com/QingFei1/R-Search|http://arxiv.org/abs/2506.04185v1|
|200|Prompt Candidates, then Distill: A Teacher-Student Framework for LLM-driven Data Annotation|Mingxuan Xia, Haobo Wang, Yixuan Li, Zewei Yu, Jindong Wang, Junbo Zhao, Runze Wu|2025-06-04|arXiv|https://github.com/MingxuanXia/CanDist|http://arxiv.org/abs/2506.03857v1|
|201|Guided Speculative Inference for Efficient Test-Time Alignment of LLMs|Jonathan Geuter, Youssef Mroueh, David Alvarez-Melis|2025-06-04|arXiv|https://github.com/j-geuter/GSI|http://arxiv.org/abs/2506.04118v1|
|202|SAVVY: Spatial Awareness via Audio-Visual LLMs through Seeing and Hearing|Mingfei Chen, Zijun Cui, Xiulong Liu, Jinlin Xiang, Caleb Zheng, Jingyuan Li, Eli Shlizerman|2025-06-04|arXiv|https://zijuncui02.github.io/SAVVY/|http://arxiv.org/abs/2506.05414v1|
|203|SmoothRot: Combining Channel-Wise Scaling and Rotation for Quantization-Friendly LLMs|Patrik Czakó, Gábor Kertész, Sándor Szénási|2025-06-04|arXiv|https://github.com/czakop/smoothrot|http://arxiv.org/abs/2506.05413v1|
|204|LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation|Ming Zhang, Yujiong Shen, Zelin Li, Huayu Sha, Binze Hu, Yuhui Wang, Chenhao Huang, Shichun Liu, Jingqi Tong, Changhao Jiang, Mingxu Chai, Zhiheng Xi, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang|2025-06-04|arXiv|https://github.com/llmeval/LLMEval-Med|http://arxiv.org/abs/2506.04078v2|
|205|Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning|Junqi Gao, Xiang Zou, YIng Ai, Dong Li, Yichen Niu, Biqing Qi, Jianxing Liu|2025-06-04|arXiv|https://github.com/gjq100/Graph-Counselor|http://arxiv.org/abs/2506.03939v1|
|206|GORACS: Group-level Optimal Transport-guided Coreset Selection for LLM-based Recommender Systems|Tiehua Mei, Hengrui Chen, Peng Yu, Jiaqing Liang, Deqing Yang|2025-06-04|arXiv|https://github.com/Mithas-114/GORACS|http://arxiv.org/abs/2506.04015v1|
|207|Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis|Kejian Zhu, Shangqing Tu, Zhuoran Jin, Lei Hou, Juanzi Li, Jun Zhao|2025-06-04|arXiv|https://github.com/GaryStack/Trustworthy-Evaluation|http://arxiv.org/abs/2506.04142v1|
|208|AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism|Zhepei Wei, Wei-Lin Chen, Xinyu Zhu, Yu Meng|2025-06-04|arXiv|https://github.com/weizhepei/AdaDecode|http://arxiv.org/abs/2506.03700v1|
|209|TracLLM: A Generic Framework for Attributing Long Context LLMs|Yanting Wang, Wei Zou, Runpeng Geng, Jinyuan Jia|2025-06-04|arXiv|https://github.com/Wang-Yanting/TracLLM|http://arxiv.org/abs/2506.04202v2|
|210|Bohdi: Heterogeneous LLM Fusion with Automatic Data Exploration|Junqi Gao, Zhichang Guo, Dazhi Zhang, Dong Li, Runze Liu, Pengfei Li, Kai Tian, Biqing Qi|2025-06-04|arXiv|https://github.com/gjq100/Bohdi|http://arxiv.org/abs/2506.15721v1|
|211|Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales|Ayuto Tsutsumi, Yuu Jinnai|2025-06-04|arXiv|https://github.com/CyberAgentA|https://doi.org/10.48550/arXiv.2506.03619|
|212|Aligning Large Language Models with Implicit Preferences from User-Generated Content|Zhaoxuan Tan, Zheng Li, Tianyi Liu, Haodong Wang, Hyokun Yun, Ming Zeng, Pei Chen, Zhihan Zhang, Yifan Gao, Ruijie Wang, Priyanka Nigam, Bing Yin, Meng Jiang|2025-06-04|arXiv|https://zhaoxuan.info/PUGC.github.io/|https://doi.org/10.48550/arXiv.2506.04463|
|213|NetPress: Dynamically Generated LLM Benchmarks for Network Applications|Yajie Zhou, Jiajun Ruan, Eric S. Wang, Sadjad Fouladi, Francis Y. Yan, Kevin Hsieh, Zaoxing Liu|2025-06-03|arXiv|https://github.com/Froot-NetSys/NetPress|http://arxiv.org/abs/2506.03231v1|
|214|SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation|Siqi Chen, Xinyu Dong, Haolei Xu, Xingyu Wu, Fei Tang, Hang Zhang, Yuchen Yan, Linjuan Wu, Wenqi Zhang, Guiyang Hou, Yongliang Shen, Weiming Lu, Yueting Zhuang|2025-06-03|arXiv|https://zju-real.github.io/SVGenius|http://arxiv.org/abs/2506.03139v1|
|215|ProcrustesGPT: Compressing LLMs with Structured Matrices and Orthogonal Transformations|Ekaterina Grishina, Mikhail Gorbunov, Maxim Rakhuba|2025-06-03|arXiv|https://github.com/GrishKate/ProcrustesGPT|http://arxiv.org/abs/2506.02818v1|
|216|NextQuill: Causal Preference Modeling for Enhancing LLM Personalization|Xiaoyan Zhao, Juntao You, Yang Zhang, Wenjie Wang, Hong Cheng, Fuli Feng, See-Kiong Ng, Tat-Seng Chua|2025-06-03|arXiv|https://github.com/juntaoyou/NextQuill|http://arxiv.org/abs/2506.02368v1|
|217|Beyond the Surface: Measuring Self-Preference in LLM Judgments|Zhi-Yuan Chen, Hao Wang, Xinyu Zhang, Enrui Hu, Yankai Lin|2025-06-03|arXiv|https://github.com/zhiyuanc2001/self-preference|http://arxiv.org/abs/2506.02592v1|
|218|Cell-o1: Training LLMs to Solve Single-Cell Reasoning Puzzles with Reinforcement Learning|Yin Fang, Qiao Jin, Guangzhi Xiong, Bowen Jin, Xianrui Zhong, Siru Ouyang, Aidong Zhang, Jiawei Han, Zhiyong Lu|2025-06-03|arXiv|https://github.com/ncbi-nlp/cell-o1|http://arxiv.org/abs/2506.02911v1|
|219|A Smart Multimodal Healthcare Copilot with Powerful LLM Reasoning|Xuejiao Zhao, Siyan Liu, Su-Yin Yang, Chunyan Miao|2025-06-03|arXiv|https://github.com/SNOWTEAM2023/MedRAG|http://arxiv.org/abs/2506.02470v1|
|220|VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning|Hao Yan, Handong Zheng, Hao Wang, Liang Yin, Xingchen Liu, Zhenbiao Cao, Xinxing Su, Zihao Chen, Jihao Wu, Minghui Liao, Chao Weng, Wei Chen, Yuliang Liu, Xiang Bai|2025-06-03|arXiv|https://github.com/yh-hust/VisuRiddles|https://doi.org/10.48550/arXiv.2506.02537|
|221|Are Large Language Models Good Temporal Graph Learners?|Shenyang Huang, Ali Parviz, Emma Kondrup, Zachary Yang, Zifeng Ding, Michael M. Bronstein, Reihaneh Rabbany, Guillaume Rabusseau|2025-06-03|arXiv|https://github.com/shenyangHuang/TGTalker|https://doi.org/10.48550/arXiv.2506.05393|
|222|LAMARL: LLM-Aided Multi-Agent Reinforcement Learning for Cooperative Policy Generation|Guobin Zhu, Rui Zhou, Wenkang Ji, Shiyu Zhao|2025-06-02|arXiv|https://windylab.github.io/LAMARL/|http://arxiv.org/abs/2506.01538v2|
|223|StochasTok: Improving Fine-Grained Subword Understanding in LLMs|Anya Sims, Thom Foster, Klara Kaleb, Tuan-Duy H. Nguyen, Joseph Lee, Jakob N. Foerster, Yee Whye Teh, Cong Lu|2025-06-02|arXiv|https://github.com/anyasims/stochastok|http://arxiv.org/abs/2506.01687v2|
|224|ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and Understanding|Junliang Ye, Zhengyi Wang, Ruowen Zhao, Shenghao Xie, Jun Zhu|2025-06-02|arXiv|https://github.com/JAMESYJL/ShapeLLM-Omni|http://arxiv.org/abs/2506.01853v1|
|225|ReGA: Representation-Guided Abstraction for Model-based Safeguarding of LLMs|Zeming Wei, Chengcan Wu, Meng Sun|2025-06-02|arXiv|https://github.com/weizeming/ReGA|http://arxiv.org/abs/2506.01770v1|
|226|MTCMB: A Multi-Task Benchmark Framework for Evaluating LLMs on Knowledge, Reasoning, and Safety in Traditional Chinese Medicine|Shufeng Kong, Xingru Yang, Yuanyuan Wei, Zijie Wang, Hao Tang, Jiuqi Qin, Shuting Lan, Yingheng Wang, Junwen Bai, Zhuangbin Chen, Zibin Zheng, Caihua Liu, Hao Liang|2025-06-02|arXiv|https://github.com/Wayyuanyuan/MTCMB|http://arxiv.org/abs/2506.01252v1|
|227|Incentivizing LLMs to Self-Verify Their Answers|Fuxiang Zhang, Jiacheng Xu, Chaojie Wang, Ce Cui, Yang Liu, Bo An|2025-06-02|arXiv|https://github.com/mansicer/self-verification|http://arxiv.org/abs/2506.01369v1|
|228|CVC: A Large-Scale Chinese Value Rule Corpus for Value Alignment of Large Language Models|Ping Wu, Guobin Shen, Dongcheng Zhao, Yuwei Wang, Yiting Dong, Yu Shi, Enmeng Lu, Feifei Zhao, Yi Zeng|2025-06-02|arXiv|https://github.com/Beijing-AISI/CVC|https://doi.org/10.48550/arXiv.2506.01495|
|229|Exploring the Potential of LLMs as Personalized Assistants: Dataset, Evaluation, and Analysis|Jisoo Mok, Ik-hwan Kim, Sangkwon Park, Sungroh Yoon|2025-06-02|arXiv|https://github.com/12kimih/HiCUPID|http://arxiv.org/abs/2506.01262v1|
|230|EvolveNav: Self-Improving Embodied Reasoning for LLM-Based Vision-Language Navigation|Bingqian Lin, Yunshuang Nie, Khun Loun Zai, Ziming Wei, Mingfei Han, Rongtao Xu, Minzhe Niu, Jianhua Han, Liang Lin, Cewu Lu, Xiaodan Liang|2025-06-02|arXiv|https://github.com/expectorlin/EvolveNav|http://arxiv.org/abs/2506.01551v1|
|231|Flow2Code: Evaluating Large Language Models for Flowchart-based Code Generation Capability|Mengliang He, Jiayi Zeng, Yankai Jiang, Wei Zhang, Zeming Liu, Xiaoming Shi, Aimin Zhou|2025-06-02|arXiv|https://github.com/hml-github/Flow2Code|https://doi.org/10.48550/arXiv.2506.02073|
|232|IF-GUIDE: Influence Function-Guided Detoxification of LLMs|Zachary Coalson, Juhan Bae, Nicholas Carlini, Sanghyun Hong|2025-06-02|arXiv|https://github.com/ztcoalson/IF-Guide|http://arxiv.org/abs/2506.01790v2|
|233|IRT-Router: Effective and Interpretable Multi-LLM Routing via Item Response Theory|Wei Song, Zhenya Huang, Cheng Cheng, Weibo Gao, Bihan Xu, GuanHao Zhao, Fei Wang, Runze Wu|2025-06-01|arXiv|https://github.com/Mercidaiha/IRT-Router|http://arxiv.org/abs/2506.01048v1|
|234|Unlabeled Data Improves Fine-Grained Image Zero-shot Classification with Multimodal LLMs|Yunqi Hong, Sohyun An, Andrew Bai, Neil Y. C. Lin, Cho-Jui Hsieh|2025-06-01|arXiv|https://github.com/yq-hong/AutoSEP|http://arxiv.org/abs/2506.03195v1|
|235|Un-considering Contextual Information: Assessing LLMs' Understanding of Indexical Elements|Metehan Oguz, Yavuz Bakman, Duygu Nur Yaldiz|2025-06-01|arXiv|https://github.com/metehanoguzz/LLMs-Indexicals-English|http://arxiv.org/abs/2506.01089v1|
|236|Taming LLMs by Scaling Learning Rates with Gradient Grouping|Siyuan Li, Juanxi Tian, Zedong Wang, Xin Jin, Zicheng Liu, Wentao Zhang, Dan Xu|2025-06-01|arXiv|https://github.com/ScalingOpt/SGG|http://arxiv.org/abs/2506.01049v1|
|237|Reconsidering LLM Uncertainty Estimation Methods in the Wild|Yavuz Bakman, Duygu Nur Yaldiz, Sungmin Kang, Tuo Zhang, Baturalp Buyukates, Salman Avestimehr, Sai Praneeth Karimireddy|2025-06-01|arXiv|https://github.com/duygunuryldz/uncertainty_in_the_wild|http://arxiv.org/abs/2506.01114v1|
|238|Probing the Geometry of Truth: Consistency and Generalization of Truth Directions in LLMs Across Logical Transformations and Question Answering Tasks|Yuntai Bao, Xuhong Zhang, Tianyu Du, Xinkui Zhao, Zhengwen Feng, Hao Peng, Jianwei Yin|2025-06-01|arXiv|https://github.com/colored-dye/truthfulness_probe_generalization|http://arxiv.org/abs/2506.00823v1|
|239|L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with Synthetic Annotations using CoTR prompting and Large Language Models|Nidhi Kowtal, Raviraj Joshi|2025-06-01|arXiv|https://github.com/l3cube-pune/MarathiNLP|https://doi.org/10.48550/arXiv.2506.00863|
|240|Probing Neural Topology of Large Language Models|Yu Zheng, Yuan Yuan, Yong Li, Paolo Santi|2025-06-01|arXiv|https://github.com/DavyMorgan/llm-graph-probing|https://doi.org/10.48550/arXiv.2506.01042|
|241|Improving Automatic Evaluation of Large Language Models (LLMs) in Biomedical Relation Extraction via LLMs-as-the-Judge|Md. Tahmid Rahman Laskar, Israt Jahan, Elham Dolatabadi, Chun Peng, Enamul Hoque, Jimmy Huang|2025-06-01|arXiv|https://github.com/tahmedge/llm_judge_biomedical_re|https://doi.org/10.48550/arXiv.2506.00777|
|242|Dynamic Chunking and Selection for Reading Comprehension of Ultra-Long Context in Large Language Models|Boheng Sheng, Jiacheng Yao, Meicong Zhang, Guoxiu He|2025-06-01|arXiv|https://github.com/ECNU-Text-Computing/DCS|https://doi.org/10.48550/arXiv.2506.00773|
|243|Doubly Robust Alignment for Large Language Models|Erhan Xu, Kai Ye, Hongyi Zhou, Luhan Zhu, Francesco Quinzan, Chengchun Shi|2025-06-01|arXiv|https://github.com/DRPO4LLM/DRPO4LLM|https://doi.org/10.48550/arXiv.2506.01183|
|244|CODEMENV: Benchmarking Large Language Models on Code Migration|Keyuan Cheng, Xudong Shen, Yihao Yang, Tengyue Wang, Yang Cao, Muhammad Asif Ali, Hanbin Wang, Lijie Hu, Di Wang|2025-06-01|arXiv|https://github.com/xdshen-ai/Benchmark-of-Code-Migration|https://doi.org/10.48550/arXiv.2506.00894|
|245|KG-TRACES: Enhancing Large Language Models with Knowledge Graph-constrained Trajectory Reasoning and Attribution Supervision|Rong Wu, Pinlong Cai, Jianbiao Mei, Licheng Wen, Tao Hu, Xuemeng Yang, Daocheng Fu, Botian Shi|2025-06-01|arXiv|https://github.com/Edaizi/KG-TRACES|https://doi.org/10.48550/arXiv.2506.00783|
|246|SafeTuneBed: A Toolkit for Benchmarking LLM Safety Alignment in Fine-Tuning|Saad Hossain, Samanvay Vajpayee, Sirisha Rambhatla|2025-05-31|arXiv|https://github.com/criticalml-uw/SafeTuneBed|http://arxiv.org/abs/2506.00676v1|
|247|Measuring Faithfulness and Abstention: An Automated Pipeline for Evaluating LLM-Generated 3-ply Case-Based Legal Arguments|Li Zhang, Morgan Gray, Jaromir Savelka, Kevin D. Ashley|2025-05-31|arXiv|https://lizhang-aiandlaw.github.io/An-Automated-Pipeline-for-Evaluating-LLM-Generated-3-ply-Case-Based-Legal-Arguments/|http://arxiv.org/abs/2506.00694v2|
|248|Revisiting LLMs as Zero-Shot Time-Series Forecasters: Small Noise Can Break Large Models|Junwoo Park, Hyuck Lee, Dohyun Lee, Daehoon Gwak, Jaegul Choo|2025-05-31|arXiv|https://github.com/junwoopark92/revisiting-LLMs-zeroshot-forecaster|http://arxiv.org/abs/2506.00457v1|
|249|Towards Multi-dimensional Evaluation of LLM Summarization across Domains and Languages|Hyangsuk Min, Yuho Lee, Minjeong Ban, Jiaqi Deng, Nicole Hee-Yeon Kim, Taewon Yun, Hang Su, Jason Cai, Hwanjun Song|2025-05-31|arXiv|https://github.com/DISL-Lab/MSumBench|http://arxiv.org/abs/2506.00549v1|
|250|Chain-of-Frames: Advancing Video Understanding in Multimodal LLMs via Frame-Aware Reasoning|Sara Ghazanfari, Francesco Croce, Nicolas Flammarion, Prashanth Krishnamurthy, Farshad Khorrami, Siddharth Garg|2025-05-31|arXiv|https://github.com/SaraGhazanfari/CoF|http://arxiv.org/abs/2506.00318v1|
|251|K-order Ranking Preference Optimization for Large Language Models|Shihao Cai, Chongming Gao, Yang Zhang, Wentao Shi, Jizhi Zhang, Keqin Bao, Qifan Wang, Fuli Feng|2025-05-31|arXiv|https://github.com/Lanyu0303/KPO|https://doi.org/10.48550/arXiv.2506.00441|
|252|CityLens: Benchmarking Large Language-Vision Models for Urban Socioeconomic Sensing|Tianhui Liu, Jie Feng, Hetian Pang, Xin Zhang, Tianjian Ouyang, Zhiyuan Zhang, Yong Li|2025-05-31|arXiv|https://github.com/tsinghua-fib-lab/CityLens|https://doi.org/10.48550/arXiv.2506.00530|
|253|Reasoning Like an Economist: Post-Training on Economic Problems Induces Strategic Generalization in LLMs|Yufa Zhou, Shaobo Wang, Xingyu Dong, Xiangqi Jin, Yifang Chen, Yue Min, Kexin Yang, Xingzhang Ren, Dayiheng Liu, Linfeng Zhang|2025-05-31|arXiv|https://github.com/MasterZhou1/Recon|http://arxiv.org/abs/2506.00577v1|
|254|CausalAbstain: Enhancing Multilingual LLMs with Causal Reasoning for Trustworthy Abstention|Yuxi Sun, Aoqi Zuo, Wei Gao, Jing Ma|2025-05-31|arXiv|https://github.com/peachch/CausalAbstain|http://arxiv.org/abs/2506.00519v2|
|255|LegalEval-Q: A New Benchmark for The Quality Evaluation of LLM-Generated Legal Text|Li yunhan, Wu gengshen|2025-05-30|arXiv|https://github.com/lyxx3rd/LegalEval-Q|http://arxiv.org/abs/2505.24826v1|
|256|HardTests: Synthesizing High-Quality Test Cases for LLM Coding|Zhongmou He, Yee Man Choi, Kexun Zhang, Jiabao Ji, Junting Zhou, Dejia Xu, Ivan Bercovich, Aidan Zhang, Lei Li|2025-05-30|arXiv|https://leililab.github.io/HardTests/|http://arxiv.org/abs/2505.24098v1|
|257|TRAPDOC: Deceiving LLM Users by Injecting Imperceptible Phantom Tokens into Documents|Hyundong Jin, Sicheol Sung, Shinwoo Park, SeungYeop Baik, Yo-Sub Han|2025-05-30|arXiv|https://github.com/jindong22/TrapDoc|http://arxiv.org/abs/2506.00089v1|
|258|SEAR: A Multimodal Dataset for Analyzing AR-LLM-Driven Social Engineering Behaviors|Tianlong Yu, Chenghang Ye, Zheyu Yang, Ziyi Zhou, Cui Tang, Zui Tao, Jun Zhang, Kailong Wang, Liting Zhou, Yang Yang, Ting Bi|2025-05-30|arXiv|https://github.com/INSLabCN/SEAR-Dataset|http://arxiv.org/abs/2505.24458v1|
|259|RAST: Reasoning Activation in LLMs via Small-model Transfer|Siru Ouyang, Xinyu Zhu, Zilin Xiao, Minhao Jiang, Yu Meng, Jiawei Han|2025-05-30|arXiv|https://ozyyshr.github.io/RAST/|http://arxiv.org/abs/2506.15710v1|
|260|MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement Learning|Yiqing Liang, Jielin Qiu, Wenhao Ding, Zuxin Liu, James Tompkin, Mengdi Xu, Mengzhou Xia, Zhengzhong Tu, Laixi Shi, Jiacheng Zhu|2025-05-30|arXiv|https://modomodo-rl.github.io/|http://arxiv.org/abs/2505.24871v2|
|261|MMAFFBen: A Multilingual and Multimodal Affective Analysis Benchmark for Evaluating LLMs and VLMs|Zhiwei Liu, Lingfei Qian, Qianqian Xie, Jimin Huang, Kailai Yang, Sophia Ananiadou|2025-05-30|arXiv|https://github.com/lzw108/MMAFFBen|http://arxiv.org/abs/2505.24423v1|
|262|DisTime: Distribution-based Time Representation for Video Large Language Models|Yingsen Zeng, Zepeng Huang, Yujie Zhong, Chengjian Feng, Jie Hu, Lin Ma, Yang Liu|2025-05-30|arXiv|https://github.com/josephzpng/DisTime|https://doi.org/10.48550/arXiv.2505.24329|
|263|GridRoute: A Benchmark for LLM-Based Route Planning with Cardinal Movement in Grid Environments|Kechen Li, Yaotian Tao, Ximing Wen, Quanwei Sun, Zifei Gong, Chang Xu, Xizhe Zhang, Tianbo Ji|2025-05-30|arXiv|https://github.com/LinChance/GridRoute|http://arxiv.org/abs/2505.24306v1|
|264|Effects of Theory of Mind and Prosocial Beliefs on Steering Human-Aligned Behaviors of LLMs in Ultimatum Games|Neemesh Yadav, Palakorn Achananuparp, Jing Jiang, Ee-Peng Lim|2025-05-30|arXiv|https://github.com/Stealth-py/UltimatumToM|http://arxiv.org/abs/2505.24255v1|
|265|ClinBench-HPB: A Clinical Benchmark for Evaluating LLMs in Hepato-Pancreato-Biliary Diseases|Yuchong Li, Xiaojun Zeng, Chihua Fang, Jian Yang, Fucang Jia, Lei Zhang|2025-05-30|arXiv|https://clinbench-hpb.github.io|http://arxiv.org/abs/2506.00095v3|
|266|Beyond Semantic Entropy: Boosting LLM Uncertainty Quantification with Pairwise Semantic Similarity|Dang Nguyen, Ali Payani, Baharan Mirzasoleiman|2025-05-30|arXiv|https://github.com/BigML-CS-UCLA/SNNE|http://arxiv.org/abs/2506.00245v1|
|267|Revisiting Epistemic Markers in Confidence Estimation: Can Markers Accurately Reflect Large Language Models&apos; Uncertainty?|Jiayu Liu, Qing Zong, Weiqi Wang, Yangqiu Song|2025-05-30|arXiv|https://github.com/HKUST-KnowComp/MarCon|https://doi.org/10.48550/arXiv.2505.24778|
|268|Period-LLM: Extending the Periodic Capability of Multimodal Large Language Model|Yuting Zhang, Hao Lu, Qingyong Hu, Yin Wang, Kaishen Yuan, Xin Liu, Kaishun Wu|2025-05-30|arXiv|https://github.com/keke-nice/Period-LLM|https://doi.org/10.48550/arXiv.2505.24476|
|269|Mixed-R1: Unified Reward Perspective For Reasoning Capability in Multimodal Large Language Models|Shilin Xu, Yanwei Li, Rui Yang, Tao Zhang, Yueyi Sun, Wei Chow, Linfeng Li, Hang Song, Qi Xu, Yunhai Tong, Xiangtai Li, Hao Fei|2025-05-30|arXiv|https://github.com/xushilin1/mixed-r1|https://doi.org/10.48550/arXiv.2505.24164|
|270|Large Language Models are Locally Linear Mappings|James R. Golden|2025-05-30|arXiv|https://github.com/jamesgolden1/llms-are-llms|https://doi.org/10.48550/arXiv.2505.24293|
|271|Sentinel: Attention Probing of Proxy Models for LLM Context Compression with an Understanding Perspective|Yong Zhang, Yanwen Huang, Ning Cheng, Yang Guo, Yun Zhu, Yanmeng Wang, Shaojun Wang, Jing Xiao|2025-05-29|arXiv|https://github.com/yzhangchuck/Sentinel|http://arxiv.org/abs/2505.23277v1|
|272|SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents|Kunlun Zhu, Jiaxun Zhang, Ziheng Qi, Nuoxing Shang, Zijia Liu, Peixuan Han, Yue Su, Haofei Yu, Jiaxuan You|2025-05-29|arXiv|https://github.com/ulab-uiuc/SafeScientist|http://arxiv.org/abs/2505.23559v1|
|273|Stairway to Success: Zero-Shot Floor-Aware Object-Goal Navigation via LLM-Driven Coarse-to-Fine Exploration|Zeying Gong, Rong Li, Tianshuai Hu, Ronghe Qiu, Lingdong Kong, Lingfeng Zhang, Yiyi Ding, Leying Zhang, Junwei Liang|2025-05-29|arXiv|https://zeying-gong.github.io/projects/ascent|http://arxiv.org/abs/2505.23019v1|
|274|ToMAP: Training Opponent-Aware LLM Persuaders with Theory of Mind|Peixuan Han, Zijia Liu, Jiaxuan You|2025-05-29|arXiv|https://github.com/ulab-uiuc/ToMAP|http://arxiv.org/abs/2505.22961v1|
|275|Don't Take the Premise for Granted: Evaluating the Premise Critique Ability of Large Language Models|Jinzhe Li, Gengxu Li, Yi Chang, Yuan Wu|2025-05-29|arXiv|https://github.com/MLGroupJLU/Premise_Critique|http://arxiv.org/abs/2505.23715v1|
|276|Probability-Consistent Preference Optimization for Enhanced LLM Reasoning|Yunqiao Yang, Houxing Ren, Zimu Lu, Ke Wang, Weikang Shi, Aojun Zhou, Junting Pan, Mingjie Zhan, Hongsheng Li|2025-05-29|arXiv|https://github.com/YunqiaoYang/PCPO|http://arxiv.org/abs/2505.23540v1|
|277|GenIC: An LLM-Based Framework for Instance Completion in Knowledge Graphs|Amel Gader, Alsayed Algergawy|2025-05-29|arXiv|https://github.com/amal-gader/genic|http://arxiv.org/abs/2505.24036v1|
|278|DenoiseRotator: Enhance Pruning Robustness for LLMs via Importance Concentration|Tianteng Gu, Bei Liu, Bo Xiao, Ke Zeng, Jiacheng Liu, Yanmin Qian|2025-05-29|arXiv|https://github.com/Axel-gu/DenoiseRotator|http://arxiv.org/abs/2505.23049v1|
|279|Leave it to the Specialist: Repair Sparse LLMs with Sparse Fine-Tuning via Sparsity Evolution|Qiao Xiao, Alan Ansell, Boqian Wu, Lu Yin, Mykola Pechenizkiy, Shiwei Liu, Decebal Constantin Mocanu|2025-05-29|arXiv|https://github.com/QiaoXiao7282/SEFT|http://arxiv.org/abs/2505.24037v1|
|280|Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models|Yiran Guo, Lijie Xu, Jie Liu, Dan Ye, Shuang Qiu|2025-05-29|arXiv|https://github.com/AIFrameResearch/SPO|https://doi.org/10.48550/arXiv.2505.23564|
|281|SocialMaze: A Benchmark for Evaluating Social Reasoning in Large Language Models|Zixiang Xu, Yanbo Wang, Yue Huang, Jiayi Ye, Haomin Zhuang, Zirui Song, Lang Gao, Chenxi Wang, Zhaorun Chen, Yujun Zhou, Sixian Li, Wang Pan, Yue Zhao, Jieyu Zhao, Xiangliang Zhang, Xiuying Chen|2025-05-29|arXiv|https://github.com/xzx34/SocialMaze|https://doi.org/10.48550/arXiv.2505.23713|
|282|BioReason: Incentivizing Multimodal Biological Reasoning within a DNA-LLM Model|Adibvafa Fallahpour, Andrew Magnuson, Purav Gupta, Shihao Ma, Jack Naimer, Arnav Shah, Haonan Duan, Omar Ibrahim, Hani Goodarzi, Chris J. Maddison, Bo Wang|2025-05-29|arXiv|https://github.com/bowang-lab/BioReason|http://arxiv.org/abs/2505.23579v1|
|283|Enabling Flexible Multi-LLM Integration for Scalable Knowledge Aggregation|Zhenglun Kong, Zheng Zhan, Shiyue Hou, Yifan Gong, Xin Meng, Pengwei Sui, Peiyan Dong, Xuan Shen, Zifeng Wang, Pu Zhao, Hao Tang, Stratis Ioannidis, Yanzhi Wang|2025-05-28|arXiv|https://github.com/ZLKong/LLM_Integration|http://arxiv.org/abs/2505.23844v1|
|284|Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO|Lai Wei, Yuting Li, Chen Wang, Yue Wang, Linghe Kong, Weiran Huang, Lichao Sun|2025-05-28|arXiv|https://github.com/waltonfuture/MM-UPT|http://arxiv.org/abs/2505.22453v1|
|285|EFIM: Efficient Serving of LLMs for Infilling Tasks with Improved KV Cache Reuse|Tianyu Guo, Hande Dong, Yichong Leng, Feng Liu, Cheater Lin, Nong Xiao, Xianwei Zhang|2025-05-28|arXiv|https://github.com/gty111/EFIM|http://arxiv.org/abs/2505.21889v2|
|286|Beyond path selection: Better LLMs for Scientific Information Extraction with MimicSFT and Relevance and Rule-induced(R$^2$)GRPO|Ran Li, Shimin Di, Yuchen Liu, Chen Jing, Yu Qiu, Lei Chen|2025-05-28|arXiv|https://github.com/ranlislz/R2GRPO|http://arxiv.org/abs/2505.22068v1|
|287|AgentDNS: A Root Domain Naming System for LLM Agents|Enfang Cui, Yujun Cheng, Rui She, Dan Liu, Zhiyuan Liang, Minxin Guo, Tianzheng Li, Qian Wei, Wenjuan Xing, Zhijie Zhong|2025-05-28|arXiv|https://github.com/agentdns|http://arxiv.org/abs/2505.22368v1|
|288|LoKI: Low-damage Knowledge Implanting of Large Language Models|Runyu Wang, Peng Ping, Zhengyu Guo, Xiaoye Zhang, Quan Shi, Liting Zhou, Tianbo Ji|2025-05-28|arXiv|https://github.com/Nexround/LoKI|https://doi.org/10.48550/arXiv.2505.22120|
|289|Do You See Me : A Multidimensional Benchmark for Evaluating Visual Perception in Multimodal LLMs|Aditya Kanade, Tanuja Ganu|2025-05-28|arXiv|https://github.com/microsoft/Do-You-See-Me|http://arxiv.org/abs/2506.02022v1|
|290|Large Language Models for Depression Recognition in Spoken Language Integrating Psychological Knowledge|Yupei Li, Shuaijie Shao, Manuel Milling, Björn W. Schuller|2025-05-28|arXiv|https://github.com/myxp-lyp/Depression-detection|https://doi.org/10.48550/arXiv.2505.22863|
|291|EULER: Enhancing the Reasoning Ability of Large Language Models through Error-Induced Learning|Zhuoyang Wu, Xinze Li, Zhenghao Liu, Yukun Yan, Zhiyuan Liu, Minghe Yu, Cheng Yang, Yu Gu, Ge Yu, Maosong Sun|2025-05-28|arXiv|https://github.com/NEUIR/EULER|https://doi.org/10.48550/arXiv.2505.22131|
|292|Characterizing Bias: Benchmarking Large Language Models in Simplified versus Traditional Chinese|Hanjia Lyu, Jiebo Luo, Jian Kang, Allison Koenecke|2025-05-28|FAccT|https://github.com/brucelyu17/SC-TC-Bench|https://doi.org/10.1145/3715275.3732182|
|293|3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model|Wenbo Hu, Yining Hong, Yanjun Wang, Leison Gao, Zibu Wei, Xingcheng Yao, Nanyun Peng, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang|2025-05-28|arXiv|https://3dllm-mem.github.io|https://doi.org/10.48550/arXiv.2505.22657|
|294|R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised and Reinforcement Learning|Yongchao Chen, Yueying Liu, Junwei Zhou, Yilun Hao, Jingquan Wang, Yang Zhang, Chuchu Fan|2025-05-27|arXiv|https://github.com/yongchao98/R1-Code-Interpreter|http://arxiv.org/abs/2505.21668v1|
|295|Accelerating RL for LLM Reasoning with Optimal Advantage Regression|Kianté Brantley, Mingyu Chen, Zhaolin Gao, Jason D. Lee, Wen Sun, Wenhao Zhan, Xuezhou Zhang|2025-05-27|arXiv|https://github.com/ZhaolinGao/A-PO|http://arxiv.org/abs/2505.20686v1|
|296|Trans-EnV: A Framework for Evaluating the Linguistic Robustness of LLMs Against English Varieties|Jiyoung Lee, Seungho Kim, Jieun Han, Jun-Min Lee, Kitaek Kim, Alice Oh, Edward Choi|2025-05-27|arXiv|https://github.com/jiyounglee-0523/TransEnV|http://arxiv.org/abs/2505.20875v1|
|297|Evaluation of LLMs in Medical Text Summarization: The Role of Vocabulary Adaptation in High OOV Settings|Gunjan Balde, Soumyadeep Roy, Mainack Mondal, Niloy Ganguly|2025-05-27|arXiv|https://github.com/gb-kgp/LLM-MedicalSummarization-Benchmark|http://arxiv.org/abs/2505.21242v1|
|298|Scaling External Knowledge Input Beyond Context Windows of LLMs via Multi-Agent Collaboration|Zijun Liu, Zhennan Wan, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Yang Liu|2025-05-27|arXiv|https://github.com/THUNLP-MT/ExtAgents|http://arxiv.org/abs/2505.21471v1|
|299|Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation|Ke Zhang, Cihan Xiao, Yiqun Mei, Jiacong Xu, Vishal M. Patel|2025-05-27|arXiv|https://bwgzk-keke.github.io/DiffPhy/|http://arxiv.org/abs/2505.21653v1|
|300|UQLegalAI@COLIEE2025: Advancing Legal Case Retrieval with Large Language Models and Graph Neural Networks|Yanran Tang, Ruihong Qiu, Zi Huang|2025-05-27|arXiv|https://github.com/yanran-tang/CaseLink|https://doi.org/10.48550/arXiv.2505.20743|
|301|A Course Correction in Steerability Evaluation: Revealing Miscalibration and Side Effects in LLMs|Trenton Chang, Tobias Schnabel, Adith Swaminathan, Jenna Wiens|2025-05-27|arXiv|https://github.com/MLD3/steerability|http://arxiv.org/abs/2505.23816v1|
|302|'Hello, World!': Making GNNs Talk with LLMs|Sunwoo Kim, Soo Yong Lee, Jaemin Yoo, Kijung Shin|2025-05-27|arXiv|https://github.com/kswoo97/GLN-Code|http://arxiv.org/abs/2505.20742v1|
|303|SELF-PERCEPT: Introspection Improves Large Language Models&apos; Detection of Multi-Person Mental Manipulation in Conversations|Danush Khanna, Pratinav Seth, Sidhaarth Sredharan Murali, Aditya Kumar Guru, Siddharth Shukla, Tanuj Tyagi, Sandeep Chaurasia, Kripabandhu Ghosh|2025-05-27|arXiv|https://github.com/danushkhanna/self-percept|https://doi.org/10.48550/arXiv.2505.20679|
|304|Active-O3: Empowering Multimodal Large Language Models with Active Perception via GRPO|Muzhi Zhu, Hao Zhong, Canyu Zhao, Zongze Du, Zheng Huang, Mingyu Liu, Hao Chen, Cheng Zou, Jingdong Chen, Ming Yang, Chunhua Shen|2025-05-27|arXiv|https://aim-uofa.github.io/ACTIVE-o3|https://doi.org/10.48550/arXiv.2505.21457|
|305|How does Misinformation Affect Large Language Model Behaviors and Preferences?|Miao Peng, Nuo Chen, Jianheng Tang, Jia Li|2025-05-27|arXiv|https://github.com/GKNL/MisBench|https://doi.org/10.48550/arXiv.2505.21608|
|306|HoliTom: Holistic Token Merging for Fast Video Large Language Models|Kele Shao, Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang|2025-05-27|arXiv|https://github.com/cokeshao/HoliTom|https://doi.org/10.48550/arXiv.2505.21334|
|307|DenseLoRA: Dense Low-Rank Adaptation of Large Language Models|Lin Mu, Xiaoyu Wang, Li Ni, Yang Li, Zhize Wu, Peiquan Jin, Yiwen Zhang|2025-05-27|arXiv|https://github.com/mulin-ahu/DenseLoRA|https://doi.org/10.48550/arXiv.2505.23808|
|308|DLP: Dynamic Layerwise Pruning in Large Language Models|Yuli Chen, Bo Cheng, Jiale Han, Yingying Zhang, Yingting Li, Shuhao Zhang|2025-05-27|arXiv|https://github.com/ironartisan/DLP|https://doi.org/10.48550/arXiv.2505.23807|
|309|CogniBench: A Legal-inspired Framework and Dataset for Assessing Cognitive Faithfulness of Large Language Models|Xiaqiang Tang, Jian Li, Keyu Hu, Du Nan, Xiaolong Li, Xi Zhang, Weigao Sun, Sihong Xie|2025-05-27|arXiv|https://github.com/FUTUREEEEEE/CogniBench|https://doi.org/10.48550/arXiv.2505.20767|
|310|LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms|Wenhu Li, Niki van Stein, Thomas Bäck, Elena Raponi|2025-05-27|arXiv|https://github.com/Ewendawi/LLaMEA-BO|https://doi.org/10.48550/arXiv.2505.21034|
|311|AMQA: An Adversarial Dataset for Benchmarking Bias of LLMs in Medicine and Healthcare|Ying Xiao, Jie Huang, Ruijuan He, Jing Xiao, Mohammad Reza Mousavi, Yepang Liu, Kezhi Li, Zhenpeng Chen, Jie M. Zhang|2025-05-26|arXiv|https://github.com/XY-Showing/AMQA|http://arxiv.org/abs/2505.19562v1|
|312|Rethinking Text-based Protein Understanding: Retrieval or LLM?|Juntong Wu, Zijing Liu, He Cao, Hao Li, Bin Feng, Zishan Shu, Ke Yu, Li Yuan, Yu Li|2025-05-26|arXiv|https://github.com/IDEA-XL/RAPM|http://arxiv.org/abs/2505.20354v1|
|313|AgentRecBench: Benchmarking LLM Agent-based Personalized Recommender Systems|Yu Shang, Peijie Liu, Yuwei Yan, Zijing Wu, Leheng Sheng, Yuanqing Yu, Chumeng Jiang, An Zhang, Fengli Xu, Yu Wang, Min Zhang, Yong Li|2025-05-26|arXiv|https://tsinghua-fib-lab.github.io/AgentSocietyChallenge/pages/overview.html|http://arxiv.org/abs/2505.19623v1|
|314|Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning|Shenao Zhang, Yaqing Wang, Yinxiao Liu, Tianqi Liu, Peter Grabowski, Eugene Ie, Zhaoran Wang, Yunxuan Li|2025-05-26|arXiv|https://github.com/shenao-zhang/BARL|http://arxiv.org/abs/2505.20561v1|
|315|HomeBench: Evaluating LLMs in Smart Homes with Valid and Invalid Instructions Across Single and Multiple Devices|Silin Li, Yuhang Guo, Jiashu Yao, Zeming Liu, Haifeng Wang|2025-05-26|arXiv|https://github.com/BITHLP/HomeBench|http://arxiv.org/abs/2505.19628v1|
|316|FinLoRA: Benchmarking LoRA Methods for Fine-Tuning LLMs on Financial Datasets|Dannong Wang, Jaisal Patel, Daochen Zha, Steve Y. Yang, Xiao-Yang Liu|2025-05-26|arXiv|https://github.com/Open-Finance-Lab/FinLoRA|http://arxiv.org/abs/2505.19819v1|
|317|BizFinBench: A Business-Driven Real-World Financial Benchmark for Evaluating LLMs|Guilong Lu, Xuntao Guo, Rongjunchen Zhang, Wenqiao Zhu, Ji Liu|2025-05-26|arXiv|https://hithink-research.github.io/BizFinBench/|http://arxiv.org/abs/2505.19457v1|
|318|DOGe: Defensive Output Generation for LLM Protection Against Knowledge Distillation|Pingzhi Li, Zhen Tan, Huaizhi Qu, Huan Liu, Tianlong Chen|2025-05-26|arXiv|https://github.com/UNITES-Lab/DOGe|http://arxiv.org/abs/2505.19504v1|
|319|Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks|Sirui Chen, Shuqin Ma, Shu Yu, Hanwang Zhang, Shengjie Zhao, Chaochao Lu|2025-05-26|arXiv|https://github.com/OpenCausaLab/Awesome-LLM-Consciousness|http://arxiv.org/abs/2505.19806v1|
|320|Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic Capabilities in LLM Compression|Peijie Dong, Zhenheng Tang, Xiang Liu, Lujun Li, Xiaowen Chu, Bo Li|2025-05-26|arXiv|https://github.com/pprp/ACBench|http://arxiv.org/abs/2505.19433v1|
|321|MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs|Zaid Alyafeai, Maged S. Al-Shaibani, Bernard Ghanem|2025-05-26|arXiv|https://github.com/IVUL-KAUST/MOLE|http://arxiv.org/abs/2505.19800v1|
|322|R3-RAG: Learning Step-by-Step Reasoning and Retrieval for LLMs via Reinforcement Learning|Yuan Li, Qi Luo, Xiaonan Li, Bufan Li, Qinyuan Cheng, Bo Wang, Yining Zheng, Yuxin Wang, Zhangyue Yin, Xipeng Qiu|2025-05-26|arXiv|https://github.com/Yuan-Li-FNLP/R3-RAG|http://arxiv.org/abs/2505.23794v1|
|323|GraphGen: Enhancing Supervised Fine-Tuning for LLMs with Knowledge-Driven Synthetic Data Generation|Zihong Chen, Wanli Jiang, Jinzhe Li, Zhonghang Yuan, Huanjun Kong, Wanli Ouyang, Nanqing Dong|2025-05-26|arXiv|https://github.com/open-sciencelab/GraphGen|http://arxiv.org/abs/2505.20416v1|
|324|WINA: Weight Informed Neuron Activation for Accelerating Large Language Model Inference|Sihan Chen, Dan Zhao, Jongwoo Ko, Colby R. Banbury, Huiping Zhuang, Luming Liang, Tianyi Chen|2025-05-26|arXiv|https://github.com/microsoft/wina|https://doi.org/10.48550/arXiv.2505.19427|
|325|LLM Meets Scene Graph: Can Large Language Models Understand and Generate Scene Graphs? A Benchmark and Empirical Study|Dongil Yang, Minjin Kim, Sunghwan Kim, Beong-woo Kwak, Minjun Park, Jinseok Hong, Woontack Woo, Jinyoung Yeo|2025-05-26|arXiv|https://github.com/docworlds/tsg-bench|https://doi.org/10.48550/arXiv.2505.19510|
|326|Token-level Accept or Reject: A Micro Alignment Approach for Large Language Models|Yang Zhang, Yu Yu, Bo Tang, Yu Zhu, Chuxiong Sun, Wenqiang Wei, Jie Hu, Zipeng Xie, Zhiyu Li, Feiyu Xiong, Edward Chung|2025-05-26|arXiv|https://github.com/IAAR-Shanghai/MARA|https://doi.org/10.48550/arXiv.2505.19743|
|327|Causal-LLaVA: Causal Disentanglement for Mitigating Hallucination in Multimodal Large Language Models|Xinmiao Hu, Chun Wang, Ruihe An, ChenYu Shao, Xiaojun Ye, Sheng Zhou, Liangcheng Li|2025-05-26|arXiv|https://github.com/IgniSavium/Causal-LLaVA|https://doi.org/10.48550/arXiv.2505.19474|
|328|DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response|Bilel Cherif, Tamás Bisztray, Richard A. Dubniczky, Aaesha Aldahmani, Saeed Alshehhi, Norbert Tihanyi|2025-05-26|arXiv|https://github.com/DFIR-Metric|https://doi.org/10.48550/arXiv.2505.19973|
|329|Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles|Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, Hao Zhou, Mingxuan Wang|2025-05-26|arXiv|https://seed-enigmata.github.io|https://doi.org/10.48550/arXiv.2505.19914|
|330|GenKI: Enhancing Open-Domain Question Answering with Knowledge Integration and Controllable Generation in Large Language Models|Tingjia Shen, Hao Wang, Chuan Qin, Ruijun Sun, Yang Song, Defu Lian, Hengshu Zhu, Enhong Chen|2025-05-26|arXiv|https://github.com/USTC-StarTeam/GenKI|https://doi.org/10.48550/arXiv.2505.19660|
|331|FoodTaxo: Generating Food Taxonomies with Large Language Models|Pascal Wullschleger, Majid Zarharan, Donnacha Daly, Marc Pouly, Jennifer Foster|2025-05-26|arXiv|https://foodtaxo.github.io/|https://doi.org/10.48550/arXiv.2505.19838|
|332|Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program|Alejandro Carrasco, Víctor Rodríguez-Fernández, Richard Linares|2025-05-26|arXiv|https://github.com/ARCLab-MIT/kspdg|https://doi.org/10.48550/arXiv.2505.19896|
|333|Automated CAD Modeling Sequence Generation from Text Descriptions via Transformer-Based Large Language Models|Jianxing Liao, Junyan Xu, Yatao Sun, Maowen Tang, Sicheng He, Jingxian Liao, Shui Yu, Yun Li, Hongguan Xiao|2025-05-26|arXiv|https://jianxliao.github.io/cadllm-page/|https://doi.org/10.48550/arXiv.2505.19490|
|334|MiniLongBench: The Low-cost Long Context Understanding Benchmark for Large Language Models|Zhongzhan Huang, Guoming Ling, Shanshan Zhong, Hefeng Wu, Liang Lin|2025-05-26|arXiv|https://github.com/MilkThink-Lab/MiniLongBench|https://doi.org/10.48550/arXiv.2505.19959|
|335|Segment First or Comprehend First? Explore the Limit of Unsupervised Word Segmentation with Large Language Models|Zihong Zhang, Liqi He, Zuchao Li, Lefei Zhang, Hai Zhao, Bo Du|2025-05-26|arXiv|https://github.com/hkr04/LLACA|https://doi.org/10.48550/arXiv.2505.19631|
|336|Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models|George Kour, Itay Nakash, Ateret Anaby-Tavor, Michal Shmueli-Scheuer|2025-05-26|arXiv|https://ibm.github.io/POBS|https://doi.org/10.48550/arXiv.2505.19621|
|337|Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for Frozen LLMs|Jaemin Kim, Hangeol Chang, Hyunmin Hwang, Choonghan Kim, Jong Chul Ye|2025-05-25|arXiv|https://github.com/hangeol/UniR|http://arxiv.org/abs/2505.19075v1|
|338|When Ethics and Payoffs Diverge: LLM Agents in Morally Charged Social Dilemmas|Steffen Backmann, David Guzman Piedrahita, Emanuel Tewolde, Rada Mihalcea, Bernhard Schölkopf, Zhijing Jin|2025-05-25|arXiv|https://github.com/sbackmann/moralsim|http://arxiv.org/abs/2505.19212v1|
|339|VerIPO: Cultivating Long Reasoning in Video-LLMs via Verifier-Gudied Iterative Policy Optimization|Yunxin Li, Xinyu Chen, Zitao Li, Zhenyu Liu, Longyue Wang, Wenhan Luo, Baotian Hu, Min Zhang|2025-05-25|arXiv|https://github.com/HITsz-TMG/VerIPO|http://arxiv.org/abs/2505.19000v1|
|340|SituatedThinker: Grounding LLM Reasoning with Real-World through Situated Thinking|Junnan Liu, Linhao Luo, Thuy-Trang Vu, Gholamreza Haffari|2025-05-25|arXiv|https://github.com/jnanliu/SituatedThinker|http://arxiv.org/abs/2505.19300v1|
|341|FP4 All the Way: Fully Quantized Training of LLMs|Brian Chmiel, Maxim Fishman, Ron Banner, Daniel Soudry|2025-05-25|arXiv|https://github.com/Anonymous1252022/fp4-all-the-way|http://arxiv.org/abs/2505.19115v1|
|342|OpenHOI: Open-World Hand-Object Interaction Synthesis with Multimodal Large Language Model|Zhenhao Zhang, Ye Shi, Lingxiao Yang, Suting Ni, Qi Ye, Jingya Wang|2025-05-25|arXiv|https://openhoi.github.io|https://doi.org/10.48550/arXiv.2505.18947|
|343|When Two LLMs Debate, Both Think They'll Win|Pradyumna Shyama Prasad, Minh Nhat Nguyen|2025-05-25|arXiv|https://github.com/pradyuprasad/llms_overconfidence|http://arxiv.org/abs/2505.19184v2|
|344|Assistant-Guided Mitigation of Teacher Preference Bias in LLM-as-a-Judge|Zhuo Liu, Moxin Li, Xun Deng, Qifan Wang, Fuli Feng|2025-05-25|arXiv|https://github.com/Liuz233/AGDe-Judge|http://arxiv.org/abs/2505.19176v1|
|345|SeRL: Self-Play Reinforcement Learning for Large Language Models with Limited Data|Wenkai Fang, Shunyu Liu, Yang Zhou, Kongcheng Zhang, Tongya Zheng, Kai-Xuan Chen, Mingli Song, Dacheng Tao|2025-05-25|arXiv|https://github.com/wantbook-book/SeRL|https://doi.org/10.48550/arXiv.2505.20347|
|346|LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models|Aida Kostikova, Zhipin Wang, Deidamea Bajri, Ole Pütz, Benjamin Paaßen, Steffen Eger|2025-05-25|arXiv|https://github.com/a-kostikova/LLLMs-Survey|https://doi.org/10.48550/arXiv.2505.19240|
|347|Can Multimodal Large Language Models Understand Spatial Relations?|Jingping Liu, Ziyan Liu, Zhedong Cen, Yan Zhou, Yinan Zou, Weiyan Zhang, Haiyun Jiang, Tong Ruan|2025-05-25|arXiv|https://github.com/ziyan-xiaoyu/SpatialMQA|https://doi.org/10.48550/arXiv.2505.19015|
|348|PM-KVQ: Progressive Mixed-precision KV Cache Quantization for Long-CoT LLMs|Tengxuan Liu, Shiyao Li, Jiayi Yang, Tianchen Zhao, Feng Zhou, Xiaohui Song, Guohao Dai, Shengen Yan, Huazhong Yang, Yu Wang|2025-05-24|arXiv|https://github.com/thu-nics/PM-KVQ|http://arxiv.org/abs/2505.18610v1|
|349|Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs|Mengqi Liao, Xiangyu Xi, Ruinian Chen, Jia Leng, Yangen Hu, Ke Zeng, Shuai Liu, Huaiyu Wan|2025-05-24|arXiv|https://github.com/LiaoMengqi/E3-RL4LLMs|http://arxiv.org/abs/2505.18573v1|
|350|Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning? A Systematic Study|Ziyang Cheng, Zhixun Li, Yuhan Li, Yixin Song, Kangyi Zhao, Dawei Cheng, Jia Li, Jeffrey Xu Yu|2025-05-24|arXiv|https://github.com/ZhixunLEE/LLM4GCL|http://arxiv.org/abs/2505.18697v1|
|351|CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs|Yiqing Zhang, Xiaozhong Liu, Fabricio Murai|2025-05-24|arXiv|https://github.com/murai-lab/CLaDMoP|http://arxiv.org/abs/2505.18527v1|
|352|$C^3$-Bench: The Things Real Disturbing LLM based Agent in Multi-Tasking|Peijie Yu, Yifan Yang, Jinjian Li, Zelong Zhang, Haorui Wang, Xiao Feng, Feng Zhang|2025-05-24|arXiv|https://github.com/yupeijei1997/C3-Bench|http://arxiv.org/abs/2505.18746v1|
|353|BTC-LLM: Efficient Sub-1-Bit LLM Quantization via Learnable Transformation and Binary Codebook|Hao Gu, Lujun Li, Zheyu Wang, Bei Liu, Qiyuan Zhu, Sirui Han, Yike Guo|2025-05-24|arXiv|https://github.com/Chooovy/BTC-LLM|http://arxiv.org/abs/2506.12040v1|
|354|G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning|Xiaojun Guo, Ang Li, Yifei Wang, Stefanie Jegelka, Yisen Wang|2025-05-24|arXiv|https://github.com/PKU-ML/G1|http://arxiv.org/abs/2505.18499v1|
|355|Using Large Language Models to Tackle Fundamental Challenges in Graph Learning: A Comprehensive Survey|Mengran Li, Pengyu Zhang, Wenbin Xing, Yijia Zheng, Klim Zaporojets, Junzhou Chen, Ronghui Zhang, Yong Zhang, Siyuan Gong, Jia Hu, Xiaolei Ma, Zhiyuan Liu, Paul Groth, Marcel Worring|2025-05-24|arXiv|https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges|https://doi.org/10.48550/arXiv.2505.18475|
|356|A Survey of LLM $\times$ DATA|Xuanhe Zhou, Junxuan He, Wei Zhou, Haodong Chen, Zirui Tang, Haoyu Zhao, Xin Tong, Guoliang Li, Youmin Chen, Jun Zhou, Zhaojun Sun, Binyuan Hui, Shuo Wang, Conghui He, Zhiyuan Liu, Jingren Zhou, Fan Wu|2025-05-24|arXiv|https://github.com/weAIDB/awsome-data-llm|http://arxiv.org/abs/2505.18458v1|
|357|Knowledge Grafting of Large Language Models|Guodong Du, Xuanning Zhou, Junlin Li, Zhuo Li, Zesheng Shi, Wanyu Lin, Ho-Kin Tang, Xiucheng Li, Fangming Liu, Wenya Wang, Min Zhang, Jing Li|2025-05-24|arXiv|https://github.com/duguodong7/GraftLLM|https://doi.org/10.48550/arXiv.2505.18502|
|358|Cross-Lingual Pitfalls: Automatic Probing Cross-Lingual Weakness of Multilingual Large Language Models|Zixiang Xu, Yanbo Wang, Yue Huang, Xiuying Chen, Jieyu Zhao, Meng Jiang, Xiangliang Zhang|2025-05-24|arXiv|https://github.com/xzx34/Cross-Lingual-Pitfalls|https://doi.org/10.48550/arXiv.2505.18673|
|359|B-score: Detecting biases in large language models using response history|An Vo, Mohammad Reza Taesiri, Daeyoung Kim, Anh Totti Nguyen|2025-05-24|arXiv|https://b-score.github.io|https://doi.org/10.48550/arXiv.2505.18545|
|360|ALPS: Attention Localization and Pruning Strategy for Efficient Alignment of Large Language Models|Hao Chen, Haoze Li, Zhiqing Xiao, Lirong Gao, Qi Zhang, Xiaomeng Hu, Ningtao Wang, Xing Fu, Junbo Zhao|2025-05-24|arXiv|https://github.com/VoiceBeer/ALPS|https://doi.org/10.48550/arXiv.2505.18799|
|361|Reinforcement Fine-Tuning Powers Reasoning Capability of Multimodal Large Language Models|Haoyuan Sun, Jiaqi Wu, Bo Xia, Yifu Luo, Yifei Zhao, Kai Qin, Xufei Lv, Tiantian Zhang, Yongzhe Chang, Xueqian Wang|2025-05-24|arXiv|https://github.com/Sun-Haoyuan23/Awesome-RL-based-Reasoning-MLLMs|https://doi.org/10.48550/arXiv.2505.18536|
|362|Fann or Flop: A Multigenre, Multiera Benchmark for Arabic Poetry Understanding in LLMs|Wafa Alghallabi, Ritesh Thawkar, Sara Ghaboura, Ketan More, Omkar Thawakar, Hisham Cholakkal, Salman Khan, Rao Muhammad Anwer|2025-05-23|arXiv|https://github.com/mbzuai-oryx/FannOrFlop|http://arxiv.org/abs/2505.18152v2|
|363|Titanus: Enabling KV Cache Pruning and Quantization On-the-Fly for LLM Acceleration|Peilin Chen, Xiaoxuan Yang|2025-05-23|arXiv|https://github.com/peilin-chen/Titanus-for-LLM-acceleration|http://arxiv.org/abs/2505.17787v1|
|364|One Model Transfer to All: On Robust Jailbreak Prompts Generation against LLMs|Linbao Li, Yannan Liu, Daojing He, Yu Li|2025-05-23|arXiv|https://github.com/LLBao/ArrAttack|http://arxiv.org/abs/2505.17598v1|
|365|On the Design of KL-Regularized Policy Gradient Algorithms for LLM Reasoning|Yifan Zhang, Yifeng Liu, Huizhuo Yuan, Yang Yuan, Quanquan Gu, Andrew C Yao|2025-05-23|arXiv|https://github.com/complex-reasoning/RPG|http://arxiv.org/abs/2505.17508v1|
|366|GeoGramBench: Benchmarking the Geometric Program Reasoning in Modern LLMs|Shixian Luo, Zezhou Zhu, Yu Yuan, Yuncheng Yang, Lianlei Shan, Yong Wu|2025-05-23|arXiv|https://github.com/LiAuto-DSR/GeoGramBench|http://arxiv.org/abs/2505.17653v1|
|367|Resolving Conflicting Evidence in Automated Fact-Checking: A Study on Retrieval-Augmented LLMs|Ziyu Ge, Yuhao Wu, Daniel Wai Kit Chin, Roy Ka-Wei Lee, Rui Cao|2025-05-23|arXiv|https://github.com/zoeyyes/CONFACT|http://arxiv.org/abs/2505.17762v1|
|368|Distilling LLM Agent into Small Models with Retrieval and Code Tools|Minki Kang, Jongwon Jeong, Seanie Lee, Jaewoong Cho, Sung Ju Hwang|2025-05-23|arXiv|https://github.com/Nardien/agent-distillation|http://arxiv.org/abs/2505.17612v1|
|369|Automating Safety Enhancement for LLM-based Agents with Synthetic Risk Scenarios|Xueyang Zhou, Weidong Wang, Lin Lu, Jiawen Shi, Guiyao Tie, Yongtian Xu, Lixing Chen, Pan Zhou, Neil Zhenqiang Gong, Lichao Sun|2025-05-23|arXiv|https://auto-safe.github.io/|http://arxiv.org/abs/2505.17735v1|
|370|ManuSearch: Democratizing Deep Search in Large Language Models with a Transparent and Open Multi-Agent Framework|Lisheng Huang, Yichen Liu, Jinhao Jiang, Rongxiang Zhang, Jiahao Yan, Junyi Li, Wayne Xin Zhao|2025-05-23|arXiv|https://github.com/RUCAIBox/ManuSearch|https://doi.org/10.48550/arXiv.2505.18105|
|371|DialogXpert: Driving Intelligent and Emotion-Aware Conversations through Online Value-Based Reinforcement Learning with LLM Priors|Tazeek Bin Abdur Rakib, Ambuj Mehrish, Lay-Ki Soon, Wern Han Lim, Soujanya Poria|2025-05-23|arXiv|https://github.com/declare-lab/dialogxpert/|http://arxiv.org/abs/2505.17795v1|
|372|GIM: Improved Interpretability for Large Language Models|Joakim Edin, Róbert Csordás, Tuukka Ruotsalo, Zhengxuan Wu, Maria Maistro, Jing Huang, Lars Maaløe|2025-05-23|arXiv|https://github.com/JoakimEdin/gim|https://doi.org/10.48550/arXiv.2505.17630|
|373|Mitigating Fine-tuning Risks in LLMs via Safety-Aware Probing Optimization|Chengcan Wu, Zhixin Zhang, Zeming Wei, Yihao Zhang, Meng Sun|2025-05-22|arXiv|https://github.com/ChengcanWu/SAP|http://arxiv.org/abs/2505.16737v1|
|374|LLM as Effective Streaming Processor: Bridging Streaming-Batch Mismatches with Group Position Encoding|Junlong Tong, Jinlan Fu, Zixuan Lin, Yingqi Fan, Anhao Zhao, Hui Su, Xiaoyu Shen|2025-05-22|arXiv|https://github.com/EIT-NLP/StreamingLLM|http://arxiv.org/abs/2505.16983v1|
|375|When Do LLMs Admit Their Mistakes? Understanding the Role of Model Belief in Retraction|Yuqing Yang, Robin Jia|2025-05-22|arXiv|https://github.com/ayyyq/llm-retraction|http://arxiv.org/abs/2505.16170v1|
|376|DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection|Yuliang Yan, Haochun Tang, Shuo Yan, Enyan Dai|2025-05-22|arXiv|https://github.com/yuliangyan0807/llm-fingerprint|http://arxiv.org/abs/2505.16530v1|
|377|EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via Action Pruning|Jiawei Liu, Qisi Chen, Jianshu Zhang, Quan Liu, Defu Lian|2025-05-22|arXiv|https://github.com/Lolo1222/EquivPruner|http://arxiv.org/abs/2505.16312v1|
|378|LLMs Are Not Scorers: Rethinking MT Evaluation with Generation-Based Methods|Hyang Cui|2025-05-22|arXiv|https://github.com/CuiNiki/LLMs-Are-Not-Scorers|http://arxiv.org/abs/2505.16129v1|
|379|R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs via Reinforcement Learning|Huatong Song, Jinhao Jiang, Wenqing Tian, Zhipeng Chen, Yuhuan Wu, Jiahao Zhao, Yingqian Min, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen|2025-05-22|arXiv|https://github.com/RUCAIBox/R1-Searcher-plus|http://arxiv.org/abs/2505.17005v1|
|380|MixAT: Combining Continuous and Discrete Adversarial Training for LLMs|Csaba Dékány, Stefan Balauca, Robin Staab, Dimitar I. Dimitrov, Martin Vechev|2025-05-22|arXiv|https://github.com/insait-institute/MixAT|http://arxiv.org/abs/2505.16947v1|
|381|SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for Enterprise Use|Hitesh Laxmichand Patel, Amit Agarwal, Arion Das, Bhargava Kumar, Srikant Panda, Priyaranjan Pattnayak, Taki Hasan Rafi, Tejaswini Kumar, Dong-Kyu Chae|2025-05-22|arXiv|https://github.com/amitbcp/multilingual_profanity|http://arxiv.org/abs/2505.17332v1|
|382|Tool-Star: Empowering LLM-Brained Multi-Tool Reasoner via Reinforcement Learning|Guanting Dong, Yifei Chen, Xiaoxi Li, Jiajie Jin, Hongjin Qian, Yutao Zhu, Hangyu Mao, Guorui Zhou, Zhicheng Dou, Ji-Rong Wen|2025-05-22|arXiv|https://github.com/dongguanting/Tool-Star|http://arxiv.org/abs/2505.16410v1|
|383|Training Long-Context LLMs Efficiently via Chunk-wise Optimization|Wenhao Li, Yuxin Zhang, Gen Luo, Daohai Yu, Rongrong Ji|2025-05-22|arXiv|https://github.com/wenhaoli-xmu/seco|http://arxiv.org/abs/2505.16710v1|
|384|Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs|Xiaoyu Xu, Xiang Yue, Yang Liu, Qingqing Ye, Haibo Hu, Minxin Du|2025-05-22|arXiv|https://github.com/XiaoyuXU1/Representational_Analysis_Tools|http://arxiv.org/abs/2505.16831v1|
|385|R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large Language Models via Share-GRPO|Huanjin Yao, Qixiang Yin, Jingyi Zhang, Min Yang, Yibo Wang, Wenhao Wu, Fei Su, Li Shen, Minghui Qiu, Dacheng Tao, Jiaxing Huang|2025-05-22|arXiv|https://github.com/HJYao00/R1-ShareVL|https://doi.org/10.48550/arXiv.2505.16673|
|386|HyGenar: An LLM-Driven Hybrid Genetic Algorithm for Few-Shot Grammar Generation|Weizhi Tang, Yixuan Li, Chris Sypherd, Elizabeth Polgreen, Vaishak Belle|2025-05-22|arXiv|https://github.com/RutaTang/HyGenar|http://arxiv.org/abs/2505.16978v1|
|387|EduBench: A Comprehensive Benchmarking Dataset for Evaluating Large Language Models in Diverse Educational Scenarios|Bin Xu, Yu Bai, Huashan Sun, Yiguan Lin, Siming Liu, Xinyue Liang, Yaolin Li, Yang Gao, Heyan Huang|2025-05-22|arXiv|https://github.com/ybai-nlp/EduBench|https://doi.org/10.48550/arXiv.2505.16160|
|388|Logic-of-Thought: Empowering Large Language Models with Logic Programs for Solving Puzzles in Natural Language|Naiqi Li, Peiyuan Liu, Zheng Liu, Tao Dai, Yong Jiang, Shu-Tao Xia|2025-05-22|arXiv|https://github.com/naiqili/Logic-of-Thought|https://doi.org/10.48550/arXiv.2505.16114|
|389|Large Language Models for Predictive Analysis: How Far Are They?|Qin Chen, Yuanyi Ren, Xiaojun Ma, Yuyang Shi|2025-05-22|arXiv|https://github.com/Cqkkkkkk/PredictiQ|https://doi.org/10.48550/arXiv.2505.17149|
|390|LLaDA-V: Large Language Diffusion Models with Visual Instruction Tuning|Zebin You, Shen Nie, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, Chongxuan Li|2025-05-22|arXiv|https://ml-gsai.github.io/LLaDA-V-demo/|https://doi.org/10.48550/arXiv.2505.16933|
|391|LIFEBench: Evaluating Length Instruction Following in Large Language Models|Wei Zhang, Zhenhong Zhou, Junfeng Fang, Rongwu Xu, Kun Wang, Yuanhe Zhang, Rui Wang, Ge Zhang, Xinfeng Li, Li Sun, Lingjuan Lyu, Yang Liu, Sen Su|2025-05-22|arXiv|https://ydyjya.github.io/LIFEBench/|https://doi.org/10.48550/arXiv.2505.16234|
|392|IFEval-Audio: Benchmarking Instruction-Following Capability in Audio-based Large Language Models|Yiming Gao, Bin Wang, Chengwei Wei, Shuo Sun, AiTi Aw|2025-05-22|arXiv|https://github.com/AudioLLMs/AudioBench/tree/main/IFEval-Audio|https://doi.org/10.48550/arXiv.2505.16774|
|393|Evaluating Large Language Model with Knowledge Oriented Language Specific Simple Question Answering|Bowen Jiang, Runchuan Zhu, Jiang Wu, Zinco Jiang, Yifan He, Junyuan Gao, Jia Yu, Rui Min, Yinfan Wang, Haote Yang, Songyang Zhang, Dahua Lin, Lijun Wu, Conghui He|2025-05-22|arXiv|https://github.com/opendatalab/KoLasSimpleQA|https://doi.org/10.48550/arXiv.2505.16591|
|394|Do Large Language Models Excel in Complex Logical Reasoning with Formal Language?|Jin Jiang, Jianing Wang, Yuchen Yan, Yang Liu, Jianhua Zhu, Mengdi Zhang, Xunliang Cai, Liangcai Gao|2025-05-22|arXiv|https://github.com/jiangjin1999/FormalEval|https://doi.org/10.48550/arXiv.2505.16998|
|395|Dimple: Discrete Diffusion Multimodal Large Language Model with Parallel Decoding|Runpeng Yu, Xinyin Ma, Xinchao Wang|2025-05-22|arXiv|https://github.com/yu-rp/Dimple|https://doi.org/10.48550/arXiv.2505.16990|
|396|ChemMLLM: Chemical Multimodal Large Language Model|Qian Tan, Dongzhan Zhou, Peng Xia, Wanhao Liu, Wanli Ouyang, Lei Bai, Yuqiang Li, Tianfan Fu|2025-05-22|arXiv|https://github.com/bbsbz/ChemMLLM|https://doi.org/10.48550/arXiv.2505.16326|
|397|CASTILLO: Characterizing Response Length Distributions of Large Language Models|Daniel F. Perez-Ramirez, Dejan Kostic, Magnus Boman|2025-05-22|arXiv|https://github.com/DanielFPerez/castillo|https://doi.org/10.48550/arXiv.2505.16881|
|398|AudioTrust: Benchmarking the Multifaceted Trustworthiness of Audio Large Language Models|Kai Li, Can Shen, Yile Liu, Jirui Han, Kelong Zheng, Xuechao Zou, Zhe Wang, Xingjian Du, Shun Zhang, Hanjun Luo, Yingbin Jin, Xinxin Xing, Ziyang Ma, Yue Liu, Xiaojun Jia, Yifan Zhang, Junfeng Fang, Kun Wang, Yibo Yan, Haoyang Li, Yiming Li, Xiaobin Zhuang, Yang Liu, Haibo Hu, Zhuo Chen, Zhizheng Wu, Xiaolin Hu, Eng-Siong Chng, XiaoFeng Wang, Wenyuan Xu, Wei Dong, Xinfeng Li|2025-05-22|arXiv|https://github.com/JusperLee/AudioTrust|https://doi.org/10.48550/arXiv.2505.16211|
|399|A Survey on the Application of Large Language Models in Scenario-Based Testing of Automated Driving Systems|Yongqi Zhao, Ji Zhou, Dong Bi, Tomislav Mihalj, Jia Hu, Arno Eichberger|2025-05-22|arXiv|https://github.com/ftgTUGraz/LLM4ADSTest|https://doi.org/10.48550/arXiv.2505.16587|
|400|A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization|Ziqing Wang, Kexin Zhang, Zihan Zhao, Yibo Wen, Abhishek Pandey, Han Liu, Kaize Ding|2025-05-22|arXiv|https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery|https://doi.org/10.48550/arXiv.2505.16094|
|401|From Problem-Solving to Teaching Problem-Solving: Aligning LLMs with Pedagogy using Reinforcement Learning|David Dinucu-Jianu, Jakub Macina, Nico Daheim, Ido Hakimi, Iryna Gurevych, Mrinmaya Sachan|2025-05-21|arXiv|https://github.com/eth-lre/PedagogicalRL|http://arxiv.org/abs/2505.15607v1|
|402|Be Careful When Fine-tuning On Open-Source LLMs: Your Fine-tuning Data Could Be Secretly Stolen!|Zhexin Zhang, Yuhao Sun, Junxiao Yang, Shiyao Cui, Hongning Wang, Minlie Huang|2025-05-21|arXiv|https://github.com/thu-coai/Backdoor-Data-Extraction|http://arxiv.org/abs/2505.15656v1|
|403|CRAKEN: Cybersecurity LLM Agent with Knowledge-Based Execution|Minghao Shao, Haoran Xi, Nanda Rani, Meet Udeshi, Venkata Sai Charan Putrevu, Kimberly Milner, Brendan Dolan-Gavitt, Sandeep Kumar Shukla, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Muhammad Shafique|2025-05-21|arXiv|https://github.com/NYU-LLM-CTF/nyuctf_agents_craken|http://arxiv.org/abs/2505.17107v1|
|404|lmgame-Bench: How Good are LLMs at Playing Games?|Lanxiang Hu, Mingjia Huo, Yuxuan Zhang, Haoyang Yu, Eric P. Xing, Ion Stoica, Tajana Rosing, Haojian Jin, Hao Zhang|2025-05-21|arXiv|https://github.com/lmgame-org/GamingAgent/lmgame-bench|http://arxiv.org/abs/2505.15146v1|
|405|STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs|Zongzhao Li, Zongyang Ma, Mingze Li, Songyou Li, Yu Rong, Tingyang Xu, Ziqi Zhang, Deli Zhao, Wenbing Huang|2025-05-21|arXiv|https://github.com/zongzhao23/STAR-R1|http://arxiv.org/abs/2505.15804v2|
|406|StepSearch: Igniting LLMs Search Ability via Step-Wise Proximal Policy Optimization|Ziliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang, Jialu Cai, Yuhang Wang, Yichao Wu|2025-05-21|arXiv|https://github.com/Zillwang/StepSearch|http://arxiv.org/abs/2505.15107v2|
|407|Traveling Across Languages: Benchmarking Cross-Lingual Consistency in Multimodal LLMs|Hao Wang, Pinzhi Huang, Jihan Yang, Saining Xie, Daisuke Kawahara|2025-05-21|arXiv|https://github.com/nlp-waseda/traveling-across-languages|http://arxiv.org/abs/2505.15075v1|
|408|An Empirical Study on Reinforcement Learning for Reasoning-Search Interleaved LLM Agents|Bowen Jin, Jinsung Yoon, Priyanka Kargupta, Sercan O. Arik, Jiawei Han|2025-05-21|arXiv|https://github.com/PeterGriffinJin/Search-R1|http://arxiv.org/abs/2505.15117v1|
|409|Towards Explainable Temporal Reasoning in Large Language Models: A Structure-Aware Generative Framework|Zihao Jiang, Ben Liu, Miao Peng, Wenjie Xu, Yao Xiao, Zhenyan Shan, Min Peng|2025-05-21|arXiv|https://github.com/carryTatum/GETER|https://doi.org/10.48550/arXiv.2505.15245|
|410|LyapLock: Bounded Knowledge Preservation in Sequential Large Language Model Editing|Peng Wang, Biyu Zhou, Xuehai Tang, Jizhong Han, Songlin Hu|2025-05-21|arXiv|https://github.com/caskcsg/LyapLock|https://doi.org/10.48550/arXiv.2505.15702|
|411|LENS: Multi-level Evaluation of Multimodal Reasoning with Large Language Models|Ruilin Yao, Bo Zhang, Jirui Huang, Xinwei Long, Yifang Zhang, Tianyu Zou, Yufei Wu, Shichao Su, Yifan Xu, Wenxi Zeng, Zhaoyu Yang, Guoyou Li, Shilan Zhang, Zichan Li, Yaxiong Chen, Shengwu Xiong, Peng Xu, Jiajun Zhang, Bowen Zhou, David A. Clifton, Luc Van Gool|2025-05-21|arXiv|https://github.com/Lens4MLLMs/lens|https://doi.org/10.48550/arXiv.2505.15616|
|412|Harnessing On-Device Large Language Model: Empirical Results and Implications for AI PC|Qingyu Song, Peiyu Liao, Wenqian Zhao, Yiwen Wang, Shoubo Hu, Hui-Ling Zhen, Ning Jiang, Mingxuan Yuan|2025-05-21|arXiv|https://github.com/simmonssong/LLMOnDevice|https://doi.org/10.48550/arXiv.2505.15030|
|413|HDLxGraph: Bridging Large Language Models and HDL Repositories via HDL Graph Databases|Pingqing Zheng, Jiayin Qin, Fuqi Zhang, Shang Wu, Yu Cao, Caiwen Ding, Yang, Zhao|2025-05-21|arXiv|https://github.com/Nick-Zheng-Q/HDLxGraph|https://doi.org/10.48550/arXiv.2505.15701|
|414|Deliberation on Priors: Trustworthy Reasoning of Large Language Models on Knowledge Graphs|Jie Ma, Ning Qu, Zhitao Gao, Rui Xing, Jun Liu, Hongbin Pei, Jiang Xie, Linyun Song, Pinghui Wang, Jing Tao, Zhou Su|2025-05-21|arXiv|https://github.com/reml-group/Deliberation-on-Priors|https://doi.org/10.48550/arXiv.2505.15210|
|415|Can Large Language Models Understand Internet Buzzwords Through User-Generated Content|Chen Huang, Junkai Luo, Xinzuo Wang, Wenqiang Lei, Jiancheng Lv|2025-05-21|arXiv|https://github.com/SCUNLP/Buzzword|https://doi.org/10.48550/arXiv.2505.15071|
|416|Boost Post-Training Quantization via Null Space Optimization for Large Language Models|Jiaqi Zhao, Miao Zhang, Weili Guan, Liqiang Nie|2025-05-21|arXiv|https://github.com/zjq0455/q2n|https://doi.org/10.48550/arXiv.2506.11044|
|417|UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models|Xiaojie Gu, Guangxu Chen, Jungang Li, Jia-Chen Gu, Xuming Hu, Kai Zhang|2025-05-20|arXiv|https://github.com/XiaojieGu/UltraEdit|https://doi.org/10.48550/arXiv.2505.14679|
|418|TinyV: Reducing False Negatives in Verification Improves RL for LLM Reasoning|Zhangchen Xu, Yuetai Li, Fengqing Jiang, Bhaskar Ramasubramanian, Luyao Niu, Bill Yuchen Lin, Radha Poovendran|2025-05-20|arXiv|https://github.com/uw-nsl/TinyV|http://arxiv.org/abs/2505.14625v2|
|419|MM-Agent: LLM as Agents for Real-world Mathematical Modeling Problem|Fan Liu, Zherui Yang, Cancheng Liu, Tianrui Song, Xiaofeng Gao, Hao Liu|2025-05-20|arXiv|https://github.com/usail-hkust/LLM-MM-Agent|http://arxiv.org/abs/2505.14148v1|
|420|Beyond Words: Multimodal LLM Knows When to Speak|Zikai Liao, Yi Ouyang, Yi-Lun Lee, Chen-Ping Yu, Yi-Hsuan Tsai, Zhaozheng Yin|2025-05-20|arXiv|https://github.com/lzk901372/MM-When2Speak|http://arxiv.org/abs/2505.14654v1|
|421|APEX: Empowering LLMs with Physics-Based Task Planning for Real-time Insight|Wanjing Huang, Weixiang Yan, Zhen Zhang, Ambuj Singh|2025-05-20|arXiv|https://github.com/hwj20/APEX_EXP|http://arxiv.org/abs/2505.13921v1|
|422|$\textttLLINBO$: Trustworthy LLM-in-the-Loop Bayesian Optimization|Chih-Yu Chang, Milad Azvar, Chinedum Okwudire, Raed Al Kontar|2025-05-20|arXiv|https://github.com/UMDataScienceLab/LLM-in-the-Loop-BO|http://arxiv.org/abs/2505.14756v1|
|423|Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity|Susav Shrestha, Brad Settlemyer, Nikoli Dryden, Narasimha Reddy|2025-05-20|arXiv|https://github.com/susavlsh10/Polar-Sparsity|http://arxiv.org/abs/2505.14884v1|
|424|Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models|Xuyang Liu, Yiyu Wang, Junpeng Ma, Linfeng Zhang|2025-05-20|arXiv|https://github.com/xuyang-liu16/VidCom2|https://doi.org/10.48550/arXiv.2505.14454|
|425|Speculative Decoding Reimagined for Multimodal Large Language Models|Luxi Lin, Zhihang Lin, Zhanpeng Zeng, Rongrong Ji|2025-05-20|arXiv|https://github.com/Lyn-Lucy/MSD|https://doi.org/10.48550/arXiv.2505.14260|
|426|Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models|Guangzhi Xiong, Eric Xie, Corey Williams, Myles Kim, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang|2025-05-20|arXiv|https://github.com/Teddy-XiongGZ/TruthHypo|https://doi.org/10.48550/arXiv.2505.14599|
|427|S2SBench: A Benchmark for Quantifying Intelligence Degradation in Speech-to-Speech Large Language Models|Yuanbo Fang, Haoze Sun, Jun Liu, Tao Zhang, Zenan Zhou, Weipeng Chen, Xiaofen Xing, Xiangmin Xu|2025-05-20|arXiv|https://github.com/undobug/S2SBench|https://doi.org/10.48550/arXiv.2505.14438|
|428|Quartet: Native FP4 Training Can Be Optimal for Large Language Models|Roberto L. Castro, Andrei Panferov, Soroush Tabesh, Oliver Sieberling, Jiale Chen, Mahdi Nikdan, Saleh Ashkboos, Dan Alistarh|2025-05-20|arXiv|https://github.com/IST-DASLab/Quartet|https://doi.org/10.48550/arXiv.2505.14669|
|429|Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models|Yuqiao Tan, Shizhu He, Kang Liu, Jun Zhao|2025-05-20|arXiv|https://github.com/Trae1ounG/Neural_Incompatibility|https://doi.org/10.48550/arXiv.2505.14436|
|430|KERL: Knowledge-Enhanced Personalized Recipe Recommendation using Large Language Models|Fnu Mohbat, Mohammed J. Zaki|2025-05-20|arXiv|https://github.com/mohbattharani/KERL|https://doi.org/10.48550/arXiv.2505.14629|
|431|DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models|Yakun Zhu, Zhongzhen Huang, Linjie Mu, Yutong Huang, Wei Nie, Jiaji Liu, Shaoting Zhang, Pengfei Liu, Xiaofan Zhang|2025-05-20|arXiv|https://github.com/SPIRAL-MED/DiagnosisArena|https://doi.org/10.48550/arXiv.2505.14107|
|432|ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models|Raghav Singhal, Kaustubh Ponkshe, Rohit Vartak, Praneeth Vepakomma|2025-05-20|arXiv|https://github.com/CERT-Lab/abba|https://doi.org/10.48550/arXiv.2505.14238|
|433|CoIn: Counting the Invisible Reasoning Tokens in Commercial Opaque LLM APIs|Guoheng Sun, Ziyao Wang, Bowei Tian, Meng Liu, Zheyu Shen, Shwai He, Yexiao He, Wanghao Ye, Yiting Wang, Ang Li|2025-05-19|arXiv|https://github.com/CASE-Lab-UMD/LLM-Auditing-CoIn|http://arxiv.org/abs/2505.13778v1|
|434|TIME: A Multi-level Benchmark for Temporal Reasoning of LLMs in Real-World Scenarios|Shaohang Wei, Wei Li, Feifan Song, Wen Luo, Tianyi Zhuang, Haochen Tan, Zhijiang Guo, Houfeng Wang|2025-05-19|arXiv|https://github.com/sylvain-wei/TIME|http://arxiv.org/abs/2505.12891v1|
|435|Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs|Zhihe Yang, Xufang Luo, Zilong Wang, Dongqi Han, Zhiyuan He, Dongsheng Li, Yunjian Xu|2025-05-19|arXiv|https://github.com/zhyang2226/AR-Lopti|http://arxiv.org/abs/2505.12929v1|
|436|Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering|Zifeng Cheng, Zhonghui Wang, Yuchen Fu, Zhiwei Jiang, Yafeng Yin, Cong Wang, Qing Gu|2025-05-19|arXiv|https://github.com/zifengcheng/CP|http://arxiv.org/abs/2505.12831v1|
|437|FEALLM: Advancing Facial Emotion Analysis in Multimodal Large Language Models with Emotional Synergy and Reasoning|Zhuozhao Hu, Kaishen Yuan, Xin Liu, Zitong Yu, Yuan Zong, Jingang Shi, Huanjing Yue, Jingyu Yang|2025-05-19|arXiv|https://github.com/953206211/FEALLM|https://doi.org/10.48550/arXiv.2505.13419|
|438|ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models|Zihao Cheng, Hongru Wang, Zeming Liu, Yuhang Guo, Yuanfang Guo, Yunhong Wang, Haifeng Wang|2025-05-19|arXiv|https://github.com/Chengziha0/ToolSpectrum|https://doi.org/10.48550/arXiv.2505.13176|
|439|On the Thinking-Language Modeling Gap in Large Language Models|Chenxi Liu, Yongqiang Chen, Tongliang Liu, James Cheng, Bo Han, Kun Zhang|2025-05-19|arXiv|https://causalcoat.github.io/lot.html|https://doi.org/10.48550/arXiv.2505.12896|
|440|Role-Playing Evaluation for Large Language Models|Yassine El Boudouri, Walter Nuninger, Julian Alvarez, Yvan Peter|2025-05-19|arXiv|https://github.com/yelboudouri/RPEval|https://doi.org/10.48550/arXiv.2505.13157|
|441|PSC: Extending Context Window of Large Language Models via Phase Shift Calibration|Wenqiao Zhu, Chao Xu, Lulu Wang, Jun Wu|2025-05-18|EMNLP|https://github.com/WNQzhu/PSC|https://aclanthology.org/2024.emnlp-main.341|
|442|UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models|Qizhou Chen, Dakan Wang, Taolin Zhang, Zaoming Yan, Chengsong You, Chengyu Wang, Xiaofeng He|2025-05-18|arXiv|https://github.com/qizhou000/UniEdit|https://doi.org/10.48550/arXiv.2505.12345|
|443|LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark|Md. Atiqur Rahman, Sabrina Islam, Mushfiqul Haque Omi|2025-05-18|arXiv|https://github.com/180041123-Atiq/MTEonLowResourceLanguage|http://arxiv.org/abs/2505.12273v1|
|444|LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning|Xinye Li, Mingqi Wan, Dianbo Sui|2025-05-18|arXiv|https://github.com/asdfo123/LLMSR-asdfo123|http://arxiv.org/abs/2505.12328v1|
|445|MARGE: Improving Math Reasoning for LLMs with Guided Exploration|Jingyue Gao, Runji Lin, Keming Lu, Bowen Yu, Junyang Lin, Jianyu Chen|2025-05-18|arXiv|https://github.com/georgao35/MARGE|http://arxiv.org/abs/2505.12500v1|
|446|HALO: Hierarchical Autonomous Logic-Oriented Orchestration for Multi-Agent LLM Systems|Zhipeng Hou, Junyi Tang, Yipeng Wang|2025-05-17|arXiv|https://github.com/23japhone/HALO|http://arxiv.org/abs/2505.13516v1|
|447|Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement|Peng Ding, Jun Kuang, Zongyu Wang, Xuezhi Cao, Xunliang Cai, Jiajun Chen, Shujian Huang|2025-05-17|arXiv|https://github.com/NJUNLP/SAGE|http://arxiv.org/abs/2505.12060v1|
|448|Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning|Yansong Ning, Wei Li, Jun Fang, Naiqiang Tan, Hao Liu|2025-05-17|arXiv|https://github.com/usail-hkust/LongShort|http://arxiv.org/abs/2505.11827v2|
|449|LLM-BABYBENCH: Understanding and Evaluating Grounded Planning and Reasoning in LLMs|Omar Choukrani, Idriss Malek, Daniil Orel, Zhuohan Xie, Zangir Iklassov, Martin Takáč, Salem Lahlou|2025-05-17|arXiv|https://github.com/choukrani/llm-babybench|http://arxiv.org/abs/2505.12135v1|
|450|Multilingual Collaborative Defense for Large Language Models|Hongliang Li, Jinan Xu, Gengping Cui, Changhao Guan, Fengran Mo, Kaiyu Huang|2025-05-17|arXiv|https://github.com/HLiang-Lee/MCD|https://doi.org/10.48550/arXiv.2505.11835|
|451|LifelongAgentBench: Evaluating LLM Agents as Lifelong Learners|Junhao Zheng, Xidi Cai, Qiuke Li, Duzhen Zhang, ZhongZhi Li, Yingying Zhang, Le Song, Qianli Ma|2025-05-17|arXiv|https://caixd-220529.github.io/LifelongAgentBench/|http://arxiv.org/abs/2505.11942v1|
|452|Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning|Puning Yang, Qizhou Wang, Zhuo Huang, Tongliang Liu, Chengqi Zhang, Bo Han|2025-05-17|arXiv|https://github.com/Puning97/SatImp-for-LLM-Unlearning|http://arxiv.org/abs/2505.11953v1|
|453|Solver-Informed RL: Grounding Large Language Models for Authentic Optimization Modeling|Yitian Chen, Jingfan Xia, Siyu Shao, Dongdong Ge, Yinyu Ye|2025-05-17|arXiv|https://github.com/Cardinal-Operations/SIRL|https://doi.org/10.48550/arXiv.2505.11792|
|454|Demystifying and Enhancing the Efficiency of Large Language Model Based Search Agents|Tiannuo Yang, Zebin Yao, Bowen Jin, Lixiao Cui, Yusen Li, Gang Wang, Xiaoguang Liu|2025-05-17|arXiv|https://github.com/tiannuo-yang/SearchAgent-X|https://doi.org/10.48550/arXiv.2505.12065|
|455|Unifying Segment Anything in Microscopy with Multimodal Large Language Model|Manyu Li, Ruian He, Zixian Zhang, Weimin Tan, Bo Yan|2025-05-16|arXiv|https://github.com/ieellee/uLLSAM|https://doi.org/10.48550/arXiv.2505.10769|
|456|IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation|Khanh-Tung Tran, Barry O'Sullivan, Hoang D. Nguyen|2025-05-16|arXiv|https://github.com/ReML-AI/IRLBench|http://arxiv.org/abs/2505.13498v1|
|457|GenKnowSub: Improving Modularity and Reusability of LLMs through General Knowledge Subtraction|Mohammadtaha Bagherifard, Sahar Rajabi, Ali Edalat, Yadollah Yaghoobzadeh|2025-05-16|arXiv|https://github.com/saharsamr/Modular-LLM|http://arxiv.org/abs/2505.10939v1|
|458|Ranked Voting based Self-Consistency of Large Language Models|Weiqin Wang, Yile Wang, Hui Huang|2025-05-16|arXiv|https://github.com/szu-tera/RankedVotingSC|https://doi.org/10.48550/arXiv.2505.10772|
|459|EmotionHallucer: Evaluating Emotion Hallucinations in Multimodal Large Language Models|Bohao Xing, Xin Liu, Guoying Zhao, Chengyu Liu, Xiaolan Fu, Heikki Kälviäinen|2025-05-16|arXiv|https://github.com/xxtars/EmotionHallucer|https://doi.org/10.48550/arXiv.2505.11405|
|460|GODBench: A Benchmark for Multimodal Large Language Models in Video Comment Art|Yiming Lei, Chenkai Zhang, Zeming Liu, Haitao Leng, Shaoguo Liu, Tingting Gao, Qingjie Liu, Yunhong Wang|2025-05-16|arXiv|https://github.com/stan-lei/GODBench-ACL2025|https://doi.org/10.48550/arXiv.2505.11436|
|461|From Trade-off to Synergy: A Versatile Symbiotic Watermarking Framework for Large Language Models|Yidan Wang, Yubing Ren, Yanan Cao, Binxing Fang|2025-05-15|arXiv|https://github.com/redwyd/SymMark|https://doi.org/10.48550/arXiv.2505.09924|
|462|ImagineBench: Evaluating Reinforcement Learning with Large Language Model Rollouts|Jing-Cheng Pang, Kaiyuan Li, Yidi Wang, Si-Hang Yang, Shengyi Jiang, Yang Yu|2025-05-15|arXiv|https://github.com/LAMDA-RL/ImagineBench|https://doi.org/10.48550/arXiv.2505.10010|
|463|On Technique Identification and Threat-Actor Attribution using LLMs and Embedding Models|Kyla Guru, Robert J. Moss, Mykel J. Kochenderfer|2025-05-15|arXiv|https://github.com/kylag/ttp_attribution|http://arxiv.org/abs/2505.11547v1|
|464|AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents|Julius Henke|2025-05-15|arXiv|https://github.com/JuliusHenke/autopentest|http://arxiv.org/abs/2505.10321v1|
|465|Do LLMs Memorize Recommendation Datasets? A Preliminary Study on MovieLens-1M|Dario Di Palma, Felice Antonio Merra, Maurizio Sfilio, Vito Walter Anelli, Fedelucio Narducci, Tommaso Di Noia|2025-05-15|arXiv|https://github.com/sisinflab/LLM-MemoryInspector|http://arxiv.org/abs/2505.10212v1|
|466|PIG: Privacy Jailbreak Attack on LLMs via Gradient-based Iterative In-Context Optimization|Yidan Wang, Yanan Cao, Yubing Ren, Fang Fang, Zheng Lin, Binxing Fang|2025-05-15|arXiv|https://github.com/redwyd/PrivacyJailbreak|http://arxiv.org/abs/2505.09921v2|
|467|Adversarial Attack on Large Language Models using Exponentiated Gradient Descent|Sajib Biswas, Mao Nishino, Samuel Jacob Chacko, Xiuwen Liu|2025-05-14|arXiv|https://github.com/sbamit/Exponentiated-Gradient-Descent-LLM-Attack|https://doi.org/10.48550/arXiv.2505.09820|
|468|LAS: Loss-less ANN-SNN Conversion for Fully Spike-Driven Large Language Models|Long Chen, Xiaotian Song, Yanan Sun|2025-05-14|arXiv|https://github.com/lc783/LAS|https://doi.org/10.48550/arXiv.2505.09659|
|469|Optimized Couplings for Watermarking Large Language Models|Dor Tsur, Carol Xuan Long, Claudio Mayrink Verdun, Hsiang Hsu, Haim H. Permuter, Flávio P. Calmon|2025-05-13|arXiv|https://github.com/Carol-Long/CC_Watermark|https://doi.org/10.48550/arXiv.2505.08878|
|470|Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era|Xixuan Hao, Yutian Jiang, Xingchen Zou, Jiabo Liu, Yifang Yin, Yuxuan Liang|2025-05-13|arXiv|https://github.com/CityMind-Lab/Awesome-Location-Intelligence|http://arxiv.org/abs/2505.09651v1|
|471|CodePDE: An Inference Framework for LLM-driven PDE Solver Generation|Shanda Li, Tanya Marwah, Junhong Shen, Weiwei Sun, Andrej Risteski, Yiming Yang, Ameet Talwalkar|2025-05-13|arXiv|https://github.com/LithiumDA/CodePDE|http://arxiv.org/abs/2505.08783v1|
|472|Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement|Haoran Ye, Jing Jin, Yuhang Xie, Xin Zhang, Guojie Song|2025-05-13|arXiv|https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics|https://doi.org/10.48550/arXiv.2505.08245|
|473|HealthBench: Evaluating Large Language Models Towards Improved Human Health|Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, Johannes Heidecke, Karan Singhal|2025-05-13|arXiv|https://github.com/openai/simple-evals|https://doi.org/10.48550/arXiv.2505.08775|
|474|A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models|Junjie Ye, Caishuang Huang, Zhuohan Chen, Wenjie Fu, Chenyuan Yang, Leyi Yang, Yilong Wu, Peng Wang, Meng Zhou, Xiaolong Yang, Tao Gui, Qi Zhang, Zhongchao Shi, Jianping Fan, Xuanjing Huang|2025-05-12|arXiv|https://github.com/Junjie-Ye/MulDimIF|https://doi.org/10.48550/arXiv.2505.07591|
|475|DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation|Jiashuo Sun, Xianrui Zhong, Sizhe Zhou, Jiawei Han|2025-05-12|arXiv|https://github.com/GasolSun36/DynamicRAG|https://doi.org/10.48550/arXiv.2505.07233|
|476|Are LLMs complicated ethical dilemma analyzers?|Jiashen, Du, Jesse Yao, Allen Liu, Zhekai Zhang|2025-05-12|arXiv|https://github.com/ALT-JS/ethicaLLM|http://arxiv.org/abs/2505.08106v1|
|477|Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs|Yifan Wei, Xiaoyan Yu, Tengfei Pan, Angsheng Li, Li Du|2025-05-12|arXiv|https://github.com/weiyifan1023/senator|http://arxiv.org/abs/2505.07184v1|
|478|GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance|Jinuk Kim, Marwa El Halabi, Wonpyo Park, Clemens JS Schaefer, Deokjae Lee, Yeonhong Park, Jae W. Lee, Hyun Oh Song|2025-05-11|arXiv|https://github.com/snu-mllab/GuidedQuant|https://doi.org/10.48550/arXiv.2505.07004|
|479|Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale|Avinash Patil, Siru Tao, Amardeep Gedhu|2025-05-11|arXiv|https://github.com/av9ash/llm_cssrs_code|http://arxiv.org/abs/2505.13480v1|
|480|From Knowledge to Reasoning: Evaluating LLMs for Ionic Liquids Research in Chemical and Biological Engineering|Gaurab Sarkar, Sougata Saha|2025-05-11|arXiv|https://github.com/sougata-ub/llms_for_ionic_liquids|http://arxiv.org/abs/2505.06964v1|
|481|MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception|Zhengye Zhang, Sirui Zhao, Shifeng Liu, Shukang Yin, Xinglong Mao, Tong Xu, Enhong Chen|2025-05-11|arXiv|https://github.com/zyzhangUstc/MELLM|http://arxiv.org/abs/2505.07007v1|
|482|POISONCRAFT: Practical Poisoning of Retrieval-Augmented Generation for Large Language Models|Yangguang Shao, Xinjie Lin, Haozheng Luo, Chengshang Hou, Gang Xiong, Jiahao Yu, Junzheng Shi|2025-05-10|arXiv|https://github.com/AndyShaw01/PoisonCraft|https://doi.org/10.48550/arXiv.2505.06579|
|483|Learn to Think: Bootstrapping LLM Reasoning Capability Through Graph Learning|Hang Gao, Chenhao Zhang, Tie Wang, Junsuo Zhao, Fengge Wu, Changwen Zheng, Huaping Liu|2025-05-09|arXiv|https://github.com/zch65458525/L2T|http://arxiv.org/abs/2505.06321v1|
|484|LiTransProQA: an LLM-based Literary Translation evaluation metric with Professional Question Answering|Ran Zhang, Wei Zhao, Lieve Macken, Steffen Eger|2025-05-08|arXiv|https://github.com/zhangr2021/TransProQA|http://arxiv.org/abs/2505.05423v2|
|485|KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification|Qianbo Zang, Christophe Zgrzendek, Igor Tchappi, Afshin Khadangi, Johannes Sedlmeir|2025-05-08|arXiv|https://github.com/QianboZang/KG-HTC|http://arxiv.org/abs/2505.05583v1|
|486|HEXGEN-TEXT2SQL: Optimizing LLM Inference Request Scheduling for Agentic Text-to-SQL Workflow|You Peng, Youhe Jiang, Chen Wang, Binhang Yuan|2025-05-08|arXiv|https://github.com/Relaxed-System-Lab/Hexgen-Flow|http://arxiv.org/abs/2505.05286v1|
|487|Prompt-Based LLMs for Position Bias-Aware Reranking in Personalized Recommendations|Md Aminul Islam, Ahmed Sayeed Faruk|2025-05-08|arXiv|https://github.com/aminul7506/LLMForReRanking|http://arxiv.org/abs/2505.04948v1|
|488|Scalable Multi-Stage Influence Function for Large Language Models via Eigenvalue-Corrected Kronecker-Factored Parameterization|Yuntai Bao, Xuhong Zhang, Tianyu Du, Xinkui Zhao, Jiang Zong, Hao Peng, Jianwei Yin|2025-05-08|arXiv|https://github.com/colored-dye/multi_stage_influence_function|https://doi.org/10.48550/arXiv.2505.05017|
|489|Unveiling Language-Specific Features in Large Language Models via Sparse Autoencoders|Boyi Deng, Yu Wan, Yidan Zhang, Baosong Yang, Fuli Feng|2025-05-08|arXiv|https://github.com/Aatrox103/multilingual-llm-features|https://doi.org/10.48550/arXiv.2505.05111|
|490|Benchmarking LLMs' Swarm intelligence|Kai Ruan, Mowen Huang, Ji-Rong Wen, Hao Sun|2025-05-07|arXiv|https://github.com/x66ccff/swarmbench|http://arxiv.org/abs/2505.04364v1|
|491|Advancing and Benchmarking Personalized Tool Invocation for LLMs|Xu Huang, Yuefeng Huang, Weiwen Liu, Xingshan Zeng, Yasheng Wang, Ruiming Tang, Hong Xie, Defu Lian|2025-05-07|arXiv|https://github.com/hyfshadow/PTBench|http://arxiv.org/abs/2505.04072v1|
|492|TrajEvo: Designing Trajectory Prediction Heuristics via LLM-driven Evolution|Zhikai Zhao, Chuanbo Hua, Federico Berto, Kanghoon Lee, Zihan Ma, Jiachen Li, Jinkyoo Park|2025-05-07|arXiv|https://github.com/ai4co/trajevo|http://arxiv.org/abs/2505.04480v1|
|493|Automatic Calibration for Membership Inference Attack on Large Language Models|Saleh Zare Zade, Yao Qiang, Xiangyu Zhou, Hui Zhu, Mohammad Amin Roshani, Prashant Khanduri, Dongxiao Zhu|2025-05-06|arXiv|https://github.com/Salehzz/ACMIA|https://doi.org/10.48550/arXiv.2505.03392|
|494|Avoid Recommending Out-of-Domain Items: Constrained Generative Recommendation with LLMs|Hao Liao, Wensheng Lu, Jianxun Lian, Mingqi Wu, Shuo Wang, Yong Zhang, Yitian Huang, Mingyang Zhou, Xing Xie|2025-05-06|arXiv|https://github.com/microsoft/RecAI|http://arxiv.org/abs/2505.03336v1|
|495|CombiBench: Benchmarking LLM Capability for Combinatorial Mathematics|Junqi Liu, Xiaohan Lin, Jonas Bayer, Yael Dillies, Weijie Jiang, Xiaodan Liang, Roman Soletskyi, Haiming Wang, Yunzhou Xie, Beibei Xiong, Zhengfeng Yang, Jujian Zhang, Lihong Zhi, Jia Li, Zhengying Liu|2025-05-06|arXiv|https://github.com/MoonshotAI/CombiBench/|http://arxiv.org/abs/2505.03171v1|
|496|Plug-and-Play AMC: Context Is King in Training-Free, Open-Set Modulation with LLMs|Mohammad Rostami, Atik Faysal, Reihaneh Gh. Roshan, Huaxia Wang, Nikhil Muralidhar, Yu-Dong Yao|2025-05-06|arXiv|https://github.com/RU-SIT/context-is-king|http://arxiv.org/abs/2505.03112v1|
|497|Sailing AI by the Stars: A Survey of Learning from Rewards in Post-Training and Test-Time Scaling of Large Language Models|Xiaobao Wu|2025-05-05|arXiv|https://github.com/bobxwu/learning-from-rewards-llm-papers|https://doi.org/10.48550/arXiv.2505.02686|
|498|LLaMA-Omni2: LLM-based Real-time Spoken Chatbot with Autoregressive Streaming Speech Synthesis|Qingkai Fang, Yan Zhou, Shoutao Guo, Shaolei Zhang, Yang Feng|2025-05-05|arXiv|https://github.com/ictnlp/LLaMA-Omni2|http://arxiv.org/abs/2505.02625v1|
|499|FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models|Zhouliang Yu, Ruotian Peng, Keyi Ding, Yizhe Li, Zhongyuan Peng, Minghao Liu, Yifan Zhang, Zheng Yuan, Huajian Xin, Wenhao Huang, Yandong Wen, Ge Zhang, Weiyang Liu|2025-05-05|arXiv|https://sphere-ai-lab.github.io/FormalMATH/|https://doi.org/10.48550/arXiv.2505.02735|
|500|Attention Mechanisms Perspective: Exploring LLM Processing of Graph-Structured Data|Zhong Guan, Likang Wu, Hongke Zhao, Ming He, Jianpin Fan|2025-05-04|arXiv|https://github.com/millioniron/LLM_exploration|http://arxiv.org/abs/2505.02130v1|
|501|MemEngine: A Unified and Modular Library for Developing Advanced Memory of LLM-based Agents|Zeyu Zhang, Quanyu Dai, Xu Chen, Rui Li, Zhongyang Li, Zhenhua Dong|2025-05-04|arXiv|https://github.com/nuster1128/MemEngine|http://arxiv.org/abs/2505.02099v1|
|502|A Survey on Inference Engines for Large Language Models: Perspectives on Optimization and Efficiency|Sihyeong Park, Sungryeol Jeon, Chaelyn Lee, Seokhun Jeon, Byung-Soo Kim, Jemin Lee|2025-05-03|arXiv|https://github.com/sihyeong/Awesome-LLM-Inference-Engine|https://doi.org/10.48550/arXiv.2505.01658|
|503|Amplifying Your Social Media Presence: Personalized Influential Content Generation with LLMs|Yuying Zhao, Yu Wang, Xueqi Cheng, Anne Marie Tumlin, Yunchao Liu, Damin Xia, Meng Jiang, Tyler Derr|2025-05-03|arXiv|https://github.com/YuyingZhao/LLM-influence-amplifier|http://arxiv.org/abs/2505.01698v1|
|504|Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities|Zhiwei Hao, Jianyuan Guo, Li Shen, Yong Luo, Han Hu, Guoxia Wang, Dianhai Yu, Yonggang Wen, Dacheng Tao|2025-05-02|arXiv|https://github.com/Hao840/Awesome-Low-Precision-Training|https://doi.org/10.48550/arXiv.2505.01043|
|505|FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing|Gaoxiang Cong, Liang Li, Jiadong Pan, Zhedong Zhang, Amin Beheshti, Anton van den Hengel, Yuankai Qi, Qingming Huang|2025-05-02|arXiv|https://galaxycong.github.io/LLM-Flow-Dubber/|http://arxiv.org/abs/2505.01263v1|
|506|WirelessAgent: Large Language Model Agents for Intelligent Wireless Networks|Jingwen Tong, Jiawei Shao, Qiong Wu, Wei Guo, Zijian Li, Zehong Lin, Jun Zhang|2025-05-02|arXiv|https://github.com/jwentong/WirelessAgent_R1|https://doi.org/10.48550/arXiv.2409.07964|
|507|DeepCritic: Deliberate Critique with Large Language Models|Wenkai Yang, Jingwen Chen, Yankai Lin, Ji-Rong Wen|2025-05-01|arXiv|https://github.com/RUCBM/DeepCritic|https://doi.org/10.48550/arXiv.2505.00662|
|508|SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation|Quang P. M. Pham, Khoi T. N. Nguyen, Nhi H. Doan, Cuong A. Pham, Kentaro Inui, Dezhen Song|2025-05-01|arXiv|https://github.com/quangpham2006/SmallPlan|http://arxiv.org/abs/2505.00831v1|
|509|LLM Ethics Benchmark: A Three-Dimensional Assessment System for Evaluating Moral Reasoning in Large Language Models|Junfeng Jiao, Saleh Afroogh, Abhejay Murali, Kevin Chen, David Atkinson, Amit Dhurandhar|2025-05-01|arXiv|https://github.com/|https://doi.org/10.48550/arXiv.2505.00853|
|510|A Survey on Large Language Model based Human-Agent Systems|Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Yuwei Cao, Dongyuan Li, Renhe Jiang, Philip S. Yu|2025-05-01|arXiv|https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-System-Papers|https://doi.org/10.48550/arXiv.2505.00753|
|511|LENSLLM: Unveiling Fine-Tuning Dynamics for LLM Selection|Xinyue Zeng, Haohui Wang, Junhong Lin, Jun Wu, Tyler Cody, Dawei Zhou|2025-05-01|arXiv|https://github.com/Susan571/LENSLLM|http://arxiv.org/abs/2505.03793v1|
|512|LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey|Henry Peng Zou, Wei-Chieh Huang, Yaozu Wu, Yankai Chen, Chunyu Miao, Hoang Nguyen, Yue Zhou, Weizhi Zhang, Liancheng Fang, Langzhou He, Yangning Li, Dongyuan Li, Renhe Jiang, Xue Liu, Philip S. Yu|2025-05-01|arXiv|https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems|http://arxiv.org/abs/2505.00753v4|
|513|Sentient Agent as a Judge: Evaluating Higher-Order Social Cognition in Large Language Models|Bang Zhang, Ruotian Ma, Qingxuan Jiang, Peisong Wang, Jiaqi Chen, Zheng Xie, Xingyu Chen, Yue Wang, Fanghua Ye, Jian Li, Yifan Yang, Zhaopeng Tu, Xiaolong Li|2025-05-01|arXiv|https://github.com/Tencent/digitalhuman/tree/main/SAGE|https://doi.org/10.48550/arXiv.2505.02847|
|514|When Reasoning Beats Scale: A 1.5B Reasoning Model Outranks 13B LLMs as Discriminator|Md Fahim Anjum|2025-04-30|arXiv|https://github.com/MDFahimAnjum/llm-planning-with-reasoning|http://arxiv.org/abs/2505.03786v1|
|515|LLM-based Interactive Imitation Learning for Robotic Manipulation|Jonas Werner, Kun Chu, Cornelius Weber, Stefan Wermter|2025-04-30|arXiv|https://github.com/Tubicor/LLM-iTeach|http://arxiv.org/abs/2504.21769v1|
|516|Computational Reasoning of Large Language Models|Haitao Wu, Zongbo Han, Joey Tianyi Zhou, Huaxi Huang, Changqing Zhang|2025-04-29|arXiv|https://github.com/HaitaoWuTJU/Turing-Machine-Bench|http://arxiv.org/abs/2504.20771v2|
|517|Turing Machine Evaluation for Large Language Model|Haitao Wu, Zongbo Han, Huaxi Huang, Changqing Zhang|2025-04-29|arXiv|https://github.com/HaitaoWuTJU/Turing-Machine-Bench|https://doi.org/10.48550/arXiv.2504.20771|
|518|X-Fusion: Introducing New Modality to Frozen Large Language Models|Sicheng Mo, Thao Nguyen, Xun Huang, Siddharth Srinivasan Iyer, Yijun Li, Yuchen Liu, Abhishek Tandon, Eli Shechtman, Krishna Kumar Singh, Yong Jae Lee, Bolei Zhou, Yuheng Li|2025-04-29|arXiv|https://sichengmo.github.io/XFusion/|https://doi.org/10.48550/arXiv.2504.20996|
|519|OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification|Shangyu Li, Juyong Jiang, Tiancheng Zhao, Jiasi Shen|2025-04-29|arXiv|https://github.com/lishangyu-hkust/OSVBench|http://arxiv.org/abs/2504.20964v1|
|520|Reinforcement Learning for Reasoning in Large Language Models with One Training Example|Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Liyuan Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, Yelong Shen|2025-04-29|arXiv|https://github.com/ypwang61/One-Shot-RLVR|https://doi.org/10.48550/arXiv.2504.20571|
|521|Evolution of Cooperation in LLM-Agent Societies: A Preliminary Study Using Different Punishment Strategies|Kavindu Warnakulasuriya, Prabhash Dissanayake, Navindu De Silva, Stephen Cranefield, Bastin Tony Roy Savarimuthu, Surangika Ranathunga, Nisansa de Silva|2025-04-28|arXiv|https://coin-workshop.github.io/coine-2025-detroit/accepted_for_presentation.html|http://arxiv.org/abs/2504.19487v1|
|522|LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects|Guangyi Liu, Pengxiang Zhao, Liang Liu, Yaxuan Guo, Han Xiao, Weifeng Lin, Yuxiang Chai, Yue Han, Shuai Ren, Hao Wang, Xiaoyu Liang, Wenhao Wang, Tianze Wu, Linghao Li, Hao Wang, Guanjing Xiong, Yong Liu, Hongsheng Li|2025-04-28|2025|https://github.com/PhoneLLM/Awesome-LLM-Powered-Phone-GUI-Agents|http://arxiv.org/abs/2504.19838v1|
|523|AutoP2C: An LLM-Based Agent Framework for Code Repository Generation from Multimodal Content in Academic Papers|Zijie Lin, Yiqing Shen, Qilin Cai, He Sun, Jinrui Zhou, Mingjun Xiao|2025-04-28|arXiv|https://github.com/shoushouyu/Automated-Paper-to-Code|http://arxiv.org/abs/2504.20115v1|
|524|BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese|Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, Yuxin Gu, Sixin Hong, Jing Ren, Jian Chen, Chao Liu, Yining Hua|2025-04-27|arXiv|https://github.com/PALIN2018/BrowseComp-ZH|https://doi.org/10.48550/arXiv.2504.19314|
|525|Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers|Dylan Bouchard, Mohit Singh Chauhan|2025-04-27|arXiv|https://github.com/cvs-health/uqlm|http://arxiv.org/abs/2504.19254v2|
|526|SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning|Jiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang, Xiaodan Liang, Zhaopeng Tu, Xiaolong Li, Kwan-Yee K. Wong|2025-04-27|arXiv|https://chen-judge.github.io/SPC/|http://arxiv.org/abs/2504.19162v1|
|527|Calibrating Translation Decoding with Quality Estimation on LLMs|Di Wu, Yibin Lei, Christof Monz|2025-04-26|arXiv|https://github.com/moore3930/calibrating-llm-mt|http://arxiv.org/abs/2504.19044v1|
|528|Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs|Mohammad Akbar-Tajari, Mohammad Taher Pilehvar, Mohammad Mahmoody|2025-04-26|arXiv|https://github.com/GoAT-pydev/Graph_of_Attacks|http://arxiv.org/abs/2504.19019v1|
|529|DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models|Jianyu Liu, Hangyu Guo, Ranjie Duan, Xingyuan Bu, Yancheng He, Shilong Li, Hui Huang, Jiaheng Liu, Yucheng Wang, Chenchen Jing, Xingwei Qu, Xiao Zhang, Yingshui Tan, Yanan Wu, Jihao Gu, Yangguang Li, Jianke Zhu|2025-04-25|NAACL|https://github.com/Kizna1ver/DREAM|https://aclanthology.org/2025.naacl-long.604/|
|530|LEAM: A Prompt-only Large Language Model-enabled Antenna Modeling Method|Tao Wu, Kexue Fu, Qiang Hua, Xinxin Liu, Muhammad Ali Imran, Bo Liu|2025-04-25|arXiv|https://github.com/TaoWu974/LEAM|https://doi.org/10.48550/arXiv.2504.18271|
|531|SORT3D: Spatial Object-centric Reasoning Toolbox for Zero-Shot 3D Grounding Using Large Language Models|Nader Zantout, Haochen Zhang, Pujith Kachana, Jinkai Qiu, Ji Zhang, Wenshan Wang|2025-04-25|arXiv|https://github.com/nzantout/SORT3D|https://doi.org/10.48550/arXiv.2504.18684|
|532|An Empirical Study on Prompt Compression for Large Language Models|Zheng Zhang, Jinyi Li, Yihuai Lan, Xiang Wang, Hao Wang|2025-04-24|arXiv|https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression|https://doi.org/10.48550/arXiv.2505.00019|
|533|Breaking the Modality Barrier: Universal Embedding Learning with Multimodal LLMs|Tiancheng Gu, Kaicheng Yang, Ziyong Feng, Xingjun Wang, Yanzhao Zhang, Dingkun Long, Yingda Chen, Weidong Cai, Jiankang Deng|2025-04-24|arXiv|https://garygutc.github.io/UniME|http://arxiv.org/abs/2504.17432v1|
|534|RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning|Zihan Wang, Kangrui Wang, Qineng Wang, Pingyue Zhang, Linjie Li, Zhengyuan Yang, Kefan Yu, Minh Nhat Nguyen, Licheng Liu, Eli Gottlieb, Monica Lam, Yiping Lu, Kyunghyun Cho, Jiajun Wu, Li Fei-Fei, Lijuan Wang, Yejin Choi, Manling Li|2025-04-24|arXiv|https://github.com/RAGEN-AI/RAGEN|http://arxiv.org/abs/2504.20073v1|
|535|Can Large Language Models Help Multimodal Language Analysis? MMLA: A Comprehensive Benchmark|Hanlei Zhang, Zhuohang Li, Yeshuang Zhu, Hua Xu, Peiwu Wang, Haige Zhu, Jie Zhou, Jinchao Zhang|2025-04-23|arXiv|https://github.com/thuiar/MMLA|https://doi.org/10.48550/arXiv.2504.16427|
|536|UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models|Yu Zheng, Longyi Liu, Yuming Lin, Jie Feng, Guozhen Zhang, Depeng Jin, Yong Li|2025-04-23|arXiv|https://github.com/tsinghua-fib-lab/PlanBench|https://doi.org/10.48550/arXiv.2504.21027|
|537|Enhancing LLM-Based Agents via Global Planning and Hierarchical Execution|Junjie Chen, Haitao Li, Jingli Yang, Yiqun Liu, Qingyao Ai|2025-04-23|arXiv|https://github.com/cjj826/GoalAct|http://arxiv.org/abs/2504.16563v1|
|538|Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control|Hannah Cyberey, David Evans|2025-04-23|arXiv|https://github.com/hannahxchen/llm-censorship-steering|http://arxiv.org/abs/2504.17130v1|
|539|PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models|Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, Chenyang Wang, Chencheng Tang, Haoling Chang, Qi Liu, Ziheng Zhou, Tianyu Zhang, Jingtian Zhang, Zhangyi Liu, Minghao Li, Yuku Zhang, Boxuan Jing, Xianqi Yin, Yutong Ren, Zizhuo Fu, Jiaming Ji, Weike Wang, Xudong Tian, Anqi Lv, Laifu Man, Jianxiang Li, Feiyu Tao, Qihua Sun, Zhou Liang, Yushu Mu, Zhongxuan Li, Jing-Jun Zhang, Shutao Zhang, Xiaotian Li, Xingqi Xia, Jiawei Lin, Zheyu Shen, Jiahang Chen, Qiuhao Xiong, Binran Wang, Fengyuan Wang, Ziyang Ni, Bohan Zhang, Fan Cui, Changkun Shao, Qing-Hong Cao, Ming-xing Luo, Yaodong Yang, Muhan Zhang, Hua Xing Zhu|2025-04-22|arXiv|https://phybench-official.github.io/phybench-demo/|https://doi.org/10.48550/arXiv.2504.16074|
|540|LiveCC: Learning Video LLM with Streaming Speech Transcription at Scale|Joya Chen, Ziyun Zeng, Yiqi Lin, Wei Li, Zejun Ma, Mike Zheng Shou|2025-04-22|arXiv|https://showlab.github.io/livecc|http://arxiv.org/abs/2504.16030v1|
|541|WALL-E 2.0: World Alignment by NeuroSymbolic Learning improves World Model-based LLM Agents|Siyu Zhou, Tianyi Zhou, Yijun Yang, Guodong Long, Deheng Ye, Jing Jiang, Chengqi Zhang|2025-04-22|arXiv|https://github.com/elated-sawyer/WALL-E|http://arxiv.org/abs/2504.15785v1|
|542|IV-Bench: A Benchmark for Image-Grounded Video Perception and Reasoning in Multimodal LLMs|David Ma, Yuanxing Zhang, Jincheng Ren, Jarvis Guo, Yifan Yao, Zhenlin Wei, Zhenzhu Yang, Zhongyuan Peng, Boyu Feng, Jun Ma, Xiao Gu, Zhoufutu Wen, King Zhu, Yancheng He, Meng Cao, Shiwen Ni, Jiaheng Liu, Wenhao Huang, Ge Zhang, Xiaojie Jin|2025-04-21|arXiv|https://github.com/multimodal-art-projection/IV-Bench|http://arxiv.org/abs/2504.15415v1|
|543|CRAVE: A Conflicting Reasoning Approach for Explainable Claim Verification Using LLMs|Yingming Zheng, Xiaoliang Liu, Peng Wu, Li Pan|2025-04-21|arXiv|https://github.com/8zym/CRAVE|http://arxiv.org/abs/2504.14905v1|
|544|VisuLogic: A Benchmark for Evaluating Visual Reasoning in Multi-modal Large Language Models|Weiye Xu, Jiahao Wang, Weiyun Wang, Zhe Chen, Wengang Zhou, Aijun Yang, Lewei Lu, Houqiang Li, Xiaohua Wang, Xizhou Zhu, Wenhai Wang, Jifeng Dai, Jinguo Zhu|2025-04-21|arXiv|https://visulogic-benchmark.github.io/VisuLogic|https://doi.org/10.48550/arXiv.2504.15279|
|545|Evaluating Judges as Evaluators: The JETTS Benchmark of LLM-as-Judges as Test-Time Scaling Evaluators|Yilun Zhou, Austin Xu, Peifeng Wang, Caiming Xiong, Shafiq Joty|2025-04-21|arXiv|https://github.com/SalesforceAIResearch/jetts-benchmark|http://arxiv.org/abs/2504.15253v1|
|546|EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models|Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, Huajun Chen|2025-04-21|arXiv|https://zjunlp.github.io/project/EasyEdit2/video|https://doi.org/10.48550/arXiv.2504.15133|
|547|Enhancing the Patent Matching Capability of Large Language Models via the Memory Graph|Qiushi Xiong, Zhipeng Xu, Zhenghao Liu, Mengjia Wang, Zulong Chen, Yue Sun, Yu Gu, Xiaohua Li, Ge Yu|2025-04-21|arXiv|https://github.com/NEUIR/MemGraph|https://doi.org/10.48550/arXiv.2504.14845|
|548|NoWag: A Unified Framework for Shape Preserving Compression of Large Language Models|Lawrence Liu, Inesh Chakrabarti, Yixiao Li, Mengdi Wang, Tuo Zhao, Lin F. Yang|2025-04-20|arXiv|https://github.com/LawrenceRLiu/NoWag|https://doi.org/10.48550/arXiv.2504.14569|
|549|Are Vision LLMs Road-Ready? A Comprehensive Benchmark for Safety-Critical Driving Video Understanding|Tong Zeng, Longfeng Wu, Liang Shi, Dawei Zhou, Feng Guo|2025-04-20|arXiv|https://github.com/tong-zeng/DVBench|http://arxiv.org/abs/2504.14526v1|
|550|CODECRASH: Stress Testing LLM Reasoning under Structural and Semantic Perturbations|Man Ho Lam, Chaozheng Wang, Jen-tse Huang, Michael R. Lyu|2025-04-19|arXiv|https://donaldlamnl.github.io/CodeCrash/|http://arxiv.org/abs/2504.14119v1|
|551|Understanding the Repeat Curse in Large Language Models from a Feature Perspective|Junchi Yao, Shu Yang, Jianhua Xu, Lijie Hu, Mengdi Li, Di Wang|2025-04-19|arXiv|https://github.com/kaustpradalab/repeat-curse-llm|https://doi.org/10.48550/arXiv.2504.14218|
|552|Towards Explainable Fake Image Detection with Multi-Modal Large Language Models|Yikun Ji, Yan Hong, Jiahui Zhan, Haoxing Chen, Jun Lan, Huijia Zhu, Weiqiang Wang, Liqing Zhang, Jianfu Zhang|2025-04-19|arXiv|https://github.com/Gennadiyev/mllm-defake|https://doi.org/10.48550/arXiv.2504.14245|
|553|Accelerating LLM Inference with Flexible N:M Sparsity via A Fully Digital Compute-in-Memory Accelerator|Akshat Ramachandran, Souvik Kundu, Arnab Raha, Shamik Kundu, Deepak K. Mathaikutty, Tushar Krishna|2025-04-19|arXiv|https://github.com/FLOW-open-project/FLOW|http://arxiv.org/abs/2504.14365v1|
|554|Integrating LLM-Generated Views into Mean-Variance Optimization Using the Black-Litterman Model|Youngbin Lee, Yejin Kim, Suin Kim, Yongjae Lee|2025-04-19|arXiv|https://github.com/youngandbin/LLM-MVO-BLM|http://arxiv.org/abs/2504.14345v1|
|555|LLM Sensitivity Evaluation Framework for Clinical Diagnosis|Chenwei Yan, Xiangling Fu, Yuxuan Xiong, Tianyi Wang, Siu Cheung Hui, Ji Wu, Xien Liu|2025-04-18|Proceedings of the 31st International Conference on Computational Linguistics, 2025|https://github.com/chenwei23333/DiagnosisQA|http://arxiv.org/abs/2504.13475v1|
|556|ZeroSumEval: An Extensible Framework For Scaling LLM Evaluation with Inter-Model Competition|Hisham A. Alyahya, Haidar Khan, Yazeed Alnumay, M Saiful Bari, Bülent Yener|2025-04-17|arXiv|https://github.com/facebookresearch/ZeroSumEval|http://arxiv.org/abs/2503.10673v1|
|557|Enhancing the Geometric Problem-Solving Ability of Multimodal LLMs via Symbolic-Neural Integration|Yicheng Pan, Zhenrong Zhang, Pengfei Hu, Jiefeng Ma, Jun Du, Jianshu Zhang, Quan Liu, Jianqing Gao, Feng Ma|2025-04-17|arXiv|https://github.com/ycpNotFound/GeoGen|http://arxiv.org/abs/2504.12773v1|
|558|EmoVoice: LLM-based Emotional Text-To-Speech Model with Freestyle Text Prompting|Guanrou Yang, Chen Yang, Qian Chen, Ziyang Ma, Wenxi Chen, Wen Wang, Tianrui Wang, Yifan Yang, Zhikang Niu, Wenrui Liu, Fan Yu, Zhihao Du, Zhifu Gao, ShiLiang Zhang, Xie Chen|2025-04-17|arXiv|https://yanghaha0908.github.io/EmoVoice/|http://arxiv.org/abs/2504.12867v1|
|559|ConExion: Concept Extraction with Large Language Models|Ebrahim Norouzi, Sven Hertling, Harald Sack|2025-04-17|arXiv|https://github.com/ISE-FIZKarlsruhe/concept_extraction|https://doi.org/10.48550/arXiv.2504.12915|
|560|A Human-AI Comparative Analysis of Prompt Sensitivity in LLM-Based Relevance Judgment|Negar Arabzadeh, Charles L. A . Clarke|2025-04-16|arXiv|https://github.com/Narabzad/prompt-sensitivity-relevance-judgements/|http://arxiv.org/abs/2504.12408v1|
|561|Modular-Cam: Modular Dynamic Camera-view Video Generation with LLM|Zirui Pan, Xin Wang, Yipeng Zhang, Hong Chen, Kwan Man Cheng, Yaofei Wu, Wenwu Zhu|2025-04-16|arXiv|https://modular-cam.github.io|http://arxiv.org/abs/2504.12048v1|
|562|HLS-Eval: A Benchmark and Framework for Evaluating LLMs on High-Level Synthesis Design Tasks|Stefan Abi-Karam, Cong Hao|2025-04-16|arXiv|https://github.com/stefanpie/hls-eval|http://arxiv.org/abs/2504.12268v1|
|563|InjectLab: A Tactical Framework for Adversarial Threat Modeling Against Large Language Models|Austin Howard|2025-04-16|arXiv|https://github.com/ahow2004/injectlab|https://doi.org/10.48550/arXiv.2505.18156|
|564|LLM-as-a-Judge: Reassessing the Performance of LLMs in Extractive QA|Xanh Ho, Jiahao Huang, Florian Boudin, Akiko Aizawa|2025-04-16|arXiv|https://github.com/Alab-NII/llm-judge-extract-qa|http://arxiv.org/abs/2504.11972v1|
|565|d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning|Siyan Zhao, Devaansh Gupta, Qinqing Zheng, Aditya Grover|2025-04-16|arXiv|https://dllm-reasoning.github.io/|https://doi.org/10.48550/arXiv.2504.12216|
|566|Offline Learning and Forgetting for Reasoning with Large Language Models|Tianwei Ni, Allen Nie, Sapana Chaudhary, Yao Liu, Huzefa Rangwala, Rasool Fakoor|2025-04-15|arXiv|https://github.com/twni2016/llm-reasoning-uft|http://arxiv.org/abs/2504.11364v3|
|567|Teaching Large Language Models to Reason through Learning and Forgetting|Tianwei Ni, Allen Nie, Sapana Chaudhary, Yao Liu, Huzefa Rangwala, Rasool Fakoor|2025-04-15|arXiv|https://github.com/twni2016/llm-reasoning-uft|https://doi.org/10.48550/arXiv.2504.11364|
|568|Using LLMs as prompt modifier to avoid biases in AI image generators|René Peinl|2025-04-15|arXiv|https://iisys-hof.github.io/llm-prompt-img-gen/|http://arxiv.org/abs/2504.11104v1|
|569|Understanding LLMs' Cross-Lingual Context Retrieval: How Good It Is And Where It Comes From|Changjiang Gao, Hankun Lin, Shujian Huang, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Jiajun Chen|2025-04-15|arXiv|https://github.com/NJUNLP/Cross-Lingual-Context-Retrieval|http://arxiv.org/abs/2504.10906v1|
|570|MT-R1-Zero: Advancing LLM-based Machine Translation via R1-Zero-like Reinforcement Learning|Zhaopeng Feng, Shaosheng Cao, Jiahan Ren, Jiayuan Su, Ruizhe Chen, Yan Zhang, Zhe Xu, Yao Hu, Jian Wu, Zuozhu Liu|2025-04-15|arXiv …, 2025|https://github.com/fzp0424/MT-R1-Zero|http://arxiv.org/abs/2504.10160v1|
|571|LLM-SRBench: A New Benchmark for Scientific Equation Discovery with Large Language Models|Parshin Shojaee, Ngoc-Hieu Nguyen, Kazem Meidani, Amir Barati Farimani, Khoa D. Doan, Chandan K. Reddy|2025-04-15|arXiv|https://github.com/deep-symbolic-mathematics/llm-srbench|https://doi.org/10.48550/arXiv.2504.10415|
|572|LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks|Soumyadeep Pal, Changsheng Wang, James Diffenderfer, Bhavya Kailkhura, Sijia Liu|2025-04-15|arXiv …, 2025|https://github.com/OPTML-Group/MU-Coreset|http://arxiv.org/abs/2504.10185v2|
|573|RadarLLM: Empowering Large Language Models to Understand Human Motion from Millimeter-wave Point Cloud Sequence|Zengyuan Lai, Jiarui Yang, Songpengcheng Xia, Lizhou Lin, Lan Sun, Renwen Wang, Jianran Liu, Qi Wu, Ling Pei|2025-04-15|arXiv|https://inowlzy.github.io/RadarLLM/|https://doi.org/10.48550/arXiv.2504.09862|
|574|Propaganda via AI? A Study on Semantic Backdoors in Large Language Models|Nay Myat Min, Long H. Pham, Yige Li, Jun Sun|2025-04-15|arXiv|https://github.com/NayMyatMin/RAVEN|https://doi.org/10.48550/arXiv.2504.12344|
|575|Probing then Editing Response Personality of Large Language Models|Tianjie Ju, Zhenyu Shao, Bowen Wang, Yujia Chen, Zhuosheng Zhang, Hao Fei, Mong-Li Lee, Wynne Hsu, Sufeng Duan, Gongshen Liu|2025-04-15|arXiv|https://github.com/universe-sky/probing-then-editing-personality|https://doi.org/10.48550/arXiv.2504.10227|
|576|Dynamic Compressing Prompts for Efficient Inference of Large Language Models|Jinwu Hu, Wei Zhang, Yufeng Wang, Yu Hu, Bin Xiao, Mingkui Tan, Qing Du|2025-04-15|arXiv|https://github.com/Fhujinwu/DCP|https://doi.org/10.48550/arXiv.2504.11004|
|577|A Dual-Space Framework for General Knowledge Distillation of Large Language Models|Xuejie Zhang, Songming Zhang, Yunlong Liang, Fandong Meng, Yufeng Chen, Jinan Xu, Jie Zhou|2025-04-15|arXiv|https://github.com/songmzhang/DSKDv2|https://doi.org/10.48550/arXiv.2504.11426|
|578|70% Size, 100% Accuracy: Lossless LLM Compression for Efficient GPU Inference via Dynamic-Length Float|Tianyi Zhang, Yang Sui, Shaochen Zhong, Vipin Chaudhary, Xia Hu, Anshumali Shrivastava|2025-04-15|arXiv|https://github.com/LeanModels/DFloat11|http://arxiv.org/abs/2504.11651v1|
|579|CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates|Ankit Kumar Shaw, Kun Jiang, Tuopu Wen, Chandan Kumar Sah, Yining Shi, Mengmeng Yang, Diange Yang, Xiaoli Lian|2025-04-14|arXiv|https://Ankit-Zefan.github.io/CleanMap/|http://arxiv.org/abs/2504.10738v1|
|580|Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025|Nitya Thakkar, Mert Yuksekgonul, Jake Silberg, Animesh Garg, Nanyun Peng, Fei Sha, Rose Yu, Carl Vondrick, James Zou|2025-04-13|arXiv|https://github.com/zou-group/review_feedback_agent|http://arxiv.org/abs/2504.09737v1|
|581|Syzygy of Thoughts: Improving LLM CoT with the Minimal Free Resolution|Chenghao Li, Chaoning Zhang, Yi Lu, Jiaquan Zhang, Qigan Sun, Xudong Wang, Jiwei Wei, Guoqing Wang, Yang Yang, Heng Tao Shen|2025-04-13|arXiv|https://github.com/dlMARiA/Syzygy-of-thoughts|http://arxiv.org/abs/2504.09566v2|
|582|Span-level Emotion-Cause-Category Triplet Extraction with Instruction Tuning LLMs and Data Augmentation|Xiangju Li, Dong Yang, Xiaogang Zhu, Faliang Huang, Peng Zhang, Zhongying Zhao|2025-04-13|arXiv|https://github.com/zxgnlp/InstruDa-LLM|http://arxiv.org/abs/2504.12331v1|
|583|How new data permeates LLM knowledge and how to dilute it|Chen Sun, Renat Aksitov, Andrey Zhmoginov, Nolan Andrew Miller, Max Vladymyrov, Ulrich Rueckert, Been Kim, Mark Sandler|2025-04-13|arXiv|https://sunchipsster1.github.io/projects/outlandish/|http://arxiv.org/abs/2504.09522v1|
|584|HalluShift: Measuring Distribution Shifts towards Hallucination Detection in LLMs|Sharanya Dasgupta, Sujoy Nath, Arkaprabha Basu, Pourya Shamsolmoali, Swagatam Das|2025-04-13|arXiv|https://github.com/sharanya-dasgupta001/hallushift|http://arxiv.org/abs/2504.09482v1|
|585|Alleviating the Fear of Losing Alignment in LLM Fine-tuning|Kang Yang, Guanhong Tao, Xun Chen, Jun Xu|2025-04-13|arXiv|https://github.com/kangyangWHU/LLMAlignment|http://arxiv.org/abs/2504.09757v1|
|586|ClinicalGPT-R1: Pushing reasoning capability of generalist disease diagnosis with large language model|Wuyang Lan, Wenzheng Wang, Changwei Ji, Guoxing Yang, Yongbo Zhang, Xiaohong Liu, Song Wu, Guangyu Wang|2025-04-13|arXiv|https://github.com/medfound/medfound|https://doi.org/10.48550/arXiv.2504.09421|
|587|DUMP: Automated Distribution-Level Curriculum Learning for RL-based LLM Post-training|Zhenting Wang, Guofeng Cui, Yu-Jhe Li, Kun Wan, Wentian Zhao|2025-04-13|arXiv|https://github.com/ZhentingWang/DUMP|http://arxiv.org/abs/2504.09710v1|
|588|SegEarth-R1: Geospatial Pixel Reasoning via Large Language Model|Kaiyu Li, Zepeng Xin, Li Pang, Chao Pang, Yupeng Deng, Jing Yao, Guisong Xia, Deyu Meng, Zhi Wang, Xiangyong Cao|2025-04-13|arXiv|https://github.com/earth-insights/SegEarth-R1|https://doi.org/10.48550/arXiv.2504.09644|
|589|Fine-tuning a Large Language Model for Automating Computational Fluid Dynamics Simulations|Zhehao Dong, Zhen Lu, Yue Yang|2025-04-13|arXiv|https://github.com/YYgroup/AutoCFD|https://doi.org/10.48550/arXiv.2504.09602|
|590|Efficient Tuning of Large Language Models for Knowledge-Grounded Dialogue Generation|Bo Zhang, Hui Ma, Dailin Li, Jian Ding, Jian Wang, Bo Xu, Hongfei Lin|2025-04-12|arXiv|https://github.com/zhangbo-nlp/KEDiT|https://doi.org/10.48550/arXiv.2504.07754|
|591|LLM4Ranking: An Easy-to-use Framework of Utilizing Large Language Models for Document Reranking|Qi Liu, Haozhe Duan, Yiqun Chen, Quanfeng Lu, Weiwei Sun, Jiaxin Mao|2025-04-12|arXiv|https://github.com/liuqi6777/llm4ranking|https://doi.org/10.48550/arXiv.2504.07439|
|592|Why We Feel: Breaking Boundaries in Emotional Reasoning with Multimodal Large Language Models|Yuxiang Lin, Jingdong Sun, Zhi-Qi Cheng, Jue Wang, Haomin Liang, Zebang Cheng, Yifei Dong, Jun-Yan He, Xiaojiang Peng, Xian-Sheng Hua|2025-04-12|arXiv|https://github.com/Lum1104/EIBench|https://doi.org/10.48550/arXiv.2504.07521|
|593|From Punchlines to Predictions: A Metric to Assess LLM Performance in Identifying Humor in Stand-Up Comedy|Adrianna Romanowski, Pedro H. V. Valois, Kazuhiro Fukui|2025-04-12|arXiv|https://github.com/swaggirl9000/humor|http://arxiv.org/abs/2504.09049v1|
|594|Revisiting LLM Evaluation through Mechanism Interpretability: a New Metric and Model Utility Law|Yixin Cao, Jiahao Ying, Yaoning Wang, Xipeng Qiu, Xuanjing Huang, Yugang Jiang|2025-04-12|arXiv …, 2025|https://github.com/ALEX-nlp/MUI-Eva|http://arxiv.org/abs/2504.07440v1|
|595|GLUS: Global-Local Reasoning Unified into A Single Large Language Model for Video Segmentation|Lang Lin, Xueyang Yu, Ziqi Pang, Yu-Xiong Wang|2025-04-12|arXiv|https://glus-video.github.io/|https://doi.org/10.48550/arXiv.2504.07962|
|596|Task Memory Engine (TME): A Structured Memory Framework with Graph-Aware Extensions for Multi-Step LLM Agent Tasks|Ye Ye|2025-04-11|arXiv|https://github.com/biubiutomato/TME-Agent|http://arxiv.org/abs/2504.08525v3|
|597|A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis|Xin Gao, Qizhi Pei, Zinan Tang, Yu Li, Honglin Lin, Jiang Wu, Conghui He, Lijun Wu|2025-04-11|arXiv|https://github.com/GX-XinGao/GRA|http://arxiv.org/abs/2504.12322v1|
|598|Model Utility Law: Evaluating LLMs beyond Performance through Mechanism Interpretable Metric|Yixin Cao, Jiahao Ying, Yaoning Wang, Xipeng Qiu, Xuanjing Huang, Yugang Jiang|2025-04-10|arXiv|https://github.com/ALEX-nlp/MUI-Eva|http://arxiv.org/abs/2504.07440v2|
|599|DeepSeek-R1 vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and Summarization?|Daniil Larionov, Sotaro Takeshita, Ran Zhang, Yanran Chen, Christoph Leiter, Zhipin Wang, Christian Greisinger, Steffen Eger|2025-04-10|arXiv|https://github.com/NL2G/reasoning-eval|http://arxiv.org/abs/2504.08120v3|
|600|Exploring the Effectiveness and Interpretability of Texts in LLM-based Time Series Models|Zhengke Sun, Hangwei Qian, Ivor Tsang|2025-04-09|arXiv|https://github.com/zachysun/TS-Lang-Exp|http://arxiv.org/abs/2504.08808v1|
|601|Right Question is Already Half the Answer: Fully Unsupervised LLM Reasoning Incentivization|Qingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, Yatao Bian|2025-04-08|arXiv|https://github.com/QingyangZhang/EMPO|http://arxiv.org/abs/2504.05812v1|
|602|StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization|Yiming Tang, Yi Fan, Chenxiao Yu, Tiankai Yang, Yue Zhao, Xiyang Hu|2025-04-08|arXiv|https://github.com/Tangyiming205069/controllable-seo|http://arxiv.org/abs/2504.05804v1|
|603|MDK12-Bench: A Multi-Discipline Benchmark for Evaluating Reasoning in Multimodal Large Language Models|Pengfei Zhou, Fanrui Zhang, Xiaopeng Peng, Zhaopan Xu, Jiaxin Ai, Yansheng Qiu, Chuanhao Li, Zhen Li, Ming Li, Yukang Feng, Jianwen Sun, Haoquan Zhang, Zizhen Li, Xiaofeng Mao, Wangbo Zhao, Kai Wang, Xiaojun Chang, Wenqi Shao, Yang You, Kaipeng Zhang|2025-04-08|arXiv|https://github.com/LanceZPF/MDK12|https://doi.org/10.48550/arXiv.2504.05782|
|604|V-MAGE: A Game Evaluation Framework for Assessing Visual-Centric Capabilities in Multimodal Large Language Models|Xiangxi Zheng, Linjie Li, Zhengyuan Yang, Ping Yu, Alex Jinpeng Wang, Rui Yan, Yuan Yao, Lijuan Wang|2025-04-08|arXiv|https://github.com/CSU-JPG/V-MAGE|https://doi.org/10.48550/arXiv.2504.06148|
|605|LLM$\times$MapReduce-V2: Entropy-Driven Convolutional Test-Time Scaling for Generating Long-Form Articles from Extremely Long Resources|Haoyu Wang, Yujia Fu, Zhu Zhang, Shuo Wang, Zirui Ren, Xiaorong Wang, Zhili Li, Chaoqun He, Bo An, Zhiyuan Liu, Maosong Sun|2025-04-08|arXiv|https://github.com/thunlp/LLMxMapReduce|http://arxiv.org/abs/2504.05732v1|
|606|Assessing Thai Dialect Performance in LLMs with Automatic Benchmarks and Human Evaluation|Peerat Limkonchotiwat, Kanruethai Masuk, Surapon Nonesung, Chalermpun Mai-On, Sarana Nutanong, Wuttikorn Ponwitayarat, Potsawee Manakul|2025-04-08|arXiv|https://github.com/mrpeerat/Thai_local_benchmark|http://arxiv.org/abs/2504.05898v1|
|607|Collab-RAG: Boosting Retrieval-Augmented Generation for Complex Question Answering via White-Box and Black-Box LLM Collaboration|Ran Xu, Wenqi Shi, Yuchen Zhuang, Yue Yu, Joyce C. Ho, Haoyu Wang, Carl Yang|2025-04-07|arXiv|https://github.com/ritaranx/Collab-RAG/|http://arxiv.org/abs/2504.04915v1|
|608|Are You Getting What You Pay For? Auditing Model Substitution in LLM APIs|Will Cai, Tianneng Shi, Xuandong Zhao, Dawn Song|2025-04-07|arXiv|https://github.com/sunblaze-ucb/llm-api-audit|http://arxiv.org/abs/2504.04715v1|
|609|Can LLM-Driven Hard Negative Sampling Empower Collaborative Filtering? Findings and Potentials|Chu Zhao, Enneng Yang, Yuting Liu, Jianzhe Zhao, Guibing Guo, Xingwei Wang|2025-04-07|arXiv|https://github.com/user683/HNLMRec|http://arxiv.org/abs/2504.04726v1|
|610|EduPlanner: LLM-Based Multi-Agent Systems for Customized and Intelligent Instructional Design|Xueqiao Zhang, Chao Zhang, Jianwen Sun, Jun Xiao, Yi Yang, Yawei Luo|2025-04-07|arXiv|https://github.com/Zc0812/Edu_Planner|http://arxiv.org/abs/2504.05370v1|
|611|Achieving binary weight and activation for LLMs using Post-Training Quantization|Siqing Song, Chuang Wang, Ruiqi Wang, Yi Yang, Xu-Yao Zhang|2025-04-07|arXiv|https://github.com/JimmyCrave/LLM-PTQ-binarization|http://arxiv.org/abs/2504.05352v3|
|612|Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models|Yubo Li, Xiaobin Shen, Xinyu Yao, Xueying Ding, Yidi Miao, Ramayya Krishnan, Rema Padman|2025-04-07|arXiv|https://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs|https://doi.org/10.48550/arXiv.2504.04717|
|613|PRIMA.CPP: Speeding Up 70B-Scale LLM Inference on Low-Resource Everyday Home Clusters|Zonghang Li, Tao Li, Wenjiao Feng, Mohsen Guizani, Hongfang Yu|2025-04-07|arXiv|https://github.com/Lizonghang/prima.cpp|http://arxiv.org/abs/2504.08791v1|
|614|SEAL: Steerable Reasoning Calibration of Large Language Models for Free|Runjin Chen, Zhenyu Zhang, Junyuan Hong, Souvik Kundu, Zhangyang Wang|2025-04-07|arXiv|https://github.com/VITA-Group/SEAL|https://doi.org/10.48550/arXiv.2504.07986|
|615|ArxivBench: Can LLMs Assist Researchers in Conducting Research?|Ning Li, Jingran Zhang, Justin Cui|2025-04-06|arXiv|https://github.com/arxivBenchLLM/arXivBench|http://arxiv.org/abs/2504.10496v1|
|616|Trust Region Preference Approximation: A simple and stable reinforcement learning algorithm for LLM reasoning|Xuerui Su, Shufang Xie, Guoqing Liu, Yingce Xia, Renqian Luo, Peiran Jin, Zhiming Ma, Yue Wang, Zun Wang, Yuting Liu|2025-04-06|arXiv|https://github.com/XueruiSu/Trust-Region-Preference-Approximation|http://arxiv.org/abs/2504.04524v1|
|617|MSL: Not All Tokens Are What You Need for Tuning LLM as a Recommender|Bohao Wang, Feng Liu, Jiawei Chen, Xingyu Lou, Changwang Zhang, Jun Wang, Yuegang Sun, Yan Feng, Chun Chen, Can Wang|2025-04-05|arXiv|https://github.com/WANGBohaO-jpg/MSL|http://arxiv.org/abs/2504.04178v1|
|618|Window Token Concatenation for Efficient Visual Large Language Models|Yifan Li, Wentao Bao, Botao Ye, Zhen Tan, Tianlong Chen, Huan Liu, Yu Kong|2025-04-05|arXiv|https://github.com/JackYFL/WiCo|https://doi.org/10.48550/arXiv.2504.04024|
|619|VocalNet: Speech LLM with Multi-Token Prediction for Faster and High-Quality Generation|Yuhao Wang, Heyang Liu, Ziyang Cheng, Ronghua Wu, Qunshan Gu, Yanfeng Wang, Yu Wang|2025-04-05|arXiv|https://github.com/SJTU-OmniAgent/VocalNet|http://arxiv.org/abs/2504.04060v1|
|620|A Benchmark for End-to-End Zero-Shot Biomedical Relation Extraction with LLMs: Experiments with OpenAI Models|Aviv Brokman, Xuguang Ai, Yuhang Jiang, Shashank Gupta, Ramakanth Kavuluru|2025-04-05|arXiv|https://github.com/bionlproc/ZeroShotRE|http://arxiv.org/abs/2504.04083v1|
|621|AiReview: An Open Platform for Accelerating Systematic Reviews with LLMs|Xinyu Mao, Teerapong Leelanupab, Martin Potthast, Harrisen Scells, Guido Zuccon|2025-04-05|arXiv|https://github.com/ielab/ai-review|http://arxiv.org/abs/2504.04193v1|
|622|A Perplexity and Menger Curvature-Based Approach for Similarity Evaluation of Large Language Models|Yuantao Zhang, Zhankui Yang|2025-04-05|arXiv|https://github.com/zyttt-coder/LLM_similarity|https://doi.org/10.48550/arXiv.2504.04216|
|623|Align to Structure: Aligning Large Language Models with Structural Information|Zae Myung Kim, Anand Ramachandran, Farideh Tavazoee, Joo-Kyung Kim, Oleg Rokhlenko, Dongyeop Kang|2025-04-04|arXiv|https://github.com/minnesotanlp/struct_align|https://doi.org/10.48550/arXiv.2504.03622|
|624|EnrichIndex: Using LLMs to Enrich Retrieval Indices Offline|Peter Baile Chen, Tomer Wolfson, Michael Cafarella, Dan Roth|2025-04-04|arXiv|https://peterbaile.github.io/enrichindex/|http://arxiv.org/abs/2504.03598v1|
|625|Measurement of LLM's Philosophies of Human Nature|Minheng Ni, Ennan Wu, Zidong Gong, Zhengyuan Yang, Linjie Li, Chung-Ching Lin, Kevin Lin, Lijuan Wang, Wangmeng Zuo|2025-04-03|arXiv|https://github.com/kodenii/M-PHNS|http://arxiv.org/abs/2504.02304v1|
|626|ZClip: Adaptive Spike Mitigation for LLM Pre-Training|Abhay Kumar, Louis Owen, Nilabhra Roy Chowdhury, Fabian Güra|2025-04-03|arXiv|https://github.com/bluorion-com/ZClip|http://arxiv.org/abs/2504.02507v1|
|627|BT-ACTION: A Test-Driven Approach for Modular Understanding of User Instruction Leveraging Behaviour Trees and LLMs|Alexander Leszczynski, Sarah Gillet, Iolanda Leite, Fethiye Irmak Dogan|2025-04-03|arXiv|https://github.com/1Eggbert7/BT_LLM|http://arxiv.org/abs/2504.02779v1|
|628|AnesBench: Multi-Dimensional Evaluation of LLM Reasoning in Anesthesiology|Xiang Feng, Wentao Jiang, Zengmao Wang, Yong Luo, Pingbo Xu, Baosheng Yu, Hua Jin, Bo Du, Jing Zhang|2025-04-03|arXiv|https://github.com/MiliLab/AnesBench|http://arxiv.org/abs/2504.02404v1|
|629|TiC-LM: A Web-Scale Benchmark for Time-Continual LLM Pretraining|Jeffrey Li, Mohammadreza Armandpour, Iman Mirzadeh, Sachin Mehta, Vaishaal Shankar, Raviteja Vemulapalli, Samy Bengio, Oncel Tuzel, Mehrdad Farajtabar, Hadi Pouransari, Fartash Faghri|2025-04-02|arXiv|https://github.com/apple/ml-tic-lm|http://arxiv.org/abs/2504.02107v1|
|630|Urban Computing in the Era of Large Language Models|Zhonghang Li, Lianghao Xia, Xubin Ren, Jiabin Tang, Tianyi Chen, Yong Xu, Chao Huang|2025-04-02|arXiv|https://github.com/HKUDS/Awesome-LLM4Urban-Papers|https://doi.org/10.48550/arXiv.2504.02009|
|631|MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits|Brandon Radosevich, John Halloran|2025-04-02|arXiv|https://github.com/leidosinc/McpSafetyScanner|http://arxiv.org/abs/2504.03767v1|
|632|Comment Staytime Prediction with LLM-enhanced Comment Understanding|Changshuo Zhang, Zihan Lin, Shukai Liu, Yongqi Liu, Han Li|2025-04-02|arXiv|https://github.com/lyingCS/KuaiComt.github.io|http://arxiv.org/abs/2504.01602v1|
|633|OmniCellTOSG: The First Cell Text-Omic Signaling Graphs Dataset for Joint LLM and GNN Modeling|Heming Zhang, Tim Xu, Dekang Cao, Shunning Liang, Lars Schimmelpfennig, Levi Kaster, Di Huang, Carlos Cruchaga, Guangfu Li, Michael Province, Yixin Chen, Philip Payne, Fuhai Li|2025-04-02|arXiv|https://github.com/FuhaiLiAiLab/OmniCellTOSG|http://arxiv.org/abs/2504.02148v1|
|634|When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning|Nishad Singhi, Hritik Bansal, Arian Hosseini, Aditya Grover, Kai-Wei Chang, Marcus Rohrbach, Anna Rohrbach|2025-04-01|arXiv|https://github.com/nishadsinghi/sc-genrm-scaling|http://arxiv.org/abs/2504.01005v1|
|635|MedReason: Eliciting Factual Medical Reasoning Steps in LLMs via Knowledge Graphs|Juncheng Wu, Wenlong Deng, Xingxuan Li, Sheng Liu, Taomian Mi, Yifan Peng, Ziyang Xu, Yi Liu, Hyunjin Cho, Chang-In Choi, Yihan Cao, Hui Ren, Xiang Li, Xiaoxiao Li, Yuyin Zhou|2025-04-01|arXiv|https://github.com/UCSC-VLAA/MedReason|http://arxiv.org/abs/2504.00993v2|
|636|m1: Unleash the Potential of Test-Time Scaling for Medical Reasoning with Large Language Models|Xiaoke Huang, Juncheng Wu, Hui Liu, Xianfeng Tang, Yuyin Zhou|2025-04-01|arXiv|https://github.com/UCSC-VLAA/m1|https://doi.org/10.48550/arXiv.2504.00869|
|637|ShortV: Efficient Multimodal Large Language Models by Freezing Visual Tokens in Ineffective Layers|Qianhao Yuan, Qingyu Zhang, Yanjiang Liu, Jiawei Chen, Yaojie Lu, Hongyu Lin, Jia Zheng, Xianpei Han, Le Sun|2025-04-01|arXiv|https://github.com/icip-cas/ShortV|https://doi.org/10.48550/arXiv.2504.00502|
|638|CrackSQL: A Hybrid SQL Dialect Translation System Powered by Large Language Models|Wei Zhou, Yuyang Gao, Xuanhe Zhou, Guoliang Li|2025-04-01|arXiv|https://github.com/weAIDB/CrackSQL|https://doi.org/10.48550/arXiv.2504.00882|
|639|RECKON: Large-scale Reference-based Efficient Knowledge Evaluation for Large Language Model|Lin Zhang, Zhouhong Gu, Xiaoran Shi, Hongwei Feng, Yanghua Xiao|2025-04-01|arXiv|https://github.com/MikeGu721/reckon|https://doi.org/10.48550/arXiv.2504.00756|
|640|SACA: A Scenario-Aware Collision Avoidance Framework for Autonomous Vehicles Integrating LLMs-Driven Reasoning|Shiyue Zhao, Junzhi Zhang, Neda Masoud, Heye Huang, Xingpeng Xia, Chengkun He|2025-03-31|arXiv|https://sean-shiyuez.github.io/SACA/|http://arxiv.org/abs/2504.00115v2|
|641|Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving|Wei Gao, Xinyu Zhou, Peng Sun, Tianwei Zhang, Yonggang Wen|2025-03-31|arXiv|https://github.com/LLMkvsys/rethink-kv-compression|https://doi.org/10.48550/arXiv.2503.24000|
|642|Text Chunking for Document Classification for Urban System Management using Large Language Models|Joshua Rodriguez, Om Sanan, Guillermo Vizarreta-Luna, Steven A. Conrad|2025-03-31|arXiv|https://github.com/josh-rodriguez-csu/ChunkingforLLMs|https://doi.org/10.48550/arXiv.2504.00274|
|643|What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models|Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, Yufei Wang, Niklas Muennighoff, Irwin King, Xue Liu, Chen Ma|2025-03-31|arXiv|https://github.com/testtimescaling/testtimescaling.github.io/|https://doi.org/10.48550/arXiv.2503.24235|
|644|SciReplicate-Bench: Benchmarking LLMs in Agent-driven Algorithmic Reproduction from Research Papers|Yanzheng Xiang, Hanqi Yan, Shuyin Ouyang, Lin Gui, Yulan He|2025-03-31|arXiv|https://github.com/xyzCS/SciReplicate-Bench|http://arxiv.org/abs/2504.00255v1|
|645|ReaLM: Reliable and Efficient Large Language Model Inference with Statistical Algorithm-Based Fault Tolerance|Tong Xie, Jiawang Zhao, Zishen Wan, Zuodong Zhang, Yuan Wang, Runsheng Wang, Ru Huang, Meng Li|2025-03-31|arXiv|https://github.com/PKU-SEC-Lab/ReaLM_DAC25/|https://doi.org/10.48550/arXiv.2503.24053|
|646|Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models|Rui Wang, Hongru Wang, Boyang Xue, Jianhui Pang, Shudong Liu, Yi Chen, Jiahao Qiu, Derek Fai Wong, Heng Ji, Kam-Fai Wong|2025-03-31|arXiv|https://github.com/DevoAllen/Awesome-Reasoning-Economy-Papers|https://doi.org/10.48550/arXiv.2503.24377|
|647|A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?|Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Wenyue Hua, Haolun Wu, Zhihan Guo, Yufei Wang, Niklas Muennighoff, Irwin King, Xue Liu, Chen Ma|2025-03-31|arXiv|https://github.com/testtimescaling/testtimescaling.github.io/|http://arxiv.org/abs/2503.24235v3|
|648|LANID: LLM-assisted New Intent Discovery|Lu Fan, Jiashu Pu, Rongsheng Zhang, Xiao-Ming Wu|2025-03-31|arXiv|https://github.com/floatSDSDS/LANID|http://arxiv.org/abs/2503.23740v1|
|649|EagleVision: Object-level Attribute Multimodal LLM for Remote Sensing|Hongxiang Jiang, Jihao Yin, Qixiong Wang, Jiaqi Feng, Guo Chen|2025-03-30|arXiv|https://github.com/XiangTodayEatsWhat/EagleVision|http://arxiv.org/abs/2503.23330v1|
|650|Agentic Large Language Models, a survey|Aske Plaat, Max J. van Duijn, Niki van Stein, Mike Preuss, Peter van der Putten, Kees Joost Batenburg|2025-03-29|arXiv|https://askeplaat.github.io/agentic-llm-survey-site/|https://doi.org/10.48550/arXiv.2503.23037|
|651|Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions|Yubo Li, Yidi Miao, Xueying Ding, Ramayya Krishnan, Rema Padman|2025-03-28|arXiv|https://github.com/yubol-bobo/MT-Consistency|https://doi.org/10.48550/arXiv.2503.22353|
|652|Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models|Zhanke Zhou, Zhaocheng Zhu, Xuan Li, Mikhail Galkin, Xiao Feng, Sanmi Koyejo, Jian Tang, Bo Han|2025-03-28|arXiv|https://github.com/tmlr-group/landscape-of-thoughts|https://doi.org/10.48550/arXiv.2503.22165|
|653|MediTools -- Medical Education Powered by LLMs|Amr Alshatnawi, Remi Sampaleanu, David Liebovitz|2025-03-28|arXiv|https://github.com/NM-Streamlit-Team/meditools|http://arxiv.org/abs/2503.22769v1|
|654|A Refined Analysis of Massive Activations in LLMs|Louis Owen, Nilabhra Roy Chowdhury, Abhay Kumar, Fabian Güra|2025-03-28|arXiv|https://github.com/bluorion-com/refine_massive_activations|http://arxiv.org/abs/2503.22329v1|
|655|QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?|Belinda Z. Li, Been Kim, Zi Wang|2025-03-28|arXiv|https://github.com/google-deepmind/questbench|http://arxiv.org/abs/2503.22674v1|
|656|SWI: Speaking with Intent in Large Language Models|Yuwei Yin, EunJeong Hwang, Giuseppe Carenini|2025-03-27|arXiv|https://github.com/YuweiYin/SWI|https://doi.org/10.48550/arXiv.2503.21544|
|657|Ignite Forecasting with SPARK: An Efficient Generative Framework for Refining LLMs in Temporal Knowledge Graph Forecasting|Gongzhu Yin, Hongli Zhang, Yi Luo, Yuchen Yang, Kun Lu, Chao Meng|2025-03-27|arXiv|https://github.com/yin-gz/SPARK|http://arxiv.org/abs/2503.22748v1|
|658|Large Language Model Agent: A Survey on Methodology, Applications and Challenges|Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiao Luo, Wei Ju, Zhiping Xiao, Yifan Wang, Meng Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, Yiqiao Jin, Fan Zhang, Xian Wu, Hanqing Zhao, Dacheng Tao, Philip S. Yu, Ming Zhang|2025-03-27|arXiv|https://github.com/luo-junyu/Awesome-Agent-Papers|https://doi.org/10.48550/arXiv.2503.21460|
|659|Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap|Tong Nie, Jian Sun, Wei Ma|2025-03-27|arXiv|https://github.com/tongnie/awesome-llm4tr|https://doi.org/10.48550/arXiv.2503.21411|
|660|Playing the Fool: Jailbreaking LLMs and Multimodal LLMs with Out-of-Distribution Strategy|Joonhyun Jeong, Seyun Bae, Yeonsung Jung, Jaeryong Hwang, Eunho Yang|2025-03-26|arXiv|https://github.com/naver-ai/JOOD|http://arxiv.org/abs/2503.20823v1|
|661|Protecting Your Video Content: Disrupting Automated Video-based LLM Annotations|Haitong Liu, Kuofeng Gao, Yang Bai, Jinmin Li, Jinxiao Shan, Tao Dai, Shu-Tao Xia|2025-03-26|arXiv|https://github.com/ttthhl/Protecting_Your_Video_Content|http://arxiv.org/abs/2503.21824v1|
|662|Dynamic Pyramid Network for Efficient Multimodal Large Language Model|Hao Ai, Kunyi Wang, Zezhou Wang, Hao Lu, Jin Tian, Yaxin Luo, Peng Xing, Jen-Yuan Huang, Huaxia Li, Gen luo|2025-03-26|arXiv|https://github.com/aihao2000/DPN-LLaVA|https://doi.org/10.48550/arXiv.2503.20322|
|663|Leveraging Implicit Sentiments: Enhancing Reliability and Validity in Psychological Trait Evaluation of LLMs|Huanhuan Ma, Haisong Gong, Xiaoyuan Yi, Xing Xie, Dongkuan Xu|2025-03-26|arXiv|https://github.com/dependentsign/CSI|http://arxiv.org/abs/2503.20182v1|
|664|Enhancing the Robustness of LLM-Generated Code: Empirical Study and Framework|ZiKe Li, MingWei Liu, Anji Li, Kaifeng He, Yanlin Wang, Xin Peng, Zibin Zheng|2025-03-26|arXiv|https://github.com/SYSUSELab/RobGen|http://arxiv.org/abs/2503.20197v1|
|665|CoLLM: A Large Language Model for Composed Image Retrieval|Chuong Huynh, Jinyu Yang, Ashish Tawari, Mubarak Shah, Son Tran, Raffay Hamid, Trishul Chilimbi, Abhinav Shrivastava|2025-03-25|arXiv|https://collm-cvpr25.github.io/|https://doi.org/10.48550/arXiv.2503.19910|
|666|PAVE: Patching and Adapting Video Large Language Models|Zhuoming Liu, Yiquan Li, Khoi Duc Nguyen, Yiwu Zhong, Yin Li|2025-03-25|arXiv|https://github.com/dragonlzm/PAVE|https://doi.org/10.48550/arXiv.2503.19794|
|667|LLM-based Agent Simulation for Maternal Health Interventions: Uncertainty Estimation and Decision-focused Evaluation|Sarah Martinson, Lingkai Kong, Cheol Woo Kim, Aparna Taneja, Milind Tambe|2025-03-25|arXiv|https://github.com/sarahmart/LLM-ABS-ARMMAN-prediction|http://arxiv.org/abs/2503.22719v1|
|668|QUAD: Quantization and Parameter-Efficient Tuning of LLM with Activation Decomposition|Yuxuan Hu, Xiaodong Chen, Cuiping Li, Hong Chen, Jing Zhang|2025-03-25|arXiv|https://github.com/hyx1999/Quad|http://arxiv.org/abs/2503.19353v1|
|669|AgentDropout: Dynamic Agent Elimination for Token-Efficient and High-Performance LLM-Based Multi-Agent Collaboration|Zhexuan Wang, Yutong Wang, Xuebo Liu, Liang Ding, Miao Zhang, Jie Liu, Min Zhang|2025-03-24|arXiv|https://github.com/wangzx1219/AgentDropout|http://arxiv.org/abs/2503.18891v1|
|670|BitDecoding: Unlocking Tensor Cores for Long-Context LLMs Decoding with Low-Bit KV Cache|Dayou Du, Shijie Cao, Jianyi Cheng, Ting Cao, Mao Yang|2025-03-24|arXiv|https://github.com/DD-DuDa/BitDecoding|http://arxiv.org/abs/2503.18773v1|
|671|LLaVAction: evaluating and training multi-modal large language models for action recognition|Shaokai Ye, Haozhe Qi, Alexander Mathis, Mackenzie W. Mathis|2025-03-24|arXiv|https://github.com/AdaptiveMotorControlLab/LLaVAction|https://doi.org/10.48550/arXiv.2503.18712|
|672|I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders|Andrey V. Galichin, Alexey Dontsov, Polina Druzhinina, Anton Razzhigaev, Oleg Y. Rogov, Elena Tutubalina, Ivan V. Oseledets|2025-03-24|arXiv|https://github.com/AIRI-Institute/SAE-Reasoning|https://doi.org/10.48550/arXiv.2503.18878|
|673|CEFW: A Comprehensive Evaluation Framework for Watermark in Large Language Models|Shuhao Zhang, Bo Cheng, Jiale Han, Yuli Chen, Zhixuan Wu, Changbao Li, Pingli Gu|2025-03-24|arXiv|https://github.com/DrankXs/BalancedWatermark|https://doi.org/10.48550/arXiv.2503.20802|
|674|Will LLMs be Professional at Fund Investment? DeepFund: A Live Arena Perspective|Changlun Li, Yao Shi, Yuyu Luo, Nan Tang|2025-03-24|arXiv|https://github.com/HKUSTDial/DeepFund|http://arxiv.org/abs/2503.18313v2|
|675|Lost in Cultural Translation: Do LLMs Struggle with Math Across Cultural Contexts?|Aabid Karim, Abdul Karim, Bhoomika Lohana, Matt Keon, Jaswinder Singh, Abdul Sattar|2025-03-23|arXiv|https://github.com/akarim23131/Lost_in_Cultural_Translation|http://arxiv.org/abs/2503.18018v1|
|676|RAIDER: Tool-Equipped Large Language Model Agent for Robotic Action Issue Detection, Explanation and Recovery|Silvia Izquierdo-Badiola, Carlos Rizzo, Guillem Alenyà|2025-03-22|arXiv|https://raider-llmagent.github.io/|https://doi.org/10.48550/arXiv.2503.17703|
|677|Safe RLHF-V: Safe Reinforcement Learning from Human Feedback in Multimodal Large Language Models|Jiaming Ji, Xinyu Chen, Rui Pan, Han Zhu, Conghui Zhang, Jiahao Li, Donghai Hong, Boyuan Chen, Jiayi Zhou, Kaile Wang, Juntao Dai, Chi-Min Chan, Sirui Han, Yike Guo, Yaodong Yang|2025-03-22|arXiv|https://github.com/SafeRLHF-V|https://doi.org/10.48550/arXiv.2503.17682|
|678|Reasoning with LLMs for Zero-Shot Vulnerability Detection|Arastoo Zibaeirad, Marco Vieira|2025-03-22|arXiv|https://github.com/Erroristotle/VulnSage|http://arxiv.org/abs/2503.17885v1|
|679|TEMPLE:Temporal Preference Learning of Video LLMs via Difficulty Scheduling and Pre-SFT Alignment|Shicheng Li, Lei Li, Kun Ouyang, Shuhuai Ren, Yuanxin Liu, Yuanxing Zhang, Fuzheng Zhang, Lingpeng Kong, Qi Liu, Xu Sun|2025-03-21|arXiv|https://github.com/lscpku/TEMPLE|http://arxiv.org/abs/2503.16929v2|
|680|RustEvo^2: An Evolving Benchmark for API Evolution in LLM-based Rust Code Generation|Linxi Liang, Jing Gong, Mingwei Liu, Chong Wang, Guangsheng Ou, Yanlin Wang, Xin Peng, Zibin Zheng|2025-03-21|arXiv|https://github.com/SYSUSELab/RustEvo|http://arxiv.org/abs/2503.16922v1|
|681|Variance Control via Weight Rescaling in LLM Pre-training|Louis Owen, Abhay Kumar, Nilabhra Roy Chowdhury, Fabian Güra|2025-03-21|arXiv|https://github.com/bluorion-com/weight_rescaling|http://arxiv.org/abs/2503.17500v1|
|682|LLM+MAP: Bimanual Robot Task Planning using Large Language Models and Planning Domain Definition Language|Kun Chu, Xufeng Zhao, Cornelius Weber, Stefan Wermter|2025-03-21|arXiv|https://github.com/Kchu/LLM-MAP|https://doi.org/10.48550/arXiv.2503.17309|
|683|Leveraging Human Production-Interpretation Asymmetries to Test LLM Cognitive Plausibility|Suet-Ying Lam, Qingcheng Zeng, Jingyi Wu, Rob Voigt|2025-03-21|arXiv|https://github.com/LingMechLab/Production-Interpretation_Asymmetries_ACL2025|http://arxiv.org/abs/2503.17579v1|
|684|Dancing with Critiques: Enhancing LLM Reasoning with Stepwise Natural Language Self-Critique|Yansi Li, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Qiuzhi Liu, Rui Wang, Zhuosheng Zhang, Zhaopeng Tu, Haitao Mi, Dong Yu|2025-03-21|arXiv|https://github.com/puddingyeah/PANEL|http://arxiv.org/abs/2503.17363v1|
|685|LEMMA: Learning from Errors for MatheMatical Advancement in LLMs|Zhuoshi Pan, Yu Li, Honglin Lin, Qizhi Pei, Zinan Tang, Wei Wu, Chenlin Ming, H. Vicky Zhao, Conghui He, Lijun Wu|2025-03-21|arXiv|https://github.com/pzs19/LEMMA|http://arxiv.org/abs/2503.17439v1|
|686|Grammar and Gameplay-aligned RL for Game Description Generation with LLMs|Tsunehiko Tanaka, Edgar Simo-Serra|2025-03-20|arXiv|https://github.com/tsunehiko/rlgdg|http://arxiv.org/abs/2503.15783v1|
|687|MathFusion: Enhancing Mathematic Problem-solving of LLM through Instruction Fusion|Qizhi Pei, Lijun Wu, Zhuoshi Pan, Yu Li, Honglin Lin, Chenlin Ming, Xin Gao, Conghui He, Rui Yan|2025-03-20|arXiv|https://github.com/QizhiPei/mathfusion|http://arxiv.org/abs/2503.16212v1|
|688|Stop Overthinking: A Survey on Efficient Reasoning for Large Language Models|Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Hanjie Chen, Xia Ben Hu|2025-03-20|arXiv|https://github.com/Eclipsess/Awesome-Efficient-Reasoning-LLMs|https://doi.org/10.48550/arXiv.2503.16419|
|689|Fin-R1: A Large Language Model for Financial Reasoning through Reinforcement Learning|Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, Chao Li, Sheng Xu, Dezhi Chen, Yun Chen, Zuo Bai, Liwen Zhang|2025-03-20|arXiv|https://github.com/SUFE-AIFLM-Lab/Fin-R1|https://doi.org/10.48550/arXiv.2503.16252|
|690|Reinforcement Learning for Reasoning in Small LLMs: What Works and What Doesn't|Quy-Anh Dang, Chris Ngo|2025-03-20|arXiv|https://github.com/knoveleng/open-rs|http://arxiv.org/abs/2503.16219v1|
|691|The Emperor's New Clothes in Benchmarking? A Rigorous Examination of Mitigation Strategies for LLM Benchmark Data Contamination|Yifan Sun, Han Wang, Dongbai Li, Gang Wang, Huan Zhang|2025-03-20|arXiv|https://github.com/ASTRAL-Group/BDC_mitigation_assessment|http://arxiv.org/abs/2503.16402v1|
|692|Hybrid-Level Instruction Injection for Video Token Compression in Multi-modal Large Language Models|Zhihang Liu, Chen-Wei Xie, Pandeng Li, Liming Zhao, Longxiang Tang, Yun Zheng, Chuanbin Liu, Hongtao Xie|2025-03-20|arXiv|https://github.com/lntzm/HICom|https://doi.org/10.48550/arXiv.2503.16036|
|693|VisNumBench: Evaluating Number Sense of Multimodal Large Language Models|Tengjin Weng, Jingyi Wang, Wenhao Jiang, Zhong Ming|2025-03-19|arXiv|https://wwwtttjjj.github.io/VisNumBench/|https://doi.org/10.48550/arXiv.2503.14939|
|694|LLaVA-MORE: A Comparative Study of LLMs and Visual Backbones for Enhanced Visual Instruction Tuning|Federico Cocchi, Nicholas Moratelli, Davide Caffagni, Sara Sarto, Lorenzo Baraldi, Marcella Cornia, Rita Cucchiara|2025-03-19|arXiv|https://github.com/aimagelab/LLaVA-MORE|http://arxiv.org/abs/2503.15621v1|
|695|Exploring Large Language Models for Word Games:Who is the Spy?|Chentian Wei, Jiewei Chen, Jinzhu Xu|2025-03-19|arXiv|https://github.com/ct-wei/Who-is-The-Spy|https://doi.org/10.48550/arXiv.2503.15235|
|696|Learning on LLM Output Signatures for gray-box Behavior Analysis|Guy Bar-Shalom, Fabrizio Frasca, Derek Lim, Yoav Gelberg, Yftah Ziser, Ran El-Yaniv, Gal Chechik, Haggai Maron|2025-03-18|arXiv|https://github.com/BarSGuy/LLM-Output-Signatures-Network|http://arxiv.org/abs/2503.14043v2|
|697|SpaceVLLM: Endowing Multimodal Large Language Model with Spatio-Temporal Video Grounding Capability|Jiankang Wang, Zhihan Zhang, Zhihang Liu, Yang Li, Jiannan Ge, Hongtao Xie, Yongdong Zhang|2025-03-18|arXiv|https://github.com/Jayce1kk/SpaceVLLM|https://doi.org/10.48550/arXiv.2503.13983|
|698|Word2Minecraft: Generating 3D Game Levels through Large Language Models|Shuo Huang, Muhammad Umair Nasir, Steven James, Julian Togelius|2025-03-18|arXiv|https://github.com/JMZ-kk/Word2Minecraft/tree/word2mc_v0|https://doi.org/10.48550/arXiv.2503.16536|
|699|Aligning Multimodal LLM with Human Preference: A Survey|Tao Yu, Yi-Fan Zhang, Chaoyou Fu, Junkang Wu, Jinda Lu, Kun Wang, Xingyu Lu, Yunhang Shen, Guibin Zhang, Dingjie Song, Yibo Yan, Tianlong Xu, Qingsong Wen, Zhang Zhang, Yan Huang, Liang Wang, Tieniu Tan|2025-03-18|arXiv|https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment|http://arxiv.org/abs/2503.14504v1|
|700|CodingGenie: A Proactive LLM-Powered Programming Assistant|Sebastian Zhao, Alan Zhu, Hussein Mozannar, David Sontag, Ameet Talwalkar, Valerie Chen|2025-03-18|arXiv|https://github.com/sebzhao/CodingGenie/|http://arxiv.org/abs/2503.14724v1|
|701|Improving LLM Video Understanding with 16 Frames Per Second|Yixuan Li, Changli Tang, Jimin Zhuang, Yudong Yang, Guangzhi Sun, Wei Li, Zejun Ma, Chao Zhang|2025-03-18|arXiv|https://github.com/bytedance/F-16|http://arxiv.org/abs/2503.13956v1|
|702|HiDe-LLaVA: Hierarchical Decoupling for Continual Instruction Tuning of Multimodal Large Language Model|Haiyang Guo, Fanhu Zeng, Ziwei Xiang, Fei Zhu, Da-Han Wang, Xu-Yao Zhang, Cheng-Lin Liu|2025-03-17|arXiv|https://github.com/Ghy0501/HiDe-LLaVA|https://doi.org/10.48550/arXiv.2503.12941|
|703|NuPlanQA: A Large-Scale Dataset and Benchmark for Multi-View Driving Scene Understanding in Multi-Modal Large Language Models|Sung-Yeon Park, Can Cui, Yunsheng Ma, Ahmadreza Moradipari, Rohit Gupta, Kyungtae Han, Ziran Wang|2025-03-17|arXiv|https://github.com/sungyeonparkk/NuPlanQA|https://doi.org/10.48550/arXiv.2503.12772|
|704|Aligning Vision to Language: Text-Free Multimodal Knowledge Graph Construction for Enhanced LLMs Reasoning|Junming Liu, Siyuan Meng, Yanting Gao, Song Mao, Pinlong Cai, Guohang Yan, Yirong Chen, Zilin Bian, Botian Shi, Ding Wang|2025-03-17|arXiv|https://github.com/Wings-Of-Disaster/VaLiK|http://arxiv.org/abs/2503.12972v1|
|705|Omnia de EgoTempo: Benchmarking Temporal Understanding of Multi-Modal LLMs in Egocentric Videos|Chiara Plizzari, Alessio Tonioni, Yongqin Xian, Achin Kulshrestha, Federico Tombari|2025-03-17|arXiv|https://github.com/google-research-datasets/egotempo|http://arxiv.org/abs/2503.13646v1|
|706|xLSTM 7B: A Recurrent LLM for Fast and Efficient Inference|Maximilian Beck, Korbinian Pöppel, Phillip Lippe, Richard Kurle, Patrick M. Blies, Günter Klambauer, Sebastian Böck, Sepp Hochreiter|2025-03-17|arXiv|https://github.com/NX-AI/xlstm|http://arxiv.org/abs/2503.13427v1|
|707|A Survey on the Memory Mechanism of Large Language Model based Agents|Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen|2025-03-16|arXiv|https://github.com/nuster1128/LLM_Agent_Memory_Survey|https://doi.org/10.48550/arXiv.2404.13501|
|708|Plausibility Vaccine: Injecting LLM Knowledge for Event Plausibility|Jacob Chmura, Jonah Dauvet, Sebastian Sabry|2025-03-16|arXiv|https://github.com/Jacob-Chmura/plausibility-vaccine|http://arxiv.org/abs/2503.12667v1|
|709|SVD-LLM V2: Optimizing Singular Value Truncation for Large Language Model Compression|Xin Wang, Samiul Alam, Zhongwei Wan, Hui Shen, Mi Zhang|2025-03-16|arXiv|https://github.com/AIoT-MLSys-Lab/SVD-LLM|https://doi.org/10.48550/arXiv.2503.12340|
|710|HKCanto-Eval: A Benchmark for Evaluating Cantonese Language Understanding and Cultural Comprehension in LLMs|Tsz Chung Cheng, Chung Shing Cheng, Chaak Ming Lau, Eugene Tin-Ho Lam, Chun Yat Wong, Hoi On Yu, Cheuk Hei Chong|2025-03-16|arXiv|https://github.com/hon9kon9ize/hkeval2025|http://arxiv.org/abs/2503.12440v1|
|711|FAILS: A Framework for Automated Collection and Analysis of LLM Service Incidents|Sándor Battaglini-Fischer, Nishanthi Srinivasan, Bálint László Szarvas, Xiaoyu Chu, Alexandru Iosup|2025-03-15|HotCloudPerf 2025|https://github.com/atlarge-research/FAILS|http://arxiv.org/abs/2503.12185v1|
|712|MT-RewardTree: A Comprehensive Framework for Advancing LLM-Based Machine Translation via Reward Modeling|Zhaopeng Feng, Jiahan Ren, Jiayuan Su, Jiamei Zheng, Zhihang Tang, Hongwei Wang, Zuozhu Liu|2025-03-15|arXiv|https://sabijun.github.io/MT_RewardTreePage|http://arxiv.org/abs/2503.12123v1|
|713|A Survey on Federated Fine-tuning of Large Language Models|Yebo Wu, Chunlin Tian, Jingguang Li, He Sun, Kahou Tam, Zhanting Zhou, Haicheng Liao, Zhijiang Guo, Li Li, Chengzhong Xu|2025-03-15|arXiv|https://github.com/Clin0212/Awesome-Federated-LLM-Learning|https://doi.org/10.48550/arXiv.2503.12016|
|714|An LLM-Integrated Framework for Completion, Management, and Tracing of STPA|Ali Raeisdanaei, Juho Kim, Michael Liao, Sparsh Kochhar|2025-03-15|arXiv|https://github.com/blueskysolarracing/stpa|http://arxiv.org/abs/2503.12043v1|
|715|FastVID: Dynamic Density Pruning for Fast Video Large Language Models|Leqi Shen, Guoqiang Gong, Tao He, Yifeng Zhang, Pengzhang Liu, Sicheng Zhao, Guiguang Ding|2025-03-14|arXiv|https://github.com/LunarShen/FastVID|https://doi.org/10.48550/arXiv.2503.11187|
|716|CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning|Hao Cui, Zahra Shamsi, Gowoon Cheon, Xuejian Ma, Shutong Li, Maria Tikhanovskaya, Peter Norgaard, Nayantara Mudur, Martyna Plomecka, Paul Raccuglia, Yasaman Bahri, Victor V. Albert, Pranesh Srinivasan, Haining Pan, Philippe Faist, Brian Rohr, Ekin Dogus Cubuk, Muratahan Aykol, Amil Merchant, Michael J. Statt, Dan Morris, Drew Purves, Elise Kleeman, Ruth Alcantara, Matthew Abraham, Muqthar Mohammad, Ean Phing VanLee, Chenfei Jiang, Elizabeth Dorfman, Eun-Ah Kim, Michael P Brenner, Viren Jain, Sameera Ponda, Subhashini Venugopalan|2025-03-14|arXiv|https://github.com/google/curie|http://arxiv.org/abs/2503.13517v2|
|717|Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space|Weichen Zhang, Zile Zhou, Zhiheng Zheng, Chen Gao, Jinqiang Cui, Yong Li, Xinlei Chen, Xiao-Ping Zhang|2025-03-14|arXiv|https://github.com/WeichenZh/Open3DVQA|https://doi.org/10.48550/arXiv.2503.11094|
|718|Broaden your SCOPE! Efficient Multi-turn Conversation Planning for LLMs using Semantic Space|Zhiliang Chen, Xinyuan Niu, Chuan-Sheng Foo, Bryan Kian Hsiang Low|2025-03-14|arXiv|https://github.com/chenzhiliang94/convo-plan-SCOPE|http://arxiv.org/abs/2503.11586v1|
|719|ASMA-Tune: Unlocking LLMs' Assembly Code Comprehension via Structural-Semantic Instruction Tuning|Xinyi Wang, Jiashui Wang, Jinbo Su, Ke Wang, Peng Chen, Yanming Liu, Long Liu, Xiang Li, Yangdong Wang, Qiyuan Chen, Rongze Chen, Chunfu Jia|2025-03-14|arXiv|https://github.com/wxy3596/ASMA-Tune|http://arxiv.org/abs/2503.11617v2|
|720|MMS-LLaMA: Efficient LLM-based Audio-Visual Speech Recognition with Minimal Multimodal Speech Tokens|Jeong Hun Yeo, Hyeongseop Rha, Se Jin Park, Yong Man Ro|2025-03-14|arXiv|https://github.com/JeongHun0716/MMS-LLaMA|http://arxiv.org/abs/2503.11315v1|
|721|Learning to Inference Adaptively for Multimodal Large Language Models|Zhuoyan Xu, Khoi Duc Nguyen, Preeti Mukherjee, Saurabh Bagchi, Somali Chaterji, Yingyu Liang, Yin Li|2025-03-13|arXiv|https://zhuoyan-xu.github.io/ada-llava/|https://doi.org/10.48550/arXiv.2503.10905|
|722|ZeroMerge: Parameter-Free KV Cache Compression for Memory-Efficient Long-Context LLMs|Xin Liu, Pei Liu, Guoming Tang|2025-03-13|arXiv|https://github.com/SusCom-Lab/ZeroMerge|http://arxiv.org/abs/2503.10714v1|
|723|ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient Long-Context LLMs|Xin Liu, Pei Liu, Guoming Tang|2025-03-13|arXiv|https://github.com/SusCom-Lab/ZSMerge|http://arxiv.org/abs/2503.10714v2|
|724|TokenCarve: Information-Preserving Visual Token Compression in Multimodal Large Language Models|Xudong Tan, Peng Ye, Chongjun Tu, Jianjian Cao, Yaoxin Yang, Lin Zhang, Dongzhan Zhou, Tao Chen|2025-03-13|arXiv|https://github.com/ShawnTan86/TokenCarve|https://doi.org/10.48550/arXiv.2503.10501|
|725|Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs|Ariba Khan, Stephen Casper, Dylan Hadfield-Menell|2025-03-13|arXiv:2503.08688, 2025|https://github.com/ariba-k/llm-cultural-alignment-evaluation|http://arxiv.org/abs/2503.08688v1|
|726|OR-LLM-Agent: Automating Modeling and Solving of Operations Research Optimization Problem with Reasoning Large Language Model|Bowen Zhang, Pengcheng Luo|2025-03-13|arXiv|https://github.com/bwz96sco/or_llm_agent|https://doi.org/10.48550/arXiv.2503.10009|
|727|4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models|Wanhua Li, Renping Zhou, Jiawei Zhou, Yingwei Song, Johannes Herter, Minghan Qin, Gao Huang, Hanspeter Pfister|2025-03-13|arXiv|https://4d-langsplat.github.io|https://doi.org/10.48550/arXiv.2503.10437|
|728|RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs|Zhongzhan Huang, Guoming Ling, Vincent S. Liang, Yupei Lin, Yandong Chen, Shanshan Zhong, Hefeng Wu, Liang Lin|2025-03-13|GoogleScholar|https://github.com/MilkThink-Lab/RouterEval|http://arxiv.org/abs/2503.10657v1|
|729|Route Sparse Autoencoder to Interpret Large Language Models|Wei Shi, Sihang Li, Tao Liang, Mingyang Wan, Guojun Ma, Xiang Wang, Xiangnan He|2025-03-13|arXiv|https://github.com/swei2001/RouteSAEs|https://doi.org/10.48550/arXiv.2503.08200|
|730|DynaCode: A Dynamic Complexity-Aware Code Benchmark for Evaluating Large Language Models in Code Generation|Wenhao Hu, Jinhao Duan, Chunchen Wei, Li Zhang, Yue Zhang, Kaidi Xu|2025-03-13|arXiv|https://github.com/HWH-2000/DynaCode|https://doi.org/10.48550/arXiv.2503.10452|
|731|Probing LLMs for Multilingual Discourse Generalization Through a Unified Label Set|Florian Eichin, Yang Janet Liu, Barbara Plank, Michael A. Hedderich|2025-03-13|arXiv|https://github.com/mainlp/discourse_probes|http://arxiv.org/abs/2503.10515v1|
|732|Adapting Large Language Models for Parameter-Efficient Log Anomaly Detection|Ying Fu Lim, Jiawen Zhu, Guansong Pang|2025-03-13|arXiv|https://github.com/mala-lab/LogADReft|https://doi.org/10.48550/arXiv.2503.08045|
|733|Towards Next-Generation Recommender Systems: A Benchmark for Personalized Recommendation Assistant with LLMs|Jiani Huang, Shijie Wang, Liang-bo Ning, Wenqi Fan, Shuaiqiang Wang, Dawei Yin, Qing Li|2025-03-12|arXiv|https://github.com/jiani-huang/RecBench|http://arxiv.org/abs/2503.09382v1|
|734|RetSTA: An LLM-Based Approach for Standardizing Clinical Fundus Image Reports|Jiushen Cai, Weihang Zhang, Hanruo Liu, Ningli Wang, Huiqi Li|2025-03-12|arXiv|https://github.com/AB-Story/RetSTA-7B|http://arxiv.org/abs/2503.09358v1|
|735|Learning to Contextualize Web Pages for Enhanced Decision Making by LLM Agents|Dongjun Lee, Juyong Lee, Kyuyoung Kim, Jihoon Tack, Jinwoo Shin, Yee Whye Teh, Kimin Lee|2025-03-12|arXiv|https://lcowiclr2025.github.io|http://arxiv.org/abs/2503.10689v1|
|736|CyberLLMInstruct: A New Dataset for Analysing Safety of Fine-Tuned LLMs Using Cyber Security Data|Adel ElZemity, Budi Arief, Shujun Li|2025-03-12|arXiv|https://github.com/Adelsamir01/CyberLLMInstruct|http://arxiv.org/abs/2503.09334v1|
|737|CASTLE: Benchmarking Dataset for Static Code Analyzers and LLMs towards CWE Detection|Richard A. Dubniczky, Krisztofer Zoltán Horvát, Tamás Bisztray, Mohamed Amine Ferrag, Lucas C. Cordeiro, Norbert Tihanyi|2025-03-12|arXiv|https://github.com/CASTLE-Benchmark|http://arxiv.org/abs/2503.09433v1|
|738|Search-R1: Training LLMs to Reason and Leverage Search Engines with Reinforcement Learning|Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, Jiawei Han|2025-03-12|arXiv|https://github.com/PeterGriffinJin/Search-R1|http://arxiv.org/abs/2503.09516v1|
|739|NVP-HRI: Zero shot natural voice and posture-based human-robot interaction via large language model|Yuzhi Lai, Shenghai Yuan, Youssef Nassar, Mingyu Fan, Thomas Weber, Matthias Rätsch|2025-03-12|Expert Syst. Appl.|https://github.com/laiyuzhi/NVP-HRI|https://doi.org/10.1016/j.eswa.2024.126360|
|740|BYOS: Knowledge-driven Large Language Models Bring Your Own Operating System More Excellent|Hongyu Lin, Yuchen Li, Haoran Luo, Kaichun Yao, Libo Zhang, Mingjie Xing, Yanjun Wu|2025-03-12|arXiv|https://github.com/LHY-24/BYOS|https://doi.org/10.48550/arXiv.2503.09663|
|741|Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models|Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, Wangxiang Che|2025-03-12|arXiv|https://long-cot.github.io/|https://doi.org/10.48550/arXiv.2503.09567|
|742|MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV Product Quantization|Zongwu Wang, Peng Xu, Fangxin Liu, Yiwei Hu, Qingxiao Sun, Gezi Li, Cheng Li, Xuan Wang, Li Jiang, Haibing Guan|2025-03-12|arXiv|https://github.com/ZongwuWang/MILLION|http://arxiv.org/abs/2504.03661v1|
|743|ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning|Ziyu Wan, Yunxiang Li, Xiaoyu Wen, Yan Song, Hanjing Wang, Linyi Yang, Mark Schmidt, Jun Wang, Weinan Zhang, Shuyue Hu, Ying Wen|2025-03-12|arXiv|https://github.com/ziyuwan/ReMA-public|http://arxiv.org/abs/2503.09501v3|
|744|What's In Your Field? Mapping Scientific Research with Knowledge Graphs and Large Language Models|Abhipsha Das, Nicholas Lourie, Siavash Golkar, Mariel Pettee|2025-03-12|arXiv|https://github.com/chiral-carbon/kg-for-science|http://arxiv.org/abs/2503.09894v1|
|745|V2Flow: Unifying Visual Tokenization and Large Language Model Vocabularies for Autoregressive Image Generation|Guiwei Zhang, Tianyu Zhang, Mohan Zhou, Yalong Bai, Biye Li|2025-03-11|arXiv|https://github.com/zhangguiwei610/V2Flow|https://doi.org/10.48550/arXiv.2503.07493|
|746|ResMoE: Space-efficient Compression of Mixture of Experts LLMs via Residual Restoration|Mengting Ai, Tianxin Wei, Yifan Chen, Zhichen Zeng, Ritchie Zhao, Girish Varatkar, Bita Darvish Rouhani, Xianfeng Tang, Hanghang Tong, Jingrui He|2025-03-11|arXiv …, 2025|https://github.com/iDEA-iSAIL-Lab-UIUC/ResMoE|http://arxiv.org/abs/2503.06881v1|
|747|Graphormer-Guided Task Planning: Beyond Static Rules with LLM Safety Perception|Wanjing Huang, Tongjie Pan, Yalan Ye|2025-03-11|arXiv:2503.06866, 2025|https://github.com/hwj20/GGTP|http://arxiv.org/abs/2503.06866v1|
|748|Enhancing Large Language Models for Hardware Verification: A Novel SystemVerilog Assertion Dataset|Anand Menon, Samit S. Miftah, Shamik Kundu, Souvik Kundu, Amisha Srivastava, Arnab Raha, Gabriel Theodor Sonnenschein, Suvadeep Banerjee, Deepak Mathaikutty, Kanad Basu|2025-03-11|arXiv|https://github.com/AnandMenon12/VERT|https://doi.org/10.48550/arXiv.2503.08923|
|749|Process-Supervised LLM Recommenders via Flow-guided Tuning|Chongming Gao, Mengyao Gao, Chenxiao Fan, Shuai Yuan, Wentao Shi, Xiangnan He|2025-03-11|arXiv …, 2025|https://github.com/Mr-Peach0301/Flower|http://arxiv.org/abs/2503.07377v1|
|750|DistiLLM-2: A Contrastive Approach Boosts the Distillation of LLMs|Jongwoo Ko, Tianyi Chen, Sungnyun Kim, Tianyu Ding, Luming Liang, Ilya Zharkov, Se-Young Yun|2025-03-11|arXiv …, 2025|https://github.com/jongwooko/distillm-2|http://arxiv.org/abs/2503.07067v1|
|751|Roamify: Designing and Evaluating an LLM Based Google Chrome Extension for Personalised Itinerary Planning|Vikranth Udandarao, Noel Abraham Tiju, Muthuraj Vairamuthu, Harsh Mistry, Dhruv Kumar|2025-03-10|arXiv|https://github.com/Roamify-Research/Roamify|http://arxiv.org/abs/2504.10489v1|
|752|Beyond Decoder-only: Large Language Models Can be Good Encoders for Machine Translation|Yingfeng Luo, Tong Zheng, Yongyu Mu, Bei Li, Qinghong Zhang, Yongqi Gao, Ziqiang Xu, Peinan Feng, Xiaoqian Liu, Tong Xiao, Jingbo Zhu|2025-03-09|arXiv|https://github.com/NiuTrans/LaMaTE|https://doi.org/10.48550/arXiv.2503.06594|
|753|InftyThink: Breaking the Length Limits of Long-Context Reasoning in Large Language Models|Yuchen Yan, Yongliang Shen, Yang Liu, Jin Jiang, Mengdi Zhang, Jian Shao, Yueting Zhuang|2025-03-09|arXiv|https://zju-real.github.io/InftyThink|https://doi.org/10.48550/arXiv.2503.06692|
|754|AutoMisty: A Multi-Agent LLM Framework for Automated Code Generation in the Misty Social Robot|Xiao Wang, Lu Dong, Sahana Rangasrinivasan, Ifeoma Nwogu, Srirangaraj Setlur, Venugopal Govindaraju|2025-03-09|arXiv|https://wangxiaoshawn.github.io/AutoMisty.html|http://arxiv.org/abs/2503.06791v1|
|755|How LLMs Learn: Tracing Internal Representations with Sparse Autoencoders|Tatsuro Inaba, Kentaro Inui, Yusuke Miyao, Yohei Oseki, Benjamin Heinzerling, Yu Takagi|2025-03-09|arXiv|https://github.com/llm-jp/llm-jp-sae|http://arxiv.org/abs/2503.06394v1|
|756|DSGBench: A Diverse Strategic Game Benchmark for Evaluating LLM-based Agents in Complex Decision-Making Environments|Wenjie Tang, Yuan Zhou, Erqiang Xu, Keyan Cheng, Minne Li, Liquan Xiao|2025-03-08|arXiv|https://github.com/DeciBrain-Group/DSGBench|http://arxiv.org/abs/2503.06047v1|
|757|Multi-Layer Visual Feature Fusion in Multimodal LLMs: Methods, Analysis, and Best Practices|Junyan Lin, Haoran Chen, Yue Fan, Yingqi Fan, Xin Jin, Hui Su, Jinlan Fu, Xiaoyu Shen|2025-03-08|arXiv|https://github.com/EIT-NLP/Layer_Select_Fuse_for_MLLM|http://arxiv.org/abs/2503.06063v1|
|758|SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?|Xudong Lu, Haohao Gao, Renshou Wu, Shuai Ren, Xiaoxin Chen, Hongsheng Li, Fangyuan Li|2025-03-08|arXiv|https://github.com/Lucky-Lance/SmartBench|http://arxiv.org/abs/2503.06029v1|
|759|Optimizing LLM Inference Throughput via Memory-aware and SLA-constrained Dynamic Batching|Bowen Pang, Kai Li, Feifan Wang|2025-03-07|arXiv|https://github.com/KevinLee1110/dynamic-batching|http://arxiv.org/abs/2503.05248v1|
|760|RocketEval: Efficient Automated LLM Evaluation via Grading Checklist|Tianjun Wei, Wei Wen, Ruizhi Qiao, Xing Sun, Jianghong Ma|2025-03-07|arXiv|https://github.com/Joinn99/RocketEval-ICLR|http://arxiv.org/abs/2503.05142v1|
|761|A Survey of Large Language Model Empowered Agents for Recommendation and Search: Towards Next-Generation Information Retrieval|Yu Zhang, Shutong Qiao, Jiaqi Zhang, Tzu-Heng Lin, Chen Gao, Yong Li|2025-03-07|arXiv|https://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search|https://doi.org/10.48550/arXiv.2503.05659|
|762|Sketch-of-Thought: Efficient LLM Reasoning with Adaptive Cognitive-Inspired Sketching|Simon A. Aytes, Jinheon Baek, Sung Ju Hwang|2025-03-07|arXiv|https://www.github.com/SimonAytes/SoT|http://arxiv.org/abs/2503.05179v1|
|763|TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for LLM-as-a-Judge|Cheng-Han Chiang, Hung-yi Lee, Michal Lukasik|2025-03-06|arXiv|https://github.com/d223302/TRACT|http://arxiv.org/abs/2503.04381v1|
|764|Insights from Rights and Wrongs: A Large Language Model for Solving Assertion Failures in RTL Design|Jie Zhou, Youshu Ji, Ning Wang, Yuchen Hu, Xinyao Jiao, Bingkun Yao, Xinwei Fang, Shuai Zhao, Nan Guan, Zhe Jiang|2025-03-06|arXiv|https://github.com/SEU-ACAL/reproduce-AssertSolver-DAC-25|https://doi.org/10.48550/arXiv.2503.04057|
|765|Keeping Yourself is Important in Downstream Tuning Multimodal Large Language Model|Wenke Huang, Jian Liang, Xianda Guo, Yiyang Fang, Guancheng Wan, Xuankun Rong, Chi Wen, Zekun Shi, Qingyun Li, Didi Zhu, Yanbiao Ma, Ke Liang, Bin Yang, He Li, Jiawei Shao, Mang Ye, Bo Du|2025-03-06|arXiv|https://github.com/WenkeHuang/Awesome-MLLM-Tuning|https://doi.org/10.48550/arXiv.2503.04543|
|766|Compositional Translation: A Novel LLM-based Approach for Low-resource Machine Translation|Armel Zebaze, Benoît Sagot, Rachel Bawden|2025-03-06|arXiv|https://github.com/ArmelRandy/compositional-translation|http://arxiv.org/abs/2503.04554v1|
|767|DSV-LFS: Unifying LLM-Driven Semantic Cues with Visual Features for Robust Few-Shot Segmentation|Amin Karimi, Charalambos Poullis|2025-03-06|arXiv|https://github.com/aminpdik/DSV-LFS|http://arxiv.org/abs/2503.04006v1|
|768|Disparities in LLM Reasoning Accuracy and Explanations: A Case Study on African American English|Runtao Zhou, Guangya Wan, Saadia Gabriel, Sheng Li, Alexander J Gates, Maarten Sap, Thomas Hartvigsen|2025-03-06|arXiv|https://github.com/Runtaozhou/dialect_bias_eval|http://arxiv.org/abs/2503.04099v1|
|769|Lost in Literalism: How Supervised Training Shapes Translationese in LLMs|Yafu Li, Ronghao Zhang, Zhilin Wang, Huajian Zhang, Leyang Cui, Yongjing Yin, Tong Xiao, Yue Zhang|2025-03-06|arXiv|https://github.com/yafuly/LLM_Translationese|http://arxiv.org/abs/2503.04369v1|
|770|Predictable Scale: Part I - Optimal Hyperparameter Scaling Law in Large Language Model Pretraining|Houyi Li, Wenzheng Zheng, Jingcheng Hu, Qiufeng Wang, Hanshan Zhang, Zili Wang, Shijie Xuyang, Yuantao Fan, Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang|2025-03-06|arXiv|https://step-law.github.io/|https://doi.org/10.48550/arXiv.2503.04715|
|771|Multi-Agent Systems Powered by Large Language Models: Applications in Swarm Intelligence|Cristian Jimenez-Romero, Alper Yegenoglu, Christian Blum|2025-03-05|arXiv|https://github.com/crjimene/swarm_gpt|https://doi.org/10.48550/arXiv.2503.03800|
|772|LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph Foundation Models|Xi Zhu, Haochen Xue, Ziwei Zhao, Wujiang Xu, Jingyuan Huang, Minghao Guo, Qifan Wang, Kaixiong Zhou, Yongfeng Zhang|2025-03-05|arXiv|https://github.com/agiresearch/PromptGFM|http://arxiv.org/abs/2503.03313v1|
|773|Improving LLM Safety Alignment with Dual-Objective Optimization|Xuandong Zhao, Will Cai, Tianneng Shi, David Huang, Licong Lin, Song Mei, Dawn Song|2025-03-05|arXiv|https://github.com/wicai24/DOOR-Alignment|http://arxiv.org/abs/2503.03710v1|
|774|LeRAAT: LLM-Enabled Real-Time Aviation Advisory Tool|Marc R. Schlichting, Vale Rasmussen, Heba Alazzeh, Houjun Liu, Kiana Jafari, Amelia F. Hardy, Dylan M. Asmar, Mykel J. Kochenderfer|2025-03-05|arXiv|https://github.com/sisl/LeRAAT/|http://arxiv.org/abs/2503.16477v1|
|775|AttackSeqBench: Benchmarking Large Language Models&apos; Understanding of Sequential Patterns in Cyber Attacks|Javier Yong, Haokai Ma, Yunshan Ma, Anis Yusof, Zhenkai Liang, Ee-Chien Chang|2025-03-05|arXiv|https://github.com/Javiery3889/AttackSeqBench|https://doi.org/10.48550/arXiv.2503.03170|
|776|Haste Makes Waste: Evaluating Planning Abilities of LLMs for Efficient and Feasible Multitasking with Time Constraints Between Actions|Zirui Wu, Xiao Liu, Jiayi Li, Lingpeng Kong, Yansong Feng|2025-03-04|arXiv|https://github.com/WilliamZR/Recipe2Plan|http://arxiv.org/abs/2503.02238v1|
|777|Shakespearean Sparks: The Dance of Hallucination and Creativity in LLMs' Decoding Layers|Zicong He, Boxuan Zhang, Lu Cheng|2025-03-04|arXiv|https://github.com/ZicongHe2002/HCL-Spark|http://arxiv.org/abs/2503.02851v1|
|778|Mask-DPO: Generalizable Fine-grained Factuality Alignment of LLMs|Yuzhe Gu, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen|2025-03-04|arXiv|https://github.com/open-compass/ANAH|http://arxiv.org/abs/2503.02846v1|
|779|It Helps to Take a Second Opinion: Teaching Smaller LLMs to Deliberate Mutually via Selective Rationale Optimisation|Sohan Patnaik, Milan Aggarwal, Sumit Bhatia, Balaji Krishnamurthy|2025-03-04|arXiv|https://github.com/Sohanpatnaik106/coalition|http://arxiv.org/abs/2503.02463v1|
|780|Wikipedia in the Era of LLMs: Evolution and Risks|Siming Huang, Yuliang Xu, Mingmeng Geng, Yao Wan, Dongping Chen|2025-03-04|arXiv|https://github.com/HSM316/LLM_Wikipedia|http://arxiv.org/abs/2503.02879v1|
|781|Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual Attention for Multimodal LLMs|Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi|2025-03-04|arXiv|https://github.com/sony/aki|http://arxiv.org/abs/2503.02597v1|
|782|LoRA-Null: Low-Rank Adaptation via Null Space for Large Language Models|Pengwei Tang, Yong Liu, Dongjie Zhang, Xing Wu, Debing Zhang|2025-03-04|arXiv|https://github.com/HungerPWAY/LoRA-Null|https://doi.org/10.48550/arXiv.2503.02659|
|783|ReSo: A Reward-driven Self-organizing LLM-based Multi-Agent System for Reasoning Tasks|Heng Zhou, Hejia Geng, Xiangyuan Xue, Li Kang, Yiran Qin, Zhiyong Wang, Zhenfei Yin, Lei Bai|2025-03-04|arXiv|https://github.com/hengzzzhou/ReSo|http://arxiv.org/abs/2503.02390v3|
|784|Measuring What Makes You Unique: Difference-Aware User Modeling for Enhancing LLM Personalization|Yilun Qiu, Xiaoyan Zhao, Yang Zhang, Yimeng Bai, Wenjie Wang, Hong Cheng, Fuli Feng, Tat-Seng Chua|2025-03-04|arXiv|https://github.com/SnowCharmQ/DPL|http://arxiv.org/abs/2503.02450v1|
|785|PromptCoT: Synthesizing Olympiad-level Problems for Mathematical Reasoning in Large Language Models|Xueliang Zhao, Wei Wu, Jian Guan, Lingpeng Kong|2025-03-04|arXiv|https://github.com/zhaoxlpku/PromptCoT|https://doi.org/10.48550/arXiv.2503.02324|
|786|OptMetaOpenFOAM: Large Language Model Driven Chain of Thought for Sensitivity Analysis and Parameter Optimization based on CFD|Yuxuan Chen, Long Zhang, Xu Zhu, Hua Zhou, Zhuyin Ren|2025-03-03|arXiv|https://github.com/Terry-cyx/MetaOpenFOAM|https://doi.org/10.48550/arXiv.2503.01273|
|787|Word Form Matters: LLMs' Semantic Reconstruction under Typoglycemia|Chenxi Wang, Tianle Gu, Zhongyu Wei, Lang Gao, Zirui Song, Xiuying Chen|2025-03-03|arXiv|https://github.com/Aurora-cx/TypoLLM|http://arxiv.org/abs/2503.01714v1|
|788|Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens|Xinsheng Wang, Mingqi Jiang, Ziyang Ma, Ziyu Zhang, Songxiang Liu, Linqin Li, Zheng Liang, Qixi Zheng, Rui Wang, Xiaoqin Feng, Weizhen Bian, Zhen Ye, Sitong Cheng, Ruibin Yuan, Zhixian Zhao, Xinfa Zhu, Jiahao Pan, Liumeng Xue, Pengcheng Zhu, Yunlin Chen, Zhifei Li, Xie Chen, Lei Xie, Yike Guo, Wei Xue|2025-03-03|arXiv|https://github.com/SparkAudio/Spark-TTS|http://arxiv.org/abs/2503.01710v1|
|789|CrowdSelect: Synthetic Instruction Data Selection with Multi-LLM Wisdom|Yisen Li, Lingfeng Yang, Wenxuan Shen, Pan Zhou, Yao Wan, Weiwei Lin, Dongping Chen|2025-03-03|arXiv|https://github.com/listentm/crowdselect|http://arxiv.org/abs/2503.01836v1|
|790|Parameter-Efficient Fine-Tuning of Large Language Models via Deconvolution in Subspace|Jia-Chen Zhang, Yu-Jie Xiong, Chun-Ming Xia, Dong-Hai Zhu, Xi-He Qiu|2025-03-03|COLING|https://github.com/Godz-z/DCFT|https://aclanthology.org/2025.coling-main.265/|
|791|MultiAgentBench: Evaluating the Collaboration and Competition of LLM agents|Kunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng Yang, Shuyi Guo, Zhe Wang, Zhenhailong Wang, Cheng Qian, Xiangru Tang, Heng Ji, Jiaxuan You|2025-03-03|arXiv|https://github.com/MultiagentBench/MARBLE|http://arxiv.org/abs/2503.01935v1|
|792|Unmasking Implicit Bias: Evaluating Persona-Prompted LLM Responses in Power-Disparate Social Scenarios|Bryan Chen Zhengyu Tan, Roy Ka-Wei Lee|2025-03-03|arXiv|https://inc0mple.github.io/Implicit_Bias_Interactive_Data_Viz|http://arxiv.org/abs/2503.01532v1|
|793|Watch Out Your Album! On the Inadvertent Privacy Memorization in Multi-Modal Large Language Models|Tianjie Ju, Yi Hua, Hao Fei, Zhenyu Shao, Yubin Zheng, Haodong Zhao, Mong-Li Lee, Wynne Hsu, Zhuosheng Zhang, Gongshen Liu|2025-03-03|arXiv|https://github.com/illusionhi/ProbingPrivacy|https://doi.org/10.48550/arXiv.2503.01208|
|794|Liger: Linearizing Large Language Models to Gated Recurrent Structures|Disen Lan, Weigao Sun, Jiaxi Hu, Jusen Du, Yu Cheng|2025-03-03|arXiv|https://github.com/OpenSparseLLMs/Linearization|https://doi.org/10.48550/arXiv.2503.01496|
|795|MiLiC-Eval: Benchmarking Multilingual LLMs for China's Minority Languages|Chen Zhang, Mingxu Tao, Zhiyuan Liao, Yansong Feng|2025-03-03|arXiv|https://github.com/luciusssss/MiLiC-Eval|http://arxiv.org/abs/2503.01150v1|
|796|Position: Don't use the CLT in LLM evals with fewer than a few hundred datapoints|Sam Bowyer, Laurence Aitchison, Desi R. Ivanova|2025-03-03|arXiv|https://github.com/sambowyer/bayes_evals|http://arxiv.org/abs/2503.01747v2|
|797|Retrieval Models Aren&apos;t Tool-Savvy: Benchmarking Tool Retrieval for Large Language Models|Zhengliang Shi, Yuhan Wang, Lingyong Yan, Pengjie Ren, Shuaiqiang Wang, Dawei Yin, Zhaochun Ren|2025-03-03|arXiv|https://github.com/mangopy/tool-retrieval-benchmark|https://doi.org/10.48550/arXiv.2503.01763|
|798|Evaluating Personalized Tool-Augmented LLMs from the Perspectives of Personalization and Proactivity|Yupu Hao, Pengfei Cao, Zhuoran Jin, Huanxuan Liao, Yubo Chen, Kang Liu, Jun Zhao|2025-03-02|arXiv|https://github.com/hypasd-art/ETAPP|http://arxiv.org/abs/2503.00771v1|
|799|HiBench: Benchmarking LLMs Capability on Hierarchical Structure Reasoning|Zhuohang Jiang, Pangjing Wu, Ziran Liang, Peter Q. Chen, Xu Yuan, Ye Jia, Jiancheng Tu, Chen Li, Peter H. F. Ng, Qing Li|2025-03-02|arXiv|https://github.com/jzzzzh/HiBench|http://arxiv.org/abs/2503.00912v1|
|800|LLMDR: LLM-Driven Deadlock Detection and Resolution in Multi-Agent Pathfinding|Seungbae Seo, Junghwan Kim, Minjeong Shin, Bongwon Suh|2025-03-02|arXiv|https://github.com/ssbacc/llmdr-dhc|http://arxiv.org/abs/2503.00717v1|
|801|U-NIAH: Unified RAG and LLM Evaluation for Long Context Needle-In-A-Haystack|Yunfan Gao, Yun Xiong, Wenlong Wu, Zijing Huang, Bohan Li, Haofen Wang|2025-03-01|arXiv|https://github.com/Tongji-KGLLM/U-NIAH|http://arxiv.org/abs/2503.00353v1|
|802|Interact, Instruct to Improve: A LLM-Driven Parallel Actor-Reasoner Framework for Enhancing Autonomous Vehicle Interactions|Shiyu Fang, Jiaqi Liu, Chengkai Xu, Chen Lv, Peng Hang, Jian Sun|2025-03-01|arXiv|https://github.com/FanGShiYuu/Actor-Reasoner|http://arxiv.org/abs/2503.00502v1|
|803|UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning|Jiawei Zhang, Shuang Yang, Bo Li|2025-02-28|arXiv|https://github.com/AI-secure/UDora|http://arxiv.org/abs/2503.01908v1|
|804|DeepRetrieval: Hacking Real Search Engines and Retrievers with Large Language Models via Reinforcement Learning|Pengcheng Jiang, Jiacheng Lin, Lang Cao, Runchu Tian, SeongKu Kang, Zifeng Wang, Jimeng Sun, Jiawei Han|2025-02-28|arXiv|https://github.com/pat-jj/DeepRetrieval|https://doi.org/10.48550/arXiv.2503.00223|
|805|UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation|Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang|2025-02-28|arXiv|https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL|http://arxiv.org/abs/2502.20984v2|
|806|InspireMusic: Integrating Super Resolution and Large Language Model for High-Fidelity Long-Form Music Generation|Chong Zhang, Yukun Ma, Qian Chen, Wen Wang, Shengkui Zhao, Zexu Pan, Hao Wang, Chongjia Ni, Trung Hieu Nguyen, Kun Zhou, Yidi Jiang, Chaohong Tan, Zhifu Gao, Zhihao Du, Bin Ma|2025-02-28|arXiv|https://github.com/FunAudioLLM/InspireMusic|https://doi.org/10.48550/arXiv.2503.00084|
|807|LLM Post-Training: A Deep Dive into Reasoning Large Language Models|Komal Kumar, Tajamul Ashraf, Omkar Thawakar, Rao Muhammad Anwer, Hisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, Phillip H. S. Torr, Salman H. Khan, Fahad Shahbaz Khan|2025-02-28|arXiv|https://github.com/mbzuai-oryx/Awesome-LLM-Post-training|https://doi.org/10.48550/arXiv.2502.21321|
|808|Palm: A Culturally Inclusive and Linguistically Diverse Dataset for Arabic LLMs|Fakhraddin Alwajih, Abdellah El Mekki, Samar Mohamed Magdy, Abdelrahim A. Elmadany, Omer Nacar, El Moatez Billah Nagoudi, Reem Abdel-Salam, Hanin Atwany, Youssef Nafea, Abdulfattah Mohammed Yahya, Rahaf Alhamouri, Hamzah A. Alsayadi, Hiba Zayed, Sara Shatnawi, Serry Sibaee, Yasir Ech-Chammakhy, Walid Al-Dhabyani, Marwa Mohamed Ali, Imen Jarraya, Ahmed Oumar El-Shangiti, Aisha Alraeesi, Mohammed Anwar Al-Ghrawi, Abdulrahman S. Al-Batati, Elgizouli Mohamed, Noha Taha Elgindi, Muhammed Saeed, Houdaifa Atou, Issam Ait Yahia, Abdelhak Bouayad, Mohammed Machrouh, Amal Makouar, Dania Alkawi, Mukhtar Mohamed, Safaa Taher Abdelfadil, Amine Ziad Ounnoughene, Rouabhia Anfel, Rwaa Assi, Ahmed Sorkatti, Mohamedou Cheikh Tourad, Anis Koubaa, Ismail Berrada, Mustafa Jarrar, Shady Shehata, Muhammad Abdul-Mageed|2025-02-28|arXiv|https://github.com/UBC-NLP/palm|http://arxiv.org/abs/2503.00151v1|
|809|A Thousand Words or An Image: Studying the Influence of Persona Modality in Multimodal LLMs|Julius Broomfield, Kartik Sharma, Srijan Kumar|2025-02-27|arXiv|https://github.com/claws-lab/persona-modality|http://arxiv.org/abs/2502.20504v1|
|810|ECCOS: Efficient Capability and Cost Coordinated Scheduling for Multi-LLM Serving|Kai Mei, Wujiang Xu, Shuhang Lin, Yongfeng Zhang|2025-02-27|arXiv|https://github.com/agiresearch/ECCOS|http://arxiv.org/abs/2502.20576v2|
|811|Adaptive Attacks Break Defenses Against Indirect Prompt Injection Attacks on LLM Agents|Qiusi Zhan, Richard Fang, Henil Shalin Panchal, Daniel Kang|2025-02-27|arXiv|https://github.com/uiuc-kang-lab/AdaptiveAttackAgent|http://arxiv.org/abs/2503.00061v2|
|812|Foot-In-The-Door: A Multi-turn Jailbreak for LLMs|Zixuan Weng, Xiaolong Jin, Jinyuan Jia, Xiangyu Zhang|2025-02-27|arXiv|https://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak|http://arxiv.org/abs/2502.19820v2|
|813|LongRoPE2: Near-Lossless LLM Context Window Scaling|Ning Shang, Li Lyna Zhang, Siyuan Wang, Gaokai Zhang, Gilsinia Lopez, Fan Yang, Weizhu Chen, Mao Yang|2025-02-27|arXiv|https://github.com/microsoft/LongRoPE|http://arxiv.org/abs/2502.20082v1|
|814|SkipPipe: Partial and Reordered Pipelining Framework for Training LLMs in Heterogeneous Networks|Nikolay Blagoev, Lydia Yiyu Chen, Oğuzhan Ersoy|2025-02-27|arXiv|https://github.com/gensyn-ai/skippipe|http://arxiv.org/abs/2502.19913v1|
|815|Beneath the Surface: How Large Language Models Reflect Hidden Bias|Jinhao Pan, Chahat Raj, Ziyu Yao, Ziwei Zhu|2025-02-27|arXiv|https://github.com/JP-25/Hidden-Bias-Benchmark|https://doi.org/10.48550/arXiv.2502.19749|
|816|SeisMoLLM: Advancing Seismic Monitoring via Cross-modal Transfer with Pre-trained Large Language Model|Xinghao Wang, Feng Liu, Rui Su, Zhihui Wang, Lei Bai, Wanli Ouyang|2025-02-27|arXiv|https://github.com/StarMoonWang/SeisMoLLM|https://doi.org/10.48550/arXiv.2502.19960|
|817|Smart Routing: Cost-Effective Multi-LLM Serving for Multi-Core AIOS|Kai Mei, Wujiang Xu, Shuhang Lin, Yongfeng Zhang|2025-02-27|arXiv|https://github.com/agiresearch/ECCOS|http://arxiv.org/abs/2502.20576v4|
|818|OmniRouter: Budget and Performance Controllable Multi-LLM Routing|Kai Mei, Wujiang Xu, Shuhang Lin, Yongfeng Zhang|2025-02-27|arXiv|https://github.com/agiresearch/OmniRouter|http://arxiv.org/abs/2502.20576v5|
|819|Erasing Without Remembering: Safeguarding Knowledge Forgetting in Large Language Models|Huazheng Wang, Yongcheng Jing, Haifeng Sun, Yingjie Wang, Jingyu Wang, Jianxin Liao, Dacheng Tao|2025-02-27|arXiv|https://github.com/MaybeLizzy/UGBench|https://doi.org/10.48550/arXiv.2502.19982|
|820|Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents|Haochen Sun, Shuwen Zhang, Lujie Niu, Lei Ren, Hao Xu, Hao Fu, Fangkun Zhao, Caixia Yuan, Xiaojie Wang|2025-02-27|arXiv|https://github.com/YusaeMeow/Collab-Overcooked|https://doi.org/10.48550/arXiv.2502.20073|
|821|Why Are Web AI Agents More Vulnerable Than Standalone LLMs? A Security Analysis|Jeffrey Yang Fan Chiang, Seungjae Lee, Jia-Bin Huang, Furong Huang, Yizheng Chen|2025-02-27|arXiv|http://vulnerable-ai-agents.github.io|http://arxiv.org/abs/2502.20383v1|
|822|Self-Training Elicits Concise Reasoning in Large Language Models|Tergel Munkhbat, Namgyu Ho, Seo Hyun Kim, Yongjin Yang, Yujin Kim, Se-Young Yun|2025-02-27|arXiv|https://github.com/TergelMunkhbat/concise-reasoning|https://doi.org/10.48550/arXiv.2502.20122|
|823|Protecting multimodal large language models against misleading visualizations|Jonathan Tonglet, Tinne Tuytelaars, Marie-Francine Moens, Iryna Gurevych|2025-02-27|arXiv|https://github.com/UKPLab/arxiv2025-misleading-visualizations|https://doi.org/10.48550/arXiv.2502.20503|
|824|Sensing and Steering Stereotypes: Extracting and Applying Gender Representation Vectors in LLMs|Hannah Cyberey, Yangfeng Ji, David Evans|2025-02-27|arXiv|https://github.com/hannahxchen/gender-bias-steering|http://arxiv.org/abs/2502.19721v1|
|825|OntologyRAG: Better and Faster Biomedical Code Mapping with Retrieval-Augmented Generation (RAG) Leveraging Ontology Knowledge Graphs and Large Language Models|Hui Feng, Yuntzu Yin, Emiliano Reynares, Jay Nanavati|2025-02-26|arXiv|https://github.com/iqvianlp/ontologyRAG|https://doi.org/10.48550/arXiv.2502.18992|
|826|TrajLLM: A Modular LLM-Enhanced Agent-Based Framework for Realistic Human Trajectory Simulation|Chenlu Ju, Jiaxin Liu, Shobhit Sinha, Hao Xue, Flora Salim|2025-02-26|arXiv|https://github.com/cju0/TrajLLM|http://arxiv.org/abs/2502.18712v1|
|827|Sparse Brains are Also Adaptive Brains: Cognitive-Load-Aware Dynamic Activation for LLMs|Yiheng Yang, Yujie Wang, Chi Ma, Lei Yu, Emmanuele Chersoni, Chu-Ren Huang|2025-02-26|arXiv|https://github.com/Oldify/CLADA|http://arxiv.org/abs/2502.19078v1|
|828|Exploring Graph Tasks with Pure LLMs: A Comprehensive Benchmark and Investigation|Yuxiang Wang, Xinnan Dai, Wenqi Fan, Yao Ma|2025-02-26|arXiv|https://github.com/myflashbarry/LLM-benchmarking|http://arxiv.org/abs/2502.18771v1|
|829|Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs|Dayu Yang, Tianyang Liu, Daoan Zhang, Antoine Simoulin, Xiaoyi Liu, Yuwei Cao, Zhaopu Teng, Xin Qian, Grey Yang, Jiebo Luo, Julian McAuley|2025-02-26|arXiv|https://github.com/dayuyang1999/Awesome-Code-Reasoning|http://arxiv.org/abs/2502.19411v1|
|830|AgentSociety Challenge: Designing LLM Agents for User Modeling and Recommendation on Web Platforms|Yuwei Yan, Yu Shang, Qingbin Zeng, Yu Li, Keyu Zhao, Zhiheng Zheng, Xuefei Ning, Tianji Wu, Shengen Yan, Yu Wang, Fengli Xu, Yong Li|2025-02-26|arXiv|https://tsinghua-fib-lab.github.io/AgentSocietyChallenge|http://arxiv.org/abs/2502.18754v1|
|831|A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs|Xuan Ding, Rui Sun, Yunjian Zhang, Xiu Yan, Yueqi Zhou, Kaihao Huang, Suzhong Fu, Angelica I Aviles-Rivero, Chuanlong Xie, Yao Zhu|2025-02-26|arXiv|https://github.com/920927/SLM-a-sliding-layer-merging-method|http://arxiv.org/abs/2502.19159v3|
|832|Judge as A Judge: Improving the Evaluation of Retrieval-Augmented Generation through the Judge-Consistency of Large Language Models|Shuliang Liu, Xinze Li, Zhenghao Liu, Yukun Yan, Cheng Yang, Zheni Zeng, Zhiyuan Liu, Maosong Sun, Ge Yu|2025-02-26|arXiv|https://github.com/OpenBMB/ConsJudge|https://doi.org/10.48550/arXiv.2502.18817|
|833|Sliding Window Attention Training for Efficient Large Language Models|Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, Xiangyu Zhao|2025-02-26|arXiv|https://github.com/Fzkuji/swat-attention|https://doi.org/10.48550/arXiv.2502.18845|
|834|ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models|Danae Sánchez Villegas, Ingo Ziegler, Desmond Elliott|2025-02-26|arXiv|https://github.com/danaesavi/ImageChain|https://doi.org/10.48550/arXiv.2502.19409|
|835|Amulet: ReAlignment During Test Time for Personalized Preference Adaptation of LLMs|Zhaowei Zhang, Fengshuo Bai, Qizhi Chen, Chengdong Ma, Mingzhi Wang, Haoran Sun, Zilong Zheng, Yaodong Yang|2025-02-26|arXiv|https://zowiezhang.github.io/projects/Amulet|http://arxiv.org/abs/2502.19148v1|
|836|JailBench: A Comprehensive Chinese Security Assessment Benchmark for Large Language Models|Shuyi Liu, Simiao Cui, Haoran Bu, Yuming Shang, Xi Zhang|2025-02-26|arXiv|https://github.com/STAIR-BUPT/JailBench|https://doi.org/10.48550/arXiv.2502.18935|
|837|Can Multimodal LLMs Perform Time Series Anomaly Detection?|Xiongxiao Xu, Haoran Wang, Yueqing Liang, Philip S. Yu, Yue Zhao, Kai Shu|2025-02-25|arXiv|https://mllm-ts.github.io|http://arxiv.org/abs/2502.17812v1|
|838|Science Across Languages: Assessing LLM Multilingual Translation of Scientific Papers|Hannah Calzi Kleidermacher, James Zou|2025-02-25|arXiv|https://hankleid.github.io/ProjectMundo|http://arxiv.org/abs/2502.17882v1|
|839|RefuteBench 2.0 -- Agentic Benchmark for Dynamic Evaluation of LLM Responses to Refutation Instruction|Jianhao Yan, Yun Luo, Yue Zhang|2025-02-25|arXiv|https://github.com/ElliottYan/RefuteBench-2.0|http://arxiv.org/abs/2502.18308v1|
|840|Problem Solved? Information Extraction Design Space for Layout-Rich Documents using LLMs|Gaye Colakoglu, Gürkan Solmaz, Jonathan Fürst|2025-02-25|arXiv|https://github.com/gayecolakoglu/LayIE-LLM|http://arxiv.org/abs/2502.18179v1|
|841|LLM Knows Geometry Better than Algebra: Numerical Understanding of LLM-Based Agents in A Trading Arena|Tianmi Ma, Jiawei Du, Wenxin Huang, Wenjie Wang, Liang Xie, Xian Zhong, Joey Tianyi Zhou|2025-02-25|arXiv|https://github.com/wekjsdvnm/Agent-Trading-Arena|http://arxiv.org/abs/2502.17967v1|
|842|Detection of LLM-Paraphrased Code and Identification of the Responsible LLM Using Coding Style Features|Shinwoo Park, Hyundong Jin, Jeong-won Cha, Yo-Sub Han|2025-02-25|arXiv|https://github.com/Shinwoo-Park/detecting_llm_paraphrased_code_via_coding_style_features|http://arxiv.org/abs/2502.17749v2|
|843|Scalable Best-of-N Selection for Large Language Models via Self-Certainty|Zhewei Kang, Xuandong Zhao, Dawn Song|2025-02-25|arXiv|https://github.com/backprop07/Self-Certainty|https://doi.org/10.48550/arXiv.2502.18581|
|844|LDGen: Enhancing Text-to-Image Synthesis via Large Language Model-Driven Language Representation|Pengzhi Li, Pengfei Yu, Zide Liu, Wei He, Xuhao Pan, Xudong Rao, Tao Wei, Wei Chen|2025-02-25|arXiv|https://zrealli.github.io/LDGen|https://doi.org/10.48550/arXiv.2502.18302|
|845|Detecting Knowledge Boundary of Vision Large Language Models by Sampling-Based Inference|Zhuo Chen, Xinyu Wang, Yong Jiang, Zhen Zhang, Xinyu Geng, Pengjun Xie, Fei Huang, Kewei Tu|2025-02-25|arXiv|https://github.com/Chord-Chen-30/VLLM-KnowledgeBoundary|https://doi.org/10.48550/arXiv.2502.18023|
|846|Harnessing Multiple Large Language Models: A Survey on LLM Ensemble|Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Dingqi Yang, Hailong Sun, Philip S. Yu|2025-02-25|arXiv|https://github.com/junchenzhi/Awesome-LLM-Ensemble|https://doi.org/10.48550/arXiv.2502.18036|
|847|Discriminative Finetuning of Generative Large Language Models without Reward Models and Preference Data|Siqi Guo, Ilgee Hong, Vicente Balmaseda, Changlong Yu, Liang Qiu, Xin Liu, Haoming Jiang, Tuo Zhao, Tianbao Yang|2025-02-25|arXiv|https://github.com/Optimization-AI/DFT|https://doi.org/10.48550/arXiv.2502.18679|
|848|Detecting LLM-Generated Korean Text through Linguistic Feature Analysis|Shinwoo Park, Shubin Kim, Do-Kyung Kim, Yo-Sub Han|2025-02-25|arXiv|https://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis|http://arxiv.org/abs/2503.00032v2|
|849|LLM-QE: Improving Query Expansion by Aligning Large Language Models with Ranking Preferences|Sijia Yao, Pengcheng Huang, Zhenghao Liu, Yu Gu, Yukun Yan, Shi Yu, Ge Yu|2025-02-24|arXiv|https://github.com/NEUIR/LLM-QE|https://doi.org/10.48550/arXiv.2502.17057|
|850|MLLMs Know Where to Look: Training-free Perception of Small Visual Details with Multimodal LLMs|Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski|2025-02-24|arXiv|https://github.com/saccharomycetes/mllms_know|http://arxiv.org/abs/2502.17422v1|
|851|Delta Decompression for MoE-based LLMs Compression|Hao Gu, Wei Li, Lujun Li, Qiyuan Zhu, Mark Lee, Shengjie Sun, Wei Xue, Yike Guo|2025-02-24|arXiv|https://github.com/lliai/D2MoE|http://arxiv.org/abs/2502.17298v1|
|852|ConvoyLLM: Dynamic Multi-Lane Convoy Control Using LLMs|Liping Lu, Zhican He, Duanfeng Chu, Rukang Wang, Saiqian Peng, Pan Zhou|2025-02-24|arXiv|https://github.com/chuduanfeng/ConvoyLLM|http://arxiv.org/abs/2502.17529v2|
|853|Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing|Yi-Kai Zhang, De-Chuan Zhan, Han-Jia Ye|2025-02-24|arXiv|https://cit-llm-routing.github.io|http://arxiv.org/abs/2502.17282v1|
|854|COSMOS: A Hybrid Adaptive Optimizer for Memory-Efficient Training of LLMs|Liming Liu, Zhenghao Xu, Zixuan Zhang, Hao Kang, Zichong Li, Chen Liang, Weizhu Chen, Tuo Zhao|2025-02-24|arXiv|https://github.com/lliu606/COSMOS|http://arxiv.org/abs/2502.17410v2|
|855|On Relation-Specific Neurons in Large Language Models|Yihong Liu, Runsheng Chen, Lea Hirlimann, Ahmad Dawar Hakimi, Mingyang Wang, Amir Hossein Kargaran, Sascha Rothe, François Yvon, Hinrich Schütze|2025-02-24|arXiv|https://github.com/cisnlp/relation-specific-neurons|https://doi.org/10.48550/arXiv.2502.17355|
|856|LongSafety: Evaluating Long-Context Safety of Large Language Models|Yida Lu, Jiale Cheng, Zhexin Zhang, Shiyao Cui, Cunxiang Wang, Xiaotao Gu, Yuxiao Dong, Jie Tang, Hongning Wang, Minlie Huang|2025-02-24|arXiv|https://github.com/thu-coai/LongSafety|https://doi.org/10.48550/arXiv.2502.16971|
|857|Introducing Visual Perception Token into Multimodal Large Language Model|Runpeng Yu, Xinyin Ma, Xinchao Wang|2025-02-24|arXiv|https://github.com/yu-rp/VisualPerceptionToken|https://doi.org/10.48550/arXiv.2502.17425|
|858|LogitLens4LLMs: Extending Logit Lens Analysis to Modern Large Language Models|Zhenyu Wang|2025-02-24|arXiv|https://github.com/zhenyu-02/LogitLens4LLMs|https://doi.org/10.48550/arXiv.2503.11667|
|859|Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs|Himanshu Beniwal, Sailesh Panda, Birudugadda Srivibhav, Mayank Singh|2025-02-24|arXiv|https://github.com/himanshubeniwal/X-BAT|http://arxiv.org/abs/2502.16901v2|
|860|ExpandR: Teaching Dense Retrievers Beyond Queries with LLM Guidance|Sijia Yao, Pengcheng Huang, Zhenghao Liu, Yu Gu, Yukun Yan, Shi Yu, Ge Yu|2025-02-24|arXiv|https://github.com/NEUIR/ExpandR|http://arxiv.org/abs/2502.17057v3|
|861|Synthetic Text Generation for Training Large Language Models via Gradient Matching|Dang Nguyen, Zeman Li, MohammadHossein Bateni, Vahab Mirrokni, Meisam Razaviyayn, Baharan Mirzasoleiman|2025-02-24|arXiv|https://github.com/BigML-CS-UCLA/GRADMM|https://doi.org/10.48550/arXiv.2502.17607|
|862|From System 1 to System 2: A Survey of Reasoning Large Language Models|Zhong-Zhi Li, Duzhen Zhang, Ming-Liang Zhang, Jiaxin Zhang, Zengyan Liu, Yuxuan Yao, Haotian Xu, Junhao Zheng, Pei-Jie Wang, Xiuyi Chen, Yingying Zhang, Fei Yin, Jiahua Dong, Zhiwei Li, Bao-Long Bi, Ling-Rui Mei, Junfeng Fang, Xiao Liang, Zhijiang Guo, Le Song, Cheng-Lin Liu|2025-02-24|arXiv|https://github.com/zzli2022/Awesome-Slow-Reason-System|https://doi.org/10.48550/arXiv.2502.17419|
|863|From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs|Ruxiao Chen, Chenguang Wang, Yuran Sun, Xilei Zhao, Susu Xu|2025-02-24|arXiv|https://github.com/SusuXu-s-Lab/FLARE|http://arxiv.org/abs/2502.17701v1|
|864|CoT-UQ: Improving Response-wise Uncertainty Quantification in LLMs with Chain-of-Thought|Boxuan Zhang, Ruqi Zhang|2025-02-24|arXiv|https://github.com/ZBox1005/CoT-UQ|http://arxiv.org/abs/2502.17214v1|
|865|AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay|Ziyi Tang, Zechuan Chen, Jiarui Yang, Jiayao Mai, Yongsen Zheng, Keze Wang, Jinrui Chen, Liang Lin|2025-02-24|arXiv|https://github.com/RndmVariableQ/AlphaAgent|http://arxiv.org/abs/2502.16789v1|
|866|CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale|Chenlong Wang, Zhaoyang Chu, Zhengxiang Cheng, Xuyi Yang, Kaiyue Qiu, Yao Wan, Zhou Zhao, Xuanhua Shi, Dongping Chen|2025-02-23|arXiv|https://github.com/Lucky-voyage/Code-Sync|https://doi.org/10.48550/arXiv.2502.16645|
|867|BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning|Haiteng Zhao, Chang Ma, Fangzhi Xu, Lingpeng Kong, Zhi-Hong Deng|2025-02-23|arXiv|https://github.com/zhao-ht/BioMaze|https://doi.org/10.48550/arXiv.2502.16660|
|868|VisFactor: Benchmarking Fundamental Visual Cognition in Multimodal Large Language Models|Jen-tse Huang, Dasen Dai, Jen-Yuan Huang, Youliang Yuan, Xiaoyuan Liu, Wenxuan Wang, Wenxiang Jiao, Pinjia He, Zhaopeng Tu|2025-02-23|arXiv|https://github.com/CUHK-ARISE/VisFactor|https://doi.org/10.48550/arXiv.2502.16435|
|869|OmniParser V2: Structured-Points-of-Thought for Unified Visual Text Parsing and Its Generality to Multimodal Large Language Models|Wenwen Yu, Zhibo Yang, Jianqiang Wan, Sibo Song, Jun Tang, Wenqing Cheng, Yuliang Liu, Xiang Bai|2025-02-22|arXiv|https://github.com/AlibabaResearch/AdvancedLiterateMachinery|https://doi.org/10.48550/arXiv.2502.16161|
|870|Mojito: LLM-Aided Motion Instructor with Jitter-Reduced Inertial Tokens|Ziwei Shan, Yaoyu He, Chengfeng Zhao, Jiashen Du, Jingyan Zhang, Qixuan Zhang, Jingyi Yu, Lan Xu|2025-02-22|arXiv|https://koyui.github.io/mojito/|http://arxiv.org/abs/2502.16175v1|
|871|Dynamic Low-Rank Sparse Adaptation for Large Language Models|Weizhong Huang, Yuxin Zhang, Xiawu Zheng, Yang Liu, Jing Lin, Yiwu Yao, Rongrong Ji|2025-02-22|arXiv|https://github.com/wzhuang-xmu/LoSA|https://doi.org/10.48550/arXiv.2502.14816|
|872|CER: Confidence Enhanced Reasoning in LLMs|Ali Razghandi, Seyed Mohammad Hadi Hosseini, Mahdieh Soleymani Baghshah|2025-02-22|arXiv …, 2025|https://github.com/|http://arxiv.org/abs/2502.14634v1|
|873|Patterns Over Principles: The Fragility of Inductive Reasoning in LLMs under Noisy Observations|Chunyang Li, Weiqi Wang, Tianshi Zheng, Yangqiu Song|2025-02-22|arXiv|https://github.com/lcy2723/Robust-Rule-Induction|http://arxiv.org/abs/2502.16169v1|
|874|CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models|Zhenhong Zhou, Zherui Li, Jie Zhang, Yuanhe Zhang, Kun Wang, Yang Liu, Qing Guo|2025-02-21|arXiv|https://github.com/zhrli324/Corba|https://doi.org/10.48550/arXiv.2502.14529|
|875|Probe Pruning: Accelerating LLMs through Dynamic Pruning via Model-Probing|Qi Le, Enmao Diao, Ziyan Wang, Xinran Wang, Jie Ding, Li Yang, Ali Anwar|2025-02-21|arXiv|https://github.com/Qi-Le1/Probe_Pruning|http://arxiv.org/abs/2502.15618v1|
|876|Plan-over-Graph: Towards Parallelable LLM Agent Schedule|Shiqi Zhang, Xinbei Ma, Zouying Cao, Zhuosheng Zhang, Hai Zhao|2025-02-21|arXiv:2502.14563, 2025|https://github.com/zsq259/Plan-over-Graph|http://arxiv.org/abs/2502.14563v1|
|877|Investigating the Adaptive Robustness with Knowledge Conflicts in LLM-based Multi-Agent Systems|Tianjie Ju, Bowen Wang, Hao Fei, Mong-Li Lee, Wynne Hsu, Yun Li, Qianren Wang, Pengzhou Cheng, Zongru Wu, Zhuosheng Zhang, Gongshen Liu|2025-02-21|arXiv|https://github.com/wbw625/MultiAgentRobustness|http://arxiv.org/abs/2502.15153v1|
|878|FormalSpecCpp: A Dataset of C++ Formal Specifications created using LLMs|Madhurima Chakraborty, Peter Pirkelbauer, Qing Yi|2025-02-21|arXiv|https://github.com/MadhuNimmo/FormalSpecCpp|http://arxiv.org/abs/2502.15217v1|
|879|Adversarial Prompt Evaluation: Systematic Benchmarking of Guardrails Against Prompt Input Attacks on LLMs|Giulio Zizzo, Giandomenico Cornacchia, Kieran Fraser, Muhammad Zaid Hameed, Ambrish Rawat, Beat Buesser, Mark Purcell, Pin-Yu Chen, Prasanna Sattigeri, Kush Varshney|2025-02-21|arXiv|https://github.com/IBM/Adversarial-Prompt-Evaluation|http://arxiv.org/abs/2502.15427v1|
|880|Transfer-Prompting: Enhancing Cross-Task Adaptation in Large Language Models via Dual-Stage Prompts Optimization|Yupeng Chang, Yi Chang, Yuan Wu|2025-02-21|arXiv|https://github.com/llm172/Transfer-Prompting|https://doi.org/10.48550/arXiv.2502.14211|
|881|Scale-Distribution Decoupling: Enabling Stable and Effective Training of Large Language Models|Ya Wang, Zhijian Zhuo, Yutao Zeng, Xun Zhou, Jian Yang, Xiaoqing Li|2025-02-21|arXiv|https://github.com/kaihemo/SDD|https://doi.org/10.48550/arXiv.2502.15499|
|882|On the logical skills of large language models: evaluations using arbitrarily complex first-order logic problems|Shokhrukh Ibragimov, Arnulf Jentzen, Benno Kuckuck|2025-02-21|arXiv|https://github.com/bkuckuck/logical-skills-of-llms|https://doi.org/10.48550/arXiv.2502.14180|
|883|MedHallu: A Comprehensive Benchmark for Detecting Medical Hallucinations in Large Language Models|Shrey Pandit, Jiawei Xu, Junyuan Hong, Zhangyang Wang, Tianlong Chen, Kaidi Xu, Ying Ding|2025-02-21|arXiv|https://medhallu.github.io/|https://doi.org/10.48550/arXiv.2502.14302|
|884|A General Pseudonymization Framework for Cloud-Based LLMs: Replacing Privacy Information in Controlled Text Generation|Shilong Hou, Ruilin Shang, Zi Long, Xianghua Fu, Yin Chen|2025-02-21|arXiv|https://github.com/Mebymeby/Pseudonymization-Framework|http://arxiv.org/abs/2502.15233v1|
|885|Protein Large Language Models: A Comprehensive Survey|Yijia Xiao, Wanjia Zhao, Junkai Zhang, Yiqiao Jin, Han Zhang, Zhicheng Ren, Renliang Sun, Haixin Wang, Guancheng Wan, Pan Lu, Xiao Luo, Yu Zhang, James Zou, Yizhou Sun, Wei Wang|2025-02-21|arXiv|https://github.com/Yijia-Xiao/Protein-LLM-Survey|https://doi.org/10.48550/arXiv.2502.17504|
|886|LServe: Efficient Long-sequence LLM Serving with Unified Sparse Attention|Shang Yang, Junxian Guo, Haotian Tang, Qinghao Hu, Guangxuan Xiao, Jiaming Tang, Yujun Lin, Zhijian Liu, Yao Lu, Song Han|2025-02-21|arXiv …, 2025|https://github.com/mit-han-lab/omniserve|http://arxiv.org/abs/2502.14866v1|
|887|Understand User Opinions of Large Language Models via LLM-Powered In-the-Moment User Experience Interviews|Mengqiao Liu, Tevin Wang, Cassandra A. Cohen, Sarah Li, Chenyan Xiong|2025-02-21|arXiv|https://github.com/cxcscmu/LLM-Interviewer|https://doi.org/10.48550/arXiv.2502.15226|
|888|From RAG to Memory: Non-Parametric Continual Learning for Large Language Models|Bernal Jiménez Gutiérrez, Yiheng Shu, Weijian Qi, Sizhe Zhou, Yu Su|2025-02-21|arXiv|https://github.com/OSU-NLP-Group/HippoRAG|https://doi.org/10.48550/arXiv.2502.14802|
|889|TurboFuzzLLM: Turbocharging Mutation-based Fuzzing for Effectively Jailbreaking Large Language Models in Practice|Aman Goel, Xian Carrie Wu, Zhe Wang, Dmitriy Bespalov, Yanjun Qi|2025-02-21|arXiv|https://github.com/amazon-science/TurboFuzzLLM|https://doi.org/10.48550/arXiv.2502.18504|
|890|STeCa: Step-level Trajectory Calibration for LLM Agent Learning|Hanlin Wang, Jian Wang, Chak Tou Leong, Wenjie Li|2025-02-21|arXiv:2502.14276, 2025|https://github.com/WangHanLinHenry/STeCa|http://arxiv.org/abs/2502.14276v1|
|891|Problem-Solving Logic Guided Curriculum In-Context Learning for LLMs Complex Reasoning|Xuetao Ma, Wenbin Jiang, Hua Huang|2025-02-21|arXiv|https://github.com/maxuetao/CurriculumICL|http://arxiv.org/abs/2502.15401v1|
|892|PredictaBoard: Benchmarking LLM Score Predictability|Lorenzo Pacchiardi, Konstantinos Voudouris, Ben Slater, Fernando Martínez-Plumed, José Hernández-Orallo, Lexin Zhou, Wout Schellaert|2025-02-21|arXiv …, 2025|https://github.com/Kinds-of-Intelligence-CFI/PredictaBoard|http://arxiv.org/abs/2502.14445v1|
|893|Middle-Layer Representation Alignment for Cross-Lingual Transfer in Fine-Tuned LLMs|Danni Liu, Jan Niehues|2025-02-21|arXiv:2502.14830, 2025|https://github.com/dannigt/mid-align|http://arxiv.org/abs/2502.14830v1|
|894|Forgotten Polygons: Multimodal Large Language Models are Shape-Blind|William Rudman, Michal Golovanevsky, Amir Bar, Vedant Palit, Yann LeCun, Carsten Eickhoff, Ritambhara Singh|2025-02-21|arXiv|https://github.com/rsinghlab/Shape-Blind|https://doi.org/10.48550/arXiv.2502.15969|
|895|Is Safety Standard Same for Everyone? User-Specific Safety Evaluation of Large Language Models|Yeonjun In, Wonjoong Kim, Kanghoon Yoon, Sungchul Kim, Md. Mehrab Tanjim, Kibum Kim, Chanyoung Park|2025-02-20|arXiv|https://github.com/yeonjun-in/U-SafeBench|https://doi.org/10.48550/arXiv.2502.15086|
|896|InductionBench: LLMs Fail in the Simplest Complexity Class|Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang Wang|2025-02-20|arXiv|https://github.com/Wenyueh/inductive_reasoning_benchmark|http://arxiv.org/abs/2502.15823v3|
|897|AI-Empowered Catalyst Discovery: A Survey from Classical Machine Learning Approaches to Large Language Models|Yuanyuan Xu, Hanchen Wang, Wenjie Zhang, Lexing Xie, Yin Chen, Flora Salim, Ying Zhang, Justin Gooding, Toby Walsh|2025-02-19|arXiv|https://github.com/LuckyGirl-XU/Awesome-Artificial-Intelligence-Empowered-Catalyst-Discovery|https://doi.org/10.48550/arXiv.2502.13626|
|898|SIFT: Grounding LLM Reasoning in Contexts via Stickers|Zihao Zeng, Xuyao Huang, Boxiu Li, Zhijie Deng|2025-02-19|arXiv|https://github.com/zhijie-group/SIFT|http://arxiv.org/abs/2502.14922v1|
|899|Judging the Judges: A Collection of LLM-Generated Relevance Judgements|Hossein A. Rahmani, Clemencia Siro, Mohammad Aliannejadi, Nick Craswell, Charles L. A. Clarke, Guglielmo Faggioli, Bhaskar Mitra, Paul Thomas, Emine Yilmaz|2025-02-19|arXiv|https://llm4eval.github.io/LLMJudge-benchmark/|http://arxiv.org/abs/2502.13908v1|
|900|DataSciBench: An LLM Agent Benchmark for Data Science|Dan Zhang, Sining Zhoubian, Min Cai, Fengzu Li, Lekang Yang, Wei Wang, Tianjiao Dong, Ziniu Hu, Jie Tang, Yisong Yue|2025-02-19|arXiv|https://github.com/THUDM/DataSciBench|http://arxiv.org/abs/2502.13897v1|
|901|Benchmarking LLMs for Political Science: A United Nations Perspective|Yueqing Liang, Liangwei Yang, Chen Wang, Congying Xia, Rui Meng, Xiongxiao Xu, Haoran Wang, Ali Payani, Kai Shu|2025-02-19|arXiv|https://github.com/yueqingliang1/UNBench|http://arxiv.org/abs/2502.14122v1|
|902|$\mathttGeLLM^3O$: Generalizing Large Language Models for Multi-property Molecule Optimization|Vishal Dey, Xiao Hu, Xia Ning|2025-02-19|arXiv|https://github.com/ninglab/GeLLMO|http://arxiv.org/abs/2502.13398v1|
|903|Proving Olympiad Inequalities by Synergizing LLMs and Symbolic Reasoning|Zenan Li, Zhaoyu Li, Wen Tang, Xian Zhang, Yuan Yao, Xujie Si, Fan Yang, Kaiyu Yang, Xiaoxing Ma|2025-02-19|arXiv|https://github.com/Lizn-zn/NeqLIPS/|http://arxiv.org/abs/2502.13834v1|
|904|An LLM-based Agent for Reliable Docker Environment Configuration|Ruida Hu, Chao Peng, Xinchen Wang, Cuiyun Gao|2025-02-19|arXiv|https://github.com/bytedance/Repo2Run|http://arxiv.org/abs/2502.13681v1|
|905|PRIV-QA: Privacy-Preserving Question Answering for Cloud Large Language Models|Guangwei Li, Yuansen Zhang, Yinggui Wang, Shoumeng Yan, Lei Wang, Tao Wei|2025-02-19|arXiv|https://github.com/ligw1998/PRIV-QA|https://doi.org/10.48550/arXiv.2502.13564|
|906|LongPO: Long Context Self-Evolution of Large Language Models through Short-to-Long Preference Optimization|Guanzheng Chen, Xin Li, Michael Qizhe Shieh, Lidong Bing|2025-02-19|arXiv|https://github.com/DAMO-NLP-SG/LongPO|https://doi.org/10.48550/arXiv.2502.13922|
|907|REFIND: Retrieval-Augmented Factuality Hallucination Detection in Large Language Models|DongGeon Lee, Hwanjo Yu|2025-02-19|arXiv|https://github.com/oneonlee/REFIND|https://doi.org/10.48550/arXiv.2502.13622|
|908|Collaborative Retrieval for Large Language Model-based Conversational Recommender Systems|Yaochen Zhu, Chao Wan, Harald Steck, Dawen Liang, Yesu Feng, Nathan Kallus, Jundong Li|2025-02-19|arXiv|https://github.com/yaochenzhu/CRAG|https://doi.org/10.48550/arXiv.2502.14137|
|909|ArtMentor: AI-Assisted Evaluation of Artworks to Explore Multimodal Large Language Models Capabilities|Chanjin Zheng, Zengyi Yu, Yilin Jiang, Mingzi Zhang, Xunuo Lu, Jing Jin, Liteng Gao|2025-02-19|arXiv|https://artmentor.github.io/|https://doi.org/10.48550/arXiv.2502.13832|
|910|Train Small, Infer Large: Memory-Efficient LoRA Training for Large Language Models|Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Yang You, Guiming Xie, Xuejian Gong, Kunlong Zhou|2025-02-19|arXiv|https://github.com/junzhang-zj/LoRAM|https://doi.org/10.48550/arXiv.2502.13533|
|911|TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation|Jialin Ouyang|2025-02-19|arXiv|https://github.com/j-bagel/treecut-math|http://arxiv.org/abs/2502.13442v1|
|912|Self-Regularization with Sparse Autoencoders for Controllable LLM-based Classification|Xuansheng Wu, Wenhao Yu, Xiaoming Zhai, Ninghao Liu|2025-02-19|arXiv|https://github.com/JacksonWuxs/Controllable_LLM_Classifier|http://arxiv.org/abs/2502.14133v2|
|913|Lost in Sequence: Do Large Language Models Understand Sequential Recommendation?|Sein Kim, Hongseok Kang, Kibum Kim, Jiwan Kim, Donghyun Kim, Minchul Yang, Kwangjin Oh, Julian McAuley, Chanyoung Park|2025-02-19|arXiv|https://github.com/Sein-Kim/LLM-SRec|https://doi.org/10.48550/arXiv.2502.13909|
|914|GeLLM3O: Generalizing Large Language Models for Multi-property Molecule Optimization|Vishal Dey, Xiao Hu, Xia Ning|2025-02-19|arXiv|https://github.com/ninglab/GeLLMO|https://doi.org/10.48550/arXiv.2502.13398|
|915|Craw4LLM: Efficient Web Crawling for LLM Pretraining|Shi Yu, Zhiyuan Liu, Chenyan Xiong|2025-02-19|arXiv|https://github.com/cxcscmu/Crawl4LLM|http://arxiv.org/abs/2502.13347v1|
|916|Investigating and Extending Homans&apos; Social Exchange Theory with Large Language Model based Agents|Lei Wang, Zheqing Zhang, Xu Chen|2025-02-18|arXiv|https://github.com/Paitesanshi/SET|https://doi.org/10.48550/arXiv.2502.12450|
|917|Trust Me, I'm Wrong: High-Certainty Hallucinations in LLMs|Adi Simhi, Itay Itzhak, Fazl Barez, Gabriel Stanovsky, Yonatan Belinkov|2025-02-18|arXiv|https://github.com/technion-cs-nlp/Trust_me_Im_wrong|http://arxiv.org/abs/2502.12964v1|
|918|SparAMX: Accelerating Compressed LLMs Token Generation on AMX-powered CPUs|Ahmed F. AbouElhamayed, Jordan Dotzel, Yash Akhauri, Chi-Chih Chang, Sameh Gobriel, J. Pablo Muñoz, Vui Seng Chua, Nilesh Jain, Mohamed S. Abdelfattah|2025-02-18|arXiv|https://github.com/IntelLabs/Hardware-Aware-Automated-Machine-Learning/tree/main/SparAMX|http://arxiv.org/abs/2502.12444v1|
|919|MoBA: Mixture of Block Attention for Long-Context LLMs|Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, Zhiqi Huang, Huan Yuan, Suting Xu, Xinran Xu, Guokun Lai, Yanru Chen, Huabin Zheng, Junjie Yan, Jianlin Su, Yuxin Wu, Neo Y. Zhang, Zhilin Yang, Xinyu Zhou, Mingxing Zhang, Jiezhong Qiu|2025-02-18|arXiv|https://github.com/MoonshotAI/MoBA|http://arxiv.org/abs/2502.13189v1|
|920|Text2World: Benchmarking Large Language Models for Symbolic World Model Generation|Mengkang Hu, Tianxing Chen, Yude Zou, Yuheng Lei, Qiguang Chen, Ming Li, Yao Mu, Hongyuan Zhang, Wenqi Shao, Ping Luo|2025-02-18|arXiv|https://text-to-world.github.io/|https://doi.org/10.48550/arXiv.2502.13092|
|921|PTQ1.61: Push the Real Limit of Extremely Low-Bit Post-Training Quantization Methods for Large Language Models|Jiaqi Zhao, Miao Zhang, Ming Wang, Yuzhang Shang, Kaihao Zhang, Weili Guan, Yaowei Wang, Min Zhang|2025-02-18|arXiv|https://github.com/zjq0455/PTQ1.61|https://doi.org/10.48550/arXiv.2502.13179|
|922|Soundwave: Less is More for Speech-Text Alignment in LLMs|Yuhao Zhang, Zhiheng Liu, Fan Bu, Ruiyu Zhang, Benyou Wang, Haizhou Li|2025-02-18|arXiv|https://github.com/FreedomIntelligence/Soundwave|http://arxiv.org/abs/2502.12900v1|
|923|G-Refer: Graph Retrieval-Augmented Large Language Model for Explainable Recommendation|Yuhan Li, Xinni Zhang, Linhao Luo, Heng Chang, Yuxiang Ren, Irwin King, Jia Li|2025-02-18|arXiv|https://github.com/Yuhan1i/G-Refer|https://doi.org/10.48550/arXiv.2502.12586|
|924|Flow-of-Options: Diversified and Improved LLM Reasoning by Thinking Through Options|Lakshmi Nair, Ian Trase, Mark Kim|2025-02-18|arXiv|https://github.com/flagshippioneering/Flow-of-Options|http://arxiv.org/abs/2502.12929v1|
|925|EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning|Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, Junge Zhang|2025-02-18|arXiv|https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO|http://arxiv.org/abs/2502.12486v1|
|926|SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings|Weikai Lu, Hao Peng, Huiping Zhuang, Cen Chen, Ziqian Zeng|2025-02-18|arXiv|https://github.com/ZeroNLP/SEA|https://doi.org/10.48550/arXiv.2502.12562|
|927|Benchmarking Post-Training Quantization in LLMs: Comprehensive Taxonomy, Unified Evaluation, and Comparative Analysis|Jiaqi Zhao, Ming Wang, Miao Zhang, Yuzhang Shang, Xuebo Liu, Yaowei Wang, Min Zhang, Liqiang Nie|2025-02-18|arXiv|https://github.com/zjq0455/PTQ_Benchmark|http://arxiv.org/abs/2502.13178v1|
|928|HPSS: Heuristic Prompting Strategy Search for LLM Evaluators|Bosi Wen, Pei Ke, Yufei Sun, Cunxiang Wang, Xiaotao Gu, Jinfeng Zhou, Jie Tang, Hongning Wang, Minlie Huang|2025-02-18|arXiv|https://github.com/thu-coai/HPSS|http://arxiv.org/abs/2502.13031v1|
|929|CMQCIC-Bench: A Chinese Benchmark for Evaluating Large Language Models in Medical Quality Control Indicator Calculation|Guangya Yu, Yanhao Li, Zongying Jiang, Yuxiong Jin, Li Dai, Yupian Lin, Ruihui Hou, Weiyan Zhang, Yongqi Fan, Qi Ye, Jingping Liu, Tong Ruan|2025-02-17|arXiv|https://github.com/YuY-2001/C-MQCIC|https://doi.org/10.48550/arXiv.2502.11703|
|930|Nuclear Deployed: Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents|Rongwu Xu, Xiaojian Li, Shuo Chen, Wei Xu|2025-02-17|arXiv|https://llm-catastrophic-risks.github.io|http://arxiv.org/abs/2502.11355v3|
|931|Language Models Can See Better: Visual Contrastive Decoding For LLM Multimodal Reasoning|Yuqi Pang, Bowen Yang, Haoqin Tu, Yun Cao, Zeyu Zhang|2025-02-17|arXiv|https://github.com/Pbhgit/MVCD|http://arxiv.org/abs/2502.11751v1|
|932|Code-Vision: Evaluating Multimodal LLMs Logic Understanding and Code Generation Capabilities|Hanbin Wang, Xiaoxuan Zhou, Zhipeng Xu, Keyuan Cheng, Yuxin Zuo, Kai Tian, Jingwei Song, Junting Lu, Wenhui Hu, Xueyang Liu|2025-02-17|arXiv|https://github.com/wanghanbinpanda/CodeVision|http://arxiv.org/abs/2502.11829v1|
|933|Bitnet.cpp: Efficient Edge Inference for Ternary LLMs|Jinheng Wang, Hansong Zhou, Ting Song, Shijie Cao, Yan Xia, Ting Cao, Jianyu Wei, Shuming Ma, Hongyu Wang, Furu Wei|2025-02-17|arXiv|https://github.com/microsoft/BitNet/tree/paper|http://arxiv.org/abs/2502.11880v1|
|934|A-MEM: Agentic Memory for LLM Agents|Wujiang Xu, Kai Mei, Hang Gao, Juntao Tan, Zujie Liang, Yongfeng Zhang|2025-02-17|arXiv|https://github.com/WujiangXu/AgenticMemory|http://arxiv.org/abs/2502.12110v5|
|935|A Survey of Personalized Large Language Models: Progress and Future Directions|Jiahong Liu, Zexuan Qiu, Zhongyang Li, Quanyu Dai, Jieming Zhu, Minda Hu, Menglin Yang, Irwin King|2025-02-17|arXiv|https://github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models|https://doi.org/10.48550/arXiv.2502.11528|
|936|"Nuclear Deployed!": Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents|Rongwu Xu, Xiaojian Li, Shuo Chen, Wei Xu|2025-02-17|arXiv|https://github.com/pillowsofwind/LLM-CBRN-Risks|http://arxiv.org/abs/2502.11355v1|
|937|Atom of Thoughts for Markov LLM Test-Time Scaling|Fengwei Teng, Zhaoyang Yu, Quan Shi, Jiayi Zhang, Chenglin Wu, Yuyu Luo|2025-02-17|arXiv|https://github.com/qixucen/atom|http://arxiv.org/abs/2502.12018v1|
|938|VRoPE: Rotary Position Embedding for Video Large Language Models|Zikang Liu, Longteng Guo, Yepeng Tang, Tongtian Yue, Junxian Cai, Kai Ma, Qingbin Liu, Xi Chen, Jing Liu|2025-02-17|arXiv|https://github.com/johncaged/VRoPE|https://doi.org/10.48550/arXiv.2502.11664|
|939|SoftCoT: Soft Chain-of-Thought for Efficient Reasoning with LLMs|Yige Xu, Xu Guo, Zhiwei Zeng, Chunyan Miao|2025-02-17|arXiv|https://github.com/xuyige/SoftCoT|http://arxiv.org/abs/2502.12134v1|
|940|KnowPath: Knowledge-enhanced Reasoning via LLM-generated Inference Paths over Knowledge Graphs|Qi Zhao, Hongyu Yang, Qi Song, Xinwei Yao, Xiangyang Li|2025-02-17|arXiv|https://github.com/tize-72/KnowPath|http://arxiv.org/abs/2502.12029v1|
|941|Idiosyncrasies in Large Language Models|Mingjie Sun, Yida Yin, Zhiqiu Xu, J. Zico Kolter, Zhuang Liu|2025-02-17|arXiv|https://eric-mingjie.github.io/llm-idiosyncrasies/index.html|https://doi.org/10.48550/arXiv.2502.12150|
|942|Can LLM Watermarks Robustly Prevent Unauthorized Knowledge Distillation?|Leyi Pan, Aiwei Liu, Shiyu Huang, Yijian Lu, Xuming Hu, Lijie Wen, Irwin King, Philip S. Yu|2025-02-17|arXiv|https://github.com/THU-BPM/Watermark-Radioactivity-Attack|http://arxiv.org/abs/2502.11598v1|
|943|RIDE: Enhancing Large Language Model Alignment through Restyled In-Context Learning Demonstration Exemplars|Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari|2025-02-17|arXiv|https://github.com/AnonymousCode-ComputerScience/RIDE|https://doi.org/10.48550/arXiv.2502.11681|
|944|Don't Get Lost in the Trees: Streamlining LLM Reasoning by Overcoming Tree Search Exploration Pitfalls|Ante Wang, Linfeng Song, Ye Tian, Dian Yu, Haitao Mi, Xiangyu Duan, Zhaopeng Tu, Jinsong Su, Dong Yu|2025-02-16|arXiv|https://github.com/Soistesimmer/Fetch|http://arxiv.org/abs/2502.11183v1|
|945|Ramp Up NTT in Record Time using GPU-Accelerated Algorithms and LLM-based Code Generation|Yu Cui, Hang Fu, Licheng Wang, Haibin Zhang|2025-02-16|arXiv|https://github.com/LMPC-Lab/GenGPUCrypto|http://arxiv.org/abs/2502.11110v1|
|946|MasRouter: Learning to Route LLMs for Multi-Agent Systems|Yanwei Yue, Guibin Zhang, Boyang Liu, Guancheng Wan, Kun Wang, Dawei Cheng, Yiyan Qi|2025-02-16|arXiv|https://github.com/yanweiyue/masrouter|http://arxiv.org/abs/2502.11133v1|
|947|G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems|Shilong Wang, Guibin Zhang, Miao Yu, Guancheng Wan, Fanci Meng, Chongye Guo, Kun Wang, Yang Wang|2025-02-16|arXiv|https://github.com/wslong20/G-safeguard|http://arxiv.org/abs/2502.11127v1|
|948|Reasoning-Augmented Conversation for Multi-Turn Jailbreak Attacks on Large Language Models|Zonghao Ying, Deyue Zhang, Zonglei Jing, Yisong Xiao, Quanchen Zou, Aishan Liu, Siyuan Liang, Xiangzheng Zhang, Xianglong Liu, Dacheng Tao|2025-02-16|arXiv|https://github.com/NY1024/RACE|https://doi.org/10.48550/arXiv.2502.11054|
|949|BoT: Breaking Long Thought Processes of o1-like Large Language Models through Backdoor Attack|Zihao Zhu, Hongbao Zhang, Mingda Zhang, Ruotong Wang, Guanzong Wu, Ke Xu, Baoyuan Wu|2025-02-16|arXiv|https://github.com/zihao-ai/BoT|https://doi.org/10.48550/arXiv.2502.12202|
|950|SURGE: On the Potential of Large Language Models as General-Purpose Surrogate Code Executors|Bohan Lyu, Siqiao Huang, Zichen Liang, Qi-An Sun, Jiaming Zhang|2025-02-16|arXiv|https://github.com/Imbernoulli/SURGE|https://doi.org/10.48550/arXiv.2502.11167|
|951|LogiDynamics: Unraveling the Dynamics of Logical Inference in Large Language Model Reasoning|Tianshi Zheng, Jiayang Cheng, Chunyang Li, Haochen Shi, Zihao Wang, Jiaxin Bai, Yangqiu Song, Ginny Y. Wong, Simon See|2025-02-16|arXiv|https://github.com/HKUST-KnowComp/LogiDynamics|https://doi.org/10.48550/arXiv.2502.11176|
|952|Unveiling Environmental Impacts of Large Language Model Serving: A Functional Unit View|Yanran Wu, Inez Hua, Yi Ding|2025-02-16|arXiv|https://github.com/jojacola/FUEL|https://doi.org/10.48550/arXiv.2502.11256|
|953|RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation|Pengcheng Jiang, Lang Cao, Ruike Zhu, Minhao Jiang, Yunyi Zhang, Jimeng Sun, Jiawei Han|2025-02-16|arXiv|https://github.com/pat-jj/RAS|http://arxiv.org/abs/2502.10996v2|
|954|MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models|Jiahao Huo, Yibo Yan, Xu Zheng, Yuanhuiyi Lyu, Xin Zou, Zhihua Wei, Xuming Hu|2025-02-16|arXiv|https://github.com/Z1zs/MMUnlearner|https://doi.org/10.48550/arXiv.2502.11051|
|955|How Do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training|Yixin Ou, Yunzhi Yao, Ningyu Zhang, Hui Jin, Jiacheng Sun, Shumin Deng, Zhenguo Li, Huajun Chen|2025-02-16|arXiv|https://github.com/zjunlp/DynamicKnowledgeCircuits|http://arxiv.org/abs/2502.11196v1|
|956|Exposing Numeracy Gaps: A Benchmark to Evaluate Fundamental Numerical Abilities in Large Language Models|Haoyang Li, Xuejia Chen, Zhanchao Xu, Darian Li, Nicole Hu, Fei Teng, Yiming Li, Luyu Qiu, Chen Jason Zhang, Qing Li, Lei Chen|2025-02-16|arXiv|https://github.com/TreeAI-Lab/NumericBench|https://doi.org/10.48550/arXiv.2502.11075|
|957|CORDIAL: Can Multimodal Large Language Models Effectively Understand Coherence Relationships?|Aashish Anantha Ramakrishnan, Aadarsh Anantha Ramakrishnan, Dongwon Lee|2025-02-16|arXiv|https://github.com/aashish2000/CORDIAL|https://doi.org/10.48550/arXiv.2502.11300|
|958|ReLearn: Unlearning via Learning for Large Language Models|Haoming Xu, Ningyuan Zhao, Liming Yang, Sendong Zhao, Shumin Deng, Mengru Wang, Bryan Hooi, Nay Oo, Huajun Chen, Ningyu Zhang|2025-02-16|arXiv|https://github.com/zjunlp/unlearn|https://doi.org/10.48550/arXiv.2502.11190|
|959|CalibQuant: 1-Bit KV Cache Quantization for Multimodal LLMs|Insu Han, Zeliang Zhang, Zhiyuan Wang, Yifan Zhu, Susan Liang, Jiani Liu, Haiting Lin, Mingjie Zhao, Chenliang Xu, Kun Wan, Wentian Zhao|2025-02-15|arXiv|https://github.com/insuhan/calibquant|http://arxiv.org/abs/2502.14882v2|
|960|Do LLMs Recognize Your Preferences? Evaluating Personalized Preference Following in LLMs|Siyan Zhao, Mingyi Hong, Yang Liu, Devamanyu Hazarika, Kaixiang Lin|2025-02-15|arXiv …, 2025|https://prefeval.github.io/|http://arxiv.org/abs/2502.09597v1|
|961|SQuARE: Sequential Question Answering Reasoning Engine for Enhanced Chain-of-Thought in Large Language Models|Daniel Fleischer, Moshe Berchansky, Gad Markovits, Moshe Wasserblat|2025-02-15|arXiv|https://github.com/IntelLabs/RAG-FiT/tree/square|https://doi.org/10.48550/arXiv.2502.09390|
|962|Injecting Domain-Specific Knowledge into Large Language Models: A Comprehensive Survey|Zirui Song, Bin Yan, Yuhan Liu, Miao Fang, Mingzhe Li, Rui Yan, Xiuying Chen|2025-02-15|arXiv|https://github.com/abilliyb/Knowledge_Injection_Survey_Papers|https://doi.org/10.48550/arXiv.2502.10708|
|963|An Empirical Analysis of Uncertainty in Large Language Model Evaluations|Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang|2025-02-15|arXiv|https://github.com/hasakiXie123/LLM-Evaluator-Uncertainty|https://doi.org/10.48550/arXiv.2502.10709|
|964|RoSTE: An Efficient Quantization-Aware Supervised Fine-Tuning Approach for Large Language Models|Quan Wei, Chung-Yiu Yau, Hoi-To Wai, Yang Katie Zhao, Dongyeop Kang, Youngsuk Park, Mingyi Hong|2025-02-15|arXiv|https://github.com/OptimAI-Lab/RoSTE|https://doi.org/10.48550/arXiv.2502.09003|
|965|EmbodiedBench: Comprehensive Benchmarking Multi-modal Large Language Models for Vision-Driven Embodied Agents|Rui Yang, Hanyang Chen, Junyu Zhang, Mark Zhao, Cheng Qian, Kangrui Wang, Qineng Wang, Teja Venkat Koripella, Marziyeh Movahedi, Manling Li, Heng Ji, Huan Zhang, Tong Zhang|2025-02-15|arXiv|https://embodiedbench.github.io|https://doi.org/10.48550/arXiv.2502.09560|
|966|LintLLM: An Open-Source Verilog Linting Framework Based on Large Language Models|Zhigang Fang, Renzhi Chen, Zhijie Yang, Yang Guo, Huadong Dai, Lei Wang|2025-02-15|arXiv|https://github.com/fangzhigang32/Static-Verilog-Analysis|https://doi.org/10.48550/arXiv.2502.10815|
|967|V2V-LLM: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multi-Modal Large Language Models|Hsu-Kuang Chiu, Ryo Hachiuma, Chien-Yi Wang, Stephen F. Smith, Yu-Chiang Frank Wang, Min-Hung Chen|2025-02-14|arXiv|https://eddyhkchiu.github.io/v2vllm.github.io/|https://doi.org/10.48550/arXiv.2502.09980|
|968|KKA: Improving Vision Anomaly Detection through Anomaly-related Knowledge from Large Language Models|Dong Chen, Zhengqing Hu, Peiguang Fan, Yueting Zhuang, Yafei Li, Qidong Liu, Xiaoheng Jiang, Mingliang Xu|2025-02-14|arXiv|https://github.com/Anfeather/KKA|https://doi.org/10.48550/arXiv.2502.14880|
|969|Large Language Diffusion Models|Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, Chongxuan Li|2025-02-14|arXiv|https://ml-gsai.github.io/LLaDA-demo/|https://doi.org/10.48550/arXiv.2502.09992|
|970|LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs - No Silver Bullet for LC or RAG Routing|Kuan Li, Liwen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Shuai Wang, Minhao Cheng|2025-02-14|arXiv|https://github.com/likuanppd/LaRA|http://arxiv.org/abs/2502.09977v1|
|971|MM-RLHF: The Next Step Forward in Multimodal LLM Alignment|Yi-Fan Zhang, Tao Yu, Haochen Tian, Chaoyou Fu, Peiyan Li, Jianshu Zeng, Wulin Xie, Yang Shi, Huanyu Zhang, Junkang Wu, Xue Wang, Yibo Hu, Bin Wen, Fan Yang, Zhang Zhang, Tingting Gao, Di Zhang, Liang Wang, Rong Jin, Tieniu Tan|2025-02-14|arXiv|https://mm-rlhf.github.io/|http://arxiv.org/abs/2502.10391v1|
|972|FinRL-DeepSeek: LLM-Infused Risk-Sensitive Reinforcement Learning for Trading Agents|Mostapha Benhenda|2025-02-13|arXiv:2502.07393, 2025|https://github.com/benstaf/FinRL_DeepSeek|http://arxiv.org/abs/2502.07393v1|
|973|Ask Patients with Patience: Enabling LLMs for Human-Centric Medical Dialogue with Grounded Reasoning|Jiayuan Zhu, Junde Wu|2025-02-13|arXiv:2502.07143, 2025|https://github.com/SuperMedIntel/AskPatients|http://arxiv.org/abs/2502.07143v1|
|974|LLM-Generated Microservice Implementations from RESTful API Definitions|Saurabh Chauhan, Zeeshan Rasheed, Abdul Malik Sami, Zheying Zhang, Jussi Rasku, Kai-Kristian Kemell, Pekka Abrahamsson|2025-02-13|arXiv|https://github.com/sirbh/code-gen|http://arxiv.org/abs/2502.09766v1|
|975|Bag of Tricks for Inference-time Computation of LLM Reasoning|Fan Liu, Wenshuo Chao, Naiqiang Tan, Hao Liu|2025-02-13|arXiv:2502.07191, 2025|https://github.com/usail-hkust/benchmark_inference_time_computation_LL|http://arxiv.org/abs/2502.07191v2|
|976|DrugImproverGPT: A Large Language Model for Drug Optimization with Fine-Tuning via Structured Policy Optimization|Xuefeng Liu, Songhao Jiang, Siyu Chen, Zhuoran Yang, Yuxin Chen, Ian T. Foster, Rick Stevens|2025-02-13|arXiv|https://github.com/xuefeng-cs/DrugImproverGPT|https://doi.org/10.48550/arXiv.2502.07237|
|977|LLMs Can Easily Learn to Reason from Demonstrations Structure, not content, is what matters!|Dacheng Li, Shiyi Cao, Tyler Griggs, Shu Liu, Xiangxi Mo, Eric Tang, Sumanth Hegde, Kourosh Hakhamaneshi, Shishir G. Patil, Matei Zaharia, Joseph E. Gonzalez, Ion Stoica|2025-02-13|arXiv …, 2025|https://github.com/NovaSky-AI/SkyThought|http://arxiv.org/abs/2502.07374v2|
|978|DarwinLM: Evolutionary Structured Pruning of Large Language Models|Shengkun Tang, Oliver Sieberling, Eldar Kurtic, Zhiqiang Shen, Dan Alistarh|2025-02-13|arXiv|https://github.com/IST-DASLab/DarwinLM|https://doi.org/10.48550/arXiv.2502.07780|
|979|Brain-Inspired Exploration of Functional Networks and Key Neurons in Large Language Models|Yiheng Liu, Xiaohui Gao, Haiyang Sun, Bao Ge, Tianming Liu, Junwei Han, Xintao Hu|2025-02-13|arXiv|https://github.com/WhatAboutMyStar/LLM_ACTIVATION|https://doi.org/10.48550/arXiv.2502.20408|
|980|The Hidden Dimensions of LLM Alignment: A Multi-Dimensional Analysis of Orthogonal Safety Directions|Wenbo Pan, Zhichao Liu, Qiguang Chen, Xiangyang Zhou, Haining Yu, Xiaohua Jia|2025-02-13|arXiv|https://github.com/BMPixel/safety-residual-space|http://arxiv.org/abs/2502.09674v4|
|981|QueryAttack: Jailbreaking Aligned Large Language Models Using Structured Non-natural Query Language|Qingsong Zou, Jingyu Xiao, Qing Li, Zhi Yan, Yuhang Wang, Li Xu, Wenxuan Wang, Kuofeng Gao, Ruoyu Li, Yong Jiang|2025-02-13|arXiv|https://github.com/horizonsinzqs/QueryAttack|http://arxiv.org/abs/2502.09723v3|
|982|LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation|Zican Dong, Junyi Li, Jinhao Jiang, Mingyu Xu, Wayne Xin Zhao, Bingning Wang, Weipeng Chen|2025-02-13|arXiv|https://github.com/RUCAIBox/LongReD|https://doi.org/10.48550/arXiv.2502.07365|
|983|Making Them a Malicious Database: Exploiting Query Code to Jailbreak Aligned Large Language Models|Qingsong Zou, Jingyu Xiao, Qing Li, Zhi Yan, Yuhang Wang, Li Xu, Wenxuan Wang, Kuofeng Gao, Ruoyu Li, Yong Jiang|2025-02-13|arXiv|https://github.com/horizonsinzqs/QueryAttack|https://doi.org/10.48550/arXiv.2502.09723|
|984|Large Language Models Meet Symbolic Provers for Logical Reasoning Evaluation|Chengwen Qi, Ren Ma, Bowen Li, He Du, Binyuan Hui, Jinwang Wu, Yuanjun Laili, Conghui He|2025-02-12|arXiv|https://github.com/opendatalab/ProverGen|https://doi.org/10.48550/arXiv.2502.06563|
|985|LCIRC: A Recurrent Compression Approach for Efficient Long-form Context and Query Dependent Modeling in LLMs|Sumin An, Junyoung Sung, Wonpyo Park, Chanjun Park, Paul Hongsuck Seo|2025-02-12|arXiv:2502.06139, 2025|https://ssuminan.github.io/LCIRC/|http://arxiv.org/abs/2502.06139v1|
|986|RALLRec: Improving Retrieval Augmented Large Language Model Recommendation with Representation Learning|Jian Xu, Sichun Luo, Xiangyu Chen, Haoming Huang, Hanxu Hou, Linqi Song|2025-02-12|arXiv|https://github.com/JianXu95/RALLRec|https://doi.org/10.48550/arXiv.2502.06101|
|987|Systematic Outliers in Large Language Models|Yongqi An, Xu Zhao, Tao Yu, Ming Tang, Jinqiao Wang|2025-02-12|arXiv|https://github.com/an-yongqi/systematic-outliers|https://doi.org/10.48550/arXiv.2502.06415|
|988|Towards Zero-Shot Anomaly Detection and Reasoning with Multimodal Large Language Models|Jiacong Xu, Shao-Yuan Lo, Bardia Safaei, Vishal M. Patel, Isht Dwivedi|2025-02-12|arXiv|https://xujiacong.github.io/Anomaly-OV/|https://doi.org/10.48550/arXiv.2502.07601|
|989|Data Augmentation to Improve Large Language Models in Food Hazard and Product Detection|Areeg Fahad Rasheed, M. Zarkoosh, Shimam Amer Chasib, Safa F. Abbas|2025-02-12|arXiv|https://github.com/AREEG94FAHAD/food-hazard-prdouct-cls|https://doi.org/10.48550/arXiv.2502.08687|
|990|Calibrating LLMs with Information-Theoretic Evidential Deep Learning|Yawei Li, David Rügamer, Bernd Bischl, Mina Rezaei|2025-02-12|arXiv:2502.06351, 2025|https://github.com/sandylaker/ib-edl|http://arxiv.org/abs/2502.06351v2|
|991|LawGPT: Knowledge-Guided Data Generation and Its Application to Legal LLM|Zhi Zhou, Kun-Yang Yu, Shi-Yu Tian, Xiao-Wen Yang, Jiang-Xin Shi, Pengxiao Song, Yi-Xuan Jin, Lan-Zhe Guo, Yu-Feng Li|2025-02-12|arXiv …, 2025|https://github.com/LAMDASZ-ML/Knowledge-Guide-Data-Generation|http://arxiv.org/abs/2502.06572v2|
|992|The foundational capabilities of large language models in predicting postoperative risks using clinical notes|Charles Alba, Bing Xue, Joanna Abraham, Thomas George Kannampallil, Chenyang Lu|2025-02-11|npj Digit. Medicine|https://github.com/cja5553/LLMs_in_perioperative_care|https://doi.org/10.1038/s41746-025-01489-2|
|993|Time2Lang: Bridging Time-Series Foundation Models and Large Language Models for Health Sensing Beyond Prompting|Arvind Pillai, Dimitris Spathis, Subigya Nepal, Amanda C Collins, Daniel M Mackin, Michael V Heinz, Tess Z Griffin, Nicholas C Jacobson, Andrew Campbell|2025-02-11|arXiv|https://github.com/arvind1609/time2lang|http://arxiv.org/abs/2502.07608v3|
|994|Demystifying Singular Defects in Large Language Models|Haoqi Wang, Tong Zhang, Mathieu Salzmann|2025-02-11|arXiv|https://github.com/haoqiwang/singular_defect|https://doi.org/10.48550/arXiv.2502.07004|
|995|Dynamic Loss-Based Sample Reweighting for Improved Large Language Model Pretraining|Daouda Sow, Herbert Woisetschläger, Saikiran Bulusu, Shiqiang Wang, Hans-Arno Jacobsen, Yingbin Liang|2025-02-10|arXiv|https://github.com/sowmaster/Sample-Level-Loss-Reweighting-ICLR-2025|https://doi.org/10.48550/arXiv.2502.06733|
|996|LLMs in Software Security: A Survey of Vulnerability Detection Techniques and Insights|Ze Sheng, Zhicheng Chen, Shuning Gu, Heqing Huang, Guofei Gu, Jeff Huang|2025-02-10|arXiv|https://github.com/OwenSanzas/LLM-For-Vulnerability-Detection|http://arxiv.org/abs/2502.07049v2|
|997|The Curse of Depth in Large Language Models|Wenfang Sun, Xinyuan Song, Pengxiang Li, Lu Yin, Yefeng Zheng, Shiwei Liu|2025-02-09|arXiv|https://github.com/lmsdss/LayerNorm-Scaling|https://doi.org/10.48550/arXiv.2502.05795|
|998|HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models|Paul Darm, Annalisa Riccardi|2025-02-09|arXiv|https://github.com/PaulDrm/targeted_intervention|http://arxiv.org/abs/2502.05945v2|
|999|Peeking Behind Closed Doors: Risks of LLM Evaluation by Private Data Curators|Hritik Bansal, Pratyush Maini|2025-02-09|arXiv|https://pratyushmaini.github.io/blog/2024/risks-private-evals/|http://arxiv.org/abs/2503.04756v1|
|1000|AutoAgent: A Fully-Automated and Zero-Code Framework for LLM Agents|Jiabin Tang, Tianyu Fan, Chao Huang|2025-02-09|arXiv|https://github.com/HKUDS/AutoAgent|http://arxiv.org/abs/2502.05957v2|
|1001|MetaChain: A Fully-Automated and Zero-Code Framework for LLM Agents|Jiabin Tang, Tianyu Fan, Chao Huang|2025-02-09|arXiv|https://github.com/HKUDS/MetaChain|http://arxiv.org/abs/2502.05957v1|
|1002|OntoTune: Ontology-Driven Self-training for Aligning Large Language Models|Zhiqiang Liu, Chengtao Gan, Junjie Wang, Yichi Zhang, Zhongpu Bo, Mengshu Sun, Huajun Chen, Wen Zhang|2025-02-08|arXiv|https://github.com/zjukg/OntoTune|https://doi.org/10.48550/arXiv.2502.05478|
