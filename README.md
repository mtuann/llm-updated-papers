# Table of Contents
1. [Large Language Models Papers](#large-language-models-papers)
2. [Other topics](#other-topics)
3. [Large Language Models Papers with Code](#large-language-models-papers-with-code)

# Large Language Models Papers
This Github repository contains a list of Federated Learning papers that are updated until **September 13, 2024**.
- The resources are collected from various sources, including arXiv, NeurIPS, ICML, ICLR, ACL, EMNLP, AAAI, IJCAI, KDD, CVPR, ICCV, ECCV, NIPS, IEEE, ACM, Springer, ScienceDirect, Wiley, Nature, Science, and other top AI/ ML conferences and journals.

---
# Other Topics
- For **Large Language Models** papers, please visit the [**LLM Repository**](https://github.com/mtuann/llm-updated-papers).
- For **Backdoor Learning** papers, please visit the [**Backdoor Learning Repository**](https://github.com/mtuann/backdoor-ai-resources).
- For **Federated Learning** papers, please visit the [**Federated Learning Repository**](https://github.com/mtuann/federated-learning-updated-papers).
- For **Machine Unlearning** papers, please visit the [**Machine Unlearning Repository**](https://github.com/mtuann/machine-unlearning-papers).

---
If you find this useful, please give us a **star** or support me by buying me a coffee.

<div style="text-align: center;">
    <a href="https://paypal.me/AIsmylife" target="_blank">
        <img src="https://www.paypalobjects.com/en_US/i/btn/btn_donateCC_LG.gif" alt="paypal">
    </a>
</div>


# Large Language Models Papers with Code
Due to GitHub repository limitations, we only include papers that provide accompanying code and sorted by the publish date. If you want to access the full list of papers, please visit the [shinyapps's website](https://mtuann.shinyapps.io/research-papers/).

---

|No.|Title|Authors|Publish Date|Venue|Code|URL|
|---|---|---|---|---|---|---|
|1|A Survey on Large Language Model based Autonomous Agents|Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen|2024-12-01|arXiv|https://github.com/Paitesanshi/LLM-Agent-Survey|https://doi.org/10.48550/arXiv.2308.11432|
|2|PneumoLLM: Harnessing the power of large language model for pneumoconiosis diagnosis|Meiyue Song, Jiarui Wang, Zhihua Yu, Jiaxin Wang, Le Yang, Yuting Lu, Baicun Li, Xue Wang, Xiaoxu Wang, Qinghua Huang, Zhijun Li, Nikolaos I. Kanellakis, Jiangfeng Liu, Jing Wang, Binglu Wang, Juntao Yang|2024-10-01|Medical Image Anal.|https://github.com/CodeMonsterPHD/PneumoLLM/tree/main|https://doi.org/10.1016/j.media.2024.103248|
|3|Enhancing text-based knowledge graph completion with zero-shot large language models: A focus on semantic enhancement|Rui Yang, Jiahao Zhu, Jianping Man, Li Fang, Yi Zhou|2024-09-27|Knowl. Based Syst.|https://github.com/sjlmg/CP-KGC|https://doi.org/10.1016/j.knosys.2024.112155|
|4|Pooling And Attention: What Are Effective Designs For LLm-Based Embedding Models?|Yixuan Tang, Yi Yang|2024-09-04|arXiv|https://github.com/yixuantt/PoolingAndAttn|http://arxiv.org/abs/2409.02727v1|
|5|Alignment-Aware Model Extraction Attacks on Large Language Models|Zi Liang, Qingqing Ye, Yanyun Wang, Sen Zhang, Yaxin Xiao, Ronghua Li, Jianliang Xu, Haibo Hu|2024-09-04|arXiv|https://github.com/liangzid/alignmentExtraction|http://arxiv.org/abs/2409.02718v1|
|6|Hypothesizing Missing Causal Variables with LLMs|Ivaxi Sheth, Sahar Abdelnabi, Mario Fritz|2024-09-04|arXiv|https://github.com/ivaxi0s/hypothesizing-causal-variable-llm|http://arxiv.org/abs/2409.02604v1|
|7|Foundations of Large Language Model Compression -- Part 1: Weight Quantization|Sean I. Young|2024-09-03|arXiv|https://github.com/seannz/cvxq|http://arxiv.org/abs/2409.02026v1|
|8|MMLU-Pro+: Evaluating Higher-Order Reasoning and Shortcut Learning in LLMs|Saeid Asgari Taghanaki, Aliasgahr Khani, Amir Khasahmadi|2024-09-03|arXiv|https://github.com/asgsaeid/mmlu-pro-plus|http://arxiv.org/abs/2409.02257v1|
|9|Booster: Tackling Harmful Fine-tuing for Large Language Models via Attenuating Harmful Perturbation|Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu|2024-09-03|arXiv|https://github.com/git-disl/Booster|http://arxiv.org/abs/2409.01586v1|
|10|Exploiting the Vulnerability of Large Language Models via Defense-Aware Architectural Backdoor|Abdullah Arafat Miah, Yu Bi|2024-09-03|arXiv|https://github.com/SiSL-URI/Arch_Backdoor_LLM|http://arxiv.org/abs/2409.01952v1|
|11|Prompt Compression with Context-Aware Sentence Encoding for Fast and Improved LLM Inference|Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, Shane Luke|2024-09-02|arXiv|https://github.com/Workday/cpc|http://arxiv.org/abs/2409.01227v2|
|12|FlashFlex: Accommodating Large Language Model Training over Heterogeneous Environment|Ran Yan, Youhe Jiang, Wangcheng Tao, Xiaonan Nie, Bin Cui, Binhang Yuan|2024-09-02|arXiv|https://github.com/Relaxed-System-Lab/FlashFlex|http://arxiv.org/abs/2409.01143v1|
|13|Large Language Models versus Classical Machine Learning: Performance in COVID-19 Mortality Prediction Using High-Dimensional Tabular Data|Mohammadreza Ghaffarzadeh-Esfahani, Mahdi Ghaffarzadeh-Esfahani, Arian Salahi-Niri, Hossein Toreyhi, Zahra Atf, Amirali Mohsenzadeh-Kermani, Mahshad Sarikhani, Zohreh Tajabadi, Fatemeh Shojaeian, Mohammad Hassan Bagheri, Aydin Feyzi, Mohammadamin Tarighatpayma, Narges Gazmeh, Fateme Heydari, Hossein Afshar, Amirreza Allahgholipour, Farid Alimardani, Ameneh Salehi, Naghmeh Asadimanesh, Mohammad Amin Khalafi, Hadis Shabanipour, Ali Moradi, Sajjad Hossein Zadeh, Omid Yazdani, Romina Esbati, Moozhan Maleki, Danial Samiei Nasr, Amirali Soheili, Hossein Majlesi, Saba Shahsavan, Alireza Soheilipour, Nooshin Goudarzi, Erfan Taherifard, Hamidreza Hatamabadi, Jamil S Samaan, Thomas Savage, Ankit Sakhuja, Ali Soroush, Girish Nadkarni, Ilad Alavi Darazam, Mohamad Amin Pourhoseingholi, Seyed Amir Ahmad Safavi-Naini|2024-09-02|arXiv|https://github.com/mohammad-gh009/Large-Language-Models-vs-Classical-Machine-learning|http://arxiv.org/abs/2409.02136v1|
|14|Automatic Pseudo-Harmful Prompt Generation for Evaluating False Refusals in Large Language Models|Bang An, Sicheng Zhu, Ruiyi Zhang, Michael-Andrei Panaitescu-Liess, Yuancheng Xu, Furong Huang|2024-09-01|arXiv|https://github.com/umd-huang-lab/FalseRefusal|http://arxiv.org/abs/2409.00598v1|
|15|Harnessing the Power of Semi-Structured Knowledge and LLMs with Triplet-Based Prefiltering for Question Answering|Derian Boer, Fabian Koch, Stefan Kramer|2024-09-01|arXiv|https://github.com/kramerlab/4StepFocus|http://arxiv.org/abs/2409.00861v1|
|16|Rome was Not Built in a Single Step: Hierarchical Prompting for LLM-based Chip Design|Andre Nakkab, Sai Qian Zhang, Ramesh Karri, Siddharth Garg|2024-09|MLCAD '24: Proceedings of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD|https://github.com/ajn313/ROME-LLM|https://dl.acm.org/doi/10.1145/3670474.3685964|
|17|LongRecipe: Recipe for Efficient Long Context Generalization in Large Language Models|Zhiyuan Hu, Yuliang Liu, Jinman Zhao, Suyuchen Wang, Yan Wang, Wei Shen, Qing Gu, Anh Tuan Luu, See-Kiong Ng, Zhiwei Jiang, Bryan Hooi|2024-08-31|arXiv|https://github.com/zhiyuanhubj/LongRecipe|http://arxiv.org/abs/2409.00509v2|
|18|MultiMath: Bridging Visual and Mathematical Reasoning for Large Language Models|Shuai Peng, Di Fu, Liangcai Gao, Xiuqin Zhong, Hongguang Fu, Zhi Tang|2024-08-30|arXiv|https://github.com/pengshuai-rin/MultiMath|http://arxiv.org/abs/2409.00147v1|
|19|A Survey on Evaluation of Large Language Models|Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, Xing Xie|2024-08-28|ACM Transactions on Intelligent Systems and Technology (TIST), Volume 15, Issue 3|https://llm-eval.github.io/|https://dl.acm.org/doi/10.1145/3641289|
|20|CBF-LLM: Safe Control for LLM Alignment|Yuya Miyaoka, Masaki Inoue|2024-08-28|arXiv|https://github.com/Mya-Mya/CBF-LLM|http://arxiv.org/abs/2408.15625v1|
|21|Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders|Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, Guilin Liu|2024-08-28|arXiv|https://github.com/NVlabs/Eagle|http://arxiv.org/abs/2408.15998v1|
|22|Efficient LLM Scheduling by Learning to Rank|Yichao Fu, Siqi Zhu, Runlong Su, Aurick Qiao, Ion Stoica, Hao Zhang|2024-08-28|arXiv|https://github.com/hao-ai-lab/vllm-ltr|http://arxiv.org/abs/2408.15792v1|
|23|RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with Rich Linguistic Semantics from Openly Available Data and Large Language Models|Junyao Ge, Yang Zheng, Kaitai Guo, Jimin Liang|2024-08-27|arXiv|https://github.com/SlytherinGe/RSTeller|http://arxiv.org/abs/2408.14744v1|
|24|PAT: Pruning-Aware Tuning for Large Language Models|Yijiang Liu, Huanrui Yang, Youxin Chen, Rongyu Zhang, Miao Wang, Yuan Du, Li Du|2024-08-27|arXiv|https://github.com/kriskrisliu/PAT_Pruning-Aware-Tuning|http://arxiv.org/abs/2408.14721v1|
|25|LyCon: Lyrics Reconstruction from the Bag-of-Words Using Large Language Models|Haven Kim, Kahyun Choi|2024-08-27|arXiv|https://github.com/havenpersona/lycon|http://arxiv.org/abs/2408.14750v1|
|26|AgentMove: Predicting Human Mobility Anywhere Using Large Language Model based Agentic Framework|Jie Feng, Yuwei Du, Jie Zhao, Yong Li|2024-08-26|arXiv|https://github.com/tsinghua-fib-lab/AgentMove|http://arxiv.org/abs/2408.13986v1|
|27|CURLoRA: Stable LLM Continual Fine-Tuning and Catastrophic Forgetting Mitigation|Muhammad Fawi|2024-08-26|arXiv|https://github.com/MNoorFawi/curlora|http://arxiv.org/abs/2408.14572v1|
|28|ConVis: Contrastive Decoding with Hallucination Visualization for Mitigating Hallucinations in Multimodal Large Language Models|Yeji Park, Deokyeong Lee, Junsuk Choe, Buru Chang|2024-08-25|arXiv|https://github.com/yejipark-m/ConVis|http://arxiv.org/abs/2408.13906v1|
|29|Vision-Language and Large Language Model Performance in Gastroenterology: GPT, Claude, Llama, Phi, Mistral, Gemma, and Quantized Models|Seyed Amir Ahmad Safavi-Naini, Shuhaib Ali, Omer Shahab, Zahra Shahhoseini, Thomas Savage, Sara Rafiee, Jamil S Samaan, Reem Al Shabeeb, Farah Ladak, Jamie O Yang, Juan Echavarria, Sumbal Babar, Aasma Shaukat, Samuel Margolis, Nicholas P Tatonetti, Girish Nadkarni, Bara El Kurdi, Ali Soroush|2024-08-25|arXiv|https://github.com/Sdamirsa/LLM-VLM-in-Gastroenterology|http://arxiv.org/abs/2409.00084v2|
|30|HRGraph: Leveraging LLMs for HR Data Knowledge Graphs with Information Propagation-based Job Recommendation|Azmine Toushik Wasi|2024-08-24|Proceedings of the 1st Workshop on Knowledge Graphs and Large Language Models (KaLLM 2024), Association for Computational Linguistics 2024|https://github.com/azminewasi/HRGraph|http://arxiv.org/abs/2408.13521v1|
|31|LlamaDuo: LLMOps Pipeline for Seamless Migration from Service LLMs to Small-Scale Local LLMs|Chansung Park, Juyong Jiang, Fan Wang, Sayak Paul, Jing Tang, Sunghun Kim|2024-08-24|arXiv|https://github.com/deep-diver/llamaduo|http://arxiv.org/abs/2408.13467v2|
|32|vitaLITy 2: Reviewing Academic Literature Using Large Language Models|Hongye An, Arpit Narechania, Emily Wall, Kai Xu|2024-08-24|arXiv|https://vitality-vis.github.io|http://arxiv.org/abs/2408.13450v1|
|33|Semantic Alignment for Multimodal Large Language Models|Tao Wu, Mengze Li, Jingyuan Chen, Wei Ji, Wang Lin, Jinyang Gao, Kun Kuang, Zhou Zhao, Fei Wu|2024-08-23|arXiv|https://mccartney01.github.io/SAM|http://arxiv.org/abs/2408.12867v1|
|34|MME-RealWorld: Could Your Multimodal LLM Challenge High-Resolution Real-World Scenarios that are Difficult for Humans?|Yi-Fan Zhang, Huanyu Zhang, Haochen Tian, Chaoyou Fu, Shuangqing Zhang, Junfei Wu, Feng Li, Kun Wang, Qingsong Wen, Zhang Zhang, Liang Wang, Rong Jin, Tieniu Tan|2024-08-23|arXiv|https://mme-realworld.github.io/|http://arxiv.org/abs/2408.13257v1|
|35|LLM-PBE: Assessing Data Privacy in Large Language Models|Qinbin Li, Junyuan Hong, Chulin Xie, Jeffrey Tan, Rachel Xin, Junyi Hou, Xavier Yin, Zhun Wang, Dan Hendrycks, Zhangyang Wang, Bo Li, Bingsheng He, Dawn Song|2024-08-23|Proceedings of the VLDB Endowment (PVLDB), Volume 17, Issue 11|https://llm-pbe.github.io/|https://dl.acm.org/doi/10.14778/3681954.3681994|
|36|LIMP: Large Language Model Enhanced Intent-aware Mobility Prediction|Songwei Li, Jie Feng, Jiawei Chi, Xinyuan Hu, Xiaomeng Zhao, Fengli Xu|2024-08-23|arXiv|https://github.com/tsinghua-fib-lab/LIMP|http://arxiv.org/abs/2408.12832v1|
|37|Generating Analytic Specifications for Data Visualization from Natural Language Queries using Large Language Models|Subham Sah, Rishab Mitra, Arpit Narechania, Alex Endert, John Stasko, Wenwen Dou|2024-08-23|arXiv|https://nl4dv.github.io|http://arxiv.org/abs/2408.13391v2|
|38|BackdoorLLM: A Comprehensive Benchmark for Backdoor Attacks on Large Language Models|Yige Li, Hanxun Huang, Yunhan Zhao, Xingjun Ma, Jun Sun|2024-08-23|arXiv|https://github.com/bboylyg/BackdoorLLM|http://arxiv.org/abs/2408.12798v1|
|39|IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities|Bin Wang, Chunyu Xie, Dawei Leng, Yuhui Yin|2024-08-23|arXiv|https://github.com/360CVGroup/Inner-Adaptor-Architecture|http://arxiv.org/abs/2408.12902v1|
|40|Aligning (Medical) LLMs for (Counterfactual) Fairness|Raphael Poulain, Hamed Fayyaz, Rahmatollah Beheshti|2024-08-22|arXiv|https://github.com/healthylaife/FairAlignmentLLM|http://arxiv.org/abs/2408.12055v1|
|41|Controllable Text Generation for Large Language Models: A Survey|Xun Liang, Hanyu Wang, Yezhaohui Wang, Shichao Song, Jiawei Yang, Simin Niu, Jie Hu, Dan Liu, Shunyu Yao, Feiyu Xiong, Zhiyu Li|2024-08-22|arXiv|https://github.com/IAAR-Shanghai/CTGSurvey|http://arxiv.org/abs/2408.12599v1|
|42|Enhanced Fine-Tuning of Lightweight Domain-Specific Q&A Model Based on Large Language Models|Shenglin Zhang, Pengtian Zhu, Minghua Ma, Jiagang Wang, Yongqian Sun, Dongwen Li, Jingyu Wang, Qianying Guo, Xiaolei Hua, Lin Zhu, Dan Pei|2024-08-22|arXiv|https://github.com/Zero-Pointer/Self-Evolution|http://arxiv.org/abs/2408.12247v2|
|43|Evidence-backed Fact Checking using RAG and Few-Shot In-Context Learning with LLMs|Ronit Singhal, Pransh Patwa, Parth Patwa, Aman Chadha, Amitava Das|2024-08-22|arXiv|https://github.com/ronit-singhal/evidence-backed-fact-checking-using-rag-and-few-shot-in-context-learning-with-llms|http://arxiv.org/abs/2408.12060v1|
|44|GenderCARE: A Comprehensive Framework for Assessing and Reducing Gender Bias in Large Language Models|Kunsheng Tang, Wenbo Zhou, Jie Zhang, Aishan Liu, Gelei Deng, Shuai Li, Peigui Qi, Weiming Zhang, Tianwei Zhang, Nenghai Yu|2024-08-22|arXiv|https://github.com/kstanghere/GenderCARE-ccs24|http://arxiv.org/abs/2408.12494v1|
|45|MDD-5k: A New Diagnostic Conversation Dataset for Mental Disorders Synthesized via Neuro-Symbolic LLM Agents|Congchi Yin, Feng Li, Shu Zhang, Zike Wang, Jun Shao, Piji Li, Jianhua Chen, Xun Jiang|2024-08-22|arXiv|https://github.com/lemonsis/MDD-5k|http://arxiv.org/abs/2408.12142v1|
|46|Reasoning Factual Knowledge in Structured Data with Large Language Models|Sirui Huang, Yanggan Gu, Xuming Hu, Zhonghao Li, Qing Li, Guandong Xu|2024-08-22|arXiv|https://github.com/EganGu/StructFact|http://arxiv.org/abs/2408.12188v1|
|47|Towards Evaluating and Building Versatile Large Language Models for Medicine|Chaoyi Wu, Pengcheng Qiu, Jinxin Liu, Hongfei Gu, Na Li, Ya Zhang, Yanfeng Wang, Weidi Xie|2024-08-22|arXiv|https://henrychur.github.io/MedS-Bench/|http://arxiv.org/abs/2408.12547v1|
|48|SimBench: A Rule-Based Multi-Turn Interaction Benchmark for Evaluating an LLM's Ability to Generate Digital Twins|Jingquan Wang, Harry Zhang, Huzaifa Mustafa Unjhawala, Peter Negrut, Shu Wang, Khailanii Slaton, Radu Serban, Jin-Long Wu, Dan Negrut|2024-08-21|arXiv|https://github.com/uwsbel/SimBench|http://arxiv.org/abs/2408.11987v1|
|49|Story3D-Agent: Exploring 3D Storytelling Visualization with Large Language Models|Yuzhou Huang, Yiran Qin, Shunlin Lu, Xintao Wang, Rui Huang, Ying Shan, Ruimao Zhang|2024-08-21|arXiv|https://yuzhou914.github.io/Story3D-Agent/|http://arxiv.org/abs/2408.11801v1|
|50|Personality Alignment of Large Language Models|Minjun Zhu, Linyi Yang, Yue Zhang|2024-08-21|arXiv|https://github.com/zhu-minjun/PAlign|http://arxiv.org/abs/2408.11779v1|
|51|SORSA: Singular Values and Orthonormal Regularized Singular Vectors Adaptation of Large Language Models|Yang Cao|2024-08-21|arXiv|https://github.com/Gunale0926/SORSA|http://arxiv.org/abs/2409.00055v1|
|52|MoE-LPR: Multilingual Extension of Large Language Models through Mixture-of-Experts with Language Priors Routing|Hao Zhou, Zhijun Wang, Shujian Huang, Xin Huang, Xue Han, Junlan Feng, Chao Deng, Weihua Luo, Jiajun Chen|2024-08-21|arXiv|https://github.com/zjwang21/MoE-LPR|http://arxiv.org/abs/2408.11396v1|
|53|Beyond Labels: Aligning Large Language Models with Human-like Reasoning|Muhammad Rafsan Kabir, Rafeed Mohammad Sultan, Ihsanul Haque Asif, Jawad Ibn Ahad, Fuad Rahman, Mohammad Ruhul Amin, Nabeel Mohammed, Shafin Rahman|2024-08-20|arXiv|https://github.com/apurba-nsu-rnd-lab/DFAR|http://arxiv.org/abs/2408.11879v1|
|54|CodeJudge-Eval: Can Large Language Models be Good Judges in Code Understanding?|Yuwei Zhao, Ziyang Luo, Yuchen Tian, Hongzhan Lin, Weixiang Yan, Annan Li, Jing Ma|2024-08-20|arXiv|https://github.com/CodeLLM-Research/CodeJudge-Eval|http://arxiv.org/abs/2408.10718v1|
|55|FLAME: Learning to Navigate with Multimodal LLM in Urban Environments|Yunzhe Xu, Yiyuan Pan, Zhe Liu, Hesheng Wang|2024-08-20|arXiv|https://flame-sjtu.github.io|http://arxiv.org/abs/2408.11051v1|
|56|LLM-Barber: Block-Aware Rebuilder for Sparsity Mask in One-Shot for Large Language Models|Yupeng Su, Ziyi Guan, Xiaoqun Liu, Tianlai Jin, Dongkuan Wu, Graziano Chesi, Ngai Wong, Hao Yu|2024-08-20|arXiv|https://github.com/YupengSu/LLM-Barber|http://arxiv.org/abs/2408.10631v1|
|57|Large Language Models for Multimodal Deformable Image Registration|Mingrui Ma, Weijie Wang, Jie Ning, Jianfeng He, Nicu Sebe, Bruno Lepri|2024-08-20|arXiv|https://github.com/ninjannn/LLM-Morph|http://arxiv.org/abs/2408.10703v1|
|58|Predicting Rewards Alongside Tokens: Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model|Chenhan Yuan, Fei Huang, Ru Peng, Keming Lu, Bowen Yu, Chang Zhou, Jingren Zhou|2024-08-20|arXiv|https://github.com/chenhan97/Otter|http://arxiv.org/abs/2408.10764v1|
|59|Putting People in LLMs' Shoes: Generating Better Answers via Question Rewriter|Junhao Chen, Bowen Wang, Zhouqiang jiang, Yuta Nakashima|2024-08-20|arXiv|https://github.com/3244we/Question-Rewriter|http://arxiv.org/abs/2408.10573v1|
|60|SysBench: Can Large Language Models Follow System Messages?|Yanzhao Qin, Tao Zhang, Tao Zhang, Yanjun Shen, Wenjing Luo, Haoze Sun, Yan Zhang, Yujing Qiao, Weipeng Chen, Zenan Zhou, Wentao Zhang, Bin Cui|2024-08-20|arXiv|https://github.com/PKU-Baichuan-MLSystemLab/SysBench|http://arxiv.org/abs/2408.10943v1|
|61|Pedestrian Attribute Recognition: A New Benchmark Dataset and A Large Language Model Augmented Framework|Jiandong Jin, Xiao Wang, Qian Zhu, Haiyang Wang, Chenglong Li|2024-08-19|arXiv|https://github.com/Event-AHU/OpenPAR|http://arxiv.org/abs/2408.09720v1|
|62|R2GenCSR: Retrieving Context Samples for Large Language Model based X-ray Medical Report Generation|Xiao Wang, Yuehang Li, Fuling Wang, Shiao Wang, Chuanfu Li, Bo Jiang|2024-08-19|arXiv|https://github.com/Event-AHU/Medical_Image_Analysis|http://arxiv.org/abs/2408.09743v1|
|63|FFAA: Multimodal Large Language Model based Explainable Open-World Face Forgery Analysis Assistant|Zhengchao Huang, Bin Xia, Zicheng Lin, Zhun Mou, Wenming Yang|2024-08-19|arXiv|https://ffaa-vl.github.io|http://arxiv.org/abs/2408.10072v1|
|64|CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language Models|Linhao Yu, Yongqi Leng, Yufei Huang, Shang Wu, Haixin Liu, Xinmeng Ji, Jiahui Zhao, Jinwang Song, Tingting Cui, Xiaoqing Cheng, Liutao Liutao, Deyi Xiong|2024-08-19|ACL|https://github.com/tjunlp-lab/CMoralEval|https://aclanthology.org/2024.findings-acl.703|
|65|Antidote: Post-fine-tuning Safety Alignment for Large Language Models against Harmful Fine-tuning|Tiansheng Huang, Gautam Bhattacharya, Pratik Joshi, Josh Kimball, Ling Liu|2024-08-18|arXiv|https://huangtiansheng.github.io/Antidote_gh_page/|http://arxiv.org/abs/2408.09600v1|
|66|PA-LLaVA: A Large Language-Vision Assistant for Human Pathology Image Understanding|Dawei Dai, Yuanhui Zhang, Long Xu, Qianlan Yang, Xiaojing Shen, Shuyin Xia, Guoyin Wang|2024-08-18|arXiv|https://github.com/ddw2AIGROUP2CQUPT/PA-LLaVA|http://arxiv.org/abs/2408.09530v1|
|67|HiAgent: Hierarchical Working Memory Management for Solving Long-Horizon Agent Tasks with Large Language Model|Mengkang Hu, Tianxing Chen, Qiguang Chen, Yao Mu, Wenqi Shao, Ping Luo|2024-08-18|arXiv|https://github.com/HiAgent2024/HiAgent|http://arxiv.org/abs/2408.09559v1|
|68|TC-RAG:Turing-Complete RAG's Case study on Medical LLM Systems|Xinke Jiang, Yue Fang, Rihong Qiu, Haoyu Zhang, Yongxin Xu, Hao Chen, Wentao Zhang, Ruizhe Zhang, Yuchen Fang, Xu Chu, Junfeng Zhao, Yasha Wang|2024-08-17|arXiv|https://https://github.com/Artessay/SAMA|http://arxiv.org/abs/2408.09199v1|
|69|Authorship Attribution in the Era of LLMs: Problems, Methodologies, and Challenges|Baixiang Huang, Canyu Chen, Kai Shu|2024-08-16|arXiv|https://llm-authorship.github.io|http://arxiv.org/abs/2408.08946v1|
|70|Fine-tuning LLMs for Autonomous Spacecraft Control: A Case Study Using Kerbal Space Program|Alejandro Carrasco, Victor Rodriguez-Fernandez, Richard Linares|2024-08-16|arXiv|https://github.com/ARCLab-MIT/kspdg|http://arxiv.org/abs/2408.08676v1|
|71|MIA-Tuner: Adapting Large Language Models as Pre-training Text Detector|Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang|2024-08-16|arXiv|https://github.com/wjfu99/MIA-Tuner|http://arxiv.org/abs/2408.08661v1|
|72|Multimodal Causal Reasoning Benchmark: Challenging Vision Large Language Models to Infer Causal Links Between Siamese Images|Zhiyuan Li, Heng Wang, Dongnan Liu, Chaoyi Zhang, Ao Ma, Jieting Long, Weidong Cai|2024-08-15|arXiv|https://github.com/Zhiyuan-Li-John/MuCR|http://arxiv.org/abs/2408.08105v1|
|73|Polaris: Open-ended Interactive Robotic Manipulation via Syn2Real Visual Grounding and Large Language Models|Tianyu Wang, Haitao Lin, Junqiu Yu, Yanwei Fu|2024-08-15|arXiv|https://star-uu-wang.github.io/Polaris/|http://arxiv.org/abs/2408.07975v1|
|74|Prefix Guidance: A Steering Wheel for Large Language Models to Defend Against Jailbreak Attacks|Jiawei Zhao, Kejiang Chen, Xiaojian Yuan, Weiming Zhang|2024-08-15|arXiv|https://github.com/weiyezhimeng/Prefix-Guidance|http://arxiv.org/abs/2408.08924v1|
|75|FactorLLM: Factorizing Knowledge via Mixture of Experts for Large Language Models|Zhongyu Zhao, Menghang Dong, Rongyu Zhang, Wenzhao Zheng, Yunpeng Zhang, Huanrui Yang, Dalong Du, Kurt Keutzer, Shanghang Zhang|2024-08-15|arXiv|https://github.com/zhenwuweihe/FactorLLM|http://arxiv.org/abs/2408.11855v1|
|76|Can Large Language Models Understand Symbolic Graphics Programs?|Zeju Qiu, Weiyang Liu, Haiwen Feng, Zhen Liu, Tim Z. Xiao, Katherine M. Collins, Joshua B. Tenenbaum, Adrian Weller, Michael J. Black, Bernhard Schölkopf|2024-08-15|arXiv|https://sgp-bench.github.io/|http://arxiv.org/abs/2408.08313v1|
|77|ArabLegalEval: A Multitask Benchmark for Assessing Arabic Legal Knowledge in Large Language Models|Faris Hijazi, Somayah AlHarbi, Abdulaziz AlHussein, Harethah Abu Shairah, Reem AlZahrani, Hebah AlShamlan, Omar Knio, George Turkiyyah|2024-08-15|Proceedings of The …, 2024|https://github.com/Thiqah/ArabLegalEval|http://arxiv.org/abs/2408.07983v1|
|78|Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities|Enneng Yang, Li Shen, Guibing Guo, Xingwei Wang, Xiaochun Cao, Jie Zhang, Dacheng Tao|2024-08-14|arXiv|https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications|http://arxiv.org/abs/2408.07666v3|
|79|Evaluating Large Language Model based Personal Information Extraction and Countermeasures|Yupei Liu, Yuqi Jia, Jinyuan Jia, Neil Zhenqiang Gong|2024-08-14|arXiv|https://github.com/liu00222/LLM-Based-Personal-Profile-Extraction|http://arxiv.org/abs/2408.07291v1|
|80|Knowledge in Superposition: Unveiling the Failures of Lifelong Knowledge Editing for Large Language Models|Chenhui Hu, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao|2024-08-14|arXiv|https://github.com/ChenhuiHu/knowledge_in_superposition|http://arxiv.org/abs/2408.07413v1|
|81|LongWriter: Unleashing 10,000+ Word Generation from Long Context LLMs|Yushi Bai, Jiajie Zhang, Xin Lv, Linzhi Zheng, Siqi Zhu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li|2024-08-13|arXiv|https://github.com/THUDM/LongWriter|http://arxiv.org/abs/2408.07055v1|
|82|Kov: Transferable and Naturalistic Black-Box LLM Attacks using Markov Decision Processes and Tree Search|Robert J. Moss|2024-08-11|arXiv|https://github.com/sisl/Kov.jl|http://arxiv.org/abs/2408.08899v1|
|83|Img-Diff: Contrastive Data Synthesis for Multimodal Large Language Models|Qirui Jiao, Daoyuan Chen, Yilun Huang, Yaliang Li, Ying Shen|2024-08-09|arXiv:2408.04594, 2024|https://github.com/modelscope/data-juicer/tree/ImgDiff|http://arxiv.org/abs/2408.04594v2|
|84|Medical Graph RAG: Towards Safe Medical Large Language Model via Graph Retrieval-Augmented Generation|Junde Wu, Jiayuan Zhu, Yunli Qi|2024-08-09|arXiv:2408.04187, 2024|https://github.com/MedicineToken/Medical-Graph-RAG/tree/main|http://arxiv.org/abs/2408.04187v1|
|85|Open-domain Implicit Format Control for Large Language Model Generation|Yiqun Yao, Wenjia Ma, Xuezhi Fang, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Jing Li, Aixin Sun, Yequan Wang|2024-08-09|arXiv …, 2024|https://github.com/cofe-ai/OIFC|http://arxiv.org/abs/2408.04392v1|
|86|Revisiting Multi-Modal LLM Evaluation|Jian Lu, Shikhar Srivastava, Junyu Chen, Robik Shrestha, Manoj Acharya, Kushal Kafle, Christopher Kanan|2024-08-09|arXiv|https://kevinlujian.github.io/MLLM_Evaluations/|http://arxiv.org/abs/2408.05334v1|
|87|SHIELD: LLM-Driven Schema Induction for Predictive Analytics in EV Battery Supply Chain Disruptions|Zhi-Qi Cheng, Yifei Dong, Aike Shi, Wei Liu, Yuzhi Hu, Jason O'Connor, Alexander Hauptmann, Kate Whitefoot|2024-08-09|arXiv|https://f1y1113.github.io/MFI/|http://arxiv.org/abs/2408.05357v1|
|88|Tabular Transfer Learning via Prompting LLMs|Jaehyun Nam, Woomin Song, Seong Hyeon Park, Jihoon Tack, Sukmin Yun, Jaehyung Kim, Kyu Hwan Oh, Jinwoo Shin|2024-08-09|arXiv|https://github.com/jaehyun513/P2T|http://arxiv.org/abs/2408.11063v1|
|89|VITA: Towards Open-Source Interactive Omni Multimodal LLM|Chaoyou Fu, Haojia Lin, Zuwei Long, Yunhang Shen, Meng Zhao, Yifan Zhang, Xiong Wang, Di Yin, Long Ma, Xiawu Zheng, Ran He, Rongrong Ji, Yunsheng Wu, Caifeng Shan, Xing Sun|2024-08-09|arXiv|https://vita-home.github.io|http://arxiv.org/abs/2408.05211v1|
|90|Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of Large Language Models|Yupeng Chang, Yi Chang, Yuan Wu|2024-08-09|arXiv:2408.04556, 2024|https://github.com/cyp-jlu-ai/BA-LoRA|http://arxiv.org/abs/2408.04556v1|
|91|ToolSandbox: A Stateful, Conversational, Interactive Evaluation Benchmark for LLM Tool Use Capabilities|Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, Zirui Wang, Ruoming Pang|2024-08-08|arXiv|https://github.com/apple/ToolSandbox|http://arxiv.org/abs/2408.04682v1|
|92|MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with Large Language Models|Haoxuan Li, Zhengmao Yang, Yunshan Ma, Yi Bin, Yang Yang, Tat-Seng Chua|2024-08-08|arXiv|https://github.com/LuminosityX/MM-Forecast|http://arxiv.org/abs/2408.04388v1|
|93|Advancing Multimodal Large Language Models with Quantization-Aware Scale Learning for Efficient Adaptation|Jingjing Xie, Yuxin Zhang, Mingbao Lin, Liujuan Cao, Rongrong Ji|2024-08-07|arXiv|https://github.com/xjjxmu/QSLAW|http://arxiv.org/abs/2408.03735v1|
|94|NACL: A General and Effective KV Cache Eviction Framework for LLMs at Inference Time|Yilong Chen, Guoxia Wang, Junyuan Shang, Shiyao Cui, Zhenyu Zhang, Tingwen Liu, Shuohuan Wang, Yu Sun, Dianhai Yu, Hua Wu|2024-08-07|arXiv|https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL|http://arxiv.org/abs/2408.03675v2|
|95|WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language Models|Prannaya Gupta, Le Qi Yau, Hao Han Low, I-Shiang Lee, Hugo Maximus Lim, Yu Xin Teoh, Jia Hng Koh, Dar Win Liew, Rishabh Bhardwaj, Rajat Bhardwaj, Soujanya Poria|2024-08-07|arXiv|https://github.com/walledai/walledeval|http://arxiv.org/abs/2408.03837v2|
|96|CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases|Xiangyan Liu, Bo Lan, Zhiyuan Hu, Yang Liu, Zhicheng Zhang, Fei Wang, Michael Shieh, Wenmeng Zhou|2024-08-07|arXiv|https://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent|http://arxiv.org/abs/2408.03910v2|
|97|Citekit: A Modular Toolkit for Large Language Model Citation Generation|Jiajun Shen, Tong Zhou, Suifeng Zhao, Yubo Chen, Kang Liu|2024-08-06|arXiv|https://github.com/SjJ1017/Citekit|http://arxiv.org/abs/2408.04662v1|
|98|OpenFactCheck: A Unified Framework for Factuality Evaluation of LLMs|Hasan Iqbal, Yuxia Wang, Minghan Wang, Georgi Georgiev, Jiahui Geng, Iryna Gurevych, Preslav Nakov|2024-08-06|arXiv|https://github.com/hasaniqbal777/openfactcheck|http://arxiv.org/abs/2408.11832v1|
|99|StructEval: Deepen and Broaden Large Language Model Assessment via Structured Evaluation|Boxi Cao, Mengjie Ren, Hongyu Lin, Xianpei Han, Feng Zhang, Junfeng Zhan, Le Sun|2024-08-06|ACL|https://github.com/c-box/StructEval|https://aclanthology.org/2024.findings-acl.314|
|100|Topic Modeling with Fine-tuning LLMs and Bag of Sentences|Johannes Schneider|2024-08-06|arXiv|https://github.com/JohnTailor/FT-Topic|http://arxiv.org/abs/2408.03099v1|
|101|ULLME: A Unified Framework for Large Language Model Embeddings with Generation-Augmented Learning|Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Thien Huu Nguyen|2024-08-06|arXiv|https://github.com/nlp-uoregon/ullme|http://arxiv.org/abs/2408.03402v1|
|102|RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation|Daniel Fleischer, Moshe Berchansky, Moshe Wasserblat, Peter Izsak|2024-08-05|arXiv|https://github.com/IntelLabs/RAGFoundry|http://arxiv.org/abs/2408.02545v1|
|103|UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks With Large Language Model|Zhaowei Li, Wei Wang, YiQing Cai, Xu Qi, Pengyu Wang, Dong Zhang, Hang Song, Botian Jiang, Zhida Huang, Tao Wang|2024-08-05|arXiv|https://github.com/lzw-lzw/UnifiedMLLM|http://arxiv.org/abs/2408.02503v1|
|104|Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in Customized Large Language Models|Zi Liang, Haibo Hu, Qingqing Ye, Yaxin Xiao, Haoyang Li|2024-08-05|arXiv|https://github.com/liangzid/PromptExtractionEval|http://arxiv.org/abs/2408.02416v1|
|105|Mini-Monkey: Multi-Scale Adaptive Cropping for Multimodal Large Language Models|Mingxin Huang, Yuliang Liu, Dingkang Liang, Lianwen Jin, Xiang Bai|2024-08-04|arXiv|https://github.com/Yuliang-Liu/Monkey|http://arxiv.org/abs/2408.02034v2|
|106|PLUGH: A Benchmark for Spatial Understanding and Reasoning in Large Language Models|Alexey Tikhonov|2024-08-03|arXiv|https://github.com/altsoph/PLUGH|http://arxiv.org/abs/2408.04648v1|
|107|MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented Generation for Pharmacovigilance|Jihye Choi, Nils Palumbo, Prasad Chalasani, Matthew M. Engelhard, Somesh Jha, Anivarya Kumar, David Page|2024-08-03|arXiv|https://github.com/jihyechoi77/malade|http://arxiv.org/abs/2408.01869v1|
|108|Agentic LLM Workflows for Generating Patient-Friendly Medical Reports|Malavikha Sudarshan, Sophie Shih, Estella Yee, Alina Yang, John Zou, Cathy Chen, Quan Zhou, Leon Chen, Chinmay Singhal, George Shih|2024-08-02|arXiv|http://github.com/malavikhasudarshan/Multi-Agent-Patient-Letter-Generation|http://arxiv.org/abs/2408.01112v2|
|109|CFBench: A Comprehensive Constraints-Following Benchmark for LLMs|Tao Zhang, Yanjun Shen, Wenjing Luo, Yan Zhang, Hao Liang, Tao Zhang, Fan Yang, Mingan Lin, Yujing Qiao, Weipeng Chen, Bin Cui, Wentao Zhang, Zenan Zhou|2024-08-02|arXiv|https://github.com/PKU-Baichuan-MLSystemLab/CFBench|http://arxiv.org/abs/2408.01122v1|
|110|Fairness in Large Language Models in Three Hour|Thang Doan Viet, Zichong Wang, Minh Nhat Nguyen, Wenbin Zhang|2024-08-02|arXiv|https://github.com/LavinWong/Fairness-in-Large-Language-Models|http://arxiv.org/abs/2408.00992v2|
|111|Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models within Perturbed Inputs|Peng Ding, Jingyu Wu, Jun Kuang, Dan Ma, Xuezhi Cao, Xunliang Cai, Shi Chen, Jiajun Chen, Shujian Huang|2024-08-02|arXiv|https://github.com/NJUNLP/Hallu-PI|http://arxiv.org/abs/2408.01355v2|
|112|Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian Rebuses|Gabriele Sarti, Tommaso Caselli, Malvina Nissim, Arianna Bisazza|2024-08-02|arXiv:2408.00584, 2024|https://github.com/gsarti/verbalized-rebus|http://arxiv.org/abs/2408.00584v1|
|113|Talk Less, Interact Better: Evaluating In-context Conversational Adaptation in Multimodal LLMs|Yilun Hua, Yoav Artzi|2024-08-02|arXiv|https://github.com/lil-lab/ICCA|http://arxiv.org/abs/2408.01417v1|
|114|ControlMLLM: Training-Free Visual Prompt Learning for Multimodal Large Language Models|Mingrui Wu, Xinyue Cai, Jiayi Ji, Jiale Li, Oucheng Huang, Gen Luo, Hao Fei, Xiaoshuai Sun, Rongrong Ji|2024-08-01|arXiv|https://github.com/mrwu-mac/ControlMLLM|https://doi.org/10.48550/arXiv.2407.21534|
|115|Understanding the Weakness of Large Language Model Agents within a Complex Android Environment|Mingzhe Xing, Rongkai Zhang, Hui Xue, Qi Chen, Fan Yang, Zhen Xiao|2024-08|KDD '24: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/AndroidArenaAgent/AndroidArena|https://dl.acm.org/doi/10.1145/3637528.3671650|
|116|RecExplainer: Aligning Large Language Models for Explaining Recommendation Models|Yuxuan Lei, Jianxun Lian, Jing Yao, Xu Huang, Defu Lian, Xing Xie|2024-08|KDD '24: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/microsoft/RecAI|https://dl.acm.org/doi/10.1145/3637528.3671802|
|117|R-Eval: A Unified Toolkit for Evaluating Domain Knowledge of Retrieval Augmented Large Language Models|Shangqing Tu, Yuanchun Wang, Jifan Yu, Yuyang Xie, Yaran Shi, Xiaozhi Wang, Jing Zhang, Lei Hou, Juanzi Li|2024-08|KDD '24: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/THU-KEG/R-Eval|https://dl.acm.org/doi/10.1145/3637528.3671564|
|118|OpenFedLLM: Training Large Language Models on Decentralized Private Data via Federated Learning|Rui Ye, Wenhao Wang, Jingyi Chai, Dihan Li, Zexi Li, Yinda Xu, Yaxin Du, Yanfeng Wang, Siheng Chen|2024-08|KDD '24: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/rui-ye/OpenFedLLM|https://dl.acm.org/doi/10.1145/3637528.3671582|
|119|AutoWebGLM: A Large Language Model-based Web Navigating Agent|Hanyu Lai, Xiao Liu, Iat Long Iong, Shuntian Yao, Yuxuan Chen, Pengbo Shen, Hao Yu, Hanchen Zhang, Xiaohan Zhang, Yuxiao Dong, Jie Tang|2024-08|KDD '24: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/THUDM/AutoWebGLM|https://dl.acm.org/doi/10.1145/3637528.3671620|
|120|Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era|Sunhao Dai, Chen Xu, Shicheng Xu, Liang Pang, Zhenhua Dong, Jun Xu|2024-08|KDD '24: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://llm-ir-bias-fairness.github.io/|https://dl.acm.org/doi/10.1145/3637528.3671458|
|121|A Survey of Large Language Models for Graphs|Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh V. Chawla, Chao Huang|2024-08|KDD '24: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://github.com/HKUDS/Awesome-LLM4Graph-Papers|https://dl.acm.org/doi/10.1145/3637528.3671460|
|122|ArcheType: A Novel Framework for Open-Source Column Type Annotation Using Large Language Models|Benjamin Feuer, Yurong Liu, Chinmay Hegde, Juliana Freire|2024-08|Proceedings of the VLDB Endowment (PVLDB), Volume 17, Issue 9|https://github.com/penfever/ArcheType|https://dl.acm.org/doi/10.14778/3665844.3665857|
|123|A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models|Wenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang, Hengyun Li, Dawei Yin, Tat-Seng Chua, Qing Li|2024-08|KDD '24: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining|https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/|https://dl.acm.org/doi/10.1145/3637528.3671470|
|124|Advancing Multimodal Large Language Models in Chart Question Answering with Visualization-Referenced Instruction Tuning|Xingchen Zeng, Haichuan Lin, Yilin Ye, Wei Zeng|2024-07-29|arXiv|https://github.com/zengxingchen/ChartQA-MLLM|https://doi.org/10.48550/arXiv.2407.20174|
|125|rLLM: Relational Table Learning with LLMs|Weichen Li, Xiaotong Huang, Jianwu Zheng, Zheng Wang, Chaokun Wang, Li Pan, Jianhua Li|2024-07-29|arXiv|https://github.com/rllm-project/rllm|http://arxiv.org/abs/2407.20157v1|
|126|Can Editing LLMs Inject Harm?|Canyu Chen, Baixiang Huang, Zekun Li, Zhaorun Chen, Shiyang Lai, Xiongxiao Xu, Jia-Chen Gu, Jindong Gu, Huaxiu Yao, Chaowei Xiao, Xifeng Yan, William Yang Wang, Philip Torr, Dawn Song, Kai Shu|2024-07-29|arXiv|https://llm-editing.github.io|http://arxiv.org/abs/2407.20224v2|
|127|CollectiveSFT: Scaling Large Language Models for Chinese Medical Benchmark with Collective Instructions in Healthcare|Jingwei Zhu, Minghuan Tan, Min Yang, Ruixue Li, Hamid Alinejad-Rokny|2024-07-29|arXiv|https://github.com/CAS-SIAT-XinHai/CollectiveSFT|https://doi.org/10.48550/arXiv.2407.19705|
|128|A Role-specific Guided Large Language Model for Ophthalmic Consultation Based on Stylistic Differentiation|Laiyi Fu, Binbin Fan, Hongkai Du, Yanxiang Feng, Chunhua Li, Huping Song|2024-07-26|arXiv|https://github.com/sperfu/EyeDoc|https://doi.org/10.48550/arXiv.2407.18483|
|129|Exploring Bengali Religious Dialect Biases in Large Language Models with Evaluation Perspectives|Azmine Toushik Wasi, Raima Islam, Mst Rafia Islam, Taki Hasan Rafi, Dong-Kyu Chae|2024-07-25|arXiv|https://heal-workshop.github.io/#:~:text=Exploring%20Bengali%20Religious%20Dialect%20Biases%20in%20Large%20Language%20Models%20with%20Evaluation%20Perspectives|https://doi.org/10.48550/arXiv.2407.18376|
|130|The Dark Side of Function Calling: Pathways to Jailbreaking Large Language Models|Zihui Wu, Haichang Gao, Jianping He, Ping Wang|2024-07-25|arXiv|https://github.com/wooozihui/jailbreakfunction|https://doi.org/10.48550/arXiv.2407.17915|
|131|Accurate and Efficient Fine-Tuning of Quantized Large Language Models Through Optimal Balance|Ao Shen, Qiang Wang, Zhiquan Lai, Xionglve Li, Dong-sheng Li|2024-07-24|arXiv|https://github.com/xiaocaigou/qbaraqahira|https://doi.org/10.48550/arXiv.2407.17029|
|132|Boosting Large Language Models with Socratic Method for Conversational Mathematics Teaching|Yuyang Ding, Hanglei Hu, Jie Zhou, Qin Chen, Bo Jiang, Liang He|2024-07-24|arXiv|https://github.com/ECNU-ICALK/SocraticMath|https://doi.org/10.48550/arXiv.2407.17349|
|133|Large Language Models for Anomaly Detection in Computational Workflows: from Supervised Fine-Tuning to In-Context Learning|Hongwei Jin, George Papadimitriou, Krishnan Raghavan, Pawel Zuk, Prasanna Balaprakash, Cong Wang, Anirban Mandal, Ewa Deelman|2024-07-24|arXiv|https://github.com/PoSeiDon-Workflows/LLM_AD|https://doi.org/10.48550/arXiv.2407.17545|
|134|Scalify: scale propagation for efficient low-precision LLM training|Paul Balança, Sam Hosegood, Carlo Luschi, Andrew Fitzgibbon|2024-07-24|arXiv|https://github.com/graphcore-research/jax-scalify|http://arxiv.org/abs/2407.17353v1|
|135|LawLuo: A Chinese Law Firm Co-run by LLM Agents|Jingyun Sun, Chengxiao Dai, Zhongze Luo, Yangbo Chang, Yang Li|2024-07-23|arXiv|https://github.com/NEFUJing/LawLuo|http://arxiv.org/abs/2407.16252v1|
|136|UniMEL: A Unified Framework for Multimodal Entity Linking with Large Language Models|Qi Liu, Yongyi He, Defu Lian, Zhi Zheng, Tong Xu, Liu Che, Enhong Chen|2024-07-23|arXiv|https://github.com/Javkonline/UniMEL|https://doi.org/10.48550/arXiv.2407.16160|
|137|Figure it Out: Analyzing-based Jailbreak Attack on Large Language Models|Shi Lin, Rongchang Li, Xun Wang, Changting Lin, Wenpeng Xing, Meng Han|2024-07-23|arXiv|https://github.com/theshi-1128/ABJ-Attack|https://doi.org/10.48550/arXiv.2407.16205|
|138|INF-LLaVA: Dual-perspective Perception for High-Resolution Multimodal Large Language Model|Yiwei Ma, Zhibin Wang, Xiaoshuai Sun, Weihuang Lin, Qiang Zhou, Jiayi Ji, Rongrong Ji|2024-07-23|arXiv|https://github.com/WeihuangLin/INF-LLaVA|https://doi.org/10.48550/arXiv.2407.16198|
|139|Do Large Language Models Have Compositional Ability? An Investigation into Limitations and Scalability|Zhuoyan Xu, Zhenmei Shi, Yingyu Liang|2024-07-22|arXiv|https://github.com/OliverXUZY/LLM_Compose|https://doi.org/10.48550/arXiv.2407.15720|
|140|Knowledge Acquisition Disentanglement for Knowledge-based Visual Question Answering with Large Language Models|Wenbin An, Feng Tian, Jiahao Nie, Wenkai Shi, Haonan Lin, Yan Chen, Qianying Wang, Yaqiang Wu, Guang Dai, Ping Chen|2024-07-22|arXiv|https://github.com/Lackel/DKA|https://doi.org/10.48550/arXiv.2407.15346|
|141|LLaST: Improved End-to-end Speech Translation System Leveraged by Large Language Models|Xi Chen, Songyang Zhang, Qibing Bai, Kai Chen, Satoshi Nakamura|2024-07-22|ACL|https://github.com/openaudiolab/LLaST|https://aclanthology.org/2024.findings-acl.416|
|142|BIGbench: A Unified Benchmark for Social Bias in Text-to-Image Generative Models Based on Multi-modal LLM|Hanjun Luo, Haoyu Huang, Ziye Deng, Xuecheng Liu, Ruizhe Chen, Zuozhu Liu|2024-07-21|arXiv|https://github.com/BIGbench2024/BIGbench2024/|http://arxiv.org/abs/2407.15240v2|
|143|Large Language Model for Verilog Generation with Golden Code Feedback|Ning Wang, Bingkun Yao, Jie Zhou, Xi Wang, Zhe Jiang, Nan Guan|2024-07-21|arXiv|https://github.com/CatIIIIIIII/veriseek|https://doi.org/10.48550/arXiv.2407.18271|
|144|Navigation Instruction Generation with BEV Perception and Large Language Models|Sheng Fan, Rui Liu, Wenguan Wang, Yi Yang|2024-07-21|arXiv|https://github.com/FanScy/BEVInstructor|https://doi.org/10.48550/arXiv.2407.15087|
|145|Prior Knowledge Integration via LLM Encoding and Pseudo Event Regulation for Video Moment Retrieval|Yiyang Jiang, Wengyu Zhang, Xulu Zhang, Xiaoyong Wei, Chang Wen Chen, Qing Li|2024-07-21|arXiv|https://github.com/fletcherjiang/LLMEPET|http://arxiv.org/abs/2407.15051v2|
|146|SynCPKL: Harnessing LLMs to Generate Synthetic Data for Commonsense Persona Knowledge Linking|Kuan-Yen Lin|2024-07-21|arXiv|https://github.com/irislin1006/CPKL|http://arxiv.org/abs/2407.15281v1|
|147|On the Design and Analysis of LLM-Based Algorithms|Yanxi Chen, Yaliang Li, Bolin Ding, Jingren Zhou|2024-07-20|arXiv|https://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm|http://arxiv.org/abs/2407.14788v1|
|148|Internal Consistency and Self-Feedback in Large Language Models: A Survey|Xun Liang, Shichao Song, Zifan Zheng, Hanyu Wang, Qingchen Yu, Xunkai Li, Rong-Hua Li, Peng Cheng, Zhonghao Wang, Feiyu Xiong, Zhiyu Li|2024-07-19|arXiv|https://github.com/IAAR-Shanghai/ICSFSurvey|https://doi.org/10.48550/arXiv.2407.14507|
|149|Beyond Code Generation: Assessing Code LLM Maturity with Postconditions|Fusen He, Juan Zhai, Minxue Pan|2024-07-19|arXiv|https://github.com/MatureModel/PostcondGen|http://arxiv.org/abs/2407.14118v1|
|150|Enhancing Zero-shot Audio Classification using Sound Attribute Knowledge from Large Language Models|Xuenan Xu, Pingyue Zhang, Ming Yan, Ji Zhang, Mengyue Wu|2024-07-19|arXiv|https://www.github.com/wsntxxn/AttrEnhZsAc|https://doi.org/10.48550/arXiv.2407.14355|
|151|ViLLa: Video Reasoning Segmentation with Large Language Model|Rongkun Zheng, Lu Qi, Xi Chen, Yi Wang, Kun Wang, Yu Qiao, Hengshuang Zhao|2024-07-18|arXiv|https://github.com/rkzheng99/ViLLa|https://doi.org/10.48550/arXiv.2407.14500|
|152|SegPoint: Segment Any Point Cloud via Large Language Model|Shuting He, Henghui Ding, Xudong Jiang, Bihan Wen|2024-07-18|arXiv|https://heshuting555.github.io/SegPoint|https://doi.org/10.48550/arXiv.2407.13761|
|153|E5-V: Universal Embeddings with Multimodal Large Language Models|Ting Jiang, Minghui Song, Zihan Zhang, Haizhen Huang, Weiwei Deng, Feng Sun, Qi Zhang, Deqing Wang, Fuzhen Zhuang|2024-07-17|arXiv|https://github.com/kongds/E5-V|https://doi.org/10.48550/arXiv.2407.12580|
|154|MoME: Mixture of Multimodal Experts for Generalist Multimodal Large Language Models|Leyang Shen, Gongwei Chen, Rui Shao, Weili Guan, Liqiang Nie|2024-07-17|arXiv|https://github.com/JiuTian-VL/MoME|https://doi.org/10.48550/arXiv.2407.12709|
|155|Patch-Level Training for Large Language Models|Chenze Shao, Fandong Meng, Jie Zhou|2024-07-17|arXiv|https://github.com/shaochenze/PatchTrain|https://doi.org/10.48550/arXiv.2407.12665|
|156|Beyond Correctness: Benchmarking Multi-dimensional Code Generation for Large Language Models|Jiasheng Zheng, Boxi Cao, Zhengzhao Ma, Ruotong Pan, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun|2024-07-16|arXiv|https://github.com/jszheng21/RACE|https://doi.org/10.48550/arXiv.2407.11470|
|157|LRQ: Optimizing Post-Training Quantization for Large Language Models by Learning Low-Rank Weight-Scaling Matrices|Jung Hyun Lee, Jeonghoon Kim, June Yong Yang, Se Jung Kwon, Eunho Yang, Kang Min Yoo, Dongsoo Lee|2024-07-16|arXiv|https://github.com/onliwad101/FlexRound_LRQ|https://doi.org/10.48550/arXiv.2407.11534|
|158|NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?|Mo Li, Songyang Zhang, Yunxin Liu, Kai Chen|2024-07-16|arXiv|https://github.com/open-compass/opencompass|http://arxiv.org/abs/2407.11963v1|
|159|Robust Utility-Preserving Text Anonymization Based on Large Language Models|Tianyu Yang, Xiaodan Zhu, Iryna Gurevych|2024-07-16|arXiv|https://github.com/UKPLab/arxiv2024-rupta|https://doi.org/10.48550/arXiv.2407.11770|
|160|VISA: Reasoning Video Object Segmentation via Large Language Models|Cilin Yan, Haochen Wang, Shilin Yan, Xiaolong Jiang, Yao Hu, Guoliang Kang, Weidi Xie, Efstratios Gavves|2024-07-16|arXiv|https://github.com/cilinyan/VISA|https://doi.org/10.48550/arXiv.2407.11325|
|161|Uncertainty is Fragile: Manipulating Uncertainty in Large Language Models|Qingcheng Zeng, Mingyu Jin, Qinkai Yu, Zhenting Wang, Wenyue Hua, Zihao Zhou, Guangyan Sun, Yanda Meng, Shiqing Ma, Qifan Wang, Felix Juefei-Xu, Kaize Ding, Fan Yang, Ruixiang Tang, Yongfeng Zhang|2024-07-15|arXiv|https://github.com/qcznlp/uncertainty_attack|https://doi.org/10.48550/arXiv.2407.11282|
|162|When AI Meets Finance (StockAgent): Large Language Model-based Stock Trading in Simulated Real-world Environments|Chong Zhang, Xinyi Liu, Mingyu Jin, Zhongmou Zhang, Lingyao Li, Zhenting Wang, Wenyue Hua, Dong Shu, Suiyuan Zhu, Xiaobo Jin, Sujian Li, Mengnan Du, Yongfeng Zhang|2024-07-15|arXiv|https://github.com/MingyuJ666/Stockagent|https://doi.org/10.48550/arXiv.2407.18957|
|163|VGBench: Evaluating Large Language Models on Vector Graphics Understanding and Generation|Bocheng Zou, Mu Cai, Jianrui Zhang, Yong Jae Lee|2024-07-15|arXiv|https://vgbench.github.io|https://doi.org/10.48550/arXiv.2407.10972|
|164|Evaluating Large Language Models with fmeval|Pola Schwöbel, Luca Franceschi, Muhammad Bilal Zafar, Keerthan Vasist, Aman Malhotra, Tomer Shenhar, Pinal Tailor, Pinar Yilmaz, Michael Diamond, Michele Donini|2024-07-15|arXiv|https://github.com/aws/fmeval|https://doi.org/10.48550/arXiv.2407.12872|
|165|Prompt Selection Matters: Enhancing Text Annotations for Social Sciences with Large Language Models|Louis Abraham, Charles Arnal, Antoine Marie|2024-07-15|arXiv|https://prompt-ultra.github.io/|https://doi.org/10.48550/arXiv.2407.10645|
|166|Learning Dynamics of LLM Finetuning|Yi Ren, Danica J. Sutherland|2024-07-15|arXiv|https://github.com/Joshua-Ren/Learning_dynamics_LLM|http://arxiv.org/abs/2407.10490v1|
|167|IDEAL: Leveraging Infinite and Dynamic Characterizations of Large Language Models for Query-focused Summarization|Jie Cao, Dian Jiao, Qiang Yan, Wenqiao Zhang, Siliang Tang, Yueting Zhuang|2024-07-15|arXiv|https://github.com/DCDmllm/IDEAL_Summary|https://doi.org/10.48550/arXiv.2407.10486|
|168|ChatLogic: Integrating Logic Programming with Large Language Models for Multi-Step Reasoning|Zhongsheng Wang, Jiamou Liu, Qiming Bao, Hongfei Rong, Jingfeng Zhang|2024-07-14|arXiv|https://github.com/Strong-AI-Lab/ChatLogic|https://doi.org/10.48550/arXiv.2407.10162|
|169|Follow the Rules: Reasoning for Video Anomaly Detection with Large Language Models|Yuchen Yang, Kwonjoon Lee, Behzad Dariush, Yinzhi Cao, Shao-Yuan Lo|2024-07-14|arXiv|https://github.com/Yuchen413/AnomalyRuler|https://doi.org/10.48550/arXiv.2407.10299|
|170|Refuse Whenever You Feel Unsafe: Improving Safety in LLMs via Decoupled Refusal Training|Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Jiahao Xu, Tian Liang, Pinjia He, Zhaopeng Tu|2024-07-12|arXiv|https://github.com/RobustNLP/DeRTa|http://arxiv.org/abs/2407.09121v1|
|171|Refusing Safe Prompts for Multi-modal Large Language Models|Zedian Shao, Hongbin Liu, Yuepeng Hu, Neil Zhenqiang Gong|2024-07-12|arXiv|https://github.com/Sadcardation/MLLM-Refusal|https://doi.org/10.48550/arXiv.2407.09050|
|172|Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors|Nico Daheim, Jakub Macina, Manu Kapur, Iryna Gurevych, Mrinmaya Sachan|2024-07-12|arXiv|https://github.com/eth-lre/verify-then-generate|https://doi.org/10.48550/arXiv.2407.09136|
|173|Mitigating Entity-Level Hallucination in Large Language Models|Weihang Su, Yichen Tang, Qingyao Ai, Changyue Wang, Zhijing Wu, Yiqun Liu|2024-07-12|arXiv|https://github.com/oneal2000/EntityHallucination|https://doi.org/10.48550/arXiv.2407.09417|
|174|Global-Local Collaborative Inference with LLM for Lidar-Based Open-Vocabulary Detection|Xingyu Peng, Yan Bai, Chen Gao, Lirong Yang, Fei Xia, Beipeng Mu, Xiaofei Wang, Si Liu|2024-07-12|arXiv|https://github.com/GradiusTwinbee/GLIS|http://arxiv.org/abs/2407.08931v1|
|175|GLBench: A Comprehensive Benchmark for Graph with Large Language Models|Yuhan Li, Peisong Wang, Xiao Zhu, Aochuan Chen, Haiyun Jiang, Deng Cai, Victor Wai Kin Chan, Jia Li|2024-07-12|arXiv|https://github.com/NineAbyss/GLBench|https://doi.org/10.48550/arXiv.2407.07457|
|176|Metron: Holistic Performance Evaluation Framework for LLM Inference Systems|Amey Agrawal, Anmol Agarwal, Nitin Kedia, Jayashree Mohan, Souvik Kundu, Nipun Kwatra, Ramachandran Ramjee, Alexey Tumanov|2024-07-11|arXiv …, 2024|https://github.com/project-metron/metron|http://arxiv.org/abs/2407.07000v1|
|177|The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective|Zhen Qin, Daoyuan Chen, Wenhao Zhang, Liuyi Yao, Yilun Huang, Bolin Ding, Yaliang Li, Shuiguang Deng|2024-07-11|arXiv|https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md|https://doi.org/10.48550/arXiv.2407.08583|
|178|SEED-Story: Multimodal Long Story Generation with Large Language Model|Shuai Yang, Yuying Ge, Yang Li, Yukang Chen, Yixiao Ge, Ying Shan, Yingcong Chen|2024-07-11|arXiv|https://github.com/TencentARC/SEED-Story|https://doi.org/10.48550/arXiv.2407.08683|
|179|Model Surgery: Modulating LLM's Behavior Via Simple Parameter Editing|Huanqian Wang, Yang Yue, Rui Lu, Jingxin Shi, Andrew Zhao, Shenzhi Wang, Shiji Song, Gao Huang|2024-07-11|arXiv|https://github.com/lucywang720/model-surgery|http://arxiv.org/abs/2407.08770v1|
|180|Hypergraph Multi-modal Large Language Model: Exploiting EEG and Eye-tracking Modalities to Evaluate Heterogeneous Responses for Video Understanding|Minghui Wu, Chenxu Zhao, Anyang Su, Donglin Di, Tianyu Fu, Da An, Min He, Ya Gao, Meng Ma, Kun Yan, Ping Wang|2024-07-11|arXiv|https://github.com/suay1113/HMLLM|https://doi.org/10.48550/arXiv.2407.08150|
|181|Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps|Yung-Sung Chuang, Linlu Qiu, Cheng-Yu Hsieh, Ranjay Krishna, Yoon Kim, James R. Glass|2024-07-11|arXiv|https://github.com/voidism/Lookback-Lens|https://doi.org/10.48550/arXiv.2407.07071|
|182|FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation|Liqun Ma, Mingjie Sun, Zhiqiang Shen|2024-07-11|arXiv:2407.07093, 2024|https://github.com/LiqunMa/FBI-LLM|http://arxiv.org/abs/2407.07093v1|
|183|Incorporating Large Language Models into Production Systems for Enhanced Task Automation and Flexibility|Yuchen Xia, Jize Zhang, Nasser Jazdi, Michael Weyrich|2024-07-11|arXiv|https://github.com/YuchenXia/GPT4IndustrialAutomation|https://doi.org/10.48550/arXiv.2407.08550|
|184|LLMBox: A Comprehensive Library for Large Language Models|Tianyi Tang, Yiwen Hu, Bingqian Li, Wenyang Luo, Zijing Qin, Haoxiang Sun, Jiapeng Wang, Shiyi Xu, Xiaoxue Cheng, Geyang Guo, Han Peng, Bowen Zheng, Yiru Tang, Yingqian Min, Yushuo Chen, Jie Chen, Yuanqian Zhao, Luran Ding, Yuhao Wang, Zican Dong, Chunxuan Xia, Junyi Li, Kun Zhou, Wayne Xin Zhao, Ji-Rong Wen|2024-07-10|arXiv|https://github.com/RUCAIBox/LLMBox|https://doi.org/10.48550/arXiv.2407.05563|
|185|TRAD: Enhancing LLM Agents with Step-Wise Thought Retrieval and Aligned Decision|Ruiwen Zhou, Yingxuan Yang, Muning Wen, Ying Wen, Wenhao Wang, Chunling Xi, Guoqiang Xu, Yong Yu, Weinan Zhang|2024-07-10|SIGIR '24: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/skyriver-2000/TRAD-Official|https://dl.acm.org/doi/10.1145/3626772.3657788|
|186|RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization|Xijie Huang, Zechun Liu, Shih-Yang Liu, Kwang-Ting Cheng|2024-07-10|arXiv|https://github.com/HuangOwen/RoLoRA|http://arxiv.org/abs/2407.08044v1|
|187|PromptLink: Leveraging Large Language Models for Cross-Source Biomedical Concept Linking|Yuzhang Xie, Jiaying Lu, Joyce Ho, Fadi B. Nahab, Xiao Hu, Carl Yang|2024-07-10|SIGIR '24: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/constantjxyz/PromptLink|https://dl.acm.org/doi/10.1145/3626772.3657904|
|188|OpenP5: An Open-Source Platform for Developing, Training, and Evaluating LLM-based Recommender Systems|Shuyuan Xu, Wenyue Hua, Yongfeng Zhang|2024-07-10|SIGIR '24: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/agiresearch/OpenP5|https://dl.acm.org/doi/10.1145/3626772.3657883|
|189|Large Language Models are Learnable Planners for Long-Term Recommendation|Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi Zhang, Qifan Wang, Fuli Feng|2024-07-10|SIGIR '24: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/jizhi-zhang/BiLLP|https://dl.acm.org/doi/10.1145/3626772.3657683|
|190|LLaRA: Large Language-Recommendation Assistant|Jiayi Liao, Sihang Li, Zhengyi Yang, Jiancan Wu, Yancheng Yuan, Xiang Wang, Xiangnan He|2024-07-10|SIGIR '24: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/ljy0ustc/LLaRA|https://dl.acm.org/doi/10.1145/3626772.3657690|
|191|LLaMAX: Scaling Linguistic Horizons of LLM by Enhancing Translation Capabilities Beyond 100 Languages|Yinquan Lu, Wenhao Zhu, Lei Li, Yu Qiao, Fei Yuan|2024-07-10|arXiv:2407.05975, 2024|https://github.com/CONE-MT/LLaMAX/|http://arxiv.org/abs/2407.05975v1|
|192|iLLM-TSC: Integration reinforcement learning and large language model for traffic signal control policy improvement|Aoyu Pang, Maonan Wang, Man-On Pun, Chung Shue Chen, Xi Xiong|2024-07-10|arXiv|https://github.com/Traffic-Alpha/iLLM-TSC|https://doi.org/10.48550/arXiv.2407.06025|
|193|Inference Performance Optimization for Large Language Models on CPUs|Pujiang He, Shan Zhou, Wenhuan Huang, Changqing Li, Duyi Wang, Bin Guo, Chen Meng, Sheng Gui, Weifei Yu, Yi Xie|2024-07-10|arXiv|https://github.com/intel/xFasterTransformer|https://doi.org/10.48550/arXiv.2407.07304|
|194|GraphGPT: Graph Instruction Tuning for Large Language Models|Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin, Chao Huang|2024-07-10|SIGIR '24: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/HKUDS/GraphGPT|https://dl.acm.org/doi/10.1145/3626772.3657775|
|195|GenArtist: Multimodal LLM as an Agent for Unified Image Generation and Editing|Zhenyu Wang, Aoxue Li, Zhenguo Li, Xihui Liu|2024-07-10|arXiv:2407.05600, 2024|https://zhenyuw16.github.io/GenArtist_page|http://arxiv.org/abs/2407.05600v1|
|196|EfficientQAT: Efficient Quantization-Aware Training for Large Language Models|Mengzhao Chen, Wenqi Shao, Peng Xu, Jiahao Wang, Peng Gao, Kaipeng Zhang, Yu Qiao, Ping Luo|2024-07-10|arXiv|https://github.com/OpenGVLab/EfficientQAT|https://doi.org/10.48550/arXiv.2407.11062|
|197|ChatUniTest: A Framework for LLM-Based Test Generation|Yinghao Chen, Zehao Hu, Chen Zhi, Junxiao Han, Shuiguang Deng, Jianwei Yin|2024-07-10|FSE 2024: Companion Proceedings of the 32nd ACM International Conference on the Foundations of Software Engineering|https://github.com/ZJU-ACES-ISE/ChatUniTest|https://dl.acm.org/doi/10.1145/3663529.3663801|
|198|Can LLMs Master Math? Investigating Large Language Models on Math Stack Exchange|Ankit Satpute, Noah Giessing, Andre Greiner-Petter, Moritz Schubotz, Olaf Teschke, Akiko Aizawa, Bela Gipp|2024-07-10|SIGIR '24: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/gipplab/LLM-Investig-MathStackExchange|https://dl.acm.org/doi/10.1145/3626772.3657945|
|199|Are Large Language Models Good at Utility Judgments?|Hengran Zhang, Ruqing Zhang, Jiafeng Guo, Maarten de Rijke, Yixing Fan, Xueqi Cheng|2024-07-10|SIGIR '24: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/ict-bigdatalab/utility_judgments|https://dl.acm.org/doi/10.1145/3626772.3657784|
|200|IDGenRec: LLM-RecSys Alignment with Textual ID Learning|Juntao Tan, Shuyuan Xu, Wenyue Hua, Yingqiang Ge, Zelong Li, Yongfeng Zhang|2024-07-10|SIGIR '24: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/agiresearch/IDGenRec|https://dl.acm.org/doi/10.1145/3626772.3657821|
|201|A Setwise Approach for Effective and Highly Efficient Zero-shot Ranking with Large Language Models|Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, Guido Zuccon|2024-07-10|SIGIR '24: Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval|https://github.com/ielab/llm-rankers|https://dl.acm.org/doi/10.1145/3626772.3657813|
|202|Etalon: Holistic Performance Evaluation Framework for LLM Inference Systems|Amey Agrawal, Anmol Agarwal, Nitin Kedia, Jayashree Mohan, Souvik Kundu, Nipun Kwatra, Ramachandran Ramjee, Alexey Tumanov|2024-07-09|arXiv|https://github.com/project-etalon/etalon|http://arxiv.org/abs/2407.07000v2|
|203|DebUnc: Mitigating Hallucinations in Large Language Model Agent Communication with Uncertainty Estimations|Luke Yoffe, Alfonso Amayuelas, William Yang Wang|2024-07-08|arXiv|https://github.com/lukeyoffe/debunc|https://doi.org/10.48550/arXiv.2407.06426|
|204|KG-FPQ: Evaluating Factuality Hallucination in LLMs with Knowledge Graph-based False Premise Questions|Yanxu Zhu, Jinlin Xiao, Yuhang Wang, Jitao Sang|2024-07-08|arXiv|https://github.com/yanxuzhu/KG-FPQ|http://arxiv.org/abs/2407.05868v1|
|205|LogicVista: Multimodal LLM Logical Reasoning Benchmark in Visual Contexts|Yijia Xiao, Edward Sun, Tianyu Liu, Wei Wang|2024-07-06|arXiv|https://github.com/Yijia-Xiao/LogicVista|http://arxiv.org/abs/2407.04973v1|
|206|Beyond Perplexity: Multi-dimensional Safety Evaluation of LLM Compression|Zhichao Xu, Ashim Gupta, Tao Li, Oliver Bentham, Vivek Srikumar|2024-07-06|arXiv|https://github.com/zhichaoxu-shufe/Beyond-Perplexity-Compression-Safety-Eval|http://arxiv.org/abs/2407.04965v2|
|207|AriGraph: Learning Knowledge Graph World Models with Episodic Memory for LLM Agents|Petr Anokhin, Nikita Semenov, Artyom Sorokin, Dmitry Evseev, Mikhail Burtsev, Evgeny Burnaev|2024-07-05|arXiv|https://github.com/AIRI-Institute/AriGraph|http://arxiv.org/abs/2407.04363v1|
|208|Towards Enhancing Coherence in Extractive Summarization: Dataset and Experiments with LLMs|Mihir Parmar, Hanieh Deilamsalehy, Franck Dernoncourt, Seunghyun Yoon, Ryan A. Rossi, Trung Bui|2024-07-05|arXiv|https://github.com/Mihir3009/Extract-AI|http://arxiv.org/abs/2407.04855v1|
|209|Automating Venture Capital: Founder assessment using LLM-powered segmentation, feature engineering and automated labeling techniques|Ekin Ozince, Yiğit Ihlamur|2024-07-05|arXiv|https://github.com/velapartners/moneyball-LLM-based-founder-features|http://arxiv.org/abs/2407.04885v1|
|210|When LLMs Play the Telephone Game: Cumulative Changes and Attractors in Iterated Cultural Transmissions|Jérémy Perez, Corentin Léger, Grgur Kovač, Cédric Colas, Gaia Molinaro, Maxime Derex, Pierre-Yves Oudeyer, Clément Moulin-Frier|2024-07-05|arXiv|https://github.com/jeremyperez2/TelephoneGameLLM|http://arxiv.org/abs/2407.04503v1|
|211|BiosERC: Integrating Biography Speakers Supported by LLMs for ERC Tasks|Jieying Xue, Minh Phuong Nguyen, Blake Matheny, Le Minh Nguyen|2024-07-05|arXiv|https://github.com/yingjie7/BiosERC|http://arxiv.org/abs/2407.04279v1|
|212|Future Events as Backdoor Triggers: Investigating Temporal Vulnerabilities in LLMs|Sara Price, Arjun Panickssery, Sam Bowman, Asa Cooper Stickland|2024-07-04|arXiv|https://github.com/sbp354/Future_triggered_backdoors|http://arxiv.org/abs/2407.04108v1|
|213|NutriBench: A Dataset for Evaluating Large Language Models in Carbohydrate Estimation from Meal Descriptions|Andong Hua, Mehak Preet Dhaliwal, Ryan Burke, Yao Qin|2024-07-04|arXiv|https://mehak126.github.io/nutribench.html|https://doi.org/10.48550/arXiv.2407.12843|
|214|The Price of Prompting: Profiling Energy Use in Large Language Models Inference|Erik Johannes Husom, Arda Goknil, Lwin Khin Shar, Sagar Sen|2024-07-04|arXiv|https://github.com/ejhusom/MELODI|https://doi.org/10.48550/arXiv.2407.16893|
|215|AutoBench: Automatic Testbench Generation and Evaluation Using LLMs for HDL Design|Ruidi Qiu, Grace Li Zhang, Rolf Drechsler, Ulf Schlichtmann, Bing Li|2024-07-04|arXiv|https://github.com/AutoBench/AutoBench|http://arxiv.org/abs/2407.03891v1|
|216|GraCoRe: Benchmarking Graph Comprehension and Complex Reasoning in Large Language Models|Zike Yuan, Ming Liu, Hui Wang, Bing Qin|2024-07-03|arXiv|https://github.com/ZIKEYUAN/GraCoRe|https://doi.org/10.48550/arXiv.2407.02936|
|217|Improving LLM Abilities in Idiomatic Translation|Sundesh Donthi, Maximilian Spencer, Om Patel, Joon Doh, Eid Rodan|2024-07-03|arXiv|https://github.com/ANON13222/ITR|http://arxiv.org/abs/2407.03518v1|
|218|Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction|Chenlong Deng, Kelong Mao, Yuyao Zhang, Zhicheng Dou|2024-07-02|arXiv|https://github.com/ChenlongDeng/ADAPT|http://arxiv.org/abs/2407.01964v3|
|219|Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models|Zihan Wang, Deli Chen, Damai Dai, Runxin Xu, Zhuoshu Li, Y. Wu|2024-07-02|arXiv|https://github.com/deepseek-ai/ESFT|https://doi.org/10.48550/arXiv.2407.01906|
|220|TokenPacker: Efficient Visual Projector for Multimodal LLM|Wentong Li, Yuqian Yuan, Jian Liu, Dongqi Tang, Song Wang, Jie Qin, Jianke Zhu, Lei Zhang|2024-07-02|arXiv|https://github.com/CircleRadon/TokenPacker|http://arxiv.org/abs/2407.02392v1|
|221|To Forget or Not? Towards Practical Knowledge Unlearning for Large Language Models|Bozhong Tian, Xiaozhuan Liang, Siyuan Cheng, Qingbin Liu, Mengru Wang, Dianbo Sui, Xi Chen, Huajun Chen, Ningyu Zhang|2024-07-02|arXiv|https://github.com/zjunlp/KnowUnDo|https://doi.org/10.48550/arXiv.2407.01920|
|222|Breaking Bias, Building Bridges: Evaluation and Mitigation of Social Biases in LLMs via Contact Hypothesis|Chahat Raj, Anjishnu Mukherjee, Aylin Caliskan, Antonios Anastasopoulos, Ziwei Zhu|2024-07-02|arXiv|https://github.com/chahatraj/breakingbias|http://arxiv.org/abs/2407.02030v1|
|223|Extracting and Encoding: Leveraging Large Language Models and Medical Knowledge to Enhance Radiological Text Representation|Pablo Messina, René Vidal, Denis Parra, Alvaro Soto, Vladimir Araujo|2024-07-02|ACL|https://github.com/PabloMessina/CXR-Fact-Encoder|https://aclanthology.org/2024.findings-acl.236|
|224|CFinBench: A Comprehensive Chinese Financial Benchmark for Large Language Models|Ying Nie, Binwei Yan, Tianyu Guo, Hao Liu, Haoyu Wang, Wei He, Binfan Zheng, Weihao Wang, Qiang Li, Weijian Sun, Yunhe Wang, Dacheng Tao|2024-07-02|arXiv|https://cfinbench.github.io/|https://doi.org/10.48550/arXiv.2407.02301|
|225|SplitLoRA: A Split Parameter-Efficient Fine-Tuning Framework for Large Language Models|Zheng Lin, Xuanjie Hu, Yuxin Zhang, Zhe Chen, Zihan Fang, Xianhao Chen, Ang Li, Praneeth Vepakomma, Yue Gao|2024-07-01|arXiv|https://fduinc.github.io/splitlora/|https://doi.org/10.48550/arXiv.2407.00952|
|226|RLingua: Improving Reinforcement Learning Sample Efficiency in Robotic Manipulations With Large Language Models|Liangliang Chen, Yutian Lei, Shiyu Jin, Ying Zhang, Liangjun Zhang|2024-07-01|IEEE Robotics and Automation Letters|https://rlingua.github.io|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10529514|
|227|MIRAI: Evaluating LLM Agents for Event Forecasting|Chenchen Ye, Ziniu Hu, Yihe Deng, Zijie Huang, Mingyu Derek Ma, Yanqiao Zhu, Wei Wang|2024-07-01|arXiv|https://mirai-llm.github.io/|http://arxiv.org/abs/2407.01231v1|
|228|FineSurE: Fine-grained Summarization Evaluation using LLMs|Hwanjun Song, Hang Su, Igor Shalyminov, Jason Cai, Saab Mansour|2024-07-01|arXiv|https://github.com/DISL-Lab/FineSurE-ACL24|http://arxiv.org/abs/2407.00908v1|
|229|Fine-grained, Multi-dimensional Summarization Evaluation with LLMs|Hwanjun Song, Hang Su, Igor Shalyminov, Jason Cai, Saab Mansour|2024-07-01|arXiv|https://github.com/DISL-Lab/FineSurE-ACL24|http://arxiv.org/abs/2407.00908v2|
|230|Exploring Advanced Large Language Models with LLMsuite|Giorgio Roffo|2024-07-01|arXiv|https://github.com/giorgioroffo/large_language_models_open_suite|https://doi.org/10.48550/arXiv.2407.12036|
|231|Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement|Zisu Huang, Xiaohua Wang, Feiran Zhang, Zhibo Xu, Cenyuan Zhang, Xiaoqing Zheng, Xuanjing Huang|2024-07-01|arXiv|https://github.com/Huangzisu/query-refinement|https://doi.org/10.48550/arXiv.2407.01461|
|232|AutoFlow: Automated Workflow Generation for Large Language Model Agents|Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu, Yongfeng Zhang|2024-07-01|arXiv|https://github.com/agiresearch/AutoFlow|https://doi.org/10.48550/arXiv.2407.12821|
|233|EconNLI: Evaluating Large Language Models on Economics Reasoning|Yue Guo, Yi Yang|2024-07-01|arXiv|https://github.com/Irenehere/EconNLI|https://doi.org/10.48550/arXiv.2407.01212|
|234|DiscoveryBench: Towards Data-Driven Discovery with Large Language Models|Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi Mishra, Abhijeetsingh Meena, Aryan Prakhar, Tirth Vora, Tushar Khot, Ashish Sabharwal, Peter Clark|2024-07-01|arXiv|https://github.com/allenai/discoverybench|https://doi.org/10.48550/arXiv.2407.01725|
|235|LLMatic: Neural Architecture Search Via Large Language Models And Quality Diversity Optimization|Muhammad Umair Nasir, Sam Earle, Julian Togelius, Steven James, Christopher W. Cleghorn|2024-07|GECCO '24: Proceedings of the Genetic and Evolutionary Computation Conference|https://github.com/umair-nasir14/LLMatic|https://dl.acm.org/doi/10.1145/3638529.3654017|
|236|LLM4GEN: Leveraging Semantic Representation of LLMs for Text-to-Image Generation|Mushui Liu, Yuhang Ma, Yang Zhen, Jun Dan, Yunlong Yu, Zeng Zhao, Zhipeng Hu, Bai Liu, Changjie Fan|2024-06-30|arXiv|https://xiaobul.github.io/LLM4GEN/|http://arxiv.org/abs/2407.00737v1|
|237|GraphArena: Benchmarking Large Language Models on Graph Computational Problems|Jianheng Tang, Qifan Zhang, Yuhan Li, Jia Li|2024-06-29|arXiv|https://github.com/squareRoot3/GraphArena|https://doi.org/10.48550/arXiv.2407.00379|
|238|LLMs-as-Instructors: Learning from Errors Toward Automating Model Improvement|Jiahao Ying, Mingbao Lin, Yixin Cao, Wei Tang, Bo Wang, Qianru Sun, Xuanjing Huang, Shuicheng Yan|2024-06-29|arXiv|https://yingjiahao14.github.io/LLMs-as-Instructors-pages/|http://arxiv.org/abs/2407.00497v1|
|239|MMRo: Are Multimodal LLMs Eligible as the Brain for In-Home Robotics?|Jinming Li, Yichen Zhu, Zhiyuan Xu, Jindong Gu, Minjie Zhu, Xin Liu, Ning Liu, Yaxin Peng, Feifei Feng, Jian Tang|2024-06-28|arXiv|https://mm-robobench.github.io/|http://arxiv.org/abs/2406.19693v1|
|240|Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs|Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang, Zutao Jiang, Mingkai Deng, Jinhong Wang, Tianhua Tao, Junbo Li, Haonan Li, Preslav Nakov, Timothy Baldwin, Zhengzhong Liu, Eric P. Xing, Xiaodan Liang, Zhiqiang Shen|2024-06-28|arXiv|https://mbzuai-llm.github.io/webpage2code/|http://arxiv.org/abs/2406.20098v1|
|241|YuLan: An Open-source Large Language Model|Yutao Zhu, Kun Zhou, Kelong Mao, Wentong Chen, Yiding Sun, Zhipeng Chen, Qian Cao, Yihan Wu, Yushuo Chen, Feng Wang, Lei Zhang, Junyi Li, Xiaolei Wang, Lei Wang, Beichen Zhang, Zican Dong, Xiaoxue Cheng, Yuhan Chen, Xinyu Tang, Yupeng Hou, Qiangqiang Ren, Xincheng Pang, Shufang Xie, Wayne Xin Zhao, Zhicheng Dou, Jiaxin Mao, Yankai Lin, Ruihua Song, Jun Xu, Xu Chen, Rui Yan, Zhewei Wei, Di Hu, Wenbing Huang, Ze-Feng Gao, Yueguo Chen, Weizheng Lu, Ji-Rong Wen|2024-06-28|arXiv|https://github.com/RUC-GSAI/YuLan-Chat|https://doi.org/10.48550/arXiv.2406.19853|
|242|Leave No Document Behind: Benchmarking Long-Context LLMs with Extended Multi-Doc QA|Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, Yongbin Li|2024-06-27|arXiv …, 2024|https://github.com/MozerWang/Loong|http://arxiv.org/abs/2406.17419v1|
|243|STBench: Assessing the Ability of Large Language Models in Spatio-Temporal Analysis|Wenbin Li, Di Yao, Ruibo Zhao, Wenjie Chen, Zijie Xu, Chengxue Luo, Chang Gong, Quanliang Jing, Haining Tan, Jingping Bi|2024-06-27|arXiv|https://github.com/LwbXc/STBench|https://doi.org/10.48550/arXiv.2406.19065|
|244|Predicting the Big Five Personality Traits in Chinese Counselling Dialogues Using Large Language Models|Yang Yan, Lizhi Ma, Anqi Li, Jingsong Ma, Zhenzhong Lan|2024-06-27|arXiv|https://github.com/kuri-leo/BigFive-LLM-Predictor|https://doi.org/10.48550/arXiv.2406.17287|
|245|Investigating How Large Language Models Leverage Internal Knowledge to Perform Complex Reasoning|Miyoung Ko, Sue Hyun Park, Joonsuk Park, Minjoon Seo|2024-06-27|arXiv|https://github.com/kaistAI/knowledge-reasoning|https://doi.org/10.48550/arXiv.2406.19502|
|246|Large Language Models are Interpretable Learners|Ruochen Wang, Si Si, Felix Yu, Dorothea Wiesmann, Cho-Jui Hsieh, Inderjit S. Dhillon|2024-06-27|arXiv|https://github.com/ruocwang/llm-symbolic-program|https://doi.org/10.48550/arXiv.2406.17224|
|247|Dual-Space Knowledge Distillation for Large Language Models|Songming Zhang, Xue Zhang, Zengkui Sun, Yufeng Chen, Jinan Xu|2024-06-27|arXiv|https://github.com/songmzhang/DSKD|https://doi.org/10.48550/arXiv.2406.17328|
|248|DIM: Dynamic Integration of Multimodal Entity Linking with Large Language Model|Shezheng Song, Shasha Li, Jie Yu, Shan Zhao, Xiaopeng Li, Jun Ma, Xiaodong Liu, Zhuo Li, Xiaoguang Mao|2024-06-27|arXiv|https://github.com/season1blue/DIM|https://doi.org/10.48550/arXiv.2407.12019|
|249|Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs|Lei Zhang, Yunshui Li, Jiaming Li, Xiaobo Xia, Jiaxi Yang, Run Luo, Minzheng Wang, Longze Chen, Junhao Liu, Min Yang|2024-06-26|arXiv|https://github.com/Hambaobao/HCP-Coder|http://arxiv.org/abs/2406.18294v2|
|250|Understand What LLM Needs: Dual Preference Alignment for Retrieval-Augmented Generation|Guanting Dong, Yutao Zhu, Chenghao Zhang, Zechen Wang, Zhicheng Dou, Ji-Rong Wen|2024-06-26|arXiv|https://github.com/dongguanting/DPA-RAG|http://arxiv.org/abs/2406.18676v1|
|251|The Surprising Effectiveness of Multimodal Large Language Models for Video Moment Retrieval|Meinardus Boris, Batra Anil, Rohrbach Anna, Rohrbach Marcus|2024-06-26|arXiv|https://github.com/sudo-Boris/mr-Blip|https://doi.org/10.48550/arXiv.2406.18113|
|252|Step-DPO: Step-wise Preference Optimization for Long-chain Reasoning of LLMs|Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, Jiaya Jia|2024-06-26|arXiv|https://github.com/dvlab-research/Step-DPO|http://arxiv.org/abs/2406.18629v1|
|253|Selective Prompting Tuning for Personalized Conversations with LLMs|Qiushi Huang, Xubo Liu, Tom Ko, Bo Wu, Wenwu Wang, Yu Zhang, Lilian Tang|2024-06-26|OpenReview|https://github.com/hqsiswiliam/SPT|http://arxiv.org/abs/2406.18187v1|
|254|Visual Reasoning and Multi-Agent Approach in Multimodal Large Language Models (MLLMs): Solving TSP and mTSP Combinatorial Challenges|Mohammed Elhenawy, Ahmad Abutahoun, Taqwa I. Alhadidi, Ahmed Jaber, Huthaifa I. Ashqar, Shadi Jaradat, Ahmed Abdelhay, Sebastien Glaser, Andry Rakotonirainy|2024-06-26|arXiv|https://github.com/ahmed-abdulhuy/Solving-TSP-and-mTSP-Combinatorial-Challenges-using-Visual-Reasoning-and-Multi-Agent-Approach-MLLMs-|https://doi.org/10.48550/arXiv.2407.00092|
|255|CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs|Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, Alexis Chevalier, Sanjeev Arora, Danqi Chen|2024-06-26|arXiv|https://charxiv.github.io/|http://arxiv.org/abs/2406.18521v1|
|256|ArzEn-LLM: Code-Switched Egyptian Arabic-English Translation and Speech Recognition Using LLMs|Ahmed Heakl, Youssef Zaghloul, Mennatullah Ali, Rania Hossam, Walid Gomaa|2024-06-26|arXiv|http://github.com/ahmedheakl/arazn-llm|http://arxiv.org/abs/2406.18120v1|
|257|A Review of Large Language Models and Autonomous Agents in Chemistry|Mayk Caldas Ramos, Christopher J. Collison, Andrew D. White|2024-06-26|arXiv|https://github.com/ur-whitelab/LLMs-in-science|https://doi.org/10.48550/arXiv.2407.01603|
|258|A Closer Look into Mixture-of-Experts in Large Language Models|Ka Man Lo, Zeyu Huang, Zihan Qiu, Zili Wang, Jie Fu|2024-06-26|arXiv|https://github.com/kamanphoebe/Look-into-MoEs|https://doi.org/10.48550/arXiv.2406.18219|
|259|BADGE: BADminton report Generation and Evaluation with LLM|Shang-Hsuan Chiang, Lin-Wei Chao, Kuang-Da Wang, Chih-Chuan Wang, Wen-Chih Peng|2024-06-26|arXiv|https://github.com/AndyChiangSH/BADGE|http://arxiv.org/abs/2406.18116v1|
|260|Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels|Razvan-Gabriel Dumitru, Vikas Yadav, Rishabh Maheshwary, Paul-Ioan Clotan, Sathwik Tejaswi Madhusudhan, Mihai Surdeanu|2024-06-25|arXiv|https://github.com/RazvanDu/LayerwiseQuant|http://arxiv.org/abs/2406.17415v2|
|261|TALEC: Teach Your LLM to Evaluate in Specific Domain with In-house Criteria by Criteria Division and Zero-shot Plus Few-shot|Kaiqi Zhang, Shuai Yuan, Honghan Zhao|2024-06-25|arXiv|https://github.com/zlkqz/auto_eval|http://arxiv.org/abs/2407.10999v1|
|262|T-MAC: CPU Renaissance via Table Lookup for Low-Bit LLM Deployment on Edge|Jianyu Wei, Shijie Cao, Ting Cao, Lingxiao Ma, Lei Wang, Yanyong Zhang, Mao Yang|2024-06-25|arXiv|https://github.com/microsoft/T-MAC|http://arxiv.org/abs/2407.00088v1|
|263|ShadowLLM: Predictor-based Contextual Sparsity for Large Language Models|Yash Akhauri, Ahmed F. AbouElhamayed, Jordan Dotzel, Zhiru Zhang, Alexander M. Rush, Safeen Huda, Mohamed S. Abdelfattah|2024-06-25|arXiv|https://github.com/abdelfattah-lab/shadow_llm/|https://doi.org/10.48550/arXiv.2406.16635|
|264|Retrieval Augmented Instruction Tuning for Open NER with Large Language Models|Tingyu Xie, Jian Zhang, Yan Zhang, Yuanyuan Liang, Qi Li, Hongwei Wang|2024-06-25|arXiv|https://github.com/Emma1066/Retrieval-Augmented-IT-OpenNER|https://doi.org/10.48550/arXiv.2406.17305|
|265|Math-LLaVA: Bootstrapping Mathematical Reasoning for Multimodal Large Language Models|Wenhao Shi, Zhiqiang Hu, Yi Bin, Junhua Liu, Yang Yang, See-Kiong Ng, Lidong Bing, Roy Ka-Wei Lee|2024-06-25|arXiv|https://github.com/HZQ950419/Math-LLaVA|https://doi.org/10.48550/arXiv.2406.17294|
|266|M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models|Rishabh Maheshwary, Vikas Yadav, Hoang Nguyen, Khyati Mahajan, Sathwik Tejaswi Madhusudhan|2024-06-25|arXiv|https://github.com/ServiceNow/M2Lingual|https://doi.org/10.48550/arXiv.2406.16783|
|267|Multi-LogiEval: Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models|Nisarg Patel, Mohith Kulkarni, Mihir Parmar, Aashna Budhiraja, Mutsumi Nakamura, Neeraj Varshney, Chitta Baral|2024-06-25|arXiv|https://github.com/Mihir3009/Multi-LogiEval|https://doi.org/10.48550/arXiv.2406.17169|
|268|Improving Arithmetic Reasoning Ability of Large Language Models through Relation Tuples, Verification and Dynamic Feedback|Zhongtao Miao, Kaiyan Zhao, Yoshimasa Tsuruoka|2024-06-25|arXiv|https://github.com/gpgg/art|https://doi.org/10.48550/arXiv.2406.17873|
|269|Crafting Customisable Characters with LLMs: Introducing SimsChat, a Persona-Driven Role-Playing Agent Framework|Bohao Yang, Dong Liu, Chen Tang, Chenghao Xiao, Kun Zhao, Chao Li, Lin Yuan, Guang Yang, Lanxiao Huang, Chenghua Lin|2024-06-25|arXiv|https://github.com/Bernard-Yang/SimsChat|http://arxiv.org/abs/2406.17962v3|
|270|Grass: Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients|Aashiq Muhamed, Oscar Li, David Woodruff, Mona Diab, Virginia Smith|2024-06-25|arXiv|https://github.com/aashiqmuhamed/GRASS|http://arxiv.org/abs/2406.17660v1|
|271|AutoDetect: Towards a Unified Framework for Automated Weakness Detection in Large Language Models|Jiale Cheng, Yida Lu, Xiaotao Gu, Pei Ke, Xiao Liu, Yuxiao Dong, Hongning Wang, Jie Tang, Minlie Huang|2024-06-25|arXiv|https://github.com/thu-coai/AutoDetect|https://doi.org/10.48550/arXiv.2406.16714|
|272|Building on Efficient Foundations: Effectively Training LLMs with Structured Feedforward Layers|Xiuying Wei, Skander Moalla, Razvan Pascanu, Caglar Gulcehre|2024-06-25|arXiv:2406.16450, 2024|https://github.com/CLAIRE-Labo/StructuredFFN/tree/main|http://arxiv.org/abs/2406.16450v1|
|273|DARG: Dynamic Evaluation of Large Language Models via Adaptive Reasoning Graph|Zhehao Zhang, Jiaao Chen, Diyi Yang|2024-06-25|arXiv|https://github.com/SALT-NLP/DARG|https://doi.org/10.48550/arXiv.2406.17271|
|274|DemoRank: Selecting Effective Demonstrations for Large Language Models in Ranking Task|Wenhan Liu, Yutao Zhu, Zhicheng Dou|2024-06-25|arXiv|https://github.com/8421BCD/DemoRank|https://doi.org/10.48550/arXiv.2406.16332|
|275|From Distributional to Overton Pluralism: Investigating Large Language Model Alignment|Thom Lake, Eunsol Choi, Greg Durrett|2024-06-25|arXiv|https://github.com/thomlake/investigating-alignment|https://doi.org/10.48550/arXiv.2406.17692|
|276|AudioBench: A Universal Benchmark for Audio Large Language Models|Bin Wang, Xunlong Zou, Geyu Lin, Shuo Sun, Zhuohan Liu, Wenyu Zhang, Zhengyuan Liu, AiTi Aw, Nancy F. Chen|2024-06-24|arXiv|https://github.com/AudioLLMs/AudioBench|https://doi.org/10.48550/arXiv.2406.16020|
|277|Can LLM Graph Reasoning Generalize beyond Pattern Memorization?|Yizhuo Zhang, Heng Wang, Shangbin Feng, Zhaoxuan Tan, Xiaochuang Han, Tianxing He, Yulia Tsvetkov|2024-06-24|arXiv …, 2024|https://github.com/MatthewYZhang/NLGift|http://arxiv.org/abs/2406.15992v1|
|278|ChatGLM: A Family of Large Language Models from GLM-130B to GLM-4 All Tools|Team GLM, :, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Dan Zhang, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Jingyu Sun, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, Zihan Wang|2024-06-24|arXiv|https://github.com/THUDM|https://doi.org/10.48550/arXiv.2406.12793|
|279|Crosslingual Capabilities and Knowledge Barriers in Multilingual Large Language Models|Lynn Chua, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Pasin Manurangsi, Amer Sinha, Chulin Xie, Chiyuan Zhang|2024-06-24|arXiv|https://github.com/google-research/crosslingual-knowledge-barriers|https://doi.org/10.48550/arXiv.2406.16135|
|280|Efficient Evolutionary Search Over Chemical Space with Large Language Models|Haorui Wang, Marta Skreta, Cher-Tian Ser, Wenhao Gao, Lingkai Kong, Felix Streith-Kalthoff, Chenru Duan, Yuchen Zhuang, Yue Yu, Yanqiao Zhu, Yuanqi Du, Alán Aspuru-Guzik, Kirill Neklyudov, Chao Zhang|2024-06-24|arXiv|http://github.com/zoom-wang112358/MOLLEO|https://doi.org/10.48550/arXiv.2406.16976|
|281|FS-RAG: A Frame Semantics Based Approach for Improved Factual Accuracy in Large Language Models|Harish Tayyar Madabushi|2024-06-24|arXiv|https://github.com/H-TayyarMadabushi/A-Frame-Semantics-based-approach-for-Improved-Factual-Accuracy-in-Large-Language-Models|https://doi.org/10.48550/arXiv.2406.16167|
|282|FastMem: Fast Memorization of Prompt Improves Context Awareness of Large Language Models|Junyi Zhu, Shuochen Liu, Yu Yu, Bo Tang, Yibo Yan, Zhiyu Li, Feiyu Xiong, Tong Xu, Matthew B. Blaschko|2024-06-24|arXiv|https://github.com/IAAR-Shanghai/FastMem|https://doi.org/10.48550/arXiv.2406.16069|
|283|Lottery Ticket Adaptation: Mitigating Destructive Interference in LLMs|Ashwinee Panda, Berivan Isik, Xiangyu Qi, Sanmi Koyejo, Tsachy Weissman, Prateek Mittal|2024-06-24|arXiv|https://github.com/kiddyboots216/lottery-ticket-adaptation|http://arxiv.org/abs/2406.16797v2|
|284|Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models|Yichen Sun, Zhixuan Chu, Zhan Qin, Kui Ren|2024-06-24|arXiv|https://github.com/TruthAI-Lab/PCIG|http://arxiv.org/abs/2406.16333v1|
|285|Can LLM be a Personalized Judge?|Yijiang River Dong, Tiancheng Hu, Nigel Collier|2024-06-24|arXiv e-prints, 2024|https://github.com/dong-river/Personalized-Judge|http://arxiv.org/abs/2406.11657v1|
|286|RuleR: Improving LLM Controllability by Rule-based Data Recycling|Ming Li, Han Chen, Chenguang Wang, Dang Nguyen, Dianqi Li, Tianyi Zhou|2024-06-23|arXiv …, 2024|https://github.com/MingLiiii/RuleR|http://arxiv.org/abs/2406.15938v1|
|287|The Music Maestro or The Musically Challenged, A Massive Music Evaluation Benchmark for Large Language Models|Jiajia Li, Lu Yang, Mingni Tang, Chenchong Chenchong, Zuchao Li, Ping Wang, Hai Zhao|2024-06-23|ACL|https://github.com/zcli-charlie/ZIQI-Eval|https://aclanthology.org/2024.findings-acl.194|
|288|Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level|Zhaopeng Feng, Ruizhe Chen, Yan Zhang, Zijie Meng, Zuozhu Liu|2024-06-23|arXiv:2406.15741, 2024|https://github.com/fzp0424/Ladder|http://arxiv.org/abs/2406.15741v1|
|289|EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer Tuning and Voting|Zhongzhi Yu, Zheng Wang, Yuhan Li, Haoran You, Ruijie Gao, Xiaoya Zhou, Sreenidhi Reedy Bommu, Yang Katie Zhao, Yingyan Celine Lin|2024-06-23|arXiv|https://github.com/GATECH-EIC/Edge-LLM|https://doi.org/10.48550/arXiv.2406.15758|
|290|ICLEval: Evaluating In-Context Learning Ability of Large Language Models|Wentong Chen, Yankai Lin, ZhenHao Zhou, HongYun Huang, Yantao Jia, Zhao Cao, Ji-Rong Wen|2024-06-22|arXiv|https://github.com/yiye3/ICLEval|https://doi.org/10.48550/arXiv.2406.14955|
|291|Identifying Inaccurate Descriptions in LLM-generated Code Comments via Test Execution|Sungmin Kang, Louis Milliken, Shin Yoo|2024-06-22|arXiv:2406.14836, 2024|https://smkang96.github.io/assets/pdf/doctest_supplementary_arxiv.pdf|http://arxiv.org/abs/2406.14836v1|
|292|InternLM-Law: An Open Source Chinese Legal Large Language Model|Zhiwei Fei, Songyang Zhang, Xiaoyu Shen, Dawei Zhu, Xiao Wang, Maosong Cao, Fengzhe Zhou, Yining Li, Wenwei Zhang, Dahua Lin, Kai Chen, Jidong Ge|2024-06-22|arXiv|https://github.com/InternLM/InternLM-Law|https://doi.org/10.48550/arXiv.2406.14887|
|293|Leveraging Passage Embeddings for Efficient Listwise Reranking with Large Language Models|Qi Liu, Bo Wang, Nan Wang, Jiaxin Mao|2024-06-22|arXiv|https://github.com/liuqi6777/pe_rank|https://doi.org/10.48550/arXiv.2406.14848|
|294|MT-Ladder: A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level|Zhaopeng Feng, Yan Zhang, Ruizhe Chen, Zijie Meng, Zuozhu Liu|2024-06-22|arXiv|https://github.com/fzp0424/Ladder|http://arxiv.org/abs/2406.15741v2|
|295|Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large Language Models without Training through Attention Calibration|Zhongzhi Yu, Zheng Wang, Yonggan Fu, Huihong Shi, Khalid Shaikh, Yingyan Celine Lin|2024-06-22|arXiv|https://github.com/GATECH-EIC/ACT|https://doi.org/10.48550/arXiv.2406.15765|
|296|video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models|Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Yuxuan Wang, Chao Zhang|2024-06-22|arXiv|https://github.com/bytedance/SALMONN/|https://doi.org/10.48550/arXiv.2406.15704|
|297|GIEBench: Towards Holistic Evaluation of Group Identity-based Empathy for Large Language Models|Leyan Wang, Yonggang Jin, Tianhao Shen, Tianyu Zheng, Xinrun Du, Chenchen Zhang, Wenhao Huang, Jiaheng Liu, Shi Wang, Ge Zhang, Liuyu Xiang, Zhaofeng He|2024-06-22|arXiv|https://github.com/GIEBench/GIEBench|https://doi.org/10.48550/arXiv.2406.14903|
|298|ESC-Eval: Evaluating Emotion Support Conversations in Large Language Models|Haiquan Zhao, Lingyu Li, Shisong Chen, Shuqi Kong, Jiaan Wang, Kexin Huang, Tianle Gu, Yixu Wang, Dandan Liang, Zhixu Li, Yan Teng, Yanghua Xiao, Yingchun Wang|2024-06-21|arXiv|https://github.com/haidequanbu/ESC-Eval|https://doi.org/10.48550/arXiv.2406.14952|
|299|GenoTEX: A Benchmark for Evaluating LLM-Based Exploration of Gene Expression Data in Alignment with Bioinformaticians|Haoyang Liu, Haohan Wang|2024-06-21|arXiv|https://github.com/Liu-Hy/GenoTex|http://arxiv.org/abs/2406.15341v1|
|300|OATH-Frames: Characterizing Online Attitudes Towards Homelessness with LLM Assistants|Jaspreet Ranjit, Brihi Joshi, Rebecca Dorn, Laura Petry, Olga Koumoundouros, Jayne Bottarini, Peichen Liu, Eric Rice, Swabha Swayamdipta|2024-06-21|arXiv|https://dill-lab.github.io/oath-frames/|http://arxiv.org/abs/2406.14883v1|
|301|FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving|Xiaohan Lin, Qingxing Cao, Yinya Huang, Haiming Wang, Jianqiao Lu, Zhengying Liu, Linqi Song, Xiaodan Liang|2024-06-20|arXiv|https://fveler.github.io/|https://doi.org/10.48550/arXiv.2406.14408|
|302|Taxonomy-Guided Zero-Shot Recommendations with LLMs|Yueqing Liang, Liangwei Yang, Chen Wang, Xiongxiao Xu, Philip S. Yu, Kai Shu|2024-06-20|arXiv|https://github.com/yueqingliang1/TaxRec|http://arxiv.org/abs/2406.14043v1|
|303|ReaLHF: Optimized RLHF Training for Large Language Models through Parameter Reallocation|Zhiyu Mei, Wei Fu, Kaiwei Li, Guangju Wang, Huanchen Zhang, Yi Wu|2024-06-20|arXiv|https://github.com/openpsi-project/ReaLHF|https://doi.org/10.48550/arXiv.2406.14088|
|304|MR-BEN: A Comprehensive Meta-Reasoning Benchmark for Large Language Models|Zhongshen Zeng, Yinhong Liu, Yingjia Wan, Jingyao Li, Pengguang Chen, Jianbo Dai, Yuxuan Yao, Rongwu Xu, Zehan Qi, Wanru Zhao, Linling Shen, Jianqiao Lu, Haochen Tan, Yukang Chen, Hao Zhang, Zhan Shi, Bailin Wang, Zhijiang Guo, Jiaya Jia|2024-06-20|arXiv|https://randolph-zeng.github.io/Mr-Ben.github.io/|https://doi.org/10.48550/arXiv.2406.13975|
|305|CityGPT: Empowering Urban Spatial Cognition of Large Language Models|Jie Feng, Yuwei Du, Tianhui Liu, Siqi Guo, Yuming Lin, Yong Li|2024-06-20|arXiv|https://github.com/tsinghua-fib-lab/CityGPT|https://doi.org/10.48550/arXiv.2406.13948|
|306|Evaluating Implicit Bias in Large Language Models by Attacking From a Psychometric Perspective|Yuchen Wen, Keping Bi, Wei Chen, Jiafeng Guo, Xueqi Cheng|2024-06-20|arXiv|https://github.com/wen112358/ImplicitBiasPsychometricEvaluation|https://doi.org/10.48550/arXiv.2406.14023|
|307|CityBench: Evaluating the Capabilities of Large Language Model as World Model|Jie Feng, Jun Zhang, Junbo Yan, Xin Zhang, Tianjian Ouyang, Tianhui Liu, Yuwei Du, Siqi Guo, Yong Li|2024-06-20|arXiv|https://github.com/tsinghua-fib-lab/CityBench|https://doi.org/10.48550/arXiv.2406.13945|
|308|CEBench: A Benchmarking Toolkit for the Cost-Effectiveness of LLM Pipelines|Wenbo Sun, Jiaqi Wang, Qiming Guo, Ziyu Li, Wenlu Wang, Rihan Hai|2024-06-20|arXiv|https://github.com/amademicnoboday12/CEBench|http://arxiv.org/abs/2407.12797v1|
|309|APEER: Automatic Prompt Engineering Enhances Large Language Model Reranking|Can Jin, Hongwu Peng, Shiyu Zhao, Zhenting Wang, Wujiang Xu, Ligong Han, Jiahui Zhao, Kai Zhong, Sanguthevar Rajasekaran, Dimitris N. Metaxas|2024-06-20|arXiv|https://github.com/jincan333/APEER|https://doi.org/10.48550/arXiv.2406.14449|
|310|BeHonest: Benchmarking Honesty of Large Language Models|Steffi Chern, Zhulin Hu, Yuqing Yang, Ethan Chern, Yuan Guo, Jiahe Jin, Binjie Wang, Pengfei Liu|2024-06-19|arXiv|https://github.com/GAIR-NLP/BeHonest|https://doi.org/10.48550/arXiv.2406.13261|
|311|Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators|Matéo Mahaut, Laura Aina, Paula Czarnowska, Momchil Hardalov, Thomas Müller, Lluís Màrquez|2024-06-19|OpenReview|https://github.com/amazon-science/factual-confidence-of-llms|http://arxiv.org/abs/2406.13415v1|
|312|Finding Blind Spots in Evaluator LLMs with Interpretable Checklists|Sumanth Doddapaneni, Mohammed Safi Ur Rahman Khan, Sshubam Verma, Mitesh M. Khapra|2024-06-19|arXiv|https://github.com/AI4Bharat/FBI|http://arxiv.org/abs/2406.13439v1|
|313|Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata|Mykhailo Poliakov, Nadiya Shvai|2024-06-19|arXiv|https://github.com/mxpoliakov/Multi-Meta-RAG|http://arxiv.org/abs/2406.13213v1|
|314|Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models|Guanting Dong, Keming Lu, Chengpeng Li, Tingyu Xia, Bowen Yu, Chang Zhou, Jingren Zhou|2024-06-19|arXiv|https://github.com/QwenLM/AutoIF|https://doi.org/10.48550/arXiv.2406.13542|
|315|MolecularGPT: Open Large Language Model (LLM) for Few-Shot Molecular Property Prediction|Yuyan Liu, Sirui Ding, Sheng Zhou, Wenqi Fan, Qiaoyu Tan|2024-06-18|arXiv|https://github.com/NYUSHCS/MolecularGPT|https://doi.org/10.48550/arXiv.2406.12950|
|316|TroL: Traversal of Layers for Large Language and Vision Models|Byung-Kwan Lee, Sangyun Chung, Chae Won Kim, Beomchan Park, Yong Man Ro|2024-06-18|arXiv|https://github.com/ByungKwanLee/TroL|https://doi.org/10.48550/arXiv.2406.12246|
|317|TourLLM: Enhancing LLMs with Tourism Knowledge|Qikai Wei, Mingzhi Yang, Jinqiang Wang, Wenwei Mao, Jiabo Xu, Huansheng Ning|2024-06-18|arXiv|https://github.com/mrweiqk/Cultour|http://arxiv.org/abs/2407.12791v1|
|318|Stealth edits for provably fixing or attacking large language models|Oliver J. Sutton, Qinghua Zhou, Wei Wang, Desmond J. Higham, Alexander N. Gorban, Alexander Bastounis, Ivan Yu. Tyukin|2024-06-18|arXiv|https://github.com/qinghua-zhou/stealth-edits|https://doi.org/10.48550/arXiv.2406.12670|
|319|SHIELD: Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation|Xiaoze Liu, Ting Sun, Tianyang Xu, Feijie Wu, Cunxiang Wang, Xiaoqian Wang, Jing Gao|2024-06-18|arXiv|https://github.com/xz-liu/SHIELD|http://arxiv.org/abs/2406.12975v1|
|320|CollabStory: Multi-LLM Collaborative Story Generation and Authorship Analysis|Saranya Venkatraman, Nafis Irtiza Tripto, Dongwon Lee|2024-06-18|arXiv|https://github.com/saranya-venkatraman/multi_llm_story_writing|http://arxiv.org/abs/2406.12665v1|
|321|Low-Redundant Optimization for Large Language Model Alignment|Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Jingyuan Wang, Ji-Rong Wen|2024-06-18|arXiv|https://github.com/RUCAIBox/ALLO|https://doi.org/10.48550/arXiv.2406.12606|
|322|IPEval: A Bilingual Intellectual Property Agency Consultation Evaluation Benchmark for Large Language Models|Qiyao Wang, Jianguo Huang, Shule Lu, Yuan Lin, Kan Xu, Liang Yang, Hongfei Lin|2024-06-18|arXiv|https://ipeval.github.io/|https://doi.org/10.48550/arXiv.2406.12386|
|323|Holmes-VAD: Towards Unbiased and Explainable Video Anomaly Detection via Multi-modal LLM|Huaxin Zhang, Xiaohao Xu, Xiang Wang, Jialong Zuo, Chuchu Han, Xiaonan Huang, Changxin Gao, Yuehuan Wang, Nong Sang|2024-06-18|arXiv|https://github.com/pipixin321/HolmesVAD|http://arxiv.org/abs/2406.12235v1|
|324|CherryRec: Enhancing News Recommendation Quality via LLM-driven Framework|Shaohuang Wang, Lun Wang, Yunhan Bu, Tianwei Huang|2024-06-18|arXiv|https://github.com/xxxxxx|http://arxiv.org/abs/2406.12243v1|
|325|VideoLLM-online: Online Video Large Language Model for Streaming Video|Joya Chen, Zhaoyang Lv, Shiwei Wu, Kevin Qinghong Lin, Chenan Song, Difei Gao, Jia-Wei Liu, Ziteng Gao, Dongxing Mao, Mike Zheng Shou|2024-06-17|arXiv|https://showlab.github.io/videollm-online|https://doi.org/10.48550/arXiv.2406.11816|
|326|Unveiling and Mitigating Bias in Mental Health Analysis with Large Language Models|Yuqing Wang, Yun Zhao, Sara Alessandra Keller, Anne A. H. de Hond, Marieke M. van Buchem, Malvika Pillai, Tina Hernandez-Boussard|2024-06-17|arXiv|https://github.com/EternityYW/BiasEval-LLM-MentalHealth|https://doi.org/10.48550/arXiv.2406.12033|
|327|Towards an End-to-End Framework for Invasive Brain Signal Decoding with Large Language Models|Sheng Feng, Heyang Liu, Yu Wang, Yanfeng Wang|2024-06-17|arXiv|https://github.com/FsFrancis15/BrainLLM|https://doi.org/10.48550/arXiv.2406.11568|
|328|Soft Prompting for Unlearning in Large Language Models|Karuna Bhaila, Minh-Hao Van, Xintao Wu|2024-06-17|arXiv|https://github.com/karuna-bhaila/llm_unlearning|https://doi.org/10.48550/arXiv.2406.12038|
|329|Probing the Decision Boundaries of In-context Learning in Large Language Models|Siyan Zhao, Tung Nguyen, Aditya Grover|2024-06-17|arXiv|https://github.com/siyan-zhao/ICL_decision_boundary|https://doi.org/10.48550/arXiv.2406.11233|
|330|Multimodal Needle in a Haystack: Benchmarking Long-Context Capability of Multimodal Large Language Models|Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, Hao Wang|2024-06-17|arXiv|https://github.com/Wang-ML-Lab/multimodal-needle-in-a-haystack|https://doi.org/10.48550/arXiv.2406.11230|
|331|LLaNA: Large Language and NeRF Assistant|Andrea Amaduzzi, Pierluigi Zama Ramirez, Giuseppe Lisanti, Samuele Salti, Luigi Di Stefano|2024-06-17|arXiv|https://andreamaduzzi.github.io/llana/|https://doi.org/10.48550/arXiv.2406.11840|
|332|Investigating Annotator Bias in Large Language Models for Hate Speech Detection|Amit Das, Zheng Zhang, Fatemeh Jamshidi, Vinija Jain, Aman Chadha, Nilanjana Raychawdhary, Mary Sandage, Lauramarie Pope, Gerry V. Dozier, Cheryl D. Seals|2024-06-17|arXiv|https://github.com/AmitDasRup123/HateSpeechCorpus|https://doi.org/10.48550/arXiv.2406.11109|
|333|Interactive Evolution: A Neural-Symbolic Self-Training Framework For Large Language Models|Fangzhi Xu, Qiushi Sun, Kanzhi Cheng, Jun Liu, Yu Qiao, Zhiyong Wu|2024-06-17|arXiv|https://github.com/xufangzhi/ENVISIONS|https://doi.org/10.48550/arXiv.2406.11736|
|334|Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging|Priyanka Kargupta, Ishika Agarwal, Dilek Hakkani-Tur, Jiawei Han|2024-06-17|arXiv|http://github.com/agarwalishika/TreeInstruct|http://arxiv.org/abs/2406.11709v2|
|335|GeoGPT4V: Towards Geometric Multi-modal Large Language Models with Geometric Image Generation|Shihao Cai, Keqin Bao, Hangyu Guo, Jizhi Zhang, Jun Song, Bo Zheng|2024-06-17|arXiv|https://github.com/Lanyu0303/GeoGPT4V_Project|https://doi.org/10.48550/arXiv.2406.11503|
|336|ExCP: Extreme LLM Checkpoint Compression via Weight-Momentum Joint Shrinking|Wenshuo Li, Xinghao Chen, Han Shu, Yehui Tang, Yunhe Wang|2024-06-17|arXiv|https://github.com/Gaffey/ExCP|http://arxiv.org/abs/2406.11257v1|
|337|DB-GPT-Hub: Towards Open Benchmarking Text-to-SQL Empowered by Large Language Models|Fan Zhou, Siqiao Xue, Danrui Qi, Wenhui Shi, Wang Zhao, Ganglin Wei, Hongyang Zhang, Caigai Jiang, Gangwei Jiang, Zhixuan Chu, Faqiang Chen|2024-06-17|arXiv|https://github.com/eosphoros-ai/DB-GPT-Hub|https://doi.org/10.48550/arXiv.2406.11434|
|338|AvaTaR: Optimizing LLM Agents for Tool-Assisted Knowledge Retrieval|Shirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis N. Ioannidis, Karthik Subbian, Jure Leskovec, James Zou|2024-06-17|arXiv|https://github.com/zou-group/avatar|http://arxiv.org/abs/2406.11200v2|
|339|Toward Optimal LLM Alignments Using Two-Player Games|Rui Zheng, Hongyi Guo, Zhihan Liu, Xiaoying Zhang, Yuanshun Yao, Xiaojun Xu, Zhaoran Wang, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang, Hang Li, Yang Liu|2024-06-16|arXiv|https://github.com/ruizheng20/gpo|http://arxiv.org/abs/2406.10977v1|
|340|SCAR: Efficient Instruction-Tuning for Large Language Models via Style Consistency-Aware Response Ranking|Zhuang Li, Yuncheng Hua, Thuy-Trang Vu, Haolan Zhan, Lizhen Qu, Gholamreza Haffari|2024-06-16|arXiv|https://github.com/zhuang-li/SCAR|https://doi.org/10.48550/arXiv.2406.10882|
|341|RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models|Zhuoran Jin, Pengfei Cao, Chenhao Wang, Zhitao He, Hongbang Yuan, Jiachun Li, Yubo Chen, Kang Liu, Jun Zhao|2024-06-16|arXiv|http://rwku-bench.github.io|https://doi.org/10.48550/arXiv.2406.10890|
|342|RUPBench: Benchmarking Reasoning Under Perturbations for Robustness Evaluation in Large Language Models|Yuqing Wang, Yun Zhao|2024-06-16|arXiv|https://github.com/EternityYW/RUPBench|https://doi.org/10.48550/arXiv.2406.11020|
|343|Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference|Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, Song Han|2024-06-16|arXiv|http://github.com/mit-han-lab/Quest|http://arxiv.org/abs/2406.10774v1|
|344|Investigating Video Reasoning Capability of Large Language Models with Tropes in Movies|Hung-Ting Su, Chun-Tong Chao, Ya-Ching Hsu, Xudong Lin, Yulei Niu, Hung-Yi Lee, Winston H. Hsu|2024-06-16|arXiv|https://ander1119.github.io/TiM|https://doi.org/10.48550/arXiv.2406.10923|
|345|A Peek into Token Bias: Large Language Models Are Not Yet Genuine Reasoners|Bowen Jiang, Yangxinyu Xie, Zhuoqun Hao, Xiaomeng Wang, Tanwi Mallick, Weijie J. Su, Camillo J. Taylor, Dan Roth|2024-06-16|arXiv|https://github.com/bowen-upenn/llm_token_bias|https://doi.org/10.48550/arXiv.2406.11050|
|346|A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery|Yu Zhang, Xiusi Chen, Bowen Jin, Sheng Wang, Shuiwang Ji, Wei Wang, Jiawei Han|2024-06-16|arXiv|https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models|https://doi.org/10.48550/arXiv.2406.10833|
|347|GUI-WORLD: A Dataset for GUI-oriented Multimodal LLM-based Agents|Dongping Chen, Yue Huang, Siyuan Wu, Jingyu Tang, Liuyi Chen, Yilin Bai, Zhigang He, Chenlong Wang, Huichi Zhou, Yiqiang Li, Tianshuo Zhou, Yue Yu, Chujie Gao, Qihui Zhang, Yi Gui, Zhen Li, Yao Wan, Pan Zhou, Jianfeng Gao, Lichao Sun|2024-06-16|arXiv|https://gui-world.github.io/|http://arxiv.org/abs/2406.10819v1|
|348|Evaluating the Generalization Ability of Quantized LLMs: Benchmark, Analysis, and Toolbox|Yijun Liu, Yuan Meng, Fang Wu, Shenhao Peng, Hang Yao, Chaoyu Guan, Chen Tang, Xinzhu Ma, Zhi Wang, Wenwu Zhu|2024-06-15|arXiv|https://github.com/TsingmaoAI/MI-optimize|http://arxiv.org/abs/2406.12928v1|
|349|Large Language Models as Surrogate Models in Evolutionary Algorithms: A Preliminary Study|Hao Hao, Xiaoqun Zhang, Aimin Zhou|2024-06-15|arXiv|https://github.com/hhyqhh/LAEA|https://doi.org/10.48550/arXiv.2406.10675|
|350|StrucText-Eval: An Autogenerated Benchmark for Evaluating Large Language Model&apos;s Ability in Structure-Rich Text Understanding|Zhouhong Gu, Haoning Ye, Zeyang Zhou, Hongwei Feng, Yanghua Xiao|2024-06-15|arXiv|https://github.com/MikeGu721/StrucText-Eval|https://doi.org/10.48550/arXiv.2406.10621|
|351|StructBench: An Autogenerated Benchmark for Evaluating Large Language Model's Ability in Structure-Rich Text Understanding|Zhouhong Gu, Haoning Ye, Zeyang Zhou, Hongwei Feng, Yanghua Xiao|2024-06-15|arXiv|https://github.com/MikeGu721/StructBench|http://arxiv.org/abs/2406.10621v1|
|352|CliBench: Multifaceted Evaluation of Large Language Models in Clinical Decisions on Diagnoses, Procedures, Lab Tests Orders and Prescriptions|Mingyu Derek Ma, Chenchen Ye, Yu Yan, Xiaoxuan Wang, Peipei Ping, Timothy S. Chang, Wei Wang|2024-06-14|arXiv|https://clibench.github.io|https://doi.org/10.48550/arXiv.2406.09923|
|353|What is the best model? Application-driven Evaluation for Large Language Models|Shiguo Lian, Kaikai Zhao, Xinhui Liu, Xuejiao Lei, Bikun Yang, Wenjing Zhang, Kai Wang, Zhaoxiang Liu|2024-06-14|arXiv|https://github.com/UnicomAI/DataSet/tree/main/TestData/GeneralAbility|https://doi.org/10.48550/arXiv.2406.10307|
|354|First Multi-Dimensional Evaluation of Flowchart Comprehension for Multimodal Large Language Models|Enming Zhang, Ruobing Yao, Huanyong Liu, Junhui Yu, Jiale Wang|2024-06-14|arXiv|https://github.com/360AILAB-NLP/FlowCE|https://doi.org/10.48550/arXiv.2406.10057|
|355|Be like a Goldfish, Don't Memorize! Mitigating Memorization in Generative LLMs|Abhimanyu Hans, Yuxin Wen, Neel Jain, John Kirchenbauer, Hamid Kazemi, Prajwal Singhania, Siddharth Singh, Gowthami Somepalli, Jonas Geiping, Abhinav Bhatele, Tom Goldstein|2024-06-14|arXiv|https://github.com/ahans30/goldfish-loss|http://arxiv.org/abs/2406.10209v1|
|356|CHiSafetyBench: A Chinese Hierarchical Safety Benchmark for Large Language Models|Wenjing Zhang, Xuejiao Lei, Zhaoxiang Liu, Meijuan An, Bikun Yang, Kaikai Zhao, Kai Wang, Shiguo Lian|2024-06-14|arXiv|https://github.com/UnicomAI/DataSet/tree/main/TestData/Safety|https://doi.org/10.48550/arXiv.2406.10311|
|357|BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages|Junho Myung, Nayeon Lee, Yi Zhou, Jiho Jin, Rifki Afina Putri, Dimosthenis Antypas, Hsuvas Borkakoty, Eunsu Kim, Carla Perez-Almendros, Abinew Ali Ayele, Víctor Gutiérrez-Basulto, Yazmín Ibáñez-García, Hwaran Lee, Shamsuddeen Hassan Muhammad, Kiwoong Park, Anar Sabuhi Rzayev, Nina White, Seid Muhie Yimam, Mohammad Taher Pilehvar, Nedjma Ousidhoum, Jose Camacho-Collados, Alice Oh|2024-06-14|arXiv|https://github.com/nlee0212/BLEnD|http://arxiv.org/abs/2406.09948v1|
|358|LLAVIDAL: Benchmarking Large Language Vision Models for Daily Activities of Living|Rajatsubhra Chakraborty, Arkaprava Sinha, Dominick Reilly, Manish Kumar Govind, Pu Wang, François Brémond, Srijan Das|2024-06-13|arXiv|https://adl-x.github.io/|https://doi.org/10.48550/arXiv.2406.09390|
|359|Sharing Matters: Analysing Neurons Across Languages and Tasks in LLMs|Weixuan Wang, Barry Haddow, Wei Peng, Alexandra Birch|2024-06-13|arXiv|https://github.com/weixuan-wang123/multilingual-neurons|http://arxiv.org/abs/2406.09265v1|
|360|SciKnowEval: Evaluating Multi-level Scientific Knowledge of Large Language Models|Kehua Feng, Keyan Ding, Weijie Wang, Xiang Zhuang, Zeyuan Wang, Ming Qin, Yu Zhao, Jianhua Yao, Qiang Zhang, Huajun Chen|2024-06-13|arXiv|https://github.com/hicai-zju/sciknoweval|https://doi.org/10.48550/arXiv.2406.09098|
|361|Living in the Moment: Can Large Language Models Grasp Co-Temporal Reasoning?|Zhaochen Su, Juntao Li, Jun Zhang, Tong Zhu, Xiaoye Qu, Pan Zhou, Yan Bowen, Yu Cheng, Min Zhang|2024-06-13|arXiv|https://github.com/zhaochen0110/Cotempqa|https://doi.org/10.48550/arXiv.2406.09072|
|362|LLM Reading Tea Leaves: Automatically Evaluating Topic Models with Large Language Models|Xiaohao Yang, He Zhao, Dinh Q. Phung, Wray L. Buntine, Lan Du|2024-06-13|arXiv|https://github.com/Xiaohao-Yang/Topic_Model_Evaluation|https://doi.org/10.48550/arXiv.2406.09008|
|363|Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs|Zhao Xu, Fan Liu, Hao Liu|2024-06-13|arXiv|https://github.com/usail-hkust/Bag_of_Tricks_for_LLM_Jailbreaking|http://arxiv.org/abs/2406.09324v1|
|364|JailbreakEval: An Integrated Toolkit for Evaluating Jailbreak Attempts Against Large Language Models|Delong Ran, Jinyuan Liu, Yichen Gong, Jingyi Zheng, Xinlei He, Tianshuo Cong, Anyu Wang|2024-06-13|arXiv|https://github.com/ThuCCSLab/JailbreakEval|https://doi.org/10.48550/arXiv.2406.09321|
|365|Investigating the translation capabilities of Large Language Models trained on parallel data only|Javier García Gilabert, Carlos Escolano, Aleix Sant Savall, Francesca de Luca Fornaciari, Audrey Mash, Xixian Liao, Maite Melero|2024-06-13|arXiv|https://github.com/projecte-aina/Plume|https://doi.org/10.48550/arXiv.2406.09140|
|366|Enhancing Psychotherapy Counseling: A Data Augmentation Pipeline Leveraging Large Language Models for Counseling Conversations|Jun-Woo Kim, Ji-Eun Han, Jun-Seok Koh, Hyeon-Tae Seo, Du-Seong Chang|2024-06-13|arXiv|https://github.com/jwkim-chat/A-Data-Augmentation-Pipeline-Leveraging-Large-Language-Models-for-Counseling-Conversations|https://doi.org/10.48550/arXiv.2406.08718|
|367|DefAn: Definitive Answer Dataset for LLMs Hallucination Evaluation|A B M Ashikur Rahman, Saeed Anwar, Muhammad Usman, Ajmal Mian|2024-06-13|arXiv|https://github.com/ashikiut/DefAn|http://arxiv.org/abs/2406.09155v1|
|368|Chain of Preference Optimization: Improving Chain-of-Thought Reasoning in LLMs|Xuan Zhang, Chao Du, Tianyu Pang, Qian Liu, Wei Gao, Min Lin|2024-06-13|arXiv|https://github.com/sail-sg/CPO|http://arxiv.org/abs/2406.09136v1|
|369|MobileAgentBench: An Efficient and User-Friendly Benchmark for Mobile LLM Agents|Luyuan Wang, Yongyu Deng, Yiwei Zha, Guodong Mao, Qinmin Wang, Tianchen Min, Wei Chen, Shoufa Chen|2024-06-12|arXiv|https://MobileAgentBench.github.io|http://arxiv.org/abs/2406.08184v1|
|370|TasTe: Teaching Large Language Models to Translate through Self-Reflection|Yutong Wang, Jiali Zeng, Xuebo Liu, Fandong Meng, Jie Zhou, Min Zhang|2024-06-12|arXiv|https://github.com/YutongWang1216/ReflectionLLMMT|https://doi.org/10.48550/arXiv.2406.08434|
|371|Reversing the Forget-Retain Objectives: An Efficient LLM Unlearning Framework from Logit Difference|Jiabao Ji, Yujian Liu, Yang Zhang, Gaowen Liu, Ramana Rao Kompella, Sijia Liu, Shiyu Chang|2024-06-12|arXiv|https://github.com/UCSB-NLP-Chang/ULD|http://arxiv.org/abs/2406.08607v1|
|372|Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing|Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, Bill Yuchen Lin|2024-06-12|arXiv|https://magpie-align.github.io/|http://arxiv.org/abs/2406.08464v1|
|373|Large Language Models Must Be Taught to Know What They Don&apos;t Know|Sanyam Kapoor, Nate Gruver, Manley Roberts, Katherine M. Collins, Arka Pal, Umang Bhatt, Adrian Weller, Samuel Dooley, Micah Goldblum, Andrew Gordon Wilson|2024-06-12|arXiv|https://github.com/activatedgeek/calibration-tuning|https://doi.org/10.48550/arXiv.2406.08391|
|374|CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery|Xiaoshuai Song, Muxi Diao, Guanting Dong, Zhengyang Wang, Yujia Fu, Runqi Qiao, Zhexu Wang, Dayuan Fu, Huangxuan Wu, Bin Liang, Weihao Zeng, Yejie Wang, Zhuoma Gongque, Jianing Yu, Qiuna Tan, Weiran Xu|2024-06-12|arXiv|https://github.com/csbench/csbench|https://doi.org/10.48550/arXiv.2406.08587|
|375|QuickLLaMA: Query-aware Inference Acceleration for Large Language Models|Jingyao Li, Han Shi, Xin Jiang, Zhenguo Li, Hong Xu, Jiaya Jia|2024-06-11|arXiv|https://github.com/dvlab-research/Q-LLM|https://doi.org/10.48550/arXiv.2406.07528|
|376|Mitigating Boundary Ambiguity and Inherent Bias for Text Classification in the Era of Large Language Models|Zhenyi Lu, Jie Tian, Wei Wei, Xiaoye Qu, Yu Cheng, Wenfeng Xie, Dangyang Chen|2024-06-11|arXiv|https://github.com/Chuge0335/PC-CoT|https://doi.org/10.48550/arXiv.2406.07001|
|377|When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models|Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Yingyan Celine Lin|2024-06-11|ICML|https://github.com/GATECH-EIC/Linearized-LLM|https://openreview.net/forum?id=7mFSaP6IiN|
|378|VulDetectBench: Evaluating the Deep Capability of Vulnerability Detection with Large Language Models|Yu Liu, Lang Gao, Mingxin Yang, Yu Xie, Ping Chen, Xiaojin Zhang, Wei Chen|2024-06-11|arXiv|https://github.com/Sweetaroo/VulDetectBench|https://doi.org/10.48550/arXiv.2406.07595|
|379|Towards more realistic evaluation of LLM-based code generation: an experimental study and beyond|Dewu Zheng, Yanlin Wang, Ensheng Shi, Ruikai Zhang, Yuchi Ma, Hongyu Zhang, Zibin Zheng|2024-06-11|arXiv|https://github.com/DeepSoftwareAnalytics/EvoEval|http://arxiv.org/abs/2406.06918v1|
|380|Scaling Large-Language-Model-based Multi-Agent Collaboration|Chen Qian, Zihao Xie, Yifei Wang, Wei Liu, Yufan Dang, Zhuoyun Du, Weize Chen, Cheng Yang, Zhiyuan Liu, Maosong Sun|2024-06-11|arXiv|https://github.com/OpenBMB/ChatDev|https://doi.org/10.48550/arXiv.2406.07155|
|381|MoreauPruner: Robust Pruning of Large Language Models against Weight Perturbations|Zixiao Wang, Jingwei Zhang, Wenqian Zhao, Farzan Farnia, Bei Yu|2024-06-11|arXiv|https://github.com/ShiningSord/MoreauPruner|https://doi.org/10.48550/arXiv.2406.07017|
|382|Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena|Aidar Myrzakhan, Sondos Mahmoud Bsharat, Zhiqiang Shen|2024-06-11|arXiv|https://github.com/VILA-Lab/Open-LLM-Leaderboard|http://arxiv.org/abs/2406.07545v1|
|383|MBBQ: A Dataset for Cross-Lingual Comparison of Stereotypes in Generative LLMs|Vera Neplenbroek, Arianna Bisazza, Raquel Fernández|2024-06-11|arXiv|https://github.com/Veranep/MBBQ|http://arxiv.org/abs/2406.07243v2|
|384|Limited Out-of-Context Knowledge Reasoning in Large Language Models|Peng Hu, Changjiang Gao, Ruiqi Gao, Jiajun Chen, Shujian Huang|2024-06-11|arXiv|https://github.com/NJUNLP/ID-OCKR|https://doi.org/10.48550/arXiv.2406.07393|
|385|LUNAR: Unsupervised LLM-based Log Parsing|Junjie Huang, Zhihan Jiang, Zhuangbin Chen, Michael R. Lyu|2024-06-11|arXiv|https://github.com/Jun-jie-Huang/LUNAR|http://arxiv.org/abs/2406.07174v2|
|386|Instruct Large Language Models to Drive like Humans|Ruijun Zhang, Xianda Guo, Wenzhao Zheng, Chenming Zhang, Kurt Keutzer, Long Chen|2024-06-11|arXiv|https://github.com/bonbon-rj/InstructDriver|https://doi.org/10.48550/arXiv.2406.07296|
|387|Evolving Subnetwork Training for Large Language Models|Hanqi Li, Lu Chen, Da Ma, Zijian Wu, Su Zhu, Kai Yu|2024-06-11|arXiv|https://github.com/OpenDFM/EST|https://doi.org/10.48550/arXiv.2406.06962|
|388|Entropy-Reinforced Planning with Large Language Models for Drug Discovery|Xuefeng Liu, Chih-chan Tien, Peng Ding, Songhao Jiang, Rick L. Stevens|2024-06-11|arXiv|https://github.com/xuefeng-cs/ERP|https://doi.org/10.48550/arXiv.2406.07025|
|389|Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study|Yichi Zhang, Yao Huang, Yitong Sun, Chang Liu, Zhe Zhao, Zhengwei Fang, Yifan Wang, Huanran Chen, Xiao Yang, Xingxing Wei, Hang Su, Yinpeng Dong, Jun Zhu|2024-06-11|arXiv|https://multi-trust.github.io/|https://doi.org/10.48550/arXiv.2406.07057|
|390|AutoSurvey: Large Language Models Can Automatically Write Surveys|Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, Wei Ye, Shikun Zhang, Yue Zhang|2024-06-10|arXiv|https://github.com/AutoSurveys/AutoSurvey|https://doi.org/10.48550/arXiv.2406.10252|
|391|How Efficient is LLM-Generated Code? A Rigorous & High-Standard Benchmark|Ruizhong Qiu, Weiliang Will Zeng, Hanghang Tong, James Ezick, Christopher Lott|2024-06-10|arXiv|https://github.com/q-rz/enamel|http://arxiv.org/abs/2406.06647v2|
|392|LLM Dataset Inference: Did you train on my dataset?|Pratyush Maini, Hengrui Jia, Nicolas Papernot, Adam Dziedzic|2024-06-10|arXiv|https://github.com/pratyushmaini/llm_dataset_inference/|http://arxiv.org/abs/2406.06443v1|
|393|Low-Rank Quantization-Aware Training for LLMs|Yelysei Bondarenko, Riccardo Del Chiaro, Markus Nagel|2024-06-10|arXiv|https://github.com/qualcomm-ai-research/LR-QAT|http://arxiv.org/abs/2406.06385v2|
|394|Recurrent Context Compression: Efficiently Expanding the Context Window of LLM|Chensen Huang, Guibo Zhu, Xuepeng Wang, Yifei Luo, Guojing Ge, Haoran Chen, Dong Yi, Jinqiao Wang|2024-06-10|arXiv|https://github.com/WUHU-G/RCC_Transformer|http://arxiv.org/abs/2406.06110v1|
|395|ShiftAddLLM: Accelerating Pretrained LLMs via Post-Training Multiplication-Less Reparameterization|Haoran You, Yipin Guo, Yichao Fu, Wei Zhou, Huihong Shi, Xiaofan Zhang, Souvik Kundu, Amir Yazdanbakhsh, Yingyan Celine Lin|2024-06-10|arXiv|https://github.com/GATECH-EIC/ShiftAddLLM|http://arxiv.org/abs/2406.05981v3|
|396|Hello Again! LLM-powered Personalized Agent for Long-term Dialogue|Hao Li, Chenghao Yang, An Zhang, Yang Deng, Xiang Wang, Tat-Seng Chua|2024-06-09|arXiv|https://github.com/leolee99/LD-Agent|http://arxiv.org/abs/2406.05925v1|
|397|How Alignment and Jailbreak Work: Explain LLM Safety through Intermediate Hidden States|Zhenhong Zhou, Haiyang Yu, Xinghua Zhang, Rongwu Xu, Fei Huang, Yongbin Li|2024-06-09|arXiv|https://github.com/ydyjya/LLM-IHS-Explanation|http://arxiv.org/abs/2406.05644v2|
|398|Soundscape Captioning using Sound Affective Quality Network and Large Language Model|Yuanbo Hou, Qiaoqiao Ren, Andrew Mitchell, Wenwu Wang, Jian Kang, Tony Belpaeme, Dick Botteldooren|2024-06-09|arXiv|https://github.com/Yuanbo2020/SoundSCaper|https://doi.org/10.48550/arXiv.2406.05914|
|399|Flow of Reasoning: Efficient Training of LLM Policy with Divergent Thinking|Fangxu Yu, Lai Jiang, Haoqiang Kang, Shibo Hao, Lianhui Qin|2024-06-09|arXiv|https://github.com/Yu-Fangxu/FoR|http://arxiv.org/abs/2406.05673v2|
|400|Data-Juicer: A One-Stop Data Processing System for Large Language Models|Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie, Zhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, Jingren Zhou|2024-06-09|SIGMOD/PODS '24: Companion of the 2024 International Conference on Management of Data|https://github.com/alibaba/data-juicer|https://dl.acm.org/doi/10.1145/3626246.3653385|
|401|Large Language Model Assisted Adversarial Robustness Neural Architecture Search|Rui Zhong, Yang Cao, Jun Yu, Masaharu Munetomo|2024-06-08|arXiv|https://github.com/RuiZhong961230/LLMO|https://doi.org/10.48550/arXiv.2406.05433|
|402|NYU CTF Dataset: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security|Minghao Shao, Sofija Jancheska, Meet Udeshi, Brendan Dolan-Gavitt, Haoran Xi, Kimberly Milner, Boyuan Chen, Max Yin, Siddharth Garg, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Muhammad Shafique|2024-06-08|arXiv|https://github.com/NYU-LLM-CTF/LLM_CTF_Database|http://arxiv.org/abs/2406.05590v1|
|403|On the Worst Prompt Performance of Large Language Models|Bowen Cao, Deng Cai, Zhisong Zhang, Yuexian Zou, Wai Lam|2024-06-08|arXiv|https://github.com/cbwbuaa/On-the-Worst-Prompt-|https://doi.org/10.48550/arXiv.2406.10248|
|404|LLM-Enhanced Bayesian Optimization for Efficient Analog Layout Constraint Generation|Guojin Chen, Keren Zhu, Seunggeun Kim, Hanqing Zhu, Yao Lai, Bei Yu, David Z. Pan|2024-06-07|arXiv|https://github.com/dekura/LLANA|http://arxiv.org/abs/2406.05250v2|
|405|Towards Semantic Equivalence of Tokenization in Multimodal LLM|Shengqiong Wu, Hao Fei, Xiangtai Li, Jiayi Ji, Hanwang Zhang, Tat-Seng Chua, Shuicheng Yan|2024-06-07|arXiv|https://chocowu.github.io/SeTok-web/|http://arxiv.org/abs/2406.05127v2|
|406|LawGPT: A Chinese Legal Knowledge-Enhanced Large Language Model|Zhi Zhou, Jiang-Xin Shi, Peng-Xiao Song, Xiao-Wen Yang, Yi-Xuan Jin, Lan-Zhe Guo, Yu-Feng Li|2024-06-07|arXiv|https://github.com/pengxiao-song/LaWGPT|https://doi.org/10.48550/arXiv.2406.04614|
|407|FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models|Rui Ye, Rui Ge, Xinyu Zhu, Jingyi Chai, Yaxin Du, Yang Liu, Yanfeng Wang, Siheng Chen|2024-06-07|arXiv|https://github.com/rui-ye/FedLLM-Bench|https://doi.org/10.48550/arXiv.2406.04845|
|408|CRiskEval: A Chinese Multi-Level Risk Evaluation Benchmark Dataset for Large Language Models|Ling Shi, Deyi Xiong|2024-06-07|arXiv|https://github.com/lingshi6565/Risk_eval|https://doi.org/10.48550/arXiv.2406.04752|
|409|An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models|Xiongtao Zhou, Jie He, Yuhua Ke, Guangyao Zhu, Víctor Gutiérrez-Basulto, Jeff Z. Pan|2024-06-07|arXiv|https://github.com/alenai97/PEFT-MLLM|https://doi.org/10.48550/arXiv.2406.05130|
|410|3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination|Jianing Yang, Xuweiyi Chen, Nikhil Madaan, Madhavan Iyengar, Shengyi Qian, David F. Fouhey, Joyce Chai|2024-06-07|arXiv|https://3d-grand.github.io|http://arxiv.org/abs/2406.05132v2|
|411|MoralBench: Moral Evaluation of LLMs|Jianchao Ji, Yutong Chen, Mingyu Jin, Wujiang Xu, Wenyue Hua, Yongfeng Zhang|2024-06-06|arXiv|https://github.com/agiresearch/MoralBench|http://arxiv.org/abs/2406.04428v1|
|412|ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models|Yuanyi Ren, Haoran Ye, Hanjun Fang, Xin Zhang, Guojie Song|2024-06-06|arXiv|https://github.com/Value4AI/ValueBench|https://doi.org/10.48550/arXiv.2406.04214|
|413|Text-to-Drive: Diverse Driving Behavior Synthesis via Large Language Models|Phat Nguyen, Tsun-Hsuan Wang, Zhang-Wei Hong, Sertac Karaman, Daniela Rus|2024-06-06|arXiv|https://text-to-drive.github.io/|https://doi.org/10.48550/arXiv.2406.04300|
|414|TESTEVAL: Benchmarking Large Language Models for Test Case Generation|Wenhan Wang, Chenyuan Yang, Zhijie Wang, Yuheng Huang, Zhaoyang Chu, Da Song, Lingming Zhang, An Ran Chen, Lei Ma|2024-06-06|arXiv|https://llm4softwaretesting.github.io|https://doi.org/10.48550/arXiv.2406.04531|
|415|PaCE: Parsimonious Concept Engineering for Large Language Models|Jinqi Luo, Tianjiao Ding, Kwan Ho Ryan Chan, Darshan Thaker, Aditya Chattopadhyay, Chris Callison-Burch, René Vidal|2024-06-06|arXiv|https://github.com/peterljq/Parsimonious-Concept-Engineering|https://doi.org/10.48550/arXiv.2406.04331|
|416|Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models|Ling Yang, Zhaochen Yu, Tianjun Zhang, Shiyi Cao, Minkai Xu, Wentao Zhang, Joseph E. Gonzalez, Bin Cui|2024-06-06|arXiv|https://github.com/YangLing0818/buffer-of-thought-llm|https://doi.org/10.48550/arXiv.2406.04271|
|417|LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text Classification|Chun Liu, Hongguang Zhang, Kainan Zhao, Xinghai Ju, Lin Yang|2024-06-06|OpenReview|https://github.com/ChunLiu-cs/LLMEmbed-ACL2024|http://arxiv.org/abs/2406.03725v1|
|418|Evaluating the Smooth Control of Attribute Intensity in Text Generation with LLMs|Shang Zhou, Feng Yao, Chengyu Dong, Zihan Wang, Jingbo Shang|2024-06-06|arXiv|https://github.com/ShangDataLab/Smooth-Control|http://arxiv.org/abs/2406.04460v1|
|419|DICE: Detecting In-distribution Contamination in LLM's Fine-tuning Phase for Math Reasoning|Shangqing Tu, Kejian Zhu, Yushi Bai, Zijun Yao, Lei Hou, Juanzi Li|2024-06-06|arXiv|https://github.com/THU-KEG/DICE|http://arxiv.org/abs/2406.04197v1|
|420|Aligning Agents like Large Language Models|Adam Jelley, Yuhan Cao, David Bignell, Sam Devlin, Tabish Rashid|2024-06-06|arXiv|https://adamjelley.github.io/aligning-agents-like-llms|https://doi.org/10.48550/arXiv.2406.04208|
|421|AgentGym: Evolving Large Language Model-based Agents across Diverse Environments|Zhiheng Xi, Yiwen Ding, Wenxiang Chen, Boyang Hong, Honglin Guo, Junzhe Wang, Dingwen Yang, Chenyang Liao, Xin Guo, Wei He, Songyang Gao, Lu Chen, Rui Zheng, Yicheng Zou, Tao Gui, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang|2024-06-06|arXiv|https://agentgym.github.io|https://doi.org/10.48550/arXiv.2406.04151|
|422|Text-like Encoding of Collaborative Information in Large Language Models for Recommendation|Yang Zhang, Keqin Bao, Ming Yang, Wenjie Wang, Fuli Feng, Xiangnan He|2024-06-05|ACL|https://github.com/zyang1580/BinLLM|https://aclanthology.org/2024.acl-long.497|
|423|Seq1F1B: Efficient Sequence-Level Pipeline Parallelism for Large Language Model Training|Ao Sun, Weilin Zhao, Xu Han, Cheng Yang, Zhiyuan Liu, Chuan Shi, Maosong Sun|2024-06-05|arXiv|https://github.com/MayDomine/Seq1F1B|https://doi.org/10.48550/arXiv.2406.03488|
|424|Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models|Peijie Dong, Lujun Li, Zhenheng Tang, Xiang Liu, Xinglin Pan, Qiang Wang, Xiaowen Chu|2024-06-05|arXiv|https://github.com/pprp/Pruner-Zero|https://doi.org/10.48550/arXiv.2406.02924|
|425|PrE-Text: Training Language Models on Private Federated Data in the Age of LLMs|Charlie Hou, Akshat Shrivastava, Hongyuan Zhan, Rylan Conway, Trang Le, Adithya Sagar, Giulia Fanti, Daniel Lazar|2024-06-05|arXiv|https://github.com/houcharlie/PrE-Text|http://arxiv.org/abs/2406.02958v1|
|426|PosterLLaVa: Constructing a Unified Multi-modal Layout Generator with LLM|Tao Yang, Yingmin Luo, Zhongang Qi, Yang Wu, Ying Shan, Chang Wen Chen|2024-06-05|arXiv|https://github.com/posterllava/PosterLLaVA|http://arxiv.org/abs/2406.02884v1|
|427|MultifacetEval: Multifaceted Evaluation to Probe LLMs in Mastering Medical Knowledge|Yuxuan Zhou, Xien Liu, Chen Ning, Ji Wu|2024-06-05|arXiv|https://github.com/THUMLP/MultifacetEval|http://arxiv.org/abs/2406.02919v1|
|428|Llumnix: Dynamic Scheduling for Large Language Model Serving|Biao Sun, Ziming Huang, Hanyu Zhao, Wencong Xiao, Xinyi Zhang, Yong Li, Wei Lin|2024-06-05|arXiv|https://github.com/AlibabaPAI/llumnix|https://doi.org/10.48550/arXiv.2406.03243|
|429|Exploring User Retrieval Integration towards Large Language Models for Cross-Domain Sequential Recommendation|Tingjia Shen, Hao Wang, Jiaqing Zhang, Sirui Zhao, Liangyue Li, Zulong Chen, Defu Lian, Enhong Chen|2024-06-05|arXiv|https://github.com/TingJShen/URLLM|https://doi.org/10.48550/arXiv.2406.03085|
|430|CSS: Contrastive Semantic Similarity for Uncertainty Quantification of LLMs|Shuang Ao, Stefan Rueger, Advaith Siddharthan|2024-06-05|arXiv|https://github.com/AoShuang92/css_uq_llms|http://arxiv.org/abs/2406.03158v1|
|431|BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents|Yifei Wang, Dizhan Xue, Shengjie Zhang, Shengsheng Qian|2024-06-05|arXiv|https://github.com/DPamK/BadAgent|http://arxiv.org/abs/2406.03007v1|
|432|HYDRA: Model Factorization Framework for Black-Box LLM Personalization|Yuchen Zhuang, Haotian Sun, Yue Yu, Rushi Qiang, Qifan Wang, Chao Zhang, Bo Dai|2024-06-05|arXiv|https://github.com/night-chen/HYDRA|http://arxiv.org/abs/2406.02888v2|
|433|Alice in Wonderland: Simple Tasks Showing Complete Reasoning Breakdown in State-Of-the-Art Large Language Models|Marianna Nezhurina, Lucia Cipolina-Kun, Mehdi Cherti, Jenia Jitsev|2024-06-04|arXiv|https://github.com/LAION-AI/AIW|https://doi.org/10.48550/arXiv.2406.02061|
|434|Hiding Text in Large Language Models: Introducing Unconditional Token Forcing Confusion|Jakub Hoscilowicz, Pawel Popiolek, Jan Rudkowski, Jedrzej Bieniasz, Artur Janicki|2024-06-04|arXiv|https://github.com/j-hoscilowic/zurek-stegano|https://doi.org/10.48550/arXiv.2406.02481|
|435|Large Language Models as Carriers of Hidden Messages|Jakub Hoscilowicz, Pawel Popiolek, Jan Rudkowski, Jedrzej Bieniasz, Artur Janicki|2024-06-04|arXiv|https://github.com/j-hoscilowic/zurek-stegano|http://arxiv.org/abs/2406.02481v2|
|436|Self-Control of LLM Behaviors by Compressing Suffix Gradient into Prefix Controller|Min Cai, Yuchen Zhang, Shichang Zhang, Fan Yin, Difan Zou, Yisong Yue, Ziniu Hu|2024-06-04|arXiv|https://llm-self-control.github.io/|http://arxiv.org/abs/2406.02721v2|
|437|XRec: Large Language Models for Explainable Recommendation|Qiyao Ma, Xubin Ren, Chao Huang|2024-06-04|arXiv|https://github.com/HKUDS/XRec|https://doi.org/10.48550/arXiv.2406.02377|
|438|SUBLLM: A Novel Efficient Architecture with Token Sequence Subsampling for LLM|Quandong Wang, Yuxuan Yuan, Xiaoyu Yang, Ruike Zhang, Kang Zhao, Wei Liu, Jian Luan, Daniel Povey, Bin Wang|2024-06-03|arXiv|https://github.com/XiaoMi/subllm|http://arxiv.org/abs/2406.06571v2|
|439|VIP: Versatile Image Outpainting Empowered by Multimodal Large Language Model|Jinze Yang, Haoran Wang, Zining Zhu, Chenglong Liu, Meng Wymond Wu, Zeke Xie, Zhong Ji, Jungong Han, Mingming Sun|2024-06-03|arXiv|https://github.com/ucasyjz/VIP|https://doi.org/10.48550/arXiv.2406.01059|
|440|Two Tales of Persona in LLMs: A Survey of Role-Playing and Personalization|Yu-Min Tseng, Yu-Chao Huang, Teng-Yun Hsiao, Wei-Lin Chen, Chao-Wei Huang, Yu Meng, Yun-Nung Chen|2024-06-03|arXiv|https://github.com/MiuLab/PersonaLLM-Survey|http://arxiv.org/abs/2406.01171v2|
|441|Towards Scalable Automated Alignment of LLMs: A Survey|Boxi Cao, Keming Lu, Xinyu Lu, Jiawei Chen, Mengjie Ren, Hao Xiang, Peilin Liu, Yaojie Lu, Ben He, Xianpei Han, Le Sun, Hongyu Lin, Bowen Yu|2024-06-03|arXiv|https://github.com/cascip/awesome-auto-alignment|http://arxiv.org/abs/2406.01252v1|
|442|The Geometry of Categorical and Hierarchical Concepts in Large Language Models|Kiho Park, Yo Joong Choe, Yibo Jiang, Victor Veitch|2024-06-03|arXiv|https://github.com/KihoPark/LLM_Categorical_Hierarchical_Representations|https://doi.org/10.48550/arXiv.2406.01506|
|443|Sparsity-Accelerated Training for Large Language Models|Da Ma, Lu Chen, Pengyu Wang, Hongshen Xu, Hanqi Li, Liangtai Sun, Su Zhu, Shuai Fan, Kai Yu|2024-06-03|arXiv|https://github.com/OpenDFM/SAT|https://doi.org/10.48550/arXiv.2406.01392|
|444|LexMatcher: Dictionary-centric Data Collection for LLM-based Machine Translation|Yongjing Yin, Jiali Zeng, Yafu Li, Fandong Meng, Yue Zhang|2024-06-03|arXiv|https://github.com/ARIES-LM/Lexmatcher-MT|http://arxiv.org/abs/2406.01441v1|
|445|Rotation and Permutation for Advanced Outlier Management and Efficient Quantization of LLMs|Haokun Lin, Haobo Xu, Yichen Wu, Jingzhi Cui, Yingtao Zhang, Linzhan Mou, Linqi Song, Zhenan Sun, Ying Wei|2024-06-03|arXiv|https://github.com/Hsu1023/DuQuant|http://arxiv.org/abs/2406.01721v1|
|446|REvolve: Reward Evolution with Large Language Models for Autonomous Driving|Rishi Hazra, Alkis Sygkounas, Andreas Persson, Amy Loutfi, Pedro Zuidberg Dos Martires|2024-06-03|arXiv|https://rishihazra.github.io/REvolve|https://doi.org/10.48550/arXiv.2406.01309|
|447|LLMs Beyond English: Scaling the Multilingual Capability of LLMs with Cross-Lingual Feedback|Wen Lai, Mohsen Mesgar, Alexander Fraser|2024-06-03|arXiv|https://github.com/boschresearch/ACL24-MLLM|http://arxiv.org/abs/2406.01771v1|
|448|Knowledge Graph in Astronomical Research with Large Language Models: Quantifying Driving Forces in Interdisciplinary Scientific Discovery|Zechang Sun, Yuan-Sen Ting, Yaobo Liang, Nan Duan, Song Huang, Zheng Cai|2024-06-03|arXiv|https://astrokg.github.io/|https://doi.org/10.48550/arXiv.2406.01391|
|449|Editing the Mind of Giants: An In-Depth Exploration of Pitfalls of Knowledge Editing in Large Language Models|Cheng-Hsun Hsueh, Paul Kuo-Ming Huang, Tzu-Han Lin, Che-Wei Liao, Hung-Chieh Fang, Chao-Wei Huang, Yun-Nung Chen|2024-06-03|arXiv|https://github.com/MiuLab/EditLLM-Survey|https://doi.org/10.48550/arXiv.2406.01436|
|450|Demystifying Platform Requirements for Diverse LLM Inference Use Cases|Abhimanyu Bambhaniya, Ritik Raj, Geonhwa Jeong, Souvik Kundu, Sudarshan Srinivasan, Midhilesh Elavazhagan, Madhu Kumar, Tushar Krishna|2024-06-03|arXiv|https://github.com/abhibambhaniya/GenZ-LLM-Analyzer|http://arxiv.org/abs/2406.01698v1|
|451|Envisioning Outlier Exposure by Large Language Models for Out-of-Distribution Detection|Chentao Cao, Zhun Zhong, Zhanke Zhou, Yang Liu, Tongliang Liu, Bo Han|2024-06-02|arXiv|https://github.com/tmlr-group/EOE|https://doi.org/10.48550/arXiv.2406.00806|
|452|Evaluating Mathematical Reasoning of Large Language Models: A Focus on Error Identification and Correction|Xiaoyuan Li, Wenjie Wang, Moxin Li, Junrong Guo, Yang Zhang, Fuli Feng|2024-06-02|arXiv|https://github.com/LittleCirc1e/EIC|https://doi.org/10.48550/arXiv.2406.00755|
|453|A Closer Look at Logical Reasoning with LLMs: The Choice of Tool Matters|Long Hei Matthew Lam, Ramya Keerthy Thatikonda, Ehsan Shareghi|2024-06-01|arXiv|https://github.com/Mattylam/Logic_Symbolic_Solvers_Experiment|http://arxiv.org/abs/2406.00284v1|
|454|Phased Instruction Fine-Tuning for Large Language Models|Wei Pang, Chuan Zhou, Xiao-Hua Zhou, Xiaojie Wang|2024-06-01|arXiv|https://github.com/xubuvd/PhasedSFT|https://doi.org/10.48550/arXiv.2406.04371|
|455|Investigating the Efficacy of Large Language Models for Code Clone Detection|Mohamad Khajezade, Jie JW Wu, Fatemeh Hendijani Fard, Gema Rodríguez-Pérez, Mohamed Sami Shehata|2024-06|2024 IEEE/ACM 32nd International Conference on Program Comprehension (ICPC)|https://github.com/mkhfring/llm-for-ccd|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556419|
|456|Knowledge-Aware Code Generation with Large Language Models|Tao Huang, Zhihong Sun, Zhi Jin, Ge Li, Chen Lyu|2024-06|2024 IEEE/ACM 32nd International Conference on Program Comprehension (ICPC)|https://github.com/CodeGeneration3/KareCoder.CCS|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556459|
|457|Interpretable Online Log Analysis Using Large Language Models with Prompt Strategies|Yilun Liu, Shimin Tao, Weibin Meng, Jingyu Wang, Wenbing Ma, Yanqing Zhao, Yuhang Chen, Hao Yang, Yanfei Jiang, Xun Chen|2024-06|2024 IEEE/ACM 32nd International Conference on Program Comprehension (ICPC)|https://github.com/lunyiliu/LogPrompt.CCS|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556497|
|458|SaySelf: Teaching LLMs to Express Confidence with Self-Reflective Rationales|Tianyang Xu, Shujin Wu, Shizhe Diao, Xiaoze Liu, Xingyao Wang, Yangyi Chen, Jing Gao|2024-05-31|arXiv|https://github.com/xu1868/SaySelf|http://arxiv.org/abs/2405.20974v2|
|459|Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis|Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, Xing Sun|2024-05-31|arXiv|https://video-mme.github.io|http://arxiv.org/abs/2405.21075v2|
|460|Improved Techniques for Optimization-Based Jailbreaking on Large Language Models|Xiaojun Jia, Tianyu Pang, Chao Du, Yihao Huang, Jindong Gu, Yang Liu, Xiaochun Cao, Min Lin|2024-05-31|arXiv|https://github.com/jiaxiaojunQAQ/I-GCG|https://doi.org/10.48550/arXiv.2405.21018|
|461|Ovis: Structural Embedding Alignment for Multimodal Large Language Model|Shiyin Lu, Yang Li, Qing-Guo Chen, Zhao Xu, Weihua Luo, Kaifu Zhang, Han-Jia Ye|2024-05-31|arXiv|https://github.com/AIDC-AI/Ovis|https://doi.org/10.48550/arXiv.2405.20797|
|462|One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for Retrieval-Augmented Large Language Models|Yutao Zhu, Zhaoheng Huang, Zhicheng Dou, Ji-Rong Wen|2024-05-30|arXiv|https://github.com/DaoD/SPRING/|https://doi.org/10.48550/arXiv.2405.19670|
|463|Xwin-LM: Strong and Scalable Alignment Practice for LLMs|Bolin Ni, JingCheng Hu, Yixuan Wei, Houwen Peng, Zheng Zhang, Gaofeng Meng, Han Hu|2024-05-30|arXiv|https://github.com/Xwin-LM/Xwin-LM|http://arxiv.org/abs/2405.20335v1|
|464|Two Optimizers Are Better Than One: LLM Catalyst Empowers Gradient-Based Optimization for Prompt Tuning|Zixian Guo, Ming Liu, Zhilong Ji, Jinfeng Bai, Yiwen Guo, Wangmeng Zuo|2024-05-30|arXiv|https://github.com/guozix/LLM-catalyst|http://arxiv.org/abs/2405.19732v3|
|465|PATIENT-Ψ: Using Large Language Models to Simulate Patients for Training Mental Health Professionals|Ruiyi Wang, Stephanie Milani, Jamie C. Chiu, Jiayin Zhi, Shaun M. Eack, Travis Labrum, Samuel M. Murphy, Nev Jones, Kate Hardy, Hong Shen, Fei Fang, Zhiyu Zoey Chen|2024-05-30|arXiv|https://github.com/ruiyiw/patient-psi|https://doi.org/10.48550/arXiv.2405.19660|
|466|Designing an Evaluation Framework for Large Language Models in Astronomy Research|John F. Wu, Alina Hyk, Kiera McCormick, Christine Ye, Simone Astarita, Elina Baral, Jo Ciuca, Jesse Cranney, Anjalie Field, Kartheik Iyer, Philipp Koehn, Jenn Kotler, Sandor Kruk, Michelle Ntampaka, Charles O&apos;Neill, Joshua E. G. Peek, Sanjib Sharma, Mikaeel Yunus|2024-05-30|arXiv|https://github.com/jsalt2024-evaluating-llms-for-astronomy/astro-arxiv-bot|https://doi.org/10.48550/arXiv.2405.20389|
|467|NoiseBoost: Alleviating Hallucination with Noise Perturbation for Multimodal Large Language Models|Kai Wu, Boyuan Jiang, Zhengkai Jiang, Qingdong He, Donghao Luo, Shengzhi Wang, Qingwen Liu, Chengjie Wang|2024-05-30|arXiv|https://kaiwu5.github.io/noiseboost|https://doi.org/10.48550/arXiv.2405.20081|
|468|Evaluating Large Language Model Biases in Persona-Steered Generation|Andy Liu, Mona Diab, Daniel Fried|2024-05-30|arXiv|https://github.com/andyjliu/persona-steered-generation-bias|https://doi.org/10.48550/arXiv.2405.20253|
|469|Detecting Hallucinations in Large Language Model Generation: A Token Probability Approach|Ernesto Quevedo, Jorge Yero, Rachel Koerner, Pablo Rivas, Tomás Cerný|2024-05-30|arXiv|https://github.com/Baylor-AI/HalluDetect|https://doi.org/10.48550/arXiv.2405.19648|
|470|Large Language Models as Planning Domain Generators (Student Abstract)|James T. Oswald, Kavitha Srinivas, Harsha Kokel, Junkyu Lee, Michael Katz, Shirin Sohrabi|2024-05-30|AAAI|https://github.com/IBM/NL2PDDL|https://doi.org/10.1609/aaai.v38i21.30491|
|471|Compressing Large Language Models using Low Rank and Low Precision Decomposition|Rajarshi Saha, Naomi Sagan, Varun Srivastava, Andrea J. Goldsmith, Mert Pilanci|2024-05-29|arXiv|https://github.com/pilancilab/caldera|https://doi.org/10.48550/arXiv.2405.18886|
|472|Enhancing Zero-Shot Facial Expression Recognition by LLM Knowledge Transfer|Zengqun Zhao, Yu Cao, Shaogang Gong, Ioannis Patras|2024-05-29|arXiv|https://github.com/zengqunzhao/Exp-CLIP|http://arxiv.org/abs/2405.19100v2|
|473|LLMs Meet Multimodal Generation and Editing: A Survey|Yingqing He, Zhaoyang Liu, Jingye Chen, Zeyue Tian, Hongyu Liu, Xiaowei Chi, Runtao Liu, Ruibin Yuan, Yazhou Xing, Wenhai Wang, Jifeng Dai, Yong Zhang, Wei Xue, Qifeng Liu, Yike Guo, Qifeng Chen|2024-05-29|arXiv|https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation|http://arxiv.org/abs/2405.19334v2|
|474|MAP-Neo: Highly Capable and Transparent Bilingual Large Language Model Series|Ge Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Y. Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, Wenhu Chen|2024-05-29|arXiv|https://map-neo.github.io/|https://doi.org/10.48550/arXiv.2405.19327|
|475|VideoTree: Adaptive Tree-based Video Representation for LLM Reasoning on Long Videos|Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, Mohit Bansal|2024-05-29|arXiv|https://videotree2024.github.io/|http://arxiv.org/abs/2405.19209v1|
|476|ORLM: Training Large Language Models for Optimization Modeling|Zhengyang Tang, Chenyu Huang, Xin Zheng, Shixi Hu, Zizhuo Wang, Dongdong Ge, Benyou Wang|2024-05-28|arXiv|https://github.com/Cardinal-Operations/ORLM|https://doi.org/10.48550/arXiv.2405.17743|
|477|TimeChara: Evaluating Point-in-Time Character Hallucination of Role-Playing Large Language Models|Jaewoo Ahn, Taehyun Lee, Junyoung Lim, Jin-Hwa Kim, Sangdoo Yun, Hwaran Lee, Gunhee Kim|2024-05-28|arXiv|https://ahnjaewoo.github.io/timechara|https://doi.org/10.48550/arXiv.2405.18027|
|478|Pipette: Automatic Fine-Grained Large Language Model Training Configurator for Real-World Clusters|Jinkyu Yim, Jaeyong Song, Yerim Choi, Jaebeen Lee, Jaewon Jung, Hongsun Jang, Jinho Lee|2024-05-28|2024 Design, Automation & Test in Europe Conference & Exhibition (DATE)|https://github.com/yimjinkyu1/date2024_pipette|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10546826|
|479|C$^3$Bench: A Comprehensive Classical Chinese Understanding Benchmark for Large Language Models|Jiahuan Cao, Yongxin Shi, Dezhi Peng, Yang Liu, Lianwen Jin|2024-05-28|arXiv|https://github.com/SCUT-DLVCLab/C3bench|http://arxiv.org/abs/2405.17732v2|
|480|Lazy Safety Alignment for Large Language Models against Harmful Fine-tuning|Tiansheng Huang, Sihao Hu, Fatih Ilhan, Selim Furkan Tekin, Ling Liu|2024-05-28|arXiv|https://github.com/git-disl/Lisa|https://doi.org/10.48550/arXiv.2405.18641|
|481|Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference|Hao Mark Chen, Wayne Luk, Ka Fai Cedric Yiu, Rui Li, Konstantin Mishchenko, Stylianos I. Venieris, Hongxiang Fan|2024-05-28|arXiv|https://github.com/hmarkc/parallel-prompt-decoding|http://arxiv.org/abs/2405.18628v2|
|482|Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing|Wei Zhao, Zhe Li, Yige Li, Ye Zhang, Jun Sun|2024-05-28|arXiv|https://github.com/ledllm/ledllm|https://doi.org/10.48550/arXiv.2405.18166|
|483|Match, Compare, or Select? An Investigation of Large Language Models for Entity Matching|Tianshu Wang, Xiaoyang Chen, Hongyu Lin, Xuanang Chen, Xianpei Han, Hao Wang, Zhenyu Zeng, Le Sun|2024-05-27|arXiv|https://github.com/tshu-w/LLM4EM|https://doi.org/10.48550/arXiv.2405.16884|
|484|Reason3D: Searching and Reasoning 3D Segmentation via Large Language Model|Kuan-Chih Huang, Xiangtai Li, Lu Qi, Shuicheng Yan, Ming-Hsuan Yang|2024-05-27|arXiv|https://KuanchihHuang.github.io/project/reason3d|https://doi.org/10.48550/arXiv.2405.17427|
|485|ReMoDetect: Reward Models Recognize Aligned LLM's Generations|Hyunseok Lee, Jihoon Tack, Jinwoo Shin|2024-05-27|arXiv|https://github.com/hyunseoklee-ai/reward_llm_detect|http://arxiv.org/abs/2405.17382v1|
|486|MotionLLM: Multimodal Motion-Language Learning with Large Language Models|Qi Wu, Yubo Zhao, Yifan Wang, Yu-Wing Tai, Chi-Keung Tang|2024-05-27|arXiv|https://knoxzhao.github.io/MotionLLM|https://doi.org/10.48550/arXiv.2405.17013|
|487|LLM-Optic: Unveiling the Capabilities of Large Language Models for Universal Visual Grounding|Haoyu Zhao, Wenhang Ge, Ying-Cong Chen|2024-05-27|arXiv|https://haoyu-zhao.github.io/LLM-Optic.github.io/|https://doi.org/10.48550/arXiv.2405.17104|
|488|Entity Alignment with Noisy Annotations from Large Language Models|Shengyuan Chen, Qinggang Zhang, Junnan Dong, Wen Hua, Qing Li, Xiao Huang|2024-05-27|arXiv|https://github.com/chensyCN/llm4ea_official|https://doi.org/10.48550/arXiv.2405.16806|
|489|Empowering Large Language Models to Set up a Knowledge Retrieval Indexer via Self-Learning|Xun Liang, Simin Niu, Zhiyu Li, Sensen Zhang, Shichao Song, Hanyu Wang, Jiawei Yang, Feiyu Xiong, Bo Tang, Chenyang Xi|2024-05-27|arXiv|https://github.com/IAAR-Shanghai/PGRAG|https://doi.org/10.48550/arXiv.2405.16933|
|490|CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs|Haoyu Wang, Bei Liu, Hang Shao, Bo Xiao, Ke Zeng, Guanglu Wan, Yanmin Qian|2024-05-27|arXiv|https://github.com/fayuge/CLAQ|http://arxiv.org/abs/2405.17233v2|
|491|Implicit Multimodal Alignment: On the Generalization of Frozen LLMs to Multimodal Inputs|Mustafa Shukor, Matthieu Cord|2024-05-26|arXiv|https://ima-lmms.github.io/|http://arxiv.org/abs/2405.16700v1|
|492|Medical MLLM is Vulnerable: Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models|Xijie Huang, Xinyuan Wang, Hantao Zhang, Yinghao Zhu, Jiawen Xi, Jingkun An, Hao Wang, Hao Liang, Chengwei Pan|2024-05-26|arXiv|https://github.com/dirtycomputer/O2M_attack|http://arxiv.org/abs/2405.20775v2|
|493|Automatically Generating Numerous Context-Driven SFT Data for LLMs across Diverse Granularity|Shanghaoran Quan|2024-05-26|arXiv|https://github.com/quanshr/AugCon|http://arxiv.org/abs/2405.16579v1|
|494|Cross-Modality Jailbreak and Mismatched Attacks on Medical Multimodal Large Language Models|Xijie Huang, Xinyuan Wang, Hantao Zhang, Jiawen Xi, Jingkun An, Hao Wang, Chengwei Pan|2024-05-26|arXiv|https://github.com/dirtycomputer/O2M_attack|https://doi.org/10.48550/arXiv.2405.20775|
|495|Cocktail: A Comprehensive Information Retrieval Benchmark with LLM-Generated Documents Integration|Sunhao Dai, Weihao Liu, Yuqi Zhou, Liang Pang, Rongju Ruan, Gang Wang, Zhenhua Dong, Jun Xu, Ji-Rong Wen|2024-05-26|arXiv|https://github.com/KID-22/Cocktail|http://arxiv.org/abs/2405.16546v1|
|496|AutoManual: Generating Instruction Manuals by LLM Agents via Interactive Environmental Learning|Minghao Chen, Yihang Li, Yanting Yang, Shiyu Yu, Binbin Lin, Xiaofei He|2024-05-25|arXiv|https://github.com/minghchen/automanual|http://arxiv.org/abs/2405.16247v1|
|497|SPP: Sparsity-Preserved Parameter-Efficient Fine-Tuning for Large Language Models|Xudong Lu, Aojun Zhou, Yuhui Xu, Renrui Zhang, Peng Gao, Hongsheng Li|2024-05-25|arXiv|https://github.com/Lucky-Lance/SPP|https://doi.org/10.48550/arXiv.2405.16057|
|498|Evaluating the Adversarial Robustness of Retrieval-Based In-Context Learning for Large Language Models|Simon Chi Lok Yu, Jie He, Pasquale Minervini, Jeff Z. Pan|2024-05-24|arXiv|https://github.com/simonucl/adv-retreival-icl|https://doi.org/10.48550/arXiv.2405.15984|
|499|Meteor: Mamba-based Traversal of Rationale for Large Language and Vision Models|Byung-Kwan Lee, Chae Won Kim, Beomchan Park, Yong Man Ro|2024-05-24|arXiv|https://github.com/ByungKwanLee/Meteor|https://doi.org/10.48550/arXiv.2405.15574|
|500|LM4LV: A Frozen Large Language Model for Low-level Vision Tasks|Boyang Zheng, Jinjin Gu, Shijun Li, Chao Dong|2024-05-24|arXiv|https://github.com/bytetriper/LM4LV|https://doi.org/10.48550/arXiv.2405.15734|
|501|Decoding at the Speed of Thought: Harnessing Parallel Decoding of Lexical Units for LLMs|Chenxi Sun, Hongzhi Zhang, Zijia Lin, Jingyuan Zhang, Fuzheng Zhang, Zhongyuan Wang, Bin Chen, Chengru Song, Di Zhang, Kun Gai, Deyi Xiong|2024-05-24|arXiv|https://github.com/tjunlp-lab/Lexical-Unit-Decoding-LUD-|http://arxiv.org/abs/2405.15208v1|
|502|A Solution-based LLM API-using Methodology for Academic Information Seeking|Yuanchun Wang, Jifan Yu, Zijun Yao, Jing Zhang, Yuyang Xie, Shangqing Tu, Yiyang Fu, Youhe Feng, Jinkai Zhang, Jingyao Zhang, Bowen Huang, Yuanyao Li, Huihui Yuan, Lei Hou, Juanzi Li, Jie Tang|2024-05-24|arXiv|https://github.com/RUCKBReasoning/SoAy|http://arxiv.org/abs/2405.15165v1|
|503|ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based Evaluation|Jingnan Zheng, Han Wang, An Zhang, Tai D. Nguyen, Jun Sun, Tat-Seng Chua|2024-05-23|arXiv|https://github.com/SophieZheng998/ALI-Agent|http://arxiv.org/abs/2405.14125v2|
|504|AlignGPT: Multi-modal Large Language Models with Adaptive Alignment Capability|Fei Zhao, Taotian Pang, Chunhui Li, Zhen Wu, Junjie Guo, Shangyu Xing, Xinyu Dai|2024-05-23|arXiv|https://aligngpt-vl.github.io/|https://doi.org/10.48550/arXiv.2405.14129|
|505|Dissociation of Faithful and Unfaithful Reasoning in LLMs|Evelyn Yee, Alice Li, Chenyu Tang, Yeon Ho Jung, Ramamohan Paturi, Leon Bergen|2024-05-23|arXiv|https://github.com/CoTErrorRecovery/CoTErrorRecovery|http://arxiv.org/abs/2405.15092v1|
|506|DuanzAI: Slang-Enhanced LLM with Prompt for Humor Understanding|Yesian Rohn|2024-05-23|arXiv|https://github.com/YesianRohn/DuanzAI|http://arxiv.org/abs/2405.15818v1|
|507|FinRobot: An Open-Source AI Agent Platform for Financial Applications using Large Language Models|Hongyang Yang, Boyu Zhang, Neng Wang, Cheng Guo, Xiaoli Zhang, Likun Lin, Junlin Wang, Tianyu Zhou, Mao Guan, Runjia Zhang, Christina Dan Wang|2024-05-23|arXiv|https://github.com/AI4Finance-Foundation/FinRobot|https://doi.org/10.48550/arXiv.2405.14767|
|508|HippoRAG: Neurobiologically Inspired Long-Term Memory for Large Language Models|Bernal Jiménez Gutiérrez, Yiheng Shu, Yu Gu, Michihiro Yasunaga, Yu Su|2024-05-23|arXiv|https://github.com/OSU-NLP-Group/HippoRAG|https://doi.org/10.48550/arXiv.2405.14831|
|509|Mitigating Quantization Errors Due to Activation Spikes in GLU-Based LLMs|Jaewoo Yang, Hayun Kim, Younghoon Kim|2024-05-23|arXiv|https://github.com/onnoo/activation-spikes|http://arxiv.org/abs/2405.14428v1|
|510|RefChecker: Reference-based Fine-grained Hallucination Checker and Benchmark for Large Language Models|Xiangkun Hu, Dongyu Ru, Lin Qiu, Qipeng Guo, Tianhang Zhang, Yang Xu, Yun Luo, Pengfei Liu, Yue Zhang, Zheng Zhang|2024-05-23|arXiv|https://github.com/amazon-science/RefChecker|https://doi.org/10.48550/arXiv.2405.14486|
|511|Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration|Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Zhen Wang, Xuelong Li|2024-05-23|arXiv|https://read-llm.github.io/|http://arxiv.org/abs/2405.14314v2|
|512|WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of Large Language Models|Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen|2024-05-23|arXiv|https://github.com/zjunlp/EasyEdit|https://doi.org/10.48550/arXiv.2405.14768|
|513|AutoCoder: Enhancing Code Large Language Model with AIEV-Instruct|Bin Lei, Yuchen Li, Qiuwu Chen|2024-05-23|arXiv|https://github.com/bin123apple/AutoCoder|https://doi.org/10.48550/arXiv.2405.14906|
|514|Prompt-Time Ontology-Driven Symbolic Knowledge Capture with Large Language Models|Tolga Çöplü, Arto Bendiken, Andrii Skomorokhov, Eduard Bateiko, Stephen Cobb, Joshua J. Bouw|2024-05-22|arXiv|https://github.com/HaltiaAI/paper-PTODSKC|https://doi.org/10.48550/arXiv.2405.14012|
|515|VTG-LLM: Integrating Timestamp Knowledge into Video LLMs for Enhanced Video Temporal Grounding|Yongxin Guo, Jingyu Liu, Mingda Li, Xiaoying Tang, Xi Chen, Bo Zhao|2024-05-22|arXiv|https://github.com/gyxxyg/VTG-LLM|http://arxiv.org/abs/2405.13382v1|
|516|LOGIN: A Large Language Model Consulted Graph Neural Network Training Framework|Yiran Qiao, Xiang Ao, Yang Liu, Jiarong Xu, Xiaoqian Sun, Qing He|2024-05-22|arXiv|https://github.com/QiaoYRan/LOGIN|https://doi.org/10.48550/arXiv.2405.13902|
|517|PitVQA: Image-grounded Text Embedding LLM for Visual Question Answering in Pituitary Surgery|Runlong He, Mengya Xu, Adrito Das, Danyal Z. Khan, Sophia Bano, Hani J. Marcus, Danail Stoyanov, Matthew J. Clarkson, Mobarakol Islam|2024-05-22|arXiv|https://github.com/mobarakol/PitVQA|http://arxiv.org/abs/2405.13949v1|
|518|Adapting Multi-modal Large Language Model to Concept Drift in the Long-tailed Open World|Xiaoyu Yang, Jie Lu, En Yu|2024-05-22|arXiv|https://github.com/Anonymous0Knight/ConceptDriftMLLMs|https://doi.org/10.48550/arXiv.2405.13459|
|519|An Empirical Study and Analysis of Text-to-Image Generation Using Large Language Model-Powered Textual Representation|Zhiyu Tan, Mengping Yang, Luozheng Qin, Hao Yang, Ye Qian, Qiang Zhou, Cheng Zhang, Hao Li|2024-05-21|arXiv|https://github.com/llm-conditioned-diffusion/llm-conditioned-diffusion|https://doi.org/10.48550/arXiv.2405.12914|
|520|Large Language Models Meet NL2Code: A Survey|Libo Qin, Qiguang Chen, Xiachong Feng, Yang Wu, Yongheng Zhang, Yinghui Li, Min Li, Wanxiang Che, Philip S. Yu|2024-05-21|ACL|https://nl2code.github.io|https://doi.org/10.18653/v1/2023.acl-long.411|
|521|SirLLM: Streaming Infinite Retentive LLM|Yao Yao, Zuchao Li, Hai Zhao|2024-05-21|OpenReview|https://github.com/Zoeyyao27/SirLLM|http://arxiv.org/abs/2405.12528v1|
|522|CLAMBER: A Benchmark of Identifying and Clarifying Ambiguous Information Needs in Large Language Models|Tong Zhang, Peixin Qin, Yang Deng, Chen Huang, Wenqiang Lei, Junhong Liu, Dingnan Jin, Hongru Liang, Tat-Seng Chua|2024-05-20|arXiv|https://github.com/zt991211/CLAMBER|https://doi.org/10.48550/arXiv.2405.12063|
|523|DOP: Diagnostic-Oriented Prompting for Large Language Models in Mathematical Correction|Hao Chen, Biaojie Zeng, Xin Lin, Liang He, Aimin Zhou|2024-05-20|arXiv|https://github.com/ChenhaoEcnuCS/Reason-Correct|https://doi.org/10.48550/arXiv.2405.12100|
|524|MathBench: Evaluating the Theory and Application Proficiency of LLMs with a Hierarchical Mathematics Benchmark|Hongwei Liu, Zilong Zheng, Yuxuan Qiao, Haodong Duan, Zhiwei Fei, Fengzhe Zhou, Wenwei Zhang, Songyang Zhang, Dahua Lin, Kai Chen|2024-05-20|OpenReview|https://github.com/open-compass/MathBench|http://arxiv.org/abs/2405.12209v1|
|525|MBIAS: Mitigating Bias in Large Language Models While Retaining Context|Shaina Raza, Ananya Raval, Veronica Chatrath|2024-05-18|arXiv|https://github.com/shainarazavi/MBIAS/tree/main|https://doi.org/10.48550/arXiv.2405.11290|
|526|Uni-MoE: Scaling Unified Multimodal LLMs with Mixture of Experts|Yunxin Li, Shenyuan Jiang, Baotian Hu, Longyue Wang, Wanqi Zhong, Wenhan Luo, Lin Ma, Min Zhang|2024-05-18|arXiv|https://uni-moe.github.io/|http://arxiv.org/abs/2405.11273v1|
|527|Benchmarking Large Language Models on CFLUE - A Chinese Financial Language Understanding Evaluation Dataset|Jie Zhu, Junhui Li, Yalong Wen, Lifan Guo|2024-05-17|arXiv|https://github.com/aliyun/cflue|https://doi.org/10.48550/arXiv.2405.10542|
|528|Layer-Condensed KV Cache for Efficient Inference of Large Language Models|Haoyi Wu, Kewei Tu|2024-05-17|arXiv|https://github.com/whyNLP/LCKV|https://doi.org/10.48550/arXiv.2405.10637|
|529|RDRec: Rationale Distillation for LLM-based Recommendation|Xinfeng Wang, Jin Cui, Yoshimi Suzuki, Fumiyo Fukumoto|2024-05-17|arXiv|https://github.com/WangXFng/RDRec|http://arxiv.org/abs/2405.10587v2|
|530|Surgical Feature-Space Decomposition of LLMs: Why, When and How?|Arnav Chavan, Nahush Lele, Deepak Gupta|2024-05-17|OpenReview|https://github.com/nyunAI/SFSD-LLM|http://arxiv.org/abs/2405.13039v1|
|531|MarkLLM: An Open-Source Toolkit for LLM Watermarking|Leyi Pan, Aiwei Liu, Zhiwei He, Zitian Gao, Xuandong Zhao, Yijian Lu, Binglin Zhou, Shuliang Liu, Hanlin Zhang, Xuming Hu, Lijie Wen, Irwin King|2024-05-16|arXiv|https://github.com/THU-BPM/MarkLLM|http://arxiv.org/abs/2405.10051v3|
|532|When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models|Xianzheng Ma, Yash Bhalgat, Brandon Smart, Shuai Chen, Xinghui Li, Jian Ding, Jindong Gu, Dave Zhenyu Chen, Songyou Peng, Jia-Wang Bian, Philip H. S. Torr, Marc Pollefeys, Matthias Nießner, Ian D. Reid, Angel X. Chang, Iro Laina, Victor Adrian Prisacariu|2024-05-16|arXiv|https://github.com/ActiveVisionLab/Awesome-LLM-3D|https://doi.org/10.48550/arXiv.2405.10255|
|533|DuetSim: Building User Simulator with Dual Large Language Models for Task-Oriented Dialogues|Xiang Luo, Zhiwen Tang, Jin Wang, Xuejie Zhang|2024-05-16|LREC/COLING|https://github.com/suntea233/DuetSim|https://aclanthology.org/2024.lrec-main.481|
|534|Libra: Building Decoupled Vision System on Large Language Models|Yifan Xu, Xiaoshan Yang, Yaguang Song, Changsheng Xu|2024-05-16|arXiv|https://github.com/YifanXu74/Libra|https://doi.org/10.48550/arXiv.2405.10140|
|535|LFED: A Literary Fiction Evaluation Dataset for Large Language Models|Linhao Yu, Qun Liu, Deyi Xiong|2024-05-16|LREC/COLING|https://github.com/tjunlp-lab/LFED|https://aclanthology.org/2024.lrec-main.915|
|536|Towards Next-Generation Steganalysis: LLMs Unleash the Power of Detecting Steganography|Minhao Bai. Jinshuai Yang, Kaiyi Pang, Huili Wang, Yongfeng Huang|2024-05-15|arXiv|https://github.com/ba0z1/Linguistic-Steganalysis-with-LLMs|http://arxiv.org/abs/2405.09090v1|
|537|Evaluating LLMs at Evaluating Temporal Generalization|Chenghao Zhu, Nuo Chen, Yufei Gao, Benyou Wang|2024-05-14|arXiv|https://github.com/FreedomIntelligence/FreshBench|http://arxiv.org/abs/2405.08460v1|
|538|Incorporating Clinical Guidelines through Adapting Multi-modal Large Language Model for Prostate Cancer PI-RADS Scoring|Tiantian Zhang, Manxi Lin, Hongda Guo, Xiaofan Zhang, Ka Fung Peter Chiu, Aasa Feragen, Qi Dou|2024-05-14|arXiv|https://github.com/med-air/PICG2scoring|https://doi.org/10.48550/arXiv.2405.08786|
|539|Is Your LLM Outdated? Evaluating LLMs at Temporal Generalization|Chenghao Zhu, Nuo Chen, Yufei Gao, Yunyi Zhang, Prayag Tiwari, Benyou Wang|2024-05-14|arXiv|https://github.com/FreedomIntelligence/FreshBench|http://arxiv.org/abs/2405.08460v2|
|540|Towards Personalized Evaluation of Large Language Models with An Anonymous Crowd-Sourcing Platform|Mingyue Cheng, Hao Zhang, Jiqian Yang, Qi Liu, Li Li, Xin Huang, Liwei Song, Zhi Li, Zhenya Huang, Enhong Chen|2024-05-13|WWW '24: Companion Proceedings of the ACM on Web Conference 2024|https://github.com/Mingyue-Cheng/Bingjian|https://dl.acm.org/doi/10.1145/3589335.3651243|
|541|Representation Learning with Large Language Models for Recommendation|Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang|2024-05-13|WWW '24: Proceedings of the ACM on Web Conference 2024|https://github.com/HKUDS/RLMRec|https://dl.acm.org/doi/10.1145/3589334.3645458|
|542|RecAI: Leveraging Large Language Models for Next-Generation Recommender Systems|Jianxun Lian, Yuxuan Lei, Xu Huang, Jing Yao, Wei Xu, Xing Xie|2024-05-13|WWW '24: Companion Proceedings of the ACM on Web Conference 2024|https://github.com/microsoft/RecAI|https://dl.acm.org/doi/10.1145/3589335.3651242|
|543|Item-side Fairness of Large Language Model-based Recommendation System|Meng Jiang, Keqin Bao, Jizhi Zhang, Wenjie Wang, Zhengyi Yang, Fuli Feng, Xiangnan He|2024-05-13|WWW '24: Proceedings of the ACM on Web Conference 2024|https://github.com/JiangM-C/IFairLRS|https://dl.acm.org/doi/10.1145/3589334.3648158|
|544|ReLLa: Retrieval-enhanced Large Language Models for Lifelong Sequential Behavior Comprehension in Recommendation|Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, Weinan Zhang|2024-05-13|WWW '24: Proceedings of the ACM on Web Conference 2024|https://github.com/LaVieEnRose365/ReLLa|https://dl.acm.org/doi/10.1145/3589334.3645467|
|545|News Recommendation with Category Description by a Large Language Model|Yuki Yada, Hayato Yamana|2024-05-13|arXiv|https://github.com/yamanalab/gpt-augmented-news-recommendation|https://doi.org/10.48550/arXiv.2405.13007|
|546|FashionReGen: LLM-Empowered Fashion Report Generation|Yujuan Ding, Yunshan Ma, Wenqi Fan, Yige Yao, Tat-Seng Chua, Qing Li|2024-05-13|WWW '24: Companion Proceedings of the ACM on Web Conference 2024|https://github.com/CompFashion/FashionReGen|https://dl.acm.org/doi/10.1145/3589335.3651232|
|547|Harnessing Multi-Role Capabilities of Large Language Models for Open-Domain Question Answering|Hongda Sun, Yuxuan Liu, Chengwei Wu, Haiyu Yan, Cheng Tai, Xin Gao, Shuo Shang, Rui Yan|2024-05-13|WWW '24: Proceedings of the ACM on Web Conference 2024|https://github.com/EthanLeo-LYX/LLMQA|https://dl.acm.org/doi/10.1145/3589334.3645670|
|548|GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks|Mengmei Zhang, Mingwei Sun, Peng Wang, Shen Fan, Yanhu Mo, Xiaoxiao Xu, Hong Liu, Cheng Yang, Chuan Shi|2024-05-13|WWW '24: Proceedings of the ACM on Web Conference 2024|https://github.com/alibaba/GraphTranslator|https://dl.acm.org/doi/10.1145/3589334.3645682|
|549|EMS-SD: Efficient Multi-sample Speculative Decoding for Accelerating Large Language Models|Yunsheng Ni, Chuanjian Liu, Yehui Tang, Kai Han, Yunhe Wang|2024-05-13|arXiv|https://github.com/niyunsheng/EMS-SD|https://doi.org/10.48550/arXiv.2405.07542|
|550|Collaborative Large Language Model for Recommender Systems|Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, Jundong Li|2024-05-13|WWW '24: Proceedings of the ACM on Web Conference 2024|https://github.com/yaochenzhu/llm4rec|https://dl.acm.org/doi/10.1145/3589334.3645347|
|551|A Systematic Investigation of Distilling Large Language Models into Cross-Encoders for Passage Re-ranking|Ferdinand Schlatt, Maik Fröbe, Harrisen Scells, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Benno Stein, Martin Potthast, Matthias Hagen|2024-05-13|arXiv|https://github.com/webis-de/msmarco-llm-distillation|https://doi.org/10.48550/arXiv.2405.07920|
|552|AIOS Compiler: LLM as Interpreter for Natural Language Programming and Flow Programming of AI Agents|Shuyuan Xu, Zelong Li, Kai Mei, Yongfeng Zhang|2024-05-11|arXiv|https://github.com/agiresearch/CoRE|http://arxiv.org/abs/2405.06907v2|
|553|ChaCha: Leveraging Large Language Models to Prompt Children to Share Their Emotions about Personal Events|Woosuk Seo, Chanmo Yang, Young-Ho Kim|2024-05-11|CHI '24: Proceedings of the CHI Conference on Human Factors in Computing Systems|https://naver-ai.github.io/chacha/|https://dl.acm.org/doi/10.1145/3613904.3642152|
|554|LaMI: Large Language Models for Multi-Modal Human-Robot Interaction|Chao Wang, Stephan Hasler, Daniel Tanneberg, Felix Ocker, Frank Joublin, Antonello Ceravola, Joerg Deigmoeller, Michael Gienger|2024-05-11|CHI EA '24: Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems|https://hri-eu.github.io/Lami/|https://dl.acm.org/doi/10.1145/3613905.3651029|
|555|Natural Language Dataset Generation Framework for Visualizations Powered by Large Language Models|Hyung-Kwon Ko, Hyeon Jeon, Gwanmo Park, Dae Hyun Kim, Nam Wook Kim, Juho Kim, Jinwook Seo|2024-05-11|CHI '24: Proceedings of the CHI Conference on Human Factors in Computing Systems|https://github.com/hyungkwonko/chart-llm|https://dl.acm.org/doi/10.1145/3613904.3642943|
|556|Pruning as a Domain-specific LLM Extractor|Nan Zhang, Yanchi Liu, Xujiang Zhao, Wei Cheng, Runxue Bao, Rui Zhang, Prasenjit Mitra, Haifeng Chen|2024-05-10|Findings of the Association for Computational Linguistics: NAACL 2024 - Findings|https://github.com/psunlpgroup/D-Pruner|http://arxiv.org/abs/2405.06275v1|
|557|PLeak: Prompt Leaking Attacks against Large Language Model Applications|Bo Hui, Haolin Yuan, Neil Zhenqiang Gong, Philippe Burlina, Yinzhi Cao|2024-05-10|arXiv|https://github.com/BHui97/PLeak|https://doi.org/10.48550/arXiv.2405.06823|
|558|Linearizing Large Language Models|Jean Mercat, Igor Vasiljevic, Sedrick Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar|2024-05-10|arXiv|https://github.com/TRI-ML/linear_open_lm|https://doi.org/10.48550/arXiv.2405.06640|
|559|LLM Discussion: Enhancing the Creativity of Large Language Models via Discussion Framework and Role-Play|Li-Chun Lu, Shou-Jen Chen, Tsung-Min Pai, Chan-Hung Yu, Hung-yi Lee, Shao-Hua Sun|2024-05-10|arXiv|https://github.com/lawraa/LLM-Discussion|https://doi.org/10.48550/arXiv.2405.06373|
|560|Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference|Zhihang Lin, Mingbao Lin, Luxi Lin, Rongrong Ji|2024-05-09|arXiv|https://github.com/lzhxmu/VTW|https://doi.org/10.48550/arXiv.2405.05803|
|561|CuMo: Scaling Multimodal LLM with Co-Upcycled Mixture-of-Experts|Jiachen Li, Xinyao Wang, Sijie Zhu, Chia-Wen Kuo, Lu Xu, Fan Chen, Jitesh Jain, Humphrey Shi, Longyin Wen|2024-05-09|arXiv|https://github.com/SHI-Labs/CuMo|http://arxiv.org/abs/2405.05949v1|
|562|LLM-QBench: A Benchmark Towards the Best Practice for Post-training Quantization of Large Language Models|Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Huang, Yunchen Zhang, Xianglong Liu, Dacheng Tao|2024-05-09|arXiv|https://github.com/ModelTC/llmc|https://doi.org/10.48550/arXiv.2405.06001|
|563|LLMC: Benchmarking Large Language Model Quantization with a Versatile Compression Toolkit|Ruihao Gong, Yang Yong, Shiqiao Gu, Yushi Huang, Chentao Lv, Yunchen Zhang, Xianglong Liu, Dacheng Tao|2024-05-09|arXiv|https://github.com/ModelTC/llmc|http://arxiv.org/abs/2405.06001v2|
|564|Probing Multimodal LLMs as World Models for Driving|Shiva Sreeram, Tsun-Hsuan Wang, Alaa Maalouf, Guy Rosman, Sertac Karaman, Daniela Rus|2024-05-09|arXiv|https://github.com/sreeramsa/DriveSim|http://arxiv.org/abs/2405.05956v1|
|565|Robots Can Feel: LLM-based Framework for Robot Ethical Reasoning|Artem Lykov, Miguel Altamirano Cabrera, Koffivi Fidèle Gbagbe, Dzmitry Tsetserukou|2024-05-09|arXiv|https://github.com/TemaLykov/robots_can_feel|http://arxiv.org/abs/2405.05824v1|
|566|DALK: Dynamic Co-Augmentation of LLMs and KG to answer Alzheimer's Disease Questions with Scientific Literature|Dawei Li, Shu Yang, Zhen Tan, Jae Young Baik, Sukwon Yun, Joseph Lee, Aaron Chacko, Bojian Hou, Duy Duong-Tran, Ying Ding, Huan Liu, Li Shen, Tianlong Chen|2024-05-08|arXiv|https://github.com/David-Li0406/DALK|http://arxiv.org/abs/2405.04819v2|
|567|Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models|Sander Land, Max Bartolo|2024-05-08|arXiv|https://github.com/cohere-ai/magikarp/|https://doi.org/10.48550/arXiv.2405.05417|
|568|Vidur: A Large-Scale Simulation Framework For LLM Inference|Amey Agrawal, Nitin Kedia, Jayashree Mohan, Ashish Panwar, Nipun Kwatra, Bhargav Gulavani, Ramachandran Ramjee, Alexey Tumanov|2024-05-08|arXiv|https://github.com/microsoft/vidur|http://arxiv.org/abs/2405.05465v2|
|569|QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving|Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, Song Han|2024-05-07|arXiv|https://github.com/mit-han-lab/qserve|http://arxiv.org/abs/2405.04532v2|
|570|Learning from Students: Applying t-Distributions to Explore Accurate and Efficient Formats for LLMs|Jordan Dotzel, Yuzong Chen, Bahaa Kotb, Sushma Prasad, Gang Wu, Sheng Li, Mohamed S. Abdelfattah, Zhiru Zhang|2024-05-06|arXiv|https://github.com/cornell-zhang/llm-datatypes|http://arxiv.org/abs/2405.03103v2|
|571|Word2World: Generating Stories and Worlds through Large Language Models|Muhammad Umair Nasir, Steven James, Julian Togelius|2024-05-06|arXiv|https://github.com/umair-nasir14/Word2World|https://doi.org/10.48550/arXiv.2405.06686|
|572|MedDoc-Bot: A Chat Tool for Comparative Analysis of Large Language Models in the Context of the Pediatric Hypertension Guideline|Mohamed Yaseen Jabarulla, Steffen Oeltze-Jafra, Philipp Beerbaum, Theodor Uden|2024-05-06|arXiv|https://github.com/yaseen28/MedDoc-Bot|https://doi.org/10.48550/arXiv.2405.03359|
|573|When LLMs Meet Cybersecurity: A Systematic Literature Review|Jie Zhang, Haoyu Bu, Hui Wen, Yu Chen, Lun Li, Hongsong Zhu|2024-05-06|arXiv|https://github.com/tmylla/Awesome-LLM4Cybersecurity|http://arxiv.org/abs/2405.03644v1|
|574|Language Evolution for Evading Social Media Regulation via LLM-Based Multi-Agent Simulation|Jinyu Cai, Jialong Li, Mingyue Zhang, Munan Li, Chen-Shu Wang, Kenji Tei|2024-05-05|2024 IEEE Congress on Evolutionary Computation (CEC)|https://github.com/BlueLinkXlGA-MAS|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10612015|
|575|NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli|Xu Wang, Cheng Li, Yi Chang, Jindong Wang, Yuan Wu|2024-05-05|arXiv|https://github.com/wangxu0820/NegativePrompt|https://doi.org/10.48550/arXiv.2405.02814|
|576|EDA Corpus: A Large Language Model Dataset for Enhanced Interaction with OpenROAD|Bing-Yue Wu, Utsav Sharma, Sai Rahul Dhanvi Kankipati, Ajay Yadav, Bintu Kappil George, Sai Ritish Guntupalli, Austin Rovinski, Vidya A. Chhabria|2024-05-04|arXiv|https://github.com/OpenROAD-Assistant/EDA-Corpus|https://doi.org/10.48550/arXiv.2405.06676|
|577|Enhancing Contextual Understanding in Large Language Models through Contrastive Decoding|Zheng Zhao, Emilio Monti, Jens Lehmann, Haytham Assem|2024-05-04|arXiv|https://github.com/amazon-science/ContextualUnderstanding-ContrastiveDecoding|https://doi.org/10.48550/arXiv.2405.02750|
|578|PICLe: Eliciting Diverse Behaviors from Large Language Models with Persona In-Context Learning|Hyeong Kyu Choi, Yixuan Li|2024-05-03|arXiv|https://github.com/deeplearning-wisc/picle|https://doi.org/10.48550/arXiv.2405.02501|
|579|ProFLingo: A Fingerprinting-based Copyright Protection Scheme for Large Language Models|Heng Jin, Chaoyu Zhang, Shanghao Shi, Wenjing Lou, Y. Thomas Hou|2024-05-03|arXiv|https://github.com/hengvt/ProFLingo|https://doi.org/10.48550/arXiv.2405.02466|
|580|A Survey on Large Language Models for Critical Societal Domains: Finance, Healthcare, and Law|Zhiyu Zoey Chen, Jing Ma, Xinlu Zhang, Nan Hao, An Yan, Armineh Nourbakhsh, Xianjun Yang, Julian J. McAuley, Linda R. Petzold, William Yang Wang|2024-05-02|arXiv|https://github.com/czyssrs/LLM_X_papers|https://doi.org/10.48550/arXiv.2405.01769|
|581|Analyzing the Role of Semantic Representations in the Era of Large Language Models|Zhijing Jin, Yuen Chen, Fernando Gonzalez Adauto, Jiarui Liu, Jiayi Zhang, Julian Michael, Bernhard Schölkopf, Mona T. Diab|2024-05-02|NAACL-HLT|https://github.com/causalNLP/amr_llm|https://doi.org/10.18653/v1/2024.naacl-long.209|
|582|Creative Problem Solving in Large Language and Vision Models - What Would it Take?|Lakshmi Nair, Evana Gizzi, Jivko Sinapov|2024-05-02|arXiv|https://github.com/lnairGT/creative-problem-solving-LLMs|https://doi.org/10.48550/arXiv.2405.01453|
|583|MiniGPT-3D: Efficiently Aligning 3D Point Clouds with Large Language Models using 2D Priors|Yuan Tang, Xu Han, Xianzhi Li, Qiao Yu, Yixue Hao, Long Hu, Min Chen|2024-05-02|arXiv|https://github.com/TangYuan96/MiniGPT-3D|https://doi.org/10.48550/arXiv.2405.01413|
|584|Characterising the Creative Process in Humans and Large Language Models|Surabhi S. Nath, Peter Dayan, Claire Stevenson|2024-05-01|arXiv|https://github.com/surabhisnath/Creative_Process|https://doi.org/10.48550/arXiv.2405.00899|
|585|Text-to-SQL Empowered by Large Language Models: A Benchmark Evaluation|Dawei Gao, Haibin Wang, Yaliang Li, Xiuyu Sun, Yichen Qian, Bolin Ding, Jingren Zhou|2024-05|Proceedings of the VLDB Endowment (PVLDB), Volume 17, Issue 5|https://github.com/BeachWang/DAIL-SQL|https://dl.acm.org/doi/10.14778/3641204.3641221|
|586|AutoDroid: LLM-powered Task Automation in Android|Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, Yunxin Liu|2024-05|ACM MobiCom '24: Proceedings of the 30th Annual International Conference on Mobile Computing and Networking|https://autodroid-sys.github.io/|https://dl.acm.org/doi/10.1145/3636534.3649379|
|587|CodeHalu: Code Hallucinations in LLMs Driven by Execution-based Verification|Yuchen Tian, Weixiang Yan, Qian Yang, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma|2024-04-30|arXiv|https://github.com/yuchen814/CodeHalu|http://arxiv.org/abs/2405.00253v2|
|588|CodeHalu: Investigating Code Hallucinations in LLMs via Execution-based Verification|Yuchen Tian, Weixiang Yan, Qian Yang, Xuandong Zhao, Qian Chen, Wen Wang, Ziyang Luo, Lei Ma, Dawn Song|2024-04-30|arXiv|https://github.com/yuchen814/CodeHalu|http://arxiv.org/abs/2405.00253v3|
|589|Do Large Language Models Understand Conversational Implicature - A case study with a chinese sitcom|Shisen Yue, Siyuan Song, Xinyuan Cheng, Hai Hu|2024-04-30|arXiv|https://github.com/sjtu-compling/llm-pragmatics|https://doi.org/10.48550/arXiv.2404.19509|
|590|Transcrib3D: 3D Referring Expression Resolution through Large Language Models|Jiading Fang, Xiangshan Tan, Shengjie Lin, Igor Vasiljevic, Vitor Guizilini, Hongyuan Mei, Rares Ambrus, Gregory Shakhnarovich, Matthew R. Walter|2024-04-30|arXiv|https://ripl.github.io/Transcrib3D|https://doi.org/10.48550/arXiv.2404.19221|
|591|Benchmarking Benchmark Leakage in Large Language Models|Ruijie Xu, Zengzhi Wang, Run-Ze Fan, Pengfei Liu|2024-04-29|arXiv|https://gair-nlp.github.io/benbench|https://doi.org/10.48550/arXiv.2404.18824|
|592|Hallucination of Multimodal Large Language Models: A Survey|Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, Mike Zheng Shou|2024-04-29|arXiv|https://github.com/showlab/Awesome-MLLM-Hallucination|https://doi.org/10.48550/arXiv.2404.18930|
|593|LLM-SR: Scientific Equation Discovery via Programming with Large Language Models|Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, Chandan K. Reddy|2024-04-29|arXiv|https://github.com/deep-symbolic-mathematics/LLM-SR|https://doi.org/10.48550/arXiv.2404.18400|
|594|SOUL: Unlocking the Power of Second-Order Optimization for LLM Unlearning|Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James Diffenderfer, Bhavya Kailkhura, Sijia Liu|2024-04-28|arXiv|https://github.com/OPTML-Group/SOUL|http://arxiv.org/abs/2404.18239v4|
|595|WorldGPT: Empowering LLM as Multimodal World Model|Zhiqi Ge, Hongzhe Huang, Mingze Zhou, Juncheng Li, Guoming Wang, Siliang Tang, Yueting Zhuang|2024-04-28|arXiv|https://github.com/DCDmllm/WorldGPT|http://arxiv.org/abs/2404.18202v1|
|596|SpecInfer: Accelerating Large Language Model Serving with Tree-based Speculative Inference and Verification|Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, Zhihao Jia|2024-04-27|ASPLOS '24: Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3|https://github.com/flexflow/FlexFlow/|https://dl.acm.org/doi/10.1145/3620666.3651335|
|597|SEED-Bench-2-Plus: Benchmarking Multimodal Large Language Models with Text-Rich Visual Comprehension|Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao Zhang, Ying Shan|2024-04-25|arXiv|https://github.com/AILab-CVC/SEED-Bench|https://doi.org/10.48550/arXiv.2404.16790|
|598|List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs|An Yan, Zhengyuan Yang, Junda Wu, Wanrong Zhu, Jianwei Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Julian McAuley, Jianfeng Gao, Lijuan Wang|2024-04-25|arXiv|https://github.com/zzxslp/SoM-LLaVA|http://arxiv.org/abs/2404.16375v1|
|599|Evaluating Class Membership Relations in Knowledge Graphs using Large Language Models|Bradley P. Allen, Paul T. Groth|2024-04-25|arXiv|https://github.com/bradleypallen/evaluating-kg-class-memberships-using-llms|https://doi.org/10.48550/arXiv.2404.17000|
|600|Continual Learning of Large Language Models: A Comprehensive Survey|Haizhou Shi, Zihao Xu, Hengyi Wang, Weiyi Qin, Wenyuan Wang, Yibin Wang, Zifeng Wang, Sayna Ebrahimi, Hao Wang|2024-04-25|arXiv|https://github.com/Wang-ML-Lab/llm-continual-learning-survey|https://doi.org/10.48550/arXiv.2404.16789|
|601|Can&apos;t say cant? Measuring and Reasoning of Dark Jargons in Large Language Models|Xu Ji, Jianyi Zhang, Ziyin Zhou, Zhangchi Zhao, Qianqian Qiao, Kaiying Han, Md. Imran Hossen, Xiali Hei|2024-04-25|arXiv|https://github.com/cistineup/CantCounter|https://doi.org/10.48550/arXiv.2405.00718|
|602|Attacks on Third-Party APIs of Large Language Models|Wanru Zhao, Vidit Khazanchi, Haodi Xing, Xuanli He, Qiongkai Xu, Nicholas Donald Lane|2024-04-24|arXiv|https://github.com/vk0812/Third-Party-Attacks-on-LLMs|https://doi.org/10.48550/arXiv.2404.16891|
|603|Chat2Scenario: Scenario Extraction From Dataset Through Utilization of Large Language Model|Yongqi Zhao, Wenbo Xiao, Tomislav Mihalj, Jia Hu, Arno Eichberger|2024-04-24|arXiv|https://github.com/ftgTUGraz/Chat2Scenario|https://doi.org/10.48550/arXiv.2404.16147|
|604|ImplicitAVE: An Open-Source Dataset and Multimodal LLMs Benchmark for Implicit Attribute Value Extraction|Henry Peng Zou, Vinay Samuel, Yue Zhou, Weizhi Zhang, Liancheng Fang, Zihe Song, Philip S. Yu, Cornelia Caragea|2024-04-24|arXiv|https://github.com/HenryPengZou/ImplicitAVE|http://arxiv.org/abs/2404.15592v1|
|605|LogicBench: Towards Systematic Evaluation of Logical Reasoning Ability of Large Language Models|Mihir Parmar, Nisarg Patel, Neeraj Varshney, Mutsumi Nakamura, Man Luo, Santosh Mashetty, Arindam Mitra, Chitta Baral|2024-04-23|ACL|https://github.com/Mihir3009/LogicBench|https://aclanthology.org/2024.acl-long.739|
|606|Rethinking LLM Memorization through the Lens of Adversarial Compression|Avi Schwarzschild, Zhili Feng, Pratyush Maini, Zachary C. Lipton, J. Zico Kolter|2024-04-23|arXiv|https://locuslab.github.io/acr-memorization|http://arxiv.org/abs/2404.15146v1|
|607|Think-Program-reCtify: 3D Situated Reasoning with Large Language Models|Qingrong He, Kejun Lin, Shizhe Chen, Anwen Hu, Qin Jin|2024-04-23|arXiv|https://qingrongh.github.io/LLM-TPC/|https://doi.org/10.48550/arXiv.2404.14705|
|608|A User-Centric Benchmark for Evaluating Large Language Models|Jiayin Wang, Fengran Mo, Weizhi Ma, Peijie Sun, Min Zhang, Jian-Yun Nie|2024-04-22|arXiv|https://github.com/Alice1998/URS|https://doi.org/10.48550/arXiv.2404.13940|
|609|How Well Can LLMs Echo Us? Evaluating AI Chatbots' Role-Play Ability with ECHO|Man Tik Ng, Hui Tung Tse, Jen-tse Huang, Jingjing Li, Wenxuan Wang, Michael R. Lyu|2024-04-22|arXiv|https://github.com/CUHK-ARISE/ECHO|http://arxiv.org/abs/2404.13957v1|
|610|Information Re-Organization Improves Reasoning in Large Language Models|Xiaoxia Cheng, Zeqi Tan, Wei Xue, Weiming Lu|2024-04-22|arXiv|https://github.com/hustcxx/InfoRE|https://doi.org/10.48550/arXiv.2404.13985|
|611|LLM-Personalize: Aligning LLM Planners with Human Preferences via Reinforced Self-Training for Housekeeping Robots|Dongge Han, Trevor McInroe, Adam Jelley, Stefano V. Albrecht, Peter Bell, Amos Storkey|2024-04-22|arXiv|https://donggehan.github.io/projectllmpersonalize/|http://arxiv.org/abs/2404.14285v1|
|612|MARIO Eval: Evaluate Your Math LLM with your Math LLM--A mathematical dataset evaluation toolkit|Boning Zhang, Chengxi Li, Kai Fan|2024-04-22|arXiv|https://github.com/MARIO-Math-Reasoning/math_evaluation|http://arxiv.org/abs/2404.13925v1|
|613|SVGEditBench: A Benchmark Dataset for Quantitative Assessment of LLM's SVG Editing Capabilities|Kunato Nishina, Yusuke Matsui|2024-04-21|arXiv|https://github.com/mti-lab/SVGEditBench|http://arxiv.org/abs/2404.13710v1|
|614|A Survey on the Memory Mechanism of Large Language Model based Agents|Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen|2024-04-21|arXiv|https://github.com/nuster1128/LLM_Agent_Memory_Survey|https://doi.org/10.48550/arXiv.2404.13501|
|615|UnibucLLM: Harnessing LLMs for Automated Prediction of Item Difficulty and Response Time for Multiple-Choice Questions|Ana-Cristina Rogoz, Radu Tudor Ionescu|2024-04-20|arXiv|https://github.com/ana-rogoz/BEA-2024|http://arxiv.org/abs/2404.13343v1|
|616|Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works|Xinfeng Yuan, Siyu Yuan, Yuhan Cui, Tianhe Lin, Xintao Wang, Rui Xu, Jiangjie Chen, Deqing Yang|2024-04-19|arXiv|https://github.com/Joanna0123/character_profiling|https://doi.org/10.48550/arXiv.2404.12726|
|617|Groma: Localized Visual Tokenization for Grounding Multimodal Large Language Models|Chuofan Ma, Yi Jiang, Jiannan Wu, Zehuan Yuan, Xiaojuan Qi|2024-04-19|arXiv|https://groma-mllm.github.io/|https://doi.org/10.48550/arXiv.2404.13013|
|618|STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases|Shirley Wu, Shiyu Zhao, Michihiro Yasunaga, Kexin Huang, Kaidi Cao, Qian Huang, Vassilis N. Ioannidis, Karthik Subbian, James Zou, Jure Leskovec|2024-04-19|arXiv|https://github.com/snap-stanford/stark|http://arxiv.org/abs/2404.13207v2|
|619|Sample Design Engineering: An Empirical Study of What Makes Good Downstream Fine-Tuning Samples for LLMs|Biyang Guo, He Wang, Wenyilin Xiao, Hong Chen, Zhuxin Lee, Songqiao Han, Hailiang Huang|2024-04-19|arXiv|https://github.com/beyondguo/LLM-Tuning|http://arxiv.org/abs/2404.13033v1|
|620|Unified Scene Representation and Reconstruction for 3D Large Language Models|Tao Chu, Pan Zhang, Xiaoyi Dong, Yuhang Zang, Qiong Liu, Jiaqi Wang|2024-04-19|arXiv|https://chtsy.github.io/uni3drr-page/|https://doi.org/10.48550/arXiv.2404.13044|
|621|Advancing the Robustness of Large Language Models through Self-Denoised Smoothing|Jiabao Ji, Bairu Hou, Zhen Zhang, Guanhua Zhang, Wenqi Fan, Qing Li, Yang Zhang, Gaowen Liu, Sijia Liu, Shiyu Chang|2024-04-18|arXiv|https://github.com/UCSB-NLP-Chang/SelfDenoise|https://doi.org/10.48550/arXiv.2404.12274|
|622|Aligning Actions and Walking to LLM-Generated Textual Descriptions|Radu Chivereanu, Adrian Cosma, Andy Catruna, Razvan Rughinis, Emilian Radoi|2024-04-18|2024 IEEE 18th International Conference on Automatic Face and Gesture Recognition, FG 2024|https://github.com/Radu1999/WalkAndText|http://arxiv.org/abs/2404.12192v1|
|623|Large Language Models in Targeted Sentiment Analysis|Nicolay Rusnachenko, Anton Golubev, Natalia V. Loukachevitch|2024-04-18|arXiv|https://github.com/nicolay-r/Reasoning-for-Sentiment-Analysis-Framework|https://doi.org/10.48550/arXiv.2404.12342|
|624|Simultaneous Interpretation Corpus Construction by Large Language Models in Distant Language Pair|Yusuke Sakai, Mana Makinae, Hidetaka Kamigaito, Taro Watanabe|2024-04-18|arXiv|https://github.com/yusuke1997/LLM-SI-Corpus|https://doi.org/10.48550/arXiv.2404.12299|
|625|Towards Large Language Models as Copilots for Theorem Proving in Lean|Peiyang Song, Kaiyu Yang, Anima Anandkumar|2024-04-18|arXiv|https://github.com/lean-dojo/LeanCopilot|https://doi.org/10.48550/arXiv.2404.12534|
|626|OVAL-Prompt: Open-Vocabulary Affordance Localization for Robot Manipulation through LLM Affordance-Grounding|Edmond Tong, Anthony Opipari, Stanley Lewis, Zhen Zeng, Odest Chadwicke Jenkins|2024-04-17|arXiv|https://ekjt.github.io/OVAL-Prompt/|http://arxiv.org/abs/2404.11000v2|
|627|Large Language Models meet Collaborative Filtering: An Efficient All-round LLM-based Recommender System|Sein Kim, Hongseok Kang, Seungyoon Choi, Donghyun Kim, Min-Chul Yang, Chanyoung Park|2024-04-17|arXiv|https://github.com/ghdtjr/A-LLMRec|https://doi.org/10.48550/arXiv.2404.11343|
|628|Hierarchical Context Merging: Better Long Context Understanding for Pre-trained LLMs|Woomin Song, Seunghyuk Oh, Sangwoo Mo, Jaehyung Kim, Sukmin Yun, Jung-Woo Ha, Jinwoo Shin|2024-04-16|arXiv|https://github.com/alinlab/HOMER|http://arxiv.org/abs/2404.10308v1|
|629|Self-playing Adversarial Language Game Enhances LLM Reasoning|Pengyu Cheng, Tianhao Hu, Han Xu, Zhisong Zhang, Yong Dai, Lei Han, Nan Du|2024-04-16|arXiv|https://github.com/Linear95/SPAG|http://arxiv.org/abs/2404.10642v2|
|630|SA-DS: A Dataset for Large Language Model-Driven AI Accelerator Design Generation|Deepak Vungarala, Mahmoud Nazzal, Mehrdad Morsali, Chao Zhang, Arnob Ghosh, Abdallah Khreishah, Shaahin Angizi|2024-04-16|arXiv|https://github.com/ACADLab/SA-DS|http://arxiv.org/abs/2404.10875v2|
|631|A Dataset for Large Language Model-Driven AI Accelerator Generation|Mahmoud Nazzal, Deepak Vungarala, Mehrdad Morsali, Chao Zhang, Arnob Ghosh, Abdallah Khreishah, Shaahin Angizi|2024-04-16|arXiv|https://github.com/ACADLab/SA-DS|https://doi.org/10.48550/arXiv.2404.10875|
|632|Demonstration of DB-GPT: Next Generation Data Interaction System Empowered by Large Language Models|Siqiao Xue, Danrui Qi, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting Chen, Hongjun Yang, Zhiping Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, Wang Zhao, Fan Zhou, Hong Yi, Shaodong Liu, Hongjun Yang, Faqiang Chen|2024-04-16|arXiv|https://github.com/eosphoros-ai/DB-GPT|https://doi.org/10.48550/arXiv.2404.10209|
|633|Balancing Speciality and Versatility: a Coarse to Fine Framework for Supervised Fine-tuning Large Language Model|Hengyuan Zhang, Yanru Wu, Dawei Li, Zacc Yang, Rui Zhao, Yong Jiang, Fei Tan|2024-04-16|ACL|https://github.com/rattlesnakey/CoFiTune|https://aclanthology.org/2024.findings-acl.445|
|634|Constructing Benchmarks and Interventions for Combating Hallucinations in LLMs|Adi Simhi, Jonathan Herzig, Idan Szpektor, Yonatan Belinkov|2024-04-15|arXiv|https://github.com/technion-cs-nlp/hallucination-mitigation|http://arxiv.org/abs/2404.09971v1|
|635|Exploring Prompting Methods for Mitigating Class Imbalance through Synthetic Data Generation with Large Language Models|Jinhee Kim, Taesung Kim, Jaegul Choo|2024-04-15|arXiv|https://github.com/seharanul17/synthetic-tabular-LLM|http://arxiv.org/abs/2404.12404v2|
|636|Memory Sharing for Large Language Model based Agents|Hang Gao, Yongfeng Zhang|2024-04-15|arXiv|https://github.com/GHupppp/MemorySharingLLM|https://doi.org/10.48550/arXiv.2404.09982|
|637|Prepacking: A Simple Method for Fast Prefilling and Increased Throughput in Large Language Models|Siyan Zhao, Daniel Israel, Guy Van den Broeck, Aditya Grover|2024-04-15|arXiv|https://github.com/siyan-zhao/prepacking|https://doi.org/10.48550/arXiv.2404.09529|
|638|When Hindsight is Not 20/20: Testing Limits on Reflective Thinking in Large Language Models|Yanhong Li, Chenghao Yang, Allyson Ettinger|2024-04-14|arXiv|https://github.com/yanhong-lbh/LLM-SelfReflection-Eval|https://doi.org/10.48550/arXiv.2404.09129|
|639|LLM-Seg: Bridging Image Segmentation and Large Language Model Reasoning|Junchi Wang, Lei Ke|2024-04-12|arXiv|https://github.com/wangjunchi/LLMSeg|https://doi.org/10.48550/arXiv.2404.08767|
|640|Data-Augmentation-Based Dialectal Adaptation for LLMs|Fahim Faisal, Antonios Anastasopoulos|2024-04-11|arXiv|https://github.com/ffaisal93/dialect_copa|http://arxiv.org/abs/2404.08092v1|
|641|DesignQA: A Multimodal Benchmark for Evaluating Large Language Models&apos; Understanding of Engineering Documentation|Anna C. Doris, Daniele Grandi, Ryan Tomich, Md Ferdous Alam, Mohammadmehdi Ataei, Hyunmin Cheong, Faez Ahmed|2024-04-11|arXiv|https://github.com/anniedoris/design_qa/|https://doi.org/10.48550/arXiv.2404.07917|
|642|Manipulating Large Language Models to Increase Product Visibility|Aounon Kumar, Himabindu Lakkaraju|2024-04-11|arXiv|https://github.com/aounon/llm-rank-optimizer|https://doi.org/10.48550/arXiv.2404.07981|
|643|UltraEval: A Lightweight Platform for Flexible and Comprehensive Evaluation for LLMs|Chaoqun He, Renjie Luo, Shengding Hu, Yuanqian Zhao, Jie Zhou, Hanghao Wu, Jiajie Zhang, Xu Han, Zhiyuan Liu, Maosong Sun|2024-04-11|arXiv|https://github.com/OpenBMB/UltraEval|http://arxiv.org/abs/2404.07584v1|
|644|LaVy: Vietnamese Multimodal Large Language Model|Chi Tran, Huong Le Thanh|2024-04-11|arXiv|https://github.com/baochi0212/LaVy|https://doi.org/10.48550/arXiv.2404.07922|
|645|Accelerating Inference in Large Language Models with a Unified Layer Skipping Strategy|Yijin Liu, Fandong Meng, Jie Zhou|2024-04-10|arXiv|https://github.com/Adaxry/Unified_Layer_Skipping|https://doi.org/10.48550/arXiv.2404.06954|
|646|Exploring Concept Depth: How Large Language Models Acquire Knowledge at Different Layers?|Mingyu Jin, Qinkai Yu, Jingyuan Huang, Qingcheng Zeng, Zhenting Wang, Wenyue Hua, Haiyan Zhao, Kai Mei, Yanda Meng, Kaize Ding, Fan Yang, Mengnan Du, Yongfeng Zhang|2024-04-10|arXiv|https://github.com/Luckfort/CD|https://doi.org/10.48550/arXiv.2404.07066|
|647|GoEX: Perspectives and Designs Towards a Runtime for Autonomous LLM Applications|Shishir G. Patil, Tianjun Zhang, Vivian Fang, Noppapon C., Roy Huang, Aaron Hao, Martin Casado, Joseph E. Gonzalez, Raluca Ada Popa, Ion Stoica|2024-04-10|arXiv|https://github.com/ShishirPatil/gorilla/|http://arxiv.org/abs/2404.06921v1|
|648|Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs|Bowen Jin, Chulin Xie, Jiawei Zhang, Kashob Kumar Roy, Yu Zhang, Zheng Li, Ruirui Li, Xianfeng Tang, Suhang Wang, Yu Meng, Jiawei Han|2024-04-10|arXiv|https://github.com/PeterGriffinJin/Graph-CoT|https://doi.org/10.48550/arXiv.2404.07103|
|649|LLMs in Biomedicine: A study on clinical Named Entity Recognition|Masoud Monajatipoor, Jiaxin Yang, Joel Stremmel, Melika Emami, Fazlolah Mohaghegh, Mozhdeh Rouhsedaghat, Kai-Wei Chang|2024-04-10|OpenReview|https://github.com/masoud-monajati/LLM_Bio_NER|http://arxiv.org/abs/2404.07376v1|
|650|Not All Contexts Are Equal: Teaching LLMs Credibility-aware Generation|Ruotong Pan, Boxi Cao, Hongyu Lin, Xianpei Han, Jia Zheng, Sirui Wang, Xunliang Cai, Le Sun|2024-04-10|OpenReview|https://github.com/panruotong/CAG|http://arxiv.org/abs/2404.06809v2|
|651|Elephants Never Forget: Memorization and Learning of Tabular Data in Large Language Models|Sebastian Bordt, Harsha Nori, Vanessa Rodrigues, Besmira Nushi, Rich Caruana|2024-04-09|arXiv|https://github.com/interpretml/LLM-Tabular-Memorization-Checker|https://doi.org/10.48550/arXiv.2404.06209|
|652|FreeEval: A Modular Framework for Trustworthy and Efficient Evaluation of Large Language Models|Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Zhengran Zeng, Wei Ye, Jindong Wang, Yue Zhang, Shikun Zhang|2024-04-09|arXiv|https://github.com/WisdomShell/FreeEval|https://doi.org/10.48550/arXiv.2404.06003|
|653|AgentQuest: A Modular Benchmark Framework to Measure Progress and Improve LLM Agents|Luca Gioacchini, Giuseppe Siracusano, Davide Sanvito, Kiril Gashteovski, David Friede, Roberto Bifulco, Carolin Lawrence|2024-04-09|Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024|https://github.com/nec-research/agentquest|http://arxiv.org/abs/2404.06411v1|
|654|Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks|Chonghua Wang, Haodong Duan, Songyang Zhang, Dahua Lin, Kai Chen|2024-04-09|arXiv|https://github.com/open-compass/Ada-LEval|http://arxiv.org/abs/2404.06480v2|
|655|Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge|Weikai Lu, Ziqian Zeng, Jianwei Wang, Zhengdong Lu, Zelin Chen, Huiping Zhuang, Cen Chen|2024-04-08|arXiv|https://github.com/ZeroNLP/Eraser|https://doi.org/10.48550/arXiv.2404.05880|
|656|LayoutLLM: Layout Instruction Tuning with Large Language Models for Document Understanding|Chuwei Luo, Yufan Shen, Zhaoqing Zhu, Qi Zheng, Zhi Yu, Cong Yao|2024-04-08|arXiv|https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/DocumentUnderstanding/LayoutLLM|https://doi.org/10.48550/arXiv.2404.05225|
|657|The Fact Selection Problem in LLM-Based Program Repair|Nikhil Parasaram, Huijie Yan, Boyu Yang, Zineb Flahy, Abriele Qudsi, Damian Ziaber, Earl Barr, Sergey Mechtaev|2024-04-08|arXiv|https://github.com/PyRepair/maniple|http://arxiv.org/abs/2404.05520v2|
|658|RoboMP2: A Robotic Multimodal Perception-Planning Framework with Multimodal Large Language Models|Qi Lv, Hao Li, Xiang Deng, Rui Shao, Michael Y. Wang, Liqiang Nie|2024-04-07|ICML|https://aopolin-lv.github.io/RoboMP2.github.io/|https://openreview.net/forum?id=eJFQROkaj0|
|659|SqueezeAttention: 2D Management of KV-Cache in LLM Inference via Layer-wise Optimal Budget|Zihao Wang, Shaoduo Gan|2024-04-07|arXiv|https://github.com/hetailang/SqueezeAttention|http://arxiv.org/abs/2404.04793v1|
|660|CLUE: A Clinical Language Understanding Evaluation for LLMs|Amin Dada, Marie Bauer, Amanda Butler Contreras, Osman Alperen Koraş, Constantin Marc Seibold, Kaleb E Smith, Jens Kleesiek|2024-04-05|arXiv|https://github.com/TIO-IKIM/CLUE|http://arxiv.org/abs/2404.04067v3|
|661|Can only LLMs do Reasoning?: Potential of Small Language Models in Task Planning|Gawon Choi, Hyemin Ahn|2024-04-05|arXiv|https://github.com/Gawon-Choi/small-LMs-Task-Planning|http://arxiv.org/abs/2404.03891v1|
|662|Hypothesis Generation with Large Language Models|Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan|2024-04-05|arXiv|https://github.com/ChicagoHAI/hypothesis_generation|https://doi.org/10.48550/arXiv.2404.04326|
|663|LongVLM: Efficient Long Video Understanding via Large Language Models|Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, Bohan Zhuang|2024-04-04|arXiv|https://github.com/ziplab/LongVLM|https://doi.org/10.48550/arXiv.2404.03384|
|664|Untangle the KNOT: Interweaving Conflicting Knowledge and Reasoning Skills in Large Language Models|Yantao Liu, Zijun Yao, Xin Lv, Yuchen Fan, Shulin Cao, Jifan Yu, Lei Hou, Juanzi Li|2024-04-04|LREC/COLING|https://github.com/THU-KEG/KNOT|https://aclanthology.org/2024.lrec-main.1493|
|665|SHROOM-INDElab at SemEval-2024 Task 6: Zero- and Few-Shot LLM-Based Classification for Hallucination Detection|Bradley P. Allen, Fina Polat, Paul Groth|2024-04-04|arXiv|https://github.com/bradleypallen/shroom|http://arxiv.org/abs/2404.03732v1|
|666|MiniGPT4-Video: Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-Textual Tokens|Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu, Jian Ding, Mohamed Elhoseiny|2024-04-04|arXiv|https://vision-cair.github.io/MiniGPT4-video/|http://arxiv.org/abs/2404.03413v1|
|667|How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?|Siye Wu, Jian Xie, Jiangjie Chen, Tinghui Zhu, Kai Zhang, Yanghua Xiao|2024-04-04|arXiv|https://github.com/Di-viner/LLM-Robustness-to-Irrelevant-Information|https://doi.org/10.48550/arXiv.2404.03302|
|668|Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations|Mahjabin Nahar, Haeseung Seo, Eun-Ju Lee, Aiping Xiong, Dongwon Lee|2024-04-04|arXiv|https://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials|http://arxiv.org/abs/2404.03745v1|
|669|Evaluating LLMs at Detecting Errors in LLM Responses|Ryo Kamoi, Sarkar Snigdha Sarathi Das, Renze Lou, Jihyun Janice Ahn, Yilun Zhao, Xiaoxin Lu, Nan Zhang, Yusen Zhang, Ranran Haoran Zhang, Sujeeth Reddy Vummanthala, Salika Dave, Shaobo Qin, Arman Cohan, Wenpeng Yin, Rui Zhang|2024-04-04|arXiv|https://github.com/psunlpgroup/ReaLMistake|http://arxiv.org/abs/2404.03602v1|
|670|BAdam: A Memory Efficient Full Parameter Training Method for Large Language Models|Qijun Luo, Hengxu Yu, Xiao Li|2024-04-03|arXiv|https://github.com/Ledzy/BAdam|https://doi.org/10.48550/arXiv.2404.02827|
|671|ChatGLM-Math: Improving Math Problem-Solving in Large Language Models with a Self-Critique Pipeline|Yifan Xu, Xiao Liu, Xinghan Liu, Zhenyu Hou, Yueyan Li, Xiaohan Zhang, Zihan Wang, Aohan Zeng, Zhengxiao Du, Wenyi Zhao, Jie Tang, Yuxiao Dong|2024-04-03|arXiv|https://github.com/THUDM/ChatGLM-Math|https://doi.org/10.48550/arXiv.2404.02893|
|672|Conifer: Improving Complex Constrained Instruction-Following Ability of Large Language Models|Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, Ruohui Huang|2024-04-03|arXiv|https://www.github.com/ConiferLM/Conifer|https://doi.org/10.48550/arXiv.2404.02823|
|673|Large Language Models for Expansion of Spoken Language Understanding Systems to New Languages|Jakub Hoscilowicz, Pawel Pawlowski, Marcin Skorupa, Marcin Sowanski, Artur Janicki|2024-04-03|arXiv|https://github.com/Samsung/MT-LLM-NLU|https://doi.org/10.48550/arXiv.2404.02588|
|674|Min-K%++: Improved Baseline for Detecting Pre-Training Data from Large Language Models|Jingyang Zhang, Jingwei Sun, Eric C. Yeats, Yang Ouyang, Martin Kuo, Jianyi Zhang, Hao Yang, Hai Helen Li|2024-04-03|arXiv|https://zjysteven.github.io/mink-plus-plus/|https://doi.org/10.48550/arXiv.2404.02936|
|675|Advancing LLM Reasoning Generalists with Preference Trees|Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie, Yankai Lin, Zhenghao Liu, Bowen Zhou, Hao Peng, Zhiyuan Liu, Maosong Sun|2024-04-02|arXiv|https://github.com/OpenBMB/Eurus|http://arxiv.org/abs/2404.02078v1|
|676|Large Language Models for Orchestrating Bimanual Robots|Kun Chu, Xufeng Zhao, Cornelius Weber, Mengdi Li, Wenhao Lu, Stefan Wermter|2024-04-02|arXiv|http://labor-agent.github.io|https://doi.org/10.48550/arXiv.2404.02018|
|677|MuxServe: Flexible Spatial-Temporal Multiplexing for Multiple LLM Serving|Jiangfei Duan, Runyu Lu, Haojie Duanmu, Xiuhong Li, Xingcheng Zhang, Dahua Lin, Ion Stoica, Hao Zhang|2024-04-02|arXiv|https://github.com/hao-ai-lab/MuxServe|http://arxiv.org/abs/2404.02015v2|
|678|Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation|Bohao Yang, Kun Zhao, Chen Tang, Liang Zhan, Chenghua Lin|2024-04-01|arXiv|https://github.com/Bernard-Yang/SIMAMR|http://arxiv.org/abs/2404.01129v2|
|679|Safe and Responsible Large Language Model : Can We Balance Bias Reduction and Language Understanding in Large Language Models?|Shaina Raza, Oluwanifemi Bamgbose, Shardul Ghuge, Fatemeh Tavakol, Deepak John Reji, Syed Raza Bashir|2024-04-01|arXiv|https://github.com/shainarazavi/Safe-Responsible-LLM|http://arxiv.org/abs/2404.01399v3|
|680|Prompt-prompted Mixture of Experts for Efficient LLM Generation|Harry Dong, Beidi Chen, Yuejie Chi|2024-04-01|arXiv|https://github.com/hdong920/GRIFFIN|http://arxiv.org/abs/2404.01365v2|
|681|PSYDIAL: Personality-based Synthetic Dialogue Generation Using Large Language Models|Ji-Eun Han, Jun-Seok Koh, Hyeon-Tae Seo, Du-Seong Chang, Kyung-Ah Sohn|2024-04-01|LREC/COLING|https://github.com/jiSilverH/psydial|https://aclanthology.org/2024.lrec-main.1166|
|682|Prompt-prompted Adaptive Structured Pruning for Efficient LLM Generation|Harry Dong, Beidi Chen, Yuejie Chi|2024-04-01|arXiv|https://github.com/hdong920/GRIFFIN|http://arxiv.org/abs/2404.01365v3|
|683|LLM Attributor: Interactive Visual Attribution for LLM Generation|Seongmin Lee, Zijie J. Wang, Aishwarya Chakravarthy, Alec Helbling, ShengYun Peng, Mansi Phute, Duen Horng Chau, Minsuk Kahng|2024-04-01|arXiv|https://github.com/poloclub/|http://arxiv.org/abs/2404.01361v1|
|684|Emphasising Structured Information: Integrating Abstract Meaning Representation into LLMs for Enhanced Open-Domain Dialogue Evaluation|Bohao Yang, Kun Zhao, Chen Tang, Dong Liu, Liang Zhan, Chenghua Lin|2024-04-01|arXiv|https://github.com/Bernard-Yang/SIMAMR|http://arxiv.org/abs/2404.01129v3|
|685|Developing Safe and Responsible Large Language Model : Can We Balance Bias Reduction and Language Understanding in Large Language Models?|Shaina Raza, Oluwanifemi Bamgbose, Shardul Ghuge, Fatemeh Tavakol, Deepak John Reji, Syed Raza Bashir|2024-04-01|arXiv|https://github.com/shainarazavi/Safe-Responsible-LLM|http://arxiv.org/abs/2404.01399v4|
|686|LITE: Modeling Environmental Ecosystems with Multimodal Large Language Models|Haoran Li, Junqi Liu, Zexian Wang, Shiyuan Luo, Xiaowei Jia, Huaxiu Yao|2024-04-01|arXiv|https://github.com/hrlics/LITE|https://doi.org/10.48550/arXiv.2404.01165|
|687|Evaluating the Factuality of Large Language Models using Large-Scale Knowledge Graphs|Xiaoze Liu, Feijie Wu, Tianyang Xu, Zhuo Chen, Yichi Zhang, Xiaoqian Wang, Jing Gao|2024-04-01|arXiv|https://github.com/xz-liu/GraphEval|https://doi.org/10.48550/arXiv.2404.00942|
|688|Harnessing Large Language Models for Training-free Video Anomaly Detection|Luca Zanella, Willi Menapace, Massimiliano Mancini, Yiming Wang, Elisa Ricci|2024-04-01|arXiv|https://lucazanella.github.io/lavad/|https://doi.org/10.48550/arXiv.2404.01014|
|689|CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs|Jingzhe Shi, Jialuo Li, Qinwei Ma, Zaiwen Yang, Huan Ma, Lei Li|2024-03-31|arXiv|https://github.com/JingzheShi/CHOPS|http://arxiv.org/abs/2404.01343v2|
|690|Comparing Bad Apples to Good Oranges: Aligning Large Language Models via Joint Preference Optimization|Hritik Bansal, Ashima Suvarna, Gantavya Bhatt, Nanyun Peng, Kai-Wei Chang, Aditya Grover|2024-03-31|arXiv|https://github.com/Hritikbansal/dove|https://doi.org/10.48550/arXiv.2404.00530|
|691|DiffAgent: Fast and Accurate Text-to-Image API Selection with Large Language Model|Lirui Zhao, Yue Yang, Kaipeng Zhang, Wenqi Shao, Yuxin Zhang, Yu Qiao, Ping Luo, Rongrong Ji|2024-03-31|arXiv|https://github.com/OpenGVLab/DiffAgent|https://doi.org/10.48550/arXiv.2404.01342|
|692|How Much are Large Language Models Contaminated? A Comprehensive Survey and the LLMSanitize Library|Mathieu Ravaut, Bosheng Ding, Fangkai Jiao, Hailin Chen, Xingxuan Li, Ruochen Zhao, Chengwei Qin, Caiming Xiong, Shafiq Joty|2024-03-31|arXiv|https://github.com/ntunlp/LLMSanitize|http://arxiv.org/abs/2404.00699v2|
|693|M3D: Advancing 3D Medical Image Analysis with Multi-Modal Large Language Models|Fan Bai, Yuxin Du, Tiejun Huang, Max Qinghu Meng, Bo Zhao|2024-03-31|arXiv|https://github.com/BAAI-DCAI/M3D|https://doi.org/10.48550/arXiv.2404.00578|
|694|PID Control-Based Self-Healing to Improve the Robustness of Large Language Models|Zhuotong Chen, Zihu Wang, Yifan Yang, Qianxiao Li, Zheng Zhang|2024-03-31|arXiv|https://github.com/zhuotongchen/PID-Control-Based-Self-Healing-to-Improve-the-Robustness-of-Large-Language-Models|https://doi.org/10.48550/arXiv.2404.00828|
|695|ProLLM: Protein Chain-of-Thoughts Enhanced LLM for Protein-Protein Interaction Prediction|Mingyu Jin, Haochen Xue, Zhenting Wang, Boming Kang, Ruosong Ye, Kaixiong Zhou, Mengnan Du, Yongfeng Zhang|2024-03-30|arXiv|https://github.com/MingyuJ666/ProLLM|http://arxiv.org/abs/2405.06649v1|
|696|ST-LLM: Large Language Models Are Effective Temporal Learners|Ruyang Liu, Chen Li, Haoran Tang, Yixiao Ge, Ying Shan, Ge Li|2024-03-30|arXiv|https://github.com/TencentARC/ST-LLM|https://doi.org/10.48550/arXiv.2404.00308|
|697|MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models|Peng Ding, Jiading Fang, Peng Li, Kangrui Wang, Xiaochen Zhou, Mo Yu, Jing Li, Matthew R. Walter, Hongyuan Mei|2024-03-29|arXiv|https://github.com/oaklight/mango/|https://doi.org/10.48550/arXiv.2403.19913|
|698|DiJiang: Efficient Large Language Models through Compact Kernelization|Hanting Chen, Liuzhi Cheng, Xutao Wang, Yuchuan Tian, Yunhe Wang|2024-03-29|ICML|https://github.com/YuchuanTian/DiJiang|https://openreview.net/forum?id=0uUHfhXdnH|
|699|JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models|Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J. Pappas, Florian Tramèr, Hamed Hassani, Eric Wong|2024-03-28|arXiv|https://github.com/JailbreakBench/jailbreakbench|https://doi.org/10.48550/arXiv.2404.01318|
|700|TableLLM: Enabling Tabular Data Manipulation by LLMs in Real Office Usage Scenarios|Xiaokang Zhang, Jing Zhang, Zeyao Ma, Yang Li, Bohan Zhang, Guanlin Li, Zijun Yao, Kangli Xu, Jinchang Zhou, Daniel Zhang-Li, Jifan Yu, Shu Zhao, Juanzi Li, Jie Tang|2024-03-28|arXiv|https://tablellm.github.io/|http://arxiv.org/abs/2403.19318v2|
|701|Long-form factuality in large language models|Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, Quoc V. Le|2024-03-27|arXiv|https://github.com/google-deepmind/long-form-factuality|https://doi.org/10.48550/arXiv.2403.18802|
|702|Quantifying and Mitigating Unimodal Biases in Multimodal Large Language Models: A Causal Perspective|Meiqi Chen, Yixin Cao, Yan Zhang, Chaochao Lu|2024-03-27|arXiv|https://opencausalab.github.io/MORE|https://doi.org/10.48550/arXiv.2403.18346|
|703|SDSAT: Accelerating LLM Inference through Speculative Decoding with Semantic Adaptive Tokens|Chengbo Liu, Yong Zhu|2024-03-27|arXiv|https://github.com/hasuoshenyun/SDSAT|http://arxiv.org/abs/2403.18647v2|
|704|Robust and Scalable Model Editing for Large Language Models|Yingfa Chen, Zhengyan Zhang, Xu Han, Chaojun Xiao, Zhiyuan Liu, Chen Chen, Kuai Li, Tao Yang, Maosong Sun|2024-03-26|LREC/COLING|https://github.com/thunlp/EREN|https://aclanthology.org/2024.lrec-main.1235|
|705|TWOLAR: a TWO-step LLM-Augmented distillation method for passage Reranking|Davide Baldelli, Junfeng Jiang, Akiko Aizawa, Paolo Torroni|2024-03-26|Advances in Information Retrieval|https://github.com/Dundalia/TWOLAR|http://arxiv.org/abs/2403.17759v1|
|706|ShapeGrasp: Zero-Shot Task-Oriented Grasping with Large Language Models through Geometric Decomposition|Samuel Li, Sarthak Bhagat, Joseph Campbell, Yaqi Xie, Woojun Kim, Katia P. Sycara, Simon Stepputtis|2024-03-26|arXiv|https://shapegrasp.github.io/|https://doi.org/10.48550/arXiv.2403.18062|
|707|Don't Trust: Verify -- Grounding LLM Quantitative Reasoning with Autoformalization|Jin Peng Zhou, Charles Staats, Wenda Li, Christian Szegedy, Kilian Q. Weinberger, Yuhuai Wu|2024-03-26|12th International Conference on Learning Representations, ICLR 2024|https://github.com/jinpz/dtv|http://arxiv.org/abs/2403.18120v1|
|708|PCToolkit: A Unified Plug-and-Play Prompt Compression Toolkit of Large Language Models|Jinyi Li, Yihuai Lan, Lei Wang, Hao Wang|2024-03-26|arXiv|https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression|https://doi.org/10.48550/arXiv.2403.17411|
|709|Naive Bayes-based Context Extension for Large Language Models|Jianlin Su, Murtadha H. M. Ahmed, Bo Wen, Luo Ao, Mingren Zhu, Yunfeng Liu|2024-03-26|NAACL-HLT|https://github.com/amurtadha/NBCE-master|https://doi.org/10.18653/v1/2024.naacl-long.431|
|710|Can multiple-choice questions really be useful in detecting the abilities of LLMs?|Wangyue Li, Liangzhi Li, Tong Xiang, Xiao Liu, Wei Deng, Noa Garcia|2024-03-26|arXiv|https://github.com/Meetyou-AI-Lab/Can-MC-Evaluate-LLMs|http://arxiv.org/abs/2403.17752v3|
|711|LLMs Are Few-Shot In-Context Low-Resource Language Learners|Samuel Cahyawijaya, Holy Lovenia, Pascale Fung|2024-03-25|arXiv|https://github.com/SamuelCahyawijaya/in-context-alignment|http://arxiv.org/abs/2403.16512v5|
|712|Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-World Multi-Turn Dialogue|Songhua Yang, Hanjie Zhao, Senbin Zhu, Guangyu Zhou, Hongfei Xu, Yuxiang Jia, Hongying Zan|2024-03-25|AAAI|https://github.com/SupritYoung/Zhongjing|https://doi.org/10.1609/aaai.v38i17.29907|
|713|TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models|Ishika Singh, David Traum, Jesse Thomason|2024-03-25|arXiv|https://glamor-usc.github.io/twostep|https://doi.org/10.48550/arXiv.2403.17246|
|714|T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering|Lei Wang, Yi Hu, Jiabang He, Xing Xu, Ning Liu, Hui Liu, Heng Tao Shen|2024-03-25|AAAI|https://github.com/T-SciQ/T-SciQ|https://doi.org/10.1609/aaai.v38i17.29884|
|715|SeqGPT: An Out-of-the-Box Large Language Model for Open Domain Sequence Understanding|Tianyu Sun, Chengyue Jiang, Chao Lou, Shen Huang, Xiaobin Wang, Wei Liu, Jiong Cai, Yangning Li, Yinghui Li, Kewei Tu, Hai-Tao Zheng, Ningyu Zhang, Pengjun Xie, Fei Huang, Yong Jiang|2024-03-25|AAAI|https://github.com/Alibaba-NLP/SeqGPT|https://doi.org/10.1609/aaai.v38i17.29917|
|716|SayCanPay: Heuristic Planning with Large Language Models Using Learnable Domain Knowledge|Rishi Hazra, Pedro Zuidberg Dos Martires, Luc De Raedt|2024-03-25|AAAI|https://rishihazra.github.io/SayCanPay/|https://doi.org/10.1609/aaai.v38i18.29991|
|717|Reasoning Runtime Behavior of a Program with LLM: How Far Are We?|Junkai Chen, Zhiyuan Pan, Xing Hu, Zhenhao Li, Ge Li, Xin Xia|2024-03-25|arXiv|https://r-eval.github.io|http://arxiv.org/abs/2403.16437v2|
|718|ProAgent: Building Proactive Cooperative Agents with Large Language Models|Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, Yitao Liang, Yaodong Yang|2024-03-25|AAAI|https://pku-proagent.github.io|https://doi.org/10.1609/aaai.v38i16.29710|
|719|OWQ: Outlier-Aware Weight Quantization for Efficient Fine-Tuning and Inference of Large Language Models|Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok Park|2024-03-25|AAAI|https://github.com/xvyaward/owq|https://doi.org/10.1609/aaai.v38i12.29237|
|720|OUTFOX: LLM-Generated Essay Detection Through In-Context Learning with Adversarially Generated Examples|Ryuto Koike, Masahiro Kaneko, Naoaki Okazaki|2024-03-25|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/ryuryukke/OUTFOX|http://arxiv.org/abs/2307.11729v3|
|721|Narrowing the Gap between Supervised and Unsupervised Sentence Representation Learning with Large Language Model|Mingxin Li, Richong Zhang, Zhijie Nie, Yongyi Mao|2024-03-25|AAAI|https://github.com/BDBC-KG-NLP/NGCSE|https://doi.org/10.1609/aaai.v38i12.29263|
|722|Can Large Language Models Understand Real-World Complex Instructions?|Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Lida Chen, Xintao Wang, Yuncheng Huang, Haoning Ye, Zihan Li, Shisong Chen, Yikai Zhang, Zhouhong Gu, Jiaqing Liang, Yanghua Xiao|2024-03-25|AAAI|https://github.com/Abbey4799/CELLO|https://doi.org/10.1609/aaai.v38i16.29777|
|723|InstUPR : Instruction-based Unsupervised Passage Reranking with Large Language Models|Chao-Wei Huang, Yun-Nung Chen|2024-03-25|arXiv|https://github.com/MiuLab/InstUPR|https://doi.org/10.48550/arXiv.2403.16435|
|724|Comp4D: LLM-Guided Compositional 4D Scene Generation|Dejia Xu, Hanwen Liang, Neel P. Bhatt, Hezhen Hu, Hanxue Liang, Konstantinos N. Plataniotis, Zhangyang Wang|2024-03-25|arXiv|https://vita-group.github.io/Comp4D/|http://arxiv.org/abs/2403.16993v1|
|725|Harnessing the power of LLMs for normative reasoning in MASs|Bastin Tony Roy Savarimuthu, Surangika Ranathunga, Stephen Cranefield|2024-03-25|arXiv|https://coin-workshop.github.io/coine-2024-auckland/accepted_papers.html|http://arxiv.org/abs/2403.16524v1|
|726|AIOS: LLM Agent Operating System|Kai Mei, Zelong Li, Shuyuan Xu, Ruosong Ye, Yingqiang Ge, Yongfeng Zhang|2024-03-25|arXiv|https://github.com/agiresearch/AIOS|http://arxiv.org/abs/2403.16971v2|
|727|BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions|Wenbo Hu, Yifan Xu, Yi Li, Weiyue Li, Zeyuan Chen, Zhuowen Tu|2024-03-25|Proceedings of the AAAI Conference on Artificial Intelligence|https://github.com/mlpc-ucsd/BLIVA|http://arxiv.org/abs/2308.09936v3|
|728|A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators|Chen Zhang, Luis Fernando D&apos;Haro, Yiming Chen, Malu Zhang, Haizhou Li|2024-03-25|AAAI|https://github.com/e0397123/comp-analysis|https://doi.org/10.1609/aaai.v38i17.29923|
|729|Exploring Large Language Model for Graph Data Understanding in Online Job Recommendations|Likang Wu, Zhaopeng Qiu, Zhi Zheng, Hengshu Zhu, Enhong Chen|2024-03-25|AAAI|https://github.com/WLiK/GLRec|https://doi.org/10.1609/aaai.v38i8.28769|
|730|From Artificially Real to Real: Leveraging Pseudo Data from Large Language Models for Low-Resource Molecule Discovery|Yuhan Chen, Nuwa Xi, Yanrui Du, Haochun Wang, Jianyu Chen, Sendong Zhao, Bing Qin|2024-03-25|AAAI|https://github.com/SCIR-HI/ArtificiallyR2R|https://doi.org/10.1609/aaai.v38i20.30198|
|731|Graph Neural Prompting with Large Language Models|Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang, Nitesh V. Chawla, Panpan Xu|2024-03-25|AAAI|https://github.com/meettyj/GNP|https://doi.org/10.1609/aaai.v38i17.29875|
|732|Benchmarking Large Language Models on Controllable Generation under Diversified Instructions|Yihan Chen, Benfeng Xu, Quan Wang, Yi Liu, Zhendong Mao|2024-03-25|AAAI|https://github.com/Xt-cyh/CoDI-Eval|https://doi.org/10.1609/aaai.v38i16.29734|
|733|FEEL: A Framework for Evaluating Emotional Support Capability with Large Language Models|Huaiwen Zhang, Yu Chen, Ming Wang, Shi Feng|2024-03-23|arXiv|https://github.com/Ansisy/FEEL|https://doi.org/10.48550/arXiv.2403.15699|
|734|Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models|Huanxuan Liao, Shizhu He, Yao Xu, Yuanzhe Zhang, Kang Liu, Shengping Liu, Jun Zhao|2024-03-22|arXiv|https://github.com/Xnhyacinth/IAG|https://doi.org/10.48550/arXiv.2403.15268|
|735|LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement|Nicholas Lee, Thanakul Wattanawong, Sehoon Kim, Karttikeya Mangalam, Sheng Shen, Gopala Anumanchipalli, Michael W. Mahoney, Kurt Keutzer, Amir Gholami|2024-03-22|arXiv|https://github.com/SqueezeAILab/LLM2LLM|http://arxiv.org/abs/2403.15042v1|
|736|LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers|Abdur Rahman Bin Md Faizullah, Ashok Urlana, Rahul Mishra|2024-03-22|arXiv|https://github.com/arbmf/LimGen|http://arxiv.org/abs/2403.15529v2|
|737|ChainLM: Empowering Large Language Models with Improved Chain-of-Thought Prompting|Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen|2024-03-21|LREC/COLING|https://github.com/RUCAIBox/ChainLM|https://aclanthology.org/2024.lrec-main.265|
|738|MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?|Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Peng Gao, Hongsheng Li|2024-03-21|arXiv|https://mathverse-cuhk.github.io|http://arxiv.org/abs/2403.14624v1|
|739|Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation|Leyuan Sun, Asako Kanezaki, Guillaume Caron, Yusuke Yoshiyasu|2024-03-21|arXiv|https://sunleyuan.github.io/ObjectNav|https://doi.org/10.48550/arXiv.2403.14163|
|740|Empowering Segmentation Ability to Multi-modal Large Language Models|Yuqi Yang, Peng-Tao Jiang, Jing Wang, Hao Zhang, Kai Zhao, Jinwei Chen, Bo Li|2024-03-21|arXiv|https://github.com/YuqiYang213/LLaVASeg|https://doi.org/10.48550/arXiv.2403.14141|
|741|Detoxifying Large Language Models via Knowledge Editing|Mengru Wang, Ningyu Zhang, Ziwen Xu, Zekun Xi, Shumin Deng, Yunzhi Yao, Qishen Zhang, Linyi Yang, Jindong Wang, Huajun Chen|2024-03-21|arXiv|https://zjunlp.github.io/project/SafeEdit|https://doi.org/10.48550/arXiv.2403.14472|
|742|Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning|Changtong Zan, Liang Ding, Li Shen, Yibing Zhen, Weifeng Liu, Dacheng Tao|2024-03-21|arXiv|https://github.com/alphadl/LanguageAware_Tuning|http://arxiv.org/abs/2403.14399v1|
|743|AutoRE: Document-Level Relation Extraction with Large Language Models|Lilong Xue, Dan Zhang, Yuxiao Dong, Jie Tang|2024-03-21|arXiv|https://github.com/THUDM/AutoRE|https://doi.org/10.48550/arXiv.2403.14888|
|744|Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity|Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park|2024-03-21|arXiv|https://github.com/starsuzi/Adaptive-RAG|https://doi.org/10.48550/arXiv.2403.14403|
|745|Benchmarking Chinese Commonsense Reasoning of LLMs: From Chinese-Specifics to Reasoning-Memorization Correlations|Jiaxing Sun, Weiquan Huang, Jiang Wu, Chenya Gu, Wei Li, Songyang Zhang, Hang Yan, Conghui He|2024-03-21|OpenReview|https://github.com/opendatalab/CHARM|http://arxiv.org/abs/2403.14112v2|
|746|FMM-Attack: A Flow-based Multi-modal Adversarial Attack on Video-based LLMs|Jinmin Li, Kuofeng Gao, Yang Bai, Jingyun Zhang, Shu-tao Xia, Yisen Wang|2024-03-20|arXiv|https://github.com/THU-Kingmin/FMM-Attack|http://arxiv.org/abs/2403.13507v2|
|747|HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models|Wenqiao Zhang, Tianwei Lin, Jiang Liu, Fangxun Shu, Haoyuan Li, Lei Zhang, Wanggui He, Hao Zhou, Zheqi Lv, Hao Jiang, Juncheng Li, Siliang Tang, Yueting Zhuang|2024-03-20|arXiv|https://github.com/DCDmllm/HyperLLaVA|https://doi.org/10.48550/arXiv.2403.13447|
|748|Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model|Peng Zhou, Jianmin Wang, Chunyan Li, Zixu Wang, Yiping Liu, Chubo Liu, Siqi Sun, Jianxin Lin, Leyi Wei, Xibao Cai, Houtim Lai, Wei Liu, Longyue Wang, Xiangxiang Zeng, Kenli Li|2024-03-20|arXiv|https://github.com/HHW-zhou/TSMMG|https://doi.org/10.48550/arXiv.2403.13244|
|749|Agent-FLAN: Designing Data and Methods of Effective Agent Tuning for Large Language Models|Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, Feng Zhao|2024-03-19|arXiv|https://github.com/InternLM/Agent-FLAN|https://doi.org/10.48550/arXiv.2403.12881|
|750|Characteristic AI Agents via Large Language Models|Xi Wang, Hongliang Dai, Shen Gao, Piji Li|2024-03-19|LREC/COLING|https://github.com/nuaa-nlp/Character100|https://aclanthology.org/2024.lrec-main.269|
|751|Instructing Large Language Models to Identify and Ignore Irrelevant Conditions|Zhenyu Wu, Chao Shen, Meng Jiang|2024-03-19|arXiv|https://wzy6642.github.io/I3C.github.io/|https://doi.org/10.48550/arXiv.2403.12744|
|752|Towards Interpretable Hate Speech Detection using Large Language Model-extracted Rationales|Ayushi Nirmal, Amrita Bhattacharjee, Paras Sheth, Huan Liu|2024-03-19|arXiv|https://github.com/AmritaBh/shield|https://doi.org/10.48550/arXiv.2403.12403|
|753|LLM3: Large Language Model-based Task and Motion Planning with Motion Failure Reasoning|Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun Zhu, Hangxin Liu|2024-03-18|arXiv|https://github.com/AssassinWS/LLM-TAMP|https://doi.org/10.48550/arXiv.2403.11552|
|754|RouterBench: A Benchmark for Multi-LLM Routing System|Qitian Jason Hu, Jacob Bieker, Xiuyu Li, Nan Jiang, Benjamin Keigwin, Gaurav Ranganath, Kurt Keutzer, Shriyash Kaustubh Upadhyay|2024-03-18|arXiv|https://github.com/withmartian/routerbench|http://arxiv.org/abs/2403.12031v2|
|755|Metaphor Understanding Challenge Dataset for LLMs|Xiaoyu Tong, Rochelle Choenni, Martha Lewis, Ekaterina Shutova|2024-03-18|OpenReview|https://github.com/xiaoyuisrain/metaphor-understanding-challenge|http://arxiv.org/abs/2403.11810v1|
|756|Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs|M. Jehanzeb Mirza, Leonid Karlinsky, Wei Lin, Sivan Doveh, Jakub Micorek, Mateusz Kozinski, Hilde Kuehne, Horst Possegger|2024-03-18|arXiv|https://jmiemirza.github.io/Meta-Prompting/|http://arxiv.org/abs/2403.11755v3|
|757|Larimar: Large Language Models with Episodic Memory Control|Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarathkrishna Swaminathan, Sihui Dai, Aurélie C. Lozano, Georgios Kollias, Vijil Chenthamarakshan, Jirí Navrátil, Soham Dan, Pin-Yu Chen|2024-03-18|arXiv|https://github.com/IBM/larimar|https://doi.org/10.48550/arXiv.2403.11901|
|758|How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming Ability in Multi-Agent Environments|Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R. Lyu|2024-03-18|arXiv|https://github.com/CUHK-ARISE/GAMABench|http://arxiv.org/abs/2403.11807v2|
|759|EnvGen: Generating and Adapting Environments via LLMs for Training Embodied Agents|Abhay Zala, Jaemin Cho, Han Lin, Jaehong Yoon, Mohit Bansal|2024-03-18|arXiv|https://envgen-llm.github.io/|http://arxiv.org/abs/2403.12014v1|
|760|Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression|Junyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie, Kelsey Lieberman, James Diffenderfer, Brian Bartoldson, Ajay Jaiswal, Kaidi Xu, Bhavya Kailkhura, Dan Hendrycks, Dawn Song, Zhangyang Wang, Bo Li|2024-03-18|arXiv|https://decoding-comp-trust.github.io|http://arxiv.org/abs/2403.15447v3|
|761|Code Soliloquies for Accurate Calculations in Large Language Models|Shashank Sonkar, Myco Le, Xinghe Chen, Naiming Liu, Debshila Basu Mallick, Richard G. Baraniuk|2024-03-18|LAK '24: Proceedings of the 14th Learning Analytics and Knowledge Conference|https://github.com/luffycodes/Tutorbot-Spock-Phys|https://dl.acm.org/doi/10.1145/3636555.3636889|
|762|Large Language Models Powered Context-aware Motion Prediction|Xiaoji Zheng, Lixiu Wu, Zhijie Yan, Yuanrong Tang, Hao Zhao, Chen Zhong, Bokui Chen, Jiangtao Gong|2024-03-17|arXiv|https://github.com/AIR-DISCOVER/LLM-Augmented-MTR|https://doi.org/10.48550/arXiv.2403.11057|
|763|ManipVQA: Injecting Robotic Affordance and Physically Grounded Information into Multi-Modal Large Language Models|Siyuan Huang, Iaroslav Ponomarenko, Zhengkai Jiang, Xiaoqi Li, Xiaobin Hu, Peng Gao, Hongsheng Li, Hao Dong|2024-03-17|arXiv|https://github.com/SiyuanHuang95/ManipVQA|https://doi.org/10.48550/arXiv.2403.11289|
|764|S3LLM: Large-Scale Scientific Software Understanding with LLMs using Source, Metadata, and Document|Kareem Shaik, Dali Wang, Weijian Zheng, Qinglei Cao, Heng Fan, Peter Schwartz, Yunhe Feng|2024-03-15|arXiv|https://github.com/ResponsibleAILab/s3llm|http://arxiv.org/abs/2403.10588v1|
|765|AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting|Yu Wang, Xiaogeng Liu, Yu Li, Muhao Chen, Chaowei Xiao|2024-03-14|arXiv|https://github.com/rain305f/AdaShield|https://doi.org/10.48550/arXiv.2403.09513|
|766|CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences|Martin Weyssow, Aton Kamanda, Houari A. Sahraoui|2024-03-14|arXiv|https://github.com/martin-wey/CodeUltraFeedback|https://doi.org/10.48550/arXiv.2403.09032|
|767|Eyes Closed, Safety On: Protecting Multimodal LLMs via Image-to-Text Transformation|Yunhao Gou, Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung, James T. Kwok, Yu Zhang|2024-03-14|arXiv|https://gyhdog99.github.io/projects/ecso/|http://arxiv.org/abs/2403.09572v2|
|768|WavCraft: Audio Editing and Generation with Large Language Models|Jinhua Liang, Huan Zhang, Haohe Liu, Yin Cao, Qiuqiang Kong, Xubo Liu, Wenwu Wang, Mark D. Plumbley, Huy Phan, Emmanouil Benetos|2024-03-14|arXiv|https://github.com/JinhuaLiang/WavCraft|http://arxiv.org/abs/2403.09527v3|
|769|LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments|Maonan Wang, Aoyu Pang, Yuheng Kan, Man-On Pun, Chung Shue Chen, Bo Huang|2024-03-13|arXiv|https://github.com/Traffic-Alpha/LLM-Assisted-Light|https://doi.org/10.48550/arXiv.2403.08337|
|770|Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era|Xuansheng Wu, Haiyan Zhao, Yaochen Zhu, Yucheng Shi, Fan Yang, Tianming Liu, Xiaoming Zhai, Wenlin Yao, Jundong Li, Mengnan Du, Ninghao Liu|2024-03-13|arXiv|https://github.com/JacksonWuxs/UsableXAI_LLM|http://arxiv.org/abs/2403.08946v1|
|771|TeaMs-RL: Teaching LLMs to Generate Better Instruction Datasets via Reinforcement Learning|Shangding Gu, Alois Knoll, Ming Jin|2024-03-13|arXiv|https://github.com/SafeRL-Lab/TeaMs-RL|http://arxiv.org/abs/2403.08694v3|
|772|OverleafCopilot: Empowering Academic Writing in Overleaf with Large Language Models|Haomin Wen, Zhenjie Wei, Yan Lin, Jiyuan Wang, Yuxuan Liang, Huaiyu Wan|2024-03-13|arXiv|https://github.com/wenhaomin/ChatGPT-PromptGenius|https://doi.org/10.48550/arXiv.2403.09733|
|773|Large Language Models are Contrastive Reasoners|Liang Yao|2024-03-13|arXiv|https://github.com/yao8839836/cp|https://doi.org/10.48550/arXiv.2403.08211|
|774|AcademiaOS: Automating Grounded Theory Development in Qualitative Research with Large Language Models|Thomas Übellacker|2024-03-13|arXiv|https://github.com/thomasuebi/academia-os|https://doi.org/10.48550/arXiv.2403.08844|
|775|Empowering Robotics with Large Language Models: osmAG Map Comprehension with LLMs|Fujing Xie, Sören Schwertfeger|2024-03-13|arXiv|https://github.com/xiefujing/LLM-osmAG-Comprehension|https://doi.org/10.48550/arXiv.2403.08228|
|776|Cultural evolution in populations of Large Language Models|Jérémy Perez, Corentin Léger, Marcela Ovando Tellez, Chris Foulon, Joan Dussauld, Pierre-Yves Oudeyer, Clément Moulin-Frier|2024-03-13|arXiv|https://github.com/jeremyperez2/LLM-Culture|https://doi.org/10.48550/arXiv.2403.08882|
|777|Can Large Language Models Identify Authorship?|Baixiang Huang, Canyu Chen, Kai Shu|2024-03-13|arXiv|https://github.com/baixianghuang/authorship-llm|https://doi.org/10.48550/arXiv.2403.08213|
|778|Empowering Robot Path Planning with Large Language Models: osmAG Map Topology & Hierarchy Comprehension with LLMs|Fujing Xie, Sören Schwertfeger|2024-03-13|arXiv|https://github.com/xiefujing/LLM-osmAG-Comprehension|http://arxiv.org/abs/2403.08228v2|
|779|LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code|Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, Ion Stoica|2024-03-12|arXiv|https://livecodebench.github.io/|https://doi.org/10.48550/arXiv.2403.07974|
|780|Truth-Aware Context Selection: Mitigating Hallucinations of Large Language Models Being Misled by Untruthful Contexts|Tian Yu, Shaolei Zhang, Yang Feng|2024-03-12|ACL|https://github.com/ictnlp/TACS|https://aclanthology.org/2024.findings-acl.645|
|781|SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression|Xin Wang, Yu Zheng, Zhongwei Wan, Mi Zhang|2024-03-12|arXiv|https://github.com/AIoT-MLSys-Lab/SVD-LLM|https://doi.org/10.48550/arXiv.2403.07378|
|782|MoAI: Mixture of All Intelligence for Large Language and Vision Models|Byung-Kwan Lee, Beomchan Park, Chae Won Kim, Yong Man Ro|2024-03-12|arXiv|https://github.com/ByungKwanLee/MoAI|https://doi.org/10.48550/arXiv.2403.07508|
|783|NavCoT: Boosting LLM-Based Vision-and-Language Navigation via Learning Disentangled Reasoning|Bingqian Lin, Yunshuang Nie, Ziming Wei, Jiaqi Chen, Shikui Ma, Jianhua Han, Hang Xu, Xiaojun Chang, Xiaodan Liang|2024-03-12|arXiv|https://github.com/expectorlin/NavCoT|http://arxiv.org/abs/2403.07376v1|
|784|DeliGrasp: Inferring Object Properties with LLMs for Adaptive Grasp Policies|William Xie, Jensen Lavering, Nikolaus Correll|2024-03-12|arXiv|https://deligrasp.github.io|http://arxiv.org/abs/2403.07832v2|
|785|Debatrix: Multi-dimensional Debate Judge with Iterative Chronological Analysis Based on LLM|Jingcong Liang, Rong Ye, Meng Han, Ruofei Lai, Xinyu Zhang, Xuanjing Huang, Zhongyu Wei|2024-03-12|arXiv|https://github.com/ljcleo/debatrix|http://arxiv.org/abs/2403.08010v3|
|786|CodeAttack: Revealing Safety Generalization Challenges of Large Language Models via Code Completion|Qibing Ren, Chang Gao, Jing Shao, Junchi Yan, Xin Tan, Wai Lam, Lizhuang Ma|2024-03-12|ACL|https://github.com/renqibing/CodeAttack|https://aclanthology.org/2024.findings-acl.679|
|787|CALF: Aligning LLMs for Time Series Forecasting via Cross-modal Fine-Tuning|Peiyuan Liu, Hang Guo, Tao Dai, Naiqi Li, Jigang Bao, Xudong Ren, Yong Jiang, Shu-Tao Xia|2024-03-12|arXiv|https://github.com/Hank0626/LLaTA|http://arxiv.org/abs/2403.07300v2|
|788|Beyond Text: Frozen Large Language Models in Visual Signal Comprehension|Lei Zhu, Fangyun Wei, Yanye Lu|2024-03-12|arXiv|https://github.com/zh460045050/V2L-Tokenizer|https://doi.org/10.48550/arXiv.2403.07874|
|789|AC-EVAL: Evaluating Ancient Chinese Language Understanding in Large Language Models|Yuting Wei, Yuanxing Xu, Xinru Wei, Simin Yang, Yangfu Zhu, Yuqing Li, Di Liu, Bin Wu|2024-03-11|arXiv|https://github.com/yuting-wei/AC-EVAL|https://doi.org/10.48550/arXiv.2403.06574|
|790|Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?|Egor Zverev, Sahar Abdelnabi, Soroush Tabesh, Mario Fritz, Christoph H. Lampert|2024-03-11|arXiv|https://github.com/egozverev/Shold-It-Be-Executed-Or-Processed|http://arxiv.org/abs/2403.06833v2|
|791|Can LLMs' Tuning Methods Work in Medical Multimodal Domain?|Jiawei Chen, Yue Jiang, Dingkang Yang, Mingcheng Li, Jinjie Wei, Ziyun Qian, Lihua Zhang|2024-03-11|arXiv|https://github.com/TIMMY-CHAN/MILE|http://arxiv.org/abs/2403.06407v1|
|792|ConspEmoLLM: Conspiracy Theory Detection Using an Emotion-Based Large Language Model|Zhiwei Liu, Boyang Liu, Paul Thompson, Kailai Yang, Raghav Jain, Sophia Ananiadou|2024-03-11|arXiv|https://github.com/lzw108/ConspEmoLLM/|https://doi.org/10.48550/arXiv.2403.06765|
|793|DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation|Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan Huang, Xiaoyi Bao, Xingang Wang|2024-03-11|arXiv|https://drivedreamer2.github.io|http://arxiv.org/abs/2403.06845v2|
|794|Generative Expressive Robot Behaviors using Large Language Models|Karthik Mahadevan, Jonathan Chien, Noah Brown, Zhuo Xu, Carolina Parada, Fei Xia, Andy Zeng, Leila Takayama, Dorsa Sadigh|2024-03-11|HRI '24: Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction|https://generative-expressive-motion.github.io/|https://dl.acm.org/doi/10.1145/3610977.3634999|
|795|Smart-Infinity: Fast Large Language Model Training using Near-Storage Processing on a Real System|Hongsun Jang, Jaeyong Song, Jaewon Jung, Jaeyoung Park, Youngsok Kim, Jinho Lee|2024-03-11|2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)|https://github.com/AIS-SNU/smart-infinity|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476401|
|796|Walert: Putting Conversational Information Seeking Knowledge into Action by Building and Evaluating a Large Language Model-Powered Chatbot|Sachin Pathiyan Cherumanal, Lin Tian, Futoon M. Abushaqra, Angel Felipe Magnossão de Paula, Kaixin Ji, Halil Ali, Danula Hettiachchi, Johanne R. Trippas, Falk Scholer, Damiano Spina|2024-03-10|CHIIR '24: Proceedings of the 2024 Conference on Human Information Interaction and Retrieval|https://github.com/rmit-ir/walert|https://dl.acm.org/doi/10.1145/3627508.3638309|
|797|Editing Conceptual Knowledge for Large Language Models|Xiaohan Wang, Shengyu Mao, Ningyu Zhang, Shumin Deng, Yunzhi Yao, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen|2024-03-10|arXiv|https://github.com/zjunlp/EasyEdit|https://doi.org/10.48550/arXiv.2403.06259|
|798|Benchmarking Large Language Models for Molecule Prediction Tasks|Zhiqiang Zhong, Kuangyu Zhou, Davide Mottin|2024-03-08|arXiv|https://github.com/zhiqiangzhongddu/LLMaMol|https://doi.org/10.48550/arXiv.2403.05075|
|799|Exploring Human-Like Translation Strategy with Large Language Models|Zhiwei He, Tian Liang, Wenxiang Jiao, Zhuosheng Zhang, Yujiu Yang, Rui Wang, Zhaopeng Tu, Shuming Shi, Xing Wang|2024-03-08|Trans. Assoc. Comput. Linguistics|https://github.com/zwhe99/MAPS-mt|https://doi.org/10.1162/tacl_a_00642|
|800|GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM|Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, Tuo Zhao|2024-03-08|arXiv|https://github.com/HaoKang-Timmy/GEAR|http://arxiv.org/abs/2403.05527v2|
|801|LLM4Decompile: Decompiling Binary Code with Large Language Models|Hanzhuo Tan, Qi Luo, Jing Li, Yuqun Zhang|2024-03-08|arXiv|https://github.com/albertan017/LLM4Decompile|https://doi.org/10.48550/arXiv.2403.05286|
|802|SecGPT: An Execution Isolation Architecture for LLM-Based Systems|Yuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, Umar Iqbal|2024-03-08|arXiv|https://github.com/llm-platform-security/SecGPT|http://arxiv.org/abs/2403.04960v1|
|803|ELLA: Equip Diffusion Models with LLM for Enhanced Semantic Alignment|Xiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, Gang Yu|2024-03-08|arXiv|https://ella-diffusion.github.io/|http://arxiv.org/abs/2403.05135v1|
|804|Automatic and Universal Prompt Injection Attacks against Large Language Models|Xiaogeng Liu, Zhiyuan Yu, Yizhe Zhang, Ning Zhang, Chaowei Xiao|2024-03-07|arXiv|https://github.com/SheltonLiu-N/Universal-Prompt-Injection|https://doi.org/10.48550/arXiv.2403.04957|
|805|CAT: Enhancing Multimodal Large Language Model to Answer Questions in Dynamic Audio-Visual Scenarios|Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip H. S. Torr, Xiaochun Cao|2024-03-07|arXiv|https://github.com/rikeilong/Bay-CAT|https://doi.org/10.48550/arXiv.2403.04640|
|806|Do Large Language Model Understand Multi-Intent Spoken Language ?|Shangjian Yin, Peijie Huang, Yuhong Xu, Haojing Huang, Jiatian Chen|2024-03-07|arXiv|https://github.com/SJY8460/SLM|https://doi.org/10.48550/arXiv.2403.04481|
|807|Evaluation of LLMs on Syntax-Aware Code Fill-in-the-Middle Tasks|Linyuan Gong, Sida Wang, Mostafa Elhoushi, Alvin Cheung|2024-03-07|arXiv|https://github.com/gonglinyuan/safim|http://arxiv.org/abs/2403.04814v3|
|808|GraphInstruct: Empowering Large Language Models with Graph Understanding and Reasoning Capability|Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi Jiang, Xing Xie, Hai Jin|2024-03-07|arXiv|https://github.com/CGCL-codes/GraphInstruct|https://doi.org/10.48550/arXiv.2403.04483|
|809|LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error|Boshi Wang, Hao Fang, Jason Eisner, Benjamin Van Durme, Yu Su|2024-03-07|OpenReview|https://github.com/microsoft/simulated-trial-and-error|http://arxiv.org/abs/2403.04746v1|
|810|Rapidly Developing High-quality Instruction Data and Evaluation Benchmark for Large Language Models with Minimal Human Effort: A Case Study on Japanese|Yikun Sun, Zhen Wan, Nobuhiro Ueda, Sakiko Yahata, Fei Cheng, Chenhui Chu, Sadao Kurohashi|2024-03-06|LREC/COLING|https://github.com/hitoshizuku7/awesome-Ja-self-instruct|https://aclanthology.org/2024.lrec-main.1184|
|811|Towards Efficient and Effective Adaptation of Large Language Models for Sequential Recommendation|Hangyu Wang, Jianghao Lin, Bo Chen, Yang Yang, Ruiming Tang, Weinan Zhang, Yong Yu|2024-03-06|arXiv|https://github.com/justarter/E2URec|https://doi.org/10.48550/arXiv.2310.01612|
|812|SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models|Yibin Chen, Yifu Yuan, Zeyu Zhang, Yan Zheng, Jinyi Liu, Fei Ni, Jianye Hao|2024-03-06|arXiv|https://sheetagent.github.io|https://doi.org/10.48550/arXiv.2403.03636|
|813|Enhancing chest X-ray datasets with privacy-preserving large language models and multi-type annotations: a data-driven approach for improved classification|Ricardo Bigolin Lanfredi, Pritam Mukherjee, Ronald M. Summers|2024-03-06|arXiv|https://github.com/rsummers11/CADLab/tree/master/MAPLEZ_LLM_report_labeler/|https://doi.org/10.48550/arXiv.2403.04024|
|814|PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion|Yiduo Guo, Zekai Zhang, Yaobo Liang, Dongyan Zhao, Nan Duan|2024-03-06|arXiv|https://github.com/gydpku/PPTC|https://doi.org/10.48550/arXiv.2311.01767|
|815|Benchmarking Hallucination in Large Language Models Based on Unanswerable Math Word Problem|Yuhong Sun, Zhangyue Yin, Qipeng Guo, Jiawen Wu, Xipeng Qiu, Hui Zhao|2024-03-06|LREC/COLING|https://github.com/Yuki-Asuuna/UMWP|https://aclanthology.org/2024.lrec-main.196|
|816|Feast Your Eyes: Mixture-of-Resolution Adaptation for Multimodal Large Language Models|Gen Luo, Yiyi Zhou, Yuxin Zhang, Xiawu Zheng, Xiaoshuai Sun, Rongrong Ji|2024-03-05|arXiv|https://github.com/luogen1996/LLaVA-HR|https://doi.org/10.48550/arXiv.2403.03003|
|817|KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents|Yuqi Zhu, Shuofei Qiao, Yixin Ou, Shumin Deng, Ningyu Zhang, Shiwei Lyu, Yue Shen, Lei Liang, Jinjie Gu, Huajun Chen|2024-03-05|arXiv|https://zjunlp.github.io/project/KnowAgent/|http://arxiv.org/abs/2403.03101v1|
|818|LLMRec: Large Language Models with Graph Augmentation for Recommendation|Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei Yin, Chao Huang|2024-03-04|WSDM '24: Proceedings of the 17th ACM International Conference on Web Search and Data Mining|https://github.com/HKUDS/LLMRec|https://dl.acm.org/doi/10.1145/3616855.3635853|
|819|NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models|Lizhou Fan, Wenyue Hua, Xiang Li, Kaijie Zhu, Mingyu Jin, Lingyao Li, Haoyang Ling, Jinkui Chi, Jindong Wang, Xin Ma, Yongfeng Zhang|2024-03-04|arXiv|https://github.com/lizhouf/NPHardEval4V|https://doi.org/10.48550/arXiv.2403.01777|
|820|Rethinking LLM Language Adaptation: A Case Study on Chinese Mixtral|Yiming Cui, Xin Yao|2024-03-04|arXiv|https://github.com/ymcui/Chinese-Mixtral|http://arxiv.org/abs/2403.01851v1|
|821|SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis|Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun Wang, Mingjun Xu, Jin Huang, Fang Xi, Jiaxi Zhuang, Yuqi Yin, Yaqi Li, Changhong Chen, Zheng Cheng, Zifeng Zhao, Linfeng Zhang, Guolin Ke|2024-03-04|arXiv|https://sci-assess.github.io/|http://arxiv.org/abs/2403.01976v4|
|822|Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study|Yuan Sui, Mengyu Zhou, Mingjie Zhou, Shi Han, Dongmei Zhang|2024-03-04|WSDM '24: Proceedings of the 17th ACM International Conference on Web Search and Data Mining|https://github.com/microsoft/TableProvider|https://dl.acm.org/doi/10.1145/3616855.3635752|
|823|Temporal Blind Spots in Large Language Models|Jonas Wallat, Adam Jatowt, Avishek Anand|2024-03-04|WSDM '24: Proceedings of the 17th ACM International Conference on Web Search and Data Mining|https://github.com/jwallat/temporalblindspots|https://dl.acm.org/doi/10.1145/3616855.3635818|
|824|Fantastic Semantics and Where to Find Them: Investigating Which Layers of Generative LLMs Reflect Lexical Semantics|Zhu Liu, Cunliang Kong, Ying Liu, Maosong Sun|2024-03-03|OpenReview|https://github.com/RyanLiut/LLM_LexSem|http://arxiv.org/abs/2403.01509v2|
|825|AutoDefense: Multi-Agent LLM Defense against Jailbreak Attacks|Yifan Zeng, Yiran Wu, Xiao Zhang, Huazheng Wang, Qingyun Wu|2024-03-02|arXiv|https://github.com/XHMY/AutoDefense|http://arxiv.org/abs/2403.04783v1|
|826|NoMAD-Attention: Efficient LLM Inference on CPUs Through Multiply-add-free Attention|Tianyi Zhang, Jonah Wonkyu Yi, Bowen Yao, Zhaozhuo Xu, Anshumali Shrivastava|2024-03-02|arXiv|https://github.com/tonyzhang617/nomad-dist|http://arxiv.org/abs/2403.01273v1|
|827|IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact|Ruikang Liu, Haoli Bai, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, Chun Yuan|2024-03-02|arXiv|https://github.com/ruikangliu/IntactKV|https://doi.org/10.48550/arXiv.2403.01241|
|828|Attribute Structuring Improves LLM-Based Evaluation of Clinical Text Summaries|Zelalem Gero, Chandan Singh, Yiqing Xie, Sheng Zhang, Tristan Naumann, Jianfeng Gao, Hoifung Poon|2024-03-01|arXiv|https://github.com/microsoft/attribute-structuring|http://arxiv.org/abs/2403.01002v1|
|829|TempCompass: Do Video LLMs Really Understand Videos?|Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, Lu Hou|2024-03-01|arXiv|https://github.com/llyx97/TempCompass|http://arxiv.org/abs/2403.00476v3|
|830|Exploring the Potential of Large Language Models (LLMs)in Learning on Graphs|Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, Jiliang Tang|2024-03|ACM SIGKDD Explorations Newsletter (SIGKDD), Volume 25, Issue 2|https://github.com/CurryTang/Graph-LLM|https://dl.acm.org/doi/10.1145/3655103.3655110|
|831|Curiosity-driven Red-teaming for Large Language Models|Zhang-Wei Hong, Idan Shenfeld, Tsun-Hsuan Wang, Yung-Sung Chuang, Aldo Pareja, James R. Glass, Akash Srivastava, Pulkit Agrawal|2024-02-29|arXiv|https://github.com/Improbable-AI/curiosity_redteam|https://doi.org/10.48550/arXiv.2402.19464|
|832|FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning|Xupeng Miao, Gabriele Oliaro, Xinhao Cheng, Mengdi Wu, Colin Unger, Zhihao Jia|2024-02-29|arXiv|https://github.com/flexflow/FlexFlow|https://doi.org/10.48550/arXiv.2402.18789|
|833|GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers|Qintong Li, Leyang Cui, Xueliang Zhao, Lingpeng Kong, Wei Bi|2024-02-29|OpenReview|https://github.com/qtli/GSM-Plus|http://arxiv.org/abs/2402.19255v1|
|834|Teaching Large Language Models an Unseen Language on the Fly|Chen Zhang, Xiao Liu, Jiuheng Lin, Yansong Feng|2024-02-29|arXiv|https://github.com/luciusssss/ZhuangBench|https://doi.org/10.48550/arXiv.2402.19167|
|835|Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models|Chen Qian, Jie Zhang, Wei Yao, Dongrui Liu, Zhenfei Yin, Yu Qiao, Yong Liu, Jing Shao|2024-02-29|arXiv|https://github.com/ChnQ/TracingLLM|https://doi.org/10.48550/arXiv.2402.19465|
|836|Evaluating Quantized Large Language Models|Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang|2024-02-28|arXiv|https://github.com/thu-nics/qllm-eval|https://doi.org/10.48550/arXiv.2402.18158|
|837|Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models|Mingjia Huo, Sai Ashish Somayajula, Youwei Liang, Ruisi Zhang, Farinaz Koushanfar, Pengtao Xie|2024-02-28|arXiv|https://github.com/mignonjia/TS_watermark|https://doi.org/10.48550/arXiv.2402.18059|
|838|The First Place Solution of WSDM Cup 2024: Leveraging Large Language Models for Conversational Multi-Doc QA|Yiming Li, Zhao Zhang|2024-02-28|arXiv|https://github.com/zhangzhao219/WSDM-Cup-2024|https://doi.org/10.48550/arXiv.2402.18385|
|839|ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training|Le Zhuo, Zewen Chi, Minghao Xu, Heyan Huang, Heqi Zheng, Conghui He, Xian-Ling Mao, Wentao Zhang|2024-02-28|OpenReview|https://protllm.github.io/project/|http://arxiv.org/abs/2403.07920v1|
|840|FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability|Congying Xia, Chen Xing, Jiangshu Du, Xinyi Yang, Yihao Feng, Ran Xu, Wenpeng Yin, Caiming Xiong|2024-02-28|OpenReview|https://github.com/SalesforceAIResearch/FoFo|http://arxiv.org/abs/2402.18667v1|
|841|Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards|Haoxiang Wang, Yong Lin, Wei Xiong, Rui Yang, Shizhe Diao, Shuang Qiu, Han Zhao, Tong Zhang|2024-02-28|arXiv|https://github.com/Haoxiang-Wang/directional-preference-alignment|http://arxiv.org/abs/2402.18571v3|
|842|Datasets for Large Language Models: A Comprehensive Survey|Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, Lianwen Jin|2024-02-28|arXiv|https://github.com/lmmlzn/Awesome-LLMs-Datasets|https://doi.org/10.48550/arXiv.2402.18041|
|843|Data Interpreter: An LLM Agent For Data Science|Sirui Hong, Yizhang Lin, Bang Liu, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Li Zhang, Lingyao Zhang, Min Yang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xiawu Zheng, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zongze Xu, Chenglin Wu|2024-02-28|arXiv|https://github.com/geekan/MetaGPT|http://arxiv.org/abs/2402.18679v3|
|844|Beyond Natural Language: LLMs Leveraging Alternative Formats for Enhanced Reasoning and Communication|Weize Chen, Chenfei Yuan, Jiarui Yuan, Yusheng Su, Chen Qian, Cheng Yang, Ruobing Xie, Zhiyuan Liu, Maosong Sun|2024-02-28|OpenReview|https://github.com/thunlp/AutoForm|http://arxiv.org/abs/2402.18439v3|
|845|A New Era in LLM Security: Exploring Security Concerns in Real-World LLM-based Systems|Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, Chaowei Xiao|2024-02-28|arXiv|https://fzwark.github.io/LLM-System-Attack-Demo/|http://arxiv.org/abs/2402.18649v1|
|846|Massive Activations in Large Language Models|Mingjie Sun, Xinlei Chen, J. Zico Kolter, Zhuang Liu|2024-02-27|arXiv|https://eric-mingjie.github.io/massive-activations/index.html|https://doi.org/10.48550/arXiv.2402.17762|
|847|Training-Free Long-Context Scaling of Large Language Models|Chenxin An, Fei Huang, Jun Zhang, Shansan Gong, Xipeng Qiu, Chang Zhou, Lingpeng Kong|2024-02-27|arXiv|https://github.com/HKUNLP/ChunkLlama|https://doi.org/10.48550/arXiv.2402.17463|
|848|The Foundational Capabilities of Large Language Models in Predicting Postoperative Risks Using Clinical Notes|Charles Alba, Bing Xue, Joanna Abraham, Thomas Kannampallil, Chenyang Lu|2024-02-27|arXiv|https://github.com/cja5553/LLMs_in_perioperative_care|http://arxiv.org/abs/2402.17493v5|
|849|Probing Multimodal Large Language Models for Global and Local Semantic Representation|Mingxu Tao, Quzhe Huang, Kun Xu, Liwei Chen, Yansong Feng, Dongyan Zhao|2024-02-27|arXiv|https://github.com/kobayashikanna01/probing_MLLM_rep|https://doi.org/10.48550/arXiv.2402.17304|
|850|TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space|Shaolei Zhang, Tian Yu, Yang Feng|2024-02-27|arXiv|https://ictnlp.github.io/TruthX-site/|https://doi.org/10.48550/arXiv.2402.17811|
|851|MATHSENSEI: A Tool-Augmented Large Language Model for Mathematical Reasoning|Debrup Das, Debopriyo Banerjee, Somak Aditya, Ashish Kulkarni|2024-02-27|arXiv|https://github.com/Debrup-61/MathSensei|https://doi.org/10.48550/arXiv.2402.17231|
|852|DS-Agent: Automated Data Science by Empowering Large Language Models with Case-Based Reasoning|Siyuan Guo, Cheng Deng, Ying Wen, Hechang Chen, Yi Chang, Jun Wang|2024-02-27|arXiv|https://github.com/guosyjlu/DS-Agent|https://doi.org/10.48550/arXiv.2402.17453|
|853|Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data|Xiao Liu, Zirui Wu, Xueqing Wu, Pan Lu, Kai-Wei Chang, Yansong Feng|2024-02-27|OpenReview|https://xxxiaol.github.io/QRData/|http://arxiv.org/abs/2402.17644v2|
|854|Evaluating Very Long-Term Conversational Memory of LLM Agents|Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, Yuwei Fang|2024-02-27|arXiv|https://snap-research.github.io/locomo/|http://arxiv.org/abs/2402.17753v1|
|855|OncoGPT: A Medical Conversational Model Tailored with Oncology Domain Expertise on a Large Language Model Meta-AI (LLaMA)|Fujian Jia, Xin Liu, Lixi Deng, Jiwen Gu, Chunchao Pu, Tunan Bai, Mengjiang Huang, Yuanzhi Lu, Kang Liu|2024-02-26|arXiv|https://github.com/OncoGPT1|https://doi.org/10.48550/arXiv.2402.16810|
|856|TEaR: Improving LLM-based Machine Translation with Systematic Self-Refinement|Zhaopeng Feng, Yan Zhang, Hao Li, Bei Wu, Jiayu Liao, Wenqiang Liu, Jun Lang, Yang Feng, Jian Wu, Zuozhu Liu|2024-02-26|arXiv|https://github.com/fzp0424/self_correct_mt|http://arxiv.org/abs/2402.16379v3|
|857|ShieldLM: Empowering LLMs as Aligned, Customizable and Explainable Safety Detectors|Zhexin Zhang, Yida Lu, Jingyuan Ma, Di Zhang, Rui Li, Pei Ke, Hao Sun, Lei Sha, Zhifang Sui, Hongning Wang, Minlie Huang|2024-02-26|arXiv|https://github.com/thu-coai/ShieldLM|http://arxiv.org/abs/2402.16444v1|
|858|SelectIT: Selective Instruction Tuning for Large Language Models via Uncertainty-Aware Self-Reflection|Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, Min Zhang|2024-02-26|arXiv|https://github.com/Blue-Raincoat/SelectIT|https://doi.org/10.48550/arXiv.2402.16705|
|859|RepoAgent: An LLM-Powered Open-Source Framework for Repository-level Code Documentation Generation|Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong, Yankai Lin, Yingli Zhang, Xiaoyin Che, Zhiyuan Liu, Maosong Sun|2024-02-26|arXiv|https://github.com/OpenBMB/RepoAgent|http://arxiv.org/abs/2402.16667v1|
|860|ProLLaMA: A Protein Large Language Model for Multi-Task Protein Language Processing|Liuzhenghao Lv, Zongying Lin, Hao Li, Yuyang Liu, Jiaxi Cui, Calvin Yu-Chian Chen, Li Yuan, Yonghong Tian|2024-02-26|arXiv|https://github.com/Lyu6PosHao/ProLLaMA|https://doi.org/10.48550/arXiv.2402.16445|
|861|Mysterious Projections: Multimodal LLMs Gain Domain-Specific Visual Capabilities Without Richer Cross-Modal Projections|Gaurav Verma, Minje Choi, Kartik Sharma, Jamelle Watson-Daniels, Sejoon Oh, Srijan Kumar|2024-02-26|arXiv|https://claws-lab.github.io/projection-in-MLLMs/|http://arxiv.org/abs/2402.16832v1|
|862|MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property|Shiwen Ni, Minghuan Tan, Yuelin Bai, Fuqiang Niu, Min Yang, Bowen Zhang, Ruifeng Xu, Xiaojun Chen, Chengming Li, Xiping Hu, Ye Li, Jianping Fan|2024-02-26|LREC/COLING|https://github.com/AI-for-Science/MoZi|https://aclanthology.org/2024.lrec-main.1018|
|863|GROUNDHOG: Grounding Large Language Models to Holistic Segmentation|Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, Joyce Chai|2024-02-26|arXiv|https://groundhog-mllm.github.io/|https://doi.org/10.48550/arXiv.2402.16846|
|864|DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models|Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang|2024-02-26|arXiv|https://github.com/WailordHe/DenseSSM|https://doi.org/10.48550/arXiv.2403.00818|
|865|Defending LLMs against Jailbreaking Attacks via Backtranslation|Yihan Wang, Zhouxing Shi, Andrew Bai, Cho-Jui Hsieh|2024-02-26|arXiv|https://github.com/YihanWang617/llm-jailbreaking-defense|http://arxiv.org/abs/2402.16459v3|
|866|Cross-Modal Projection in Multimodal LLMs Doesn't Really Project Visual Attributes to Textual Space|Gaurav Verma, Minje Choi, Kartik Sharma, Jamelle Watson-Daniels, Sejoon Oh, Srijan Kumar|2024-02-26|arXiv|https://claws-lab.github.io/projection-in-MLLMs/|http://arxiv.org/abs/2402.16832v2|
|867|How Large Language Models Encode Context Knowledge? A Layer-Wise Probing Study|Tianjie Ju, Weiwei Sun, Wei Du, Xinwei Yuan, Zhaochun Ren, Gongshen Liu|2024-02-25|LREC/COLING|https://github.com/Jometeorie/probing_llama|https://aclanthology.org/2024.lrec-main.722|
|868|LLMs with Chain-of-Thought Are Non-Causal Reasoners|Guangsheng Bao, Hongbo Zhang, Linyi Yang, Cunxiang Wang, Yue Zhang|2024-02-25|arXiv|https://github.com/StevenZHB/CoT_Causal_Analysis|http://arxiv.org/abs/2402.16048v1|
|869|InstructEdit: Instruction-based Knowledge Editing for Large Language Models|Bozhong Tian, Siyuan Cheng, Xiaozhuan Liang, Ningyu Zhang, Yi Hu, Kouying Xue, Yanjie Gou, Xi Chen, Huajun Chen|2024-02-25|arXiv|https://github.com/zjunlp/EasyEdit|https://doi.org/10.48550/arXiv.2402.16123|
|870|Defending Large Language Models against Jailbreak Attacks via Semantic Smoothing|Jiabao Ji, Bairu Hou, Alexander Robey, George J. Pappas, Hamed Hassani, Yang Zhang, Eric Wong, Shiyu Chang|2024-02-25|arXiv|https://github.com/UCSB-NLP-Chang/SemanticSmooth|https://doi.org/10.48550/arXiv.2402.16192|
|871|How Can LLM Guide RL? A Value-Based Approach|Shenao Zhang, Sirui Zheng, Shuqi Ke, Zhihan Liu, Wanxin Jin, Jianbo Yuan, Yingxiang Yang, Hongxia Yang, Zhaoran Wang|2024-02-25|arXiv|https://github.com/agentification/Language-Integrated-VI|http://arxiv.org/abs/2402.16181v1|
|872|DrAttack: Prompt Decomposition and Reconstruction Makes Powerful LLM Jailbreakers|Xirui Li, Ruochen Wang, Minhao Cheng, Tianyi Zhou, Cho-Jui Hsieh|2024-02-25|arXiv|https://github.com/xirui-li/DrAttack|http://arxiv.org/abs/2402.16914v2|
|873|Stepwise Self-Consistent Mathematical Reasoning with Large Language Models|Zilong Zhao, Yao Rong, Dongyang Guo, Emek Gözlüklü, Emir Gülboy, Enkelejda Kasneci|2024-02-24|arXiv|https://github.com/zhao-zilong/ssc-cot|https://doi.org/10.48550/arXiv.2402.17786|
|874|An Empirical Study of Data Ability Boundary in LLMs' Math Reasoning|Zui Chen, Yezeng Chen, Jiaqi Han, Zhijie Huang, Ji Qi, Yi Zhou|2024-02-23|OpenReview|https://github.com/cyzhh/MMOS|http://arxiv.org/abs/2403.00799v1|
|875|GraphEdit: Large Language Models for Graph Structure Learning|Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang Pang, Tat-Seng Chua, Chao Huang|2024-02-23|arXiv|https://github.com/HKUDS/GraphEdit|https://doi.org/10.48550/arXiv.2402.15183|
|876|Interactive-KBQA: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models|Guanming Xiong, Junwei Bao, Wen Zhao|2024-02-23|arXiv|https://github.com/JimXiongGM/Interactive-KBQA|https://doi.org/10.48550/arXiv.2402.15131|
|877|KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models|Zhuohao Yu, Chang Gao, Wenjin Yao, Yidong Wang, Wei Ye, Jindong Wang, Xing Xie, Yue Zhang, Shikun Zhang|2024-02-23|arXiv|https://github.com/zhuohaoyu/KIEval|https://doi.org/10.48550/arXiv.2402.15043|
|878|Machine Unlearning of Pre-trained Large Language Models|Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, Xiang Yue|2024-02-23|arXiv|https://github.com/yaojin17/Unlearning_LLM|https://doi.org/10.48550/arXiv.2402.15159|
|879|UFO: a Unified and Flexible Framework for Evaluating Factuality of Large Language Models|Zhaoheng Huang, Zhicheng Dou, Yutao Zhu, Ji-Rong Wen|2024-02-22|arXiv|https://github.com/WaldenRUC/UFO|https://doi.org/10.48550/arXiv.2402.14690|
|880|MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues|Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, Wanli Ouyang|2024-02-22|arXiv|https://github.com/mtbench101/mt-bench-101|https://doi.org/10.48550/arXiv.2402.14762|
|881|Take the Bull by the Horns: Hard Sample-Reweighted Continual Training Improves LLM Generalization|Xuxi Chen, Zhendong Wang, Daouda Sow, Junjie Yang, Tianlong Chen, Yingbin Liang, Mingyuan Zhou, Zhangyang Wang|2024-02-22|arXiv|https://github.com/VITA-Group/HardFocusTraining|http://arxiv.org/abs/2402.14270v2|
|882|Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models|Xudong Lu, Qi Liu, Yuhui Xu, Aojun Zhou, Siyuan Huang, Bo Zhang, Junchi Yan, Hongsheng Li|2024-02-22|arXiv|https://github.com/Lucky-Lance/Expert_Sparsity|https://doi.org/10.48550/arXiv.2402.14800|
|883|GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data|Lele Cao, Valentin Buchner, Zineb Senane, Fangkai Yang|2024-02-22|arXiv|https://github.com/llcresearch/GenCeption|http://arxiv.org/abs/2402.14973v3|
|884|Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation|Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Noboru Koshizuka, Chuan Xiao|2024-02-22|arXiv|https://github.com/Wangjw6/LLMob/|https://doi.org/10.48550/arXiv.2402.14744|
|885|Hint-before-Solving Prompting: Guiding LLMs to Effectively Utilize Encoded Knowledge|Jinlan Fu, Shenzhen Huangfu, Hang Yan, See-Kiong Ng, Xipeng Qiu|2024-02-22|OpenReview|https://github.com/jinlanfu/HSP|http://arxiv.org/abs/2402.14310v1|
|886|Data Science with LLMs and Interpretable Models|Sebastian Bordt, Ben Lengerich, Harsha Nori, Rich Caruana|2024-02-22|arXiv|https://github.com/interpretml/TalkToEBM|http://arxiv.org/abs/2402.14474v1|
|887|Introducing GenCeption for Multimodal LLM Benchmarking: You May Bypass Annotations|Lele Cao, Valentin Buchner, Zineb Senane, Fangkai Yang|2024-02-22|arXiv|https://github.com/llcresearch/GenCeption|http://arxiv.org/abs/2402.14973v2|
|888|GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis|Yueqi Xie, Minghong Fang, Renjie Pi, Neil Gong|2024-02-21|OpenReview|https://github.com/xyq7/GradSafe|http://arxiv.org/abs/2402.13494v2|
|889|Round Trip Translation Defence against Large Language Model Jailbreaking Attacks|Canaan Yung, Hadi Mohaghegh Dolatabadi, Sarah M. Erfani, Christopher Leckie|2024-02-21|arXiv|https://github.com/Cancanxxx/Round_Trip_Translation_Defence|https://doi.org/10.48550/arXiv.2402.13517|
|890|Privacy-Preserving Instructions for Aligning Large Language Models|Da Yu, Peter Kairouz, Sewoong Oh, Zheng Xu|2024-02-21|arXiv|https://github.com/google-research/google-research/tree/master/dp_instructions|https://doi.org/10.48550/arXiv.2402.13659|
|891|OMGEval: An Open Multilingual Generative Evaluation Benchmark for Large Language Models|Yang Liu, Meng Xu, Shuo Wang, Liner Yang, Haoyu Wang, Zhenghao Liu, Cunliang Kong, Yun Chen, Yang Liu, Maosong Sun, Erhong Yang|2024-02-21|arXiv|https://github.com/blcuicall/OMGEval|https://doi.org/10.48550/arXiv.2402.13524|
|892|MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms|Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, Srijan Kumar|2024-02-21|arXiv|https://github.com/claws-lab/MMSoc|https://doi.org/10.48550/arXiv.2402.14154|
|893|PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain|Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, Baobao Chang|2024-02-21|arXiv|https://github.com/pkunlp-icler/PCA-EVAL|https://doi.org/10.48550/arXiv.2402.15527|
|894|Dynamic Evaluation of Large Language Models by Meta Probing Agents|Kaijie Zhu, Jindong Wang, Qinlin Zhao, Ruochen Xu, Xing Xie|2024-02-21|ICML|https://github.com/microsoft/promptbench|https://openreview.net/forum?id=DwTgy1hXXo|
|895|CriticBench: Evaluating Large Language Models as Critic|Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, Xian-ling Mao|2024-02-21|arXiv|https://github.com/open-compass/CriticBench|https://doi.org/10.48550/arXiv.2402.13764|
|896|Coercing LLMs to do and reveal (almost) anything|Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, Tom Goldstein|2024-02-21|arXiv|https://github.com/JonasGeiping/carving|http://arxiv.org/abs/2402.14020v1|
|897|Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models|Zhiwei He, Binglin Zhou, Hongkun Hao, Aiwei Liu, Xing Wang, Zhaopeng Tu, Zhuosheng Zhang, Rui Wang|2024-02-21|arXiv|https://github.com/zwhe99/X-SIR|https://doi.org/10.48550/arXiv.2402.14007|
|898|CODIS: Benchmarking Context-Dependent Visual Comprehension for Multimodal Large Language Models|Fuwen Luo, Chi Chen, Zihao Wan, Zhaolu Kang, Qidong Yan, Yingjie Li, Xiaolong Wang, Siyu Wang, Ziyue Wang, Xiaoyue Mi, Peng Li, Ning Ma, Maosong Sun, Yang Liu|2024-02-21|arXiv|https://thunlp-mt.github.io/CODIS|https://doi.org/10.48550/arXiv.2402.13607|
|899|ARL2: Aligning Retrievers for Black-box Large Language Models via Self-guided Adaptive Relevance Labeling|Lingxi Zhang, Yue Yu, Kuan Wang, Chao Zhang|2024-02-21|arXiv|https://github.com/zhanglingxi-cs/ARL2|https://doi.org/10.48550/arXiv.2402.13542|
|900|CIF-Bench: A Chinese Instruction-Following Benchmark for Evaluating the Generalizability of Large Language Models|Yizhi Li, Ge Zhang, Xingwei Qu, Jiali Li, Zhaoqun Li, Zekun Wang, Hao Li, Ruibin Yuan, Yinghao Ma, Kai Zhang, Wangchunshu Zhou, Yiming Liang, Lei Zhang, Lei Ma, Jiajun Zhang, Zuowen Li, Stephen W. Huang, Chenghua Lin, Wenhu Chen, Jie Fu|2024-02-20|ACL|https://yizhilll.github.io/CIF-Bench/|https://aclanthology.org/2024.findings-acl.739|
|901|Large Language Model-based Human-Agent Collaboration for Complex Task Solving|Xueyang Feng, Zhi-Yuan Chen, Yujia Qin, Yankai Lin, Xu Chen, Zhiyuan Liu, Ji-Rong Wen|2024-02-20|arXiv|https://github.com/XueyangFeng/ReHAC|https://doi.org/10.48550/arXiv.2402.12914|
|902|Learning to Check: Unleashing Potentials for Self-Correction in Large Language Models|Che Zhang, Zhenyang Xiao, Chengcheng Han, Yixin Lian, Yuejian Fang|2024-02-20|arXiv|https://github.com/bammt/Learn-to-check|https://doi.org/10.48550/arXiv.2402.13035|
|903|Me LLaMA: Foundation Large Language Models for Medical Applications|Qianqian Xie, Qingyu Chen, Aokun Chen, Cheng Peng, Yan Hu, Fongci Lin, Xueqing Peng, Jimin Huang, Jeffrey Zhang, Vipina Kuttichi Keloth, Xingyu Zhou, Huan He, Lucila Ohno-Machido, Yonghui Wu, Hua Xu, Jiang Bian|2024-02-20|arXiv|https://github.com/BIDS-Xu-Lab/Me-LLaMA|https://doi.org/10.48550/arXiv.2402.12749|
|904|Model Composition for Multimodal Large Language Models|Chi Chen, Yiyang Du, Zheng Fang, Ziyue Wang, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun, Yang Liu|2024-02-20|arXiv|https://github.com/THUNLP-MT/ModelCompose|https://doi.org/10.48550/arXiv.2402.12750|
|905|MuLan: Multimodal-LLM Agent for Progressive and Interactive Multi-Object Diffusion|Sen Li, Ruochen Wang, Cho-Jui Hsieh, Minhao Cheng, Tianyi Zhou|2024-02-20|arXiv|https://github.com/measure-infinity/mulan-code|http://arxiv.org/abs/2402.12741v2|
|906|TreeEval: Benchmark-Free Evaluation of Large Language Models through Tree Planning|Xiang Li, Yunshi Lan, Chao Yang|2024-02-20|arXiv|https://github.com/Ashura5/TreeEval|https://doi.org/10.48550/arXiv.2402.13125|
|907|What if LLMs Have Different World Views: Simulating Alien Civilizations with LLM-based Agents|Mingyu Jin, Beichen Wang, Zhaoqian Xue, Suiyuan Zhu, Wenyue Hua, Hua Tang, Kai Mei, Mengnan Du, Yongfeng Zhang|2024-02-20|arXiv|https://github.com/agiresearch/AlienAgent|http://arxiv.org/abs/2402.13184v2|
|908|MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs|Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Chenyang Tao, Dimitrios Dimitriadis, Salman Avestimehr|2024-02-19|OpenReview|https://github.com/Ybakman/LLM_Uncertainity|http://arxiv.org/abs/2402.11756v3|
|909|Remember This Event That Year? Assessing Temporal Information and Reasoning in Large Language Models|Himanshu Beniwal, Dishant Patel, Kowsik Nandagopan D, Hritik Ladia, Ankit Yadav, Mayank Singh|2024-02-19|arXiv|https://github.com/lingoiitgn/TempUN|https://doi.org/10.48550/arXiv.2402.11997|
|910|How Interpretable are Reasoning Explanations from Prompting Large Language Models?|Wei Jie Yeo, Ranjan Satapathy, Rick Siow Mong Goh, Erik Cambria|2024-02-19|arXiv|https://github.com/SenticNet/CoT_interpretability|https://doi.org/10.48550/arXiv.2402.11863|
|911|Shall We Talk: Exploring Spontaneous Collaborations of Competing LLM Agents|Zengqing Wu, Shuyuan Zheng, Qianying Liu, Xu Han, Brian Inhyuk Kwon, Makoto Onizuka, Shaojie Tang, Run Peng, Chuan Xiao|2024-02-19|arXiv|https://github.com/wuzengqing001225/SABM_ShallWeTalk|http://arxiv.org/abs/2402.12327v1|
|912|Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs|Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, Ji-Rong Wen|2024-02-19|OpenReview|https://github.com/plageon/SlimPLM|http://arxiv.org/abs/2402.12052v3|
|913|Learning to Edit: Aligning LLMs with Knowledge Editing|Yuxin Jiang, Yufei Wang, Chuhan Wu, Wanjun Zhong, Xingshan Zeng, Jiahui Gao, Liangyou Li, Xin Jiang, Lifeng Shang, Ruiming Tang, Qun Liu, Wei Wang|2024-02-19|OpenReview|https://github.com/YJiangcm/LTE|http://arxiv.org/abs/2402.11905v2|
|914|Shall We Team Up: Exploring Spontaneous Cooperation of Competing LLM Agents|Zengqing Wu, Run Peng, Shuyuan Zheng, Qianying Liu, Xu Han, Brian Inhyuk Kwon, Makoto Onizuka, Shaojie Tang, Chuan Xiao|2024-02-19|arXiv|https://github.com/wuzengqing001225/SABM_ShallWeTeamUp|http://arxiv.org/abs/2402.12327v2|
|915|Emulated Disalignment: Safety Alignment for Large Language Models May Backfire!|Zhanhui Zhou, Jie Liu, Zhichen Dong, Jiaheng Liu, Chao Yang, Wanli Ouyang, Yu Qiao|2024-02-19|arXiv|https://github.com/ZHZisZZ/emulated-disalignment|https://doi.org/10.48550/arXiv.2402.12343|
|916|ArtPrompt: ASCII Art-based Jailbreak Attacks against Aligned LLMs|Fengqing Jiang, Zhangchen Xu, Luyao Niu, Zhen Xiang, Bhaskar Ramasubramanian, Bo Li, Radha Poovendran|2024-02-19|arXiv|https://github.com/uw-nsl/ArtPrompt|http://arxiv.org/abs/2402.11753v4|
|917|A Chinese Dataset for Evaluating the Safeguards in Large Language Models|Yuxia Wang, Zenan Zhai, Haonan Li, Xudong Han, Lizhi Lin, Zhenxuan Zhang, Jingru Zhao, Preslav Nakov, Timothy Baldwin|2024-02-19|arXiv|https://github.com/Libr-AI/do-not-answer|https://doi.org/10.48550/arXiv.2402.12193|
|918|AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling|Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin Zhang, Ruibin Yuan, Ge Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui, Tianxiang Sun, Yugang Jiang, Xipeng Qiu|2024-02-19|OpenReview|https://junzhan2000.github.io/AnyGPT.github.io/|http://arxiv.org/abs/2402.12226v3|
|919|EmoBench: Evaluating the Emotional Intelligence of Large Language Models|Sahand Sabour, Siyang Liu, Zheyuan Zhang, June M. Liu, Jinfeng Zhou, Alvionna S. Sunaryo, Juanzi Li, Tatia M. C. Lee, Rada Mihalcea, Minlie Huang|2024-02-19|arXiv|https://github.com/Sahandfer/EmoBench|https://doi.org/10.48550/arXiv.2402.12071|
|920|Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models|Jiahao Ying, Yixin Cao, Yushi Bai, Qianru Sun, Bo Wang, Wei Tang, Zhaojun Ding, Yizhe Yang, Xuanjing Huang, Shuicheng Yan|2024-02-19|arXiv|https://yingjiahao14.github.io/Automating-DatasetUpdates/|http://arxiv.org/abs/2402.11894v3|
|921|Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models|Loka Li, Guangyi Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric P. Xing, Kun Zhang|2024-02-19|arXiv|https://github.com/MBZUAI-CLeaR/IoE-Prompting|https://doi.org/10.48550/arXiv.2402.12563|
|922|Aligning Modalities in Vision Large Language Models via Preference Fine-tuning|Yiyang Zhou, Chenhang Cui, Rafael Rafailov, Chelsea Finn, Huaxiu Yao|2024-02-18|arXiv|https://github.com/YiyangZhou/POVID|https://doi.org/10.48550/arXiv.2402.11411|
|923|ModelGPT: Unleashing LLM's Capabilities for Tailored Model Generation|Zihao Tang, Zheqi Lv, Shengyu Zhang, Fei Wu, Kun Kuang|2024-02-18|arXiv|https://github.com/IshiKura-a/ModelGPT|http://arxiv.org/abs/2402.12408v1|
|924|Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark|Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D. Lee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, Tianlong Chen|2024-02-18|arXiv|https://github.com/ZO-Bench/ZO-LLM|http://arxiv.org/abs/2402.11592v3|
|925|Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement|Wenda Xu, Guanglei Zhu, Xuandong Zhao, Liangming Pan, Lei Li, William Yang Wang|2024-02-18|arXiv|https://github.com/xu1998hz/llm_self_bias|http://arxiv.org/abs/2402.11436v2|
|926|Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?|Guijin Son, Sangwon Baek, Sangdae Nam, Ilgyun Jeong, Seungone Kim|2024-02-18|arXiv|https://github.com/guijinSON/MTI-Bench|https://doi.org/10.48550/arXiv.2402.11597|
|927|BESA: Pruning Large Language Models with Blockwise Parameter-Efficient Sparsity Allocation|Peng Xu, Wenqi Shao, Mengzhao Chen, Shitao Tang, Kaipeng Zhang, Peng Gao, Fengwei An, Yu Qiao, Ping Luo|2024-02-18|arXiv|https://github.com/OpenGVLab/LLMPrune-BESA|https://doi.org/10.48550/arXiv.2402.16880|
|928|Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network|Lin Chen, Fengli Xu, Nian Li, Zhenyu Han, Meng Wang, Yong Li, Pan Hui|2024-02-18|arXiv|https://github.com/LinChen-65/ReStruct|https://doi.org/10.48550/arXiv.2402.11518|
|929|Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs|Siyuan Wang, Zhongyu Wei, Yejin Choi, Xiang Ren|2024-02-18|OpenReview|https://github.com/SiyuanWangw/ULogic|http://arxiv.org/abs/2402.11442v3|
|930|Benchmark Self-Evolving: A Multi-Agent Framework for Dynamic LLM Evaluation|Siyuan Wang, Zhuohan Long, Zhihao Fan, Zhongyu Wei, Xuanjing Huang|2024-02-18|arXiv|https://github.com/NanshineLoong/Self-Evolving-Benchmark|http://arxiv.org/abs/2402.11443v1|
|931|Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models|Wenxuan Wang, Yihang Su, Jingyuan Huan, Jie Liu, Wenting Chen, Yudi Zhang, Cheng-Yi Li, Kao-Jung Chang, Xiaohan Xin, Linlin Shen, Michael R. Lyu|2024-02-17|arXiv|https://asclepius-med.github.io/|https://doi.org/10.48550/arXiv.2402.11217|
|932|Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models|Sijia Chen, Baochun Li, Di Niu|2024-02-17|arXiv|https://github.com/iQua/llmpebase/tree/main/examples/BoTReasoning|https://doi.org/10.48550/arXiv.2402.11140|
|933|CoLLaVO: Crayon Large Language and Vision mOdel|Byung-Kwan Lee, Beomchan Park, Chae Won Kim, Yong Man Ro|2024-02-17|arXiv|https://github.com/ByungKwanLee/CoLLaVO|https://doi.org/10.48550/arXiv.2402.11248|
|934|Dissecting Human and LLM Preferences|Junlong Li, Fan Zhou, Shichao Sun, Yikai Zhang, Hai Zhao, Pengfei Liu|2024-02-17|arXiv|https://github.com/GAIR-NLP/Preference-Dissection|http://arxiv.org/abs/2402.11296v1|
|935|Watch Out for Your Agents! Investigating Backdoor Threats to LLM-Based Agents|Wenkai Yang, Xiaohan Bi, Yankai Lin, Sishuo Chen, Jie Zhou, Xu Sun|2024-02-17|arXiv|https://github.com/lancopku/agent-backdoor-attacks|http://arxiv.org/abs/2402.11208v1|
|936|When is Tree Search Useful for LLM Planning? It Depends on the Discriminator|Ziru Chen, Michael White, Raymond Mooney, Ali Payani, Yu Su, Huan Sun|2024-02-16|OpenReview|https://github.com/OSU-NLP-Group/llm-planning-eval|http://arxiv.org/abs/2402.10890v2|
|937|When LLMs Meet Cunning Questions: A Fallacy Understanding Benchmark for Large Language Models|Yinghui Li, Qingyu Zhou, Yuanzhen Luo, Shirong Ma, Yangning Li, Hai-Tao Zheng, Xuming Hu, Philip S. Yu|2024-02-16|arXiv|https://github.com/THUKElab/FLUB|https://doi.org/10.48550/arXiv.2402.11100|
|938|ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages|Junjie Ye, Sixian Li, Guanyu Li, Caishuang Huang, Songyang Gao, Yilong Wu, Qi Zhang, Tao Gui, Xuanjing Huang|2024-02-16|arXiv|https://github.com/Junjie-Ye/ToolSword|https://doi.org/10.48550/arXiv.2402.10753|
|939|Large Language Models as Zero-shot Dialogue State Tracker through Function Calling|Zekun Li, Zhiyu Zoey Chen, Mike Ross, Patrick Huber, Seungwhan Moon, Zhaojiang Lin, Xin Luna Dong, Adithya Sagar, Xifeng Yan, Paul A. Crook|2024-02-16|ACL|https://github.com/facebookresearch/FnCTOD|https://aclanthology.org/2024.acl-long.471|
|940|DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows|Ajay Patel, Colin Raffel, Chris Callison-Burch|2024-02-16|OpenReview|https://github.com/datadreamer-dev/DataDreamer|http://arxiv.org/abs/2402.10379v2|
|941|BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation|Dayou Du, Yijia Zhang, Shijie Cao, Jiaqi Guo, Ting Cao, Xiaowen Chu, Ningyi Xu|2024-02-16|arXiv|https://github.com/DD-DuDa/BitDistiller|http://arxiv.org/abs/2402.10631v1|
|942|Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs|Yeonhong Park, Jake Hyun, SangLyul Cho, Bonggeun Sim, Jae W. Lee|2024-02-16|arXiv|https://github.com/SNU-ARC/any-precision-llm|http://arxiv.org/abs/2402.10517v4|
|943|AI Hospital: Benchmarking Large Language Models in a Multi-agent Medical Interaction Simulator|Zhihao Fan, Jialong Tang, Wei Chen, Siyuan Wang, Zhongyu Wei, Jun Xi, Fei Huang, Jingren Zhou|2024-02-15|arXiv|https://github.com/LibertFan/AI_Hospital|http://arxiv.org/abs/2402.09742v3|
|944|NutePrune: Efficient Progressive Pruning with Numerous Teachers for Large Language Models|Shengrui Li, Junzhe Chen, Xueting Han, Jing Bai|2024-02-15|arXiv|https://github.com/Lucius-lsr/NutePrune|https://doi.org/10.48550/arXiv.2402.09773|
|945|PAL: Proxy-Guided Black-Box Attack on Large Language Models|Chawin Sitawarin, Norman Mu, David A. Wagner, Alexandre Araujo|2024-02-15|arXiv|https://github.com/chawins/pal|https://doi.org/10.48550/arXiv.2402.09674|
|946|SwissNYF: Tool Grounded LLM Agents for Black Box Setting|Somnath Sendhil Kumar, Dhruv Jain, Eshaan Agarwal, Raunak Pandey|2024-02-15|arXiv|https://github.com/iclr-dummy-user/SwissNYF|http://arxiv.org/abs/2402.10051v1|
|947|Uncertainty Quantification for In-Context Learning of Large Language Models|Chen Ling, Xujiang Zhao, Xuchao Zhang, Wei Cheng, Yanchi Liu, Yiyou Sun, Mika Oishi, Takao Osaki, Katsushi Matsuda, Jie Ji, Guangji Bai, Liang Zhao, Haifeng Chen|2024-02-15|NAACL-HLT|https://github.com/lingchen0331/UQ_ICL|https://doi.org/10.18653/v1/2024.naacl-long.184|
|948|Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference|Harry Dong, Xinyu Yang, Zhenyu Zhang, Zhangyang Wang, Yuejie Chi, Beidi Chen|2024-02-14|arXiv|https://github.com/hdong920/LESS|http://arxiv.org/abs/2402.09398v2|
|949|SLEB: Streamlining LLMs through Redundancy Verification and Elimination of Transformer Blocks|Jiwon Song, Kyungseok Oh, Taesu Kim, Hyungjun Kim, Yulhwa Kim, Jae-Joon Kim|2024-02-14|arXiv|https://github.com/jiwonsong-dev/SLEB|http://arxiv.org/abs/2402.09025v4|
|950|Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey|Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, Yu Qiao|2024-02-14|Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024|https://github.com/niconi19/LLM-conversation-safety|http://arxiv.org/abs/2402.09283v3|
|951|AQA-Bench: An Interactive Benchmark for Evaluating LLMs' Sequential Reasoning Ability|Siwei Yang, Bingchen Zhao, Cihang Xie|2024-02-14|arXiv|https://github.com/UCSC-VLAA/AQA-Bench|http://arxiv.org/abs/2402.09404v1|
|952|COLD-Attack: Jailbreaking LLMs with Stealthiness and Controllability|Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, Bin Hu|2024-02-13|arXiv|https://github.com/Yu-Fangxu/COLD-Attack|http://arxiv.org/abs/2402.08679v2|
|953|LLaGA: Large Language and Graph Assistant|Runjin Chen, Tong Zhao, Ajay Kumar Jaiswal, Neil Shah, Zhangyang Wang|2024-02-13|arXiv|https://github.com/VITA-Group/LLaGA|https://doi.org/10.48550/arXiv.2402.08170|
|954|Test-Time Backdoor Attacks on Multimodal Large Language Models|Dong Lu, Tianyu Pang, Chao Du, Qian Liu, Xianjun Yang, Min Lin|2024-02-13|arXiv|https://sail-sg.github.io/AnyDoor/|https://doi.org/10.48550/arXiv.2402.08577|
|955|VerMCTS: Synthesizing Multi-Step Programs using a Verifier, a Large Language Model, and Tree Search|David Brandfonbrener, Simon Henniger, Sibi Raja, Tarun Prasad, Chloe Loughridge, Federico Cassano, Sabrina Ruixin Hu, Jianang Yang, William E. Byrd, Robert Zinkov, Nada Amin|2024-02-13|arXiv|https://github.com/namin/llm-verified-with-monte-carlo-tree-search|http://arxiv.org/abs/2402.08147v2|
|956|eCeLLM: Generalizing Large Language Models for E-commerce from Large-scale, High-quality Instruction Data|Bo Peng, Xinyi Ling, Ziru Chen, Huan Sun, Xia Ning|2024-02-13|arXiv|https://ninglab.github.io/eCeLLM|https://doi.org/10.48550/arXiv.2402.08831|
|957|Agent Smith: A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast|Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, Min Lin|2024-02-13|arXiv|https://sail-sg.github.io/Agent-Smith/|http://arxiv.org/abs/2402.08567v2|
|958|BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection|Kang Zhang, Osamu Yoshie, Weiran Huang|2024-02-12|arXiv|https://github.com/Neviim96/BreakGPT|https://doi.org/10.48550/arXiv.2402.07536|
|959|PoisonedRAG: Knowledge Poisoning Attacks to Retrieval-Augmented Generation of Large Language Models|Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia|2024-02-12|arXiv|https://github.com/sleeepeer/PoisonedRAG|https://doi.org/10.48550/arXiv.2402.07867|
|960|Refined Direct Preference Optimization with Synthetic Data for Behavioral Alignment of LLMs|Víctor Gallego|2024-02-12|arXiv|https://github.com/vicgalle/refined-dpo|http://arxiv.org/abs/2402.08005v1|
|961|Relative Preference Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse Prompts|Yueqin Yin, Zhendong Wang, Yi Gu, Hai Huang, Weizhu Chen, Mingyuan Zhou|2024-02-12|arXiv|https://github.com/yinyueqin/relative-preference-optimization|http://arxiv.org/abs/2402.10958v2|
|962|GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators|Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Dong Zhang, Zhehuai Chen, Eng Siong Chng|2024-02-10|arXiv|https://github.com/YUCHEN005/GenTranslate|https://doi.org/10.48550/arXiv.2402.06894|
|963|UrbanKGent: A Unified Large Language Model Agent Framework for Urban Knowledge Graph Construction|Yansong Ning, Hao Liu|2024-02-10|arXiv|https://github.com/usail-hkust/UrbanKGent|https://doi.org/10.48550/arXiv.2402.06861|
|964|Gemini Goes to Med School: Exploring the Capabilities of Multimodal Large Language Models on Medical Challenge Problems &amp; Hallucinations|Ankit Pal, Malaikannan Sankarasubbu|2024-02-10|arXiv|https://github.com/promptslab/RosettaEval|https://doi.org/10.48550/arXiv.2402.07023|
|965|Debating with More Persuasive LLMs Leads to More Truthful Answers|Akbir Khan, John Hughes, Dan Valentine, Laura Ruis, Kshitij Sachan, Ansh Radhakrishnan, Edward Grefenstette, Samuel R. Bowman, Tim Rocktäschel, Ethan Perez|2024-02-09|arXiv|https://github.com/ucl-dark/llm_debate|http://arxiv.org/abs/2402.06782v3|
|966|InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning|Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, Yudong Wang, Zijian Wu, Shuaibin Li, Fengzhe Zhou, Hongwei Liu, Songyang Zhang, Wenwei Zhang, Hang Yan, Xipeng Qiu, Jiayu Wang, Kai Chen, Dahua Lin|2024-02-09|arXiv|https://github.com/InternLM/InternLM-Math|https://doi.org/10.48550/arXiv.2402.06332|
|967|Benchmarking Large Language Models on Communicative Medical Coaching: A Dataset and a Novel System|Hengguan Huang, Songtao Wang, Hongfu Liu, Hao Wang, Ye Wang|2024-02-08|ACL|https://github.com/zerowst/Chatcoach|https://aclanthology.org/2024.findings-acl.94|
|968|Driving Everywhere with Large Language Model Policy Adaptation|Boyi Li, Yue Wang, Jiageng Mao, Boris Ivanovic, Sushant Veer, Karen Leung, Marco Pavone|2024-02-08|arXiv|https://boyiliee.github.io/llada|https://doi.org/10.48550/arXiv.2402.05932|
|969|EmojiCrypt: Prompt Encryption for Secure Communication with Large Language Models|Guo Lin, Wenyue Hua, Yongfeng Zhang|2024-02-08|arXiv|https://github.com/agiresearch/EmojiCrypt|https://doi.org/10.48550/arXiv.2402.05868|
|970|Rapid Optimization for Jailbreaking LLMs via Subconscious Exploitation and Echopraxia|Guangyu Shen, Siyuan Cheng, Kaiyuan Zhang, Guanhong Tao, Shengwei An, Lu Yan, Zhuo Zhang, Shiqing Ma, Xiangyu Zhang|2024-02-08|arXiv|https://github.com/SolidShen/RIPPLE_official/tree/official|http://arxiv.org/abs/2402.05467v1|
|971|SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models|Dongyang Liu, Renrui Zhang, Longtian Qiu, Siyuan Huang, Weifeng Lin, Shitian Zhao, Shijie Geng, Ziyi Lin, Peng Jin, Kaipeng Zhang, Wenqi Shao, Chao Xu, Conghui He, Junjun He, Hao Shao, Pan Lu, Yu Qiao, Hongsheng Li, Peng Gao|2024-02-08|ICML|https://github.com/Alpha-VLLM/LLaMA2-Accessory|https://openreview.net/forum?id=tDMlQkJRhZ|
|972|Self-Alignment of Large Language Models via Monopolylogue-based Social Scene Simulation|Xianghe Pang, Shuo Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng Wang, Siheng Chen|2024-02-08|arXiv|https://shuotang123.github.io/MATRIX|https://doi.org/10.48550/arXiv.2402.05699|
|973|Training Large Language Models for Reasoning through Reverse Curriculum Reinforcement Learning|Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi Zhang, Xuanjing Huang|2024-02-08|arXiv|https://github.com/WooooDyy/LLM-Reverse-Curriculum-RL|https://doi.org/10.48550/arXiv.2402.05808|
|974|SALAD-Bench: A Hierarchical and Comprehensive Safety Benchmark for Large Language Models|Lijun Li, Bowen Dong, Ruohui Wang, Xuhao Hu, Wangmeng Zuo, Dahua Lin, Yu Qiao, Jing Shao|2024-02-07|arXiv|https://github.com/OpenSafetyLab/SALAD-BENCH|https://doi.org/10.48550/arXiv.2402.05044|
|975|MLLM-as-a-Judge: Assessing Multimodal LLM-as-a-Judge with Vision-Language Benchmark|Dongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Yao Wan, Pan Zhou, Lichao Sun|2024-02-07|arXiv|https://mllm-judge.github.io/|http://arxiv.org/abs/2402.04788v3|
|976|MEMORYLLM: Towards Self-Updatable Large Language Models|Yu Wang, Yifan Gao, Xiusi Chen, Haoming Jiang, Shiyang Li, Jingfeng Yang, Qingyu Yin, Zheng Li, Xian Li, Bing Yin, Jingbo Shang, Julian J. McAuley|2024-02-07|arXiv|https://github.com/wangyu-ustc/MemoryLLM|https://doi.org/10.48550/arXiv.2402.04624|
|977|InfLLM: Training-Free Long-Context Extrapolation for LLMs with an Efficient Context Memory|Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Maosong Sun|2024-02-07|arXiv|https://github.com/thunlp/InfLLM|http://arxiv.org/abs/2402.04617v2|
|978|Leveraging LLMs for Unsupervised Dense Retriever Ranking|Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, Guido Zuccon|2024-02-07|arXiv|https://github.com/ielab/larmor/|http://arxiv.org/abs/2402.04853v2|
|979|A Sober Look at LLMs for Material Discovery: Are They Actually Good for Bayesian Optimization Over Molecules?|Agustinus Kristiadi, Felix Strieth-Kalthoff, Marta Skreta, Pascal Poupart, Alán Aspuru-Guzik, Geoff Pleiss|2024-02-07|arXiv|https://github.com/wiseodd/lapeft-bayesopt|http://arxiv.org/abs/2402.05015v2|
|980|ApiQ: Finetuning of 2-Bit Quantized Large Language Model|Baohao Liao, Christian Herold, Shahram Khadivi, Christof Monz|2024-02-07|arXiv|https://github.com/BaohaoLiao/ApiQ|https://doi.org/10.48550/arXiv.2402.05147|
|981|ANLS* - A Universal Document Processing Metric for Generative Large Language Models|David Peer, Philemon Schöpf, Volckmar Nebendahl, Alexander Rietzler, Sebastian Stabinger|2024-02-06|arXiv|https://github.com/deepopinion/anls_star_metric|https://doi.org/10.48550/arXiv.2402.03848|
|982|BiLLM: Pushing the Limit of Post-Training Quantization for LLMs|Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, Xiaojuan Qi|2024-02-06|arXiv|https://github.com/Aaronhuang-778/BiLLM|http://arxiv.org/abs/2402.04291v2|
|983|Discovery of the Hidden World with Large Language Models|Chenxi Liu, Yongqiang Chen, Tongliang Liu, Mingming Gong, James Cheng, Bo Han, Kun Zhang|2024-02-06|arXiv|https://causalcoat.github.io/|https://doi.org/10.48550/arXiv.2402.03941|
|984|DistiLLM: Towards Streamlined Distillation for Large Language Models|Jongwoo Ko, Sungnyun Kim, Tianyi Chen, Se-Young Yun|2024-02-06|arXiv|https://github.com/jongwooko/distillm|https://doi.org/10.48550/arXiv.2402.03898|
|985|Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models|Brenda Y. Miao, Christopher Y. K. Williams, Ebenezer Chinedu-Eneh, Travis Zack, Emily Alsentzer, Atul J. Butte, Irene Y. Chen|2024-02-06|arXiv|https://github.com/BMiao10/contraceptive-switching|https://doi.org/10.48550/arXiv.2402.03597|
|986|Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs|Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, Ondřej Dušek|2024-02-06|OpenReview|https://leak-llm.github.io/|http://arxiv.org/abs/2402.03927v2|
|987|EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models|Yixin Ou, Ningyu Zhang, Honghao Gui, Ziwen Xu, Shuofei Qiao, Yida Xue, Runnan Fang, Kangwei Liu, Lei Li, Zhen Bi, Guozhou Zheng, Huajun Chen|2024-02-05|arXiv|https://zjunlp.github.io/project/EasyInstruct|https://doi.org/10.48550/arXiv.2402.03049|
|988|Graph-enhanced Large Language Models in Asynchronous Plan Reasoning|Fangru Lin, Emanuele La Malfa, Valentin Hofmann, Elle Michelle Yang, Anthony G. Cohn, Janet B. Pierrehumbert|2024-02-05|arXiv|https://github.com/fangru-lin/graph-llm-asynchow-plan|https://doi.org/10.48550/arXiv.2402.02805|
|989|Shortened LLaMA: Depth Pruning for Large Language Models with Comparison of Retraining Methods|Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells, Shinkook Choi, Junho Shin, Hyoung-Kyu Song|2024-02-05|arXiv|https://github.com/Nota-NetsPresso/shortened-llm|http://arxiv.org/abs/2402.02834v2|
|990|Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models|Zhiyuan Hu, Chumin Liu, Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan Luu, Junxian He, Pang Wei Koh, Bryan Hooi|2024-02-05|arXiv|https://github.com/zhiyuanhubj/UoT|https://doi.org/10.48550/arXiv.2402.03271|
|991|LQER: Low-Rank Quantization Error Reconstruction for LLMs|Cheng Zhang, Jianyi Cheng, George A. Constantinides, Yiren Zhao|2024-02-04|arXiv|https://github.com/ChengZhang-98/lqer|http://arxiv.org/abs/2402.02446v3|
|992|A Survey of Large Language Models in Finance (FinLLMs)|Jean Lee, Nicholas Stevens, Soyeon Caren Han, Minseok Song|2024-02-04|arXiv|https://github.com/adlnlp/FinLLMs|https://doi.org/10.48550/arXiv.2402.02315|
|993|GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model|Xuanchang Zhang, Zhuosheng Zhang, Hai Zhao|2024-02-04|arXiv|https://github.com/thunderous77/GLaPE|https://doi.org/10.48550/arXiv.2402.02408|
|994|Break the Sequential Dependency of LLM Inference Using Lookahead Decoding|Yichao Fu, Peter Bailis, Ion Stoica, Hao Zhang|2024-02-03|arXiv|https://github.com/hao-ai-lab/LookaheadDecoding|http://arxiv.org/abs/2402.02057v1|
|995|Safety Fine-Tuning at (Almost) No Cost: A Baseline for Vision Large Language Models|Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, Timothy M. Hospedales|2024-02-03|arXiv|https://github.com/ys-zong/VLGuard|https://doi.org/10.48550/arXiv.2402.02207|
|996|Distilling LLMs' Decomposition Abilities into Compact Language Models|Denis Tarasov, Kumar Shridhar|2024-02-02|arXiv|https://github.com/DT6A/GSM8K-AI-SubQ|http://arxiv.org/abs/2402.01812v1|
|997|KB-Plugin: A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases|Jiajie Zhang, Shulin Cao, Linmei Hu, Ling Feng, Lei Hou, Juanzi Li|2024-02-02|arXiv|https://github.com/THU-KEG/KB-Plugin|https://doi.org/10.48550/arXiv.2402.01619|
|998|Large Language Models as Hyper-Heuristics for Combinatorial Optimization|Haoran Ye, Jiarui Wang, Zhiguang Cao, Federico Berto, Chuanbo Hua, Haeyeon Kim, Jinkyoo Park, Guojie Song|2024-02-02|arXiv|https://github.com/ai4co/LLM-as-HH|http://arxiv.org/abs/2402.01145v2|
|999|Leveraging Large Language Models for Structure Learning in Prompted Weak Supervision|Jinyan Su, Peilin Yu, Jieyu Zhang, Stephen H. Bach|2024-02-02|2023 IEEE International Conference on Big Data (BigData)|https://github.com/BatsResearch/su-bigdata23-code|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386190|
|1000|LimSim++: A Closed-Loop Platform for Deploying Multimodal LLMs in Autonomous Driving|Daocheng Fu, Wenjie Lei, Licheng Wen, Pinlong Cai, Song Mao, Min Dou, Botian Shi, Yu Qiao|2024-02-02|arXiv|https://pjlab-adg.github.io/limsim-plus/|http://arxiv.org/abs/2402.01246v2|
|1001|Vaccine: Perturbation-aware Alignment for Large Language Model|Tiansheng Huang, Sihao Hu, Ling Liu|2024-02-02|arXiv|https://github.com/git-disl/Vaccine|https://doi.org/10.48550/arXiv.2402.01109|
|1002|Vaccine: Perturbation-aware Alignment for Large Language Models against Harmful Fine-tuning|Tiansheng Huang, Sihao Hu, Ling Liu|2024-02-02|arXiv|https://github.com/git-disl/Vaccine|http://arxiv.org/abs/2402.01109v4|
|1003|Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents|Zelong Li, Wenyue Hua, Hao Wang, He Zhu, Yongfeng Zhang|2024-02-01|arXiv|https://github.com/agiresearch/Formal-LLM|http://arxiv.org/abs/2402.00798v3|
|1004|When Benchmarks are Targets: Revealing the Sensitivity of Large Language Model Leaderboards|Norah Alzahrani, Hisham Abdullah Alyahya, Yazeed Alnumay, Sultan Alrashed, Shaykhah Alsubaie, Yousef Almushayqih, Faisal Mirza, Nouf Alotaibi, Nora Al-Twairesh, Areeb Alowisheq, M. Saiful Bari, Haidar Khan|2024-02-01|ACL|https://github.com/National-Center-for-AI-Saudi-Arabia/lm-evaluation-harness|https://aclanthology.org/2024.acl-long.744|
|1005|Safety of Multimodal Large Language Models on Images and Text|Xin Liu, Yichen Zhu, Yunshi Lan, Chao Yang, Yu Qiao|2024-02-01|arXiv|https://github.com/isXinLiu/MLLM-Safety-Collection|https://doi.org/10.48550/arXiv.2402.00357|
|1006|Exploring Spatial Schema Intuitions in Large Language and Vision Models|Philipp Wicke, Lennart Wachowiak|2024-02-01|arXiv|https://cisnlp.github.io/Spatial_Schemas/|https://doi.org/10.48550/arXiv.2402.00956|
|1007|Executable Code Actions Elicit Better LLM Agents|Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, Heng Ji|2024-02-01|arXiv|https://github.com/xingyaoww/code-act|http://arxiv.org/abs/2402.01030v4|
|1008|EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models|Xuchen Pan, Yanxi Chen, Yaliang Li, Bolin Ding, Jingren Zhou|2024-02-01|arXiv|https://github.com/pan-x-c/EE-LLM|https://doi.org/10.48550/arXiv.2402.00518|
|1009|Harnessing the Power of Prompt-based Techniques for Generating School-Level Questions using Large Language Models|Subhankar Maity, Aniket Deroy, Sudeshna Sarkar|2024-02|FIRE '23: Proceedings of the 15th Annual Meeting of the Forum for Information Retrieval Evaluation|https://github.com/my625/PromptQG|https://dl.acm.org/doi/10.1145/3632754.3632755|
|1010|BurstGPT: A Real-world Workload Dataset to Optimize LLM Serving Systems|Yuxin Wang, Yuhan Chen, Zeyu Li, Xueze Kang, Zhenheng Tang, Xin He, Rui Guo, Xin Wang, Qiang Wang, Amelie Chi Zhou, Xiaowen Chu|2024-01-31|arXiv|https://github.com/HPMLL/BurstGPT|http://arxiv.org/abs/2401.17644v3|
|1011|I Think, Therefore I am: Benchmarking Awareness of Large Language Models Using AwareBench|Yuan Li, Yue Huang, Yuli Lin, Siyuan Wu, Yao Wan, Lichao Sun|2024-01-31|OpenReview|https://github.com/HowieHwong/Awareness-in-LLM|http://arxiv.org/abs/2401.17882v2|
|1012|LongAlign: A Recipe for Long Context Alignment of Large Language Models|Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, Juanzi Li|2024-01-31|arXiv|https://github.com/THUDM/LongAlign|https://doi.org/10.48550/arXiv.2401.18058|
|1013|Neighboring Perturbations of Knowledge Editing on Large Language Models|Jun-Yu Ma, Jia-Chen Gu, Ningyu Zhang, Zhen-Hua Ling|2024-01-31|arXiv|https://github.com/mjy1111/PEAK|https://doi.org/10.48550/arXiv.2401.17623|
|1014|Proximity QA: Unleashing the Power of Multi-Modal Large Language Models for Spatial Proximity Analysis|Jianing Li, Xi Nan, Ming Lu, Li Du, Shanghang Zhang|2024-01-31|arXiv|https://github.com/NorthSummer/ProximityQA|https://doi.org/10.48550/arXiv.2401.17862|
|1015|SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering|Xiaopeng Li, Shasha Li, Shezheng Song, Huijun Liu, Bin Ji, Xi Wang, Jun Ma, Jie Yu, Xiaodong Liu, Jing Wang, Weimin Zhang|2024-01-31|arXiv|https://github.com/xpq-tech/SWEA|https://doi.org/10.48550/arXiv.2401.17809|
|1016|MT-Eval: A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models|Wai-Chung Kwan, Xingshan Zeng, Yuxin Jiang, Yufei Wang, Liangyou Li, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong|2024-01-30|arXiv|https://github.com/KwanWaiChung/MT-Eval|https://doi.org/10.48550/arXiv.2401.16745|
|1017|Weak-to-Strong Jailbreaking on Large Language Models|Xuandong Zhao, Xianjun Yang, Tianyu Pang, Chao Du, Lei Li, Yu-Xiang Wang, William Yang Wang|2024-01-30|arXiv|https://github.com/XuandongZhao/weak-to-strong|https://doi.org/10.48550/arXiv.2401.17256|
|1018|Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex Scenarios|Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, Xin Jiang, Ruifeng Xu, Qun Liu|2024-01-30|arXiv|https://github.com/JoeYing1019/UltraTool|http://arxiv.org/abs/2401.17167v3|
|1019|LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation|Yuan Chiang, Elvis Hsieh, Chia-Hong Chou, Janosh Riebesell|2024-01-30|arXiv|https://github.com/chiang-yuan/llamp|https://doi.org/10.48550/arXiv.2401.17244|
|1020|Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate|Steffi Chern, Ethan Chern, Graham Neubig, Pengfei Liu|2024-01-30|arXiv|https://github.com/GAIR-NLP/scaleeval|https://doi.org/10.48550/arXiv.2401.16788|
|1021|Scaling Sparse Fine-Tuning to Large Language Models|Alan Ansell, Ivan Vulic, Hannah Sterz, Anna Korhonen, Edoardo M. Ponti|2024-01-29|arXiv|https://github.com/AlanAnsell/peft|https://doi.org/10.48550/arXiv.2401.16405|
|1022|SelectLLM: Can LLMs Select Important Instructions to Annotate?|Ritik Sachin Parkar, Jaehyung Kim, Jong Inn Park, Dongyeop Kang|2024-01-29|OpenReview|https://github.com/minnesotanlp/select-llm|http://arxiv.org/abs/2401.16553v5|
|1023|Contextualization Distillation from Large Language Model for Knowledge Graph Completion|Dawei Li, Zhen Tan, Tianlong Chen, Huan Liu|2024-01-28|EACL|https://github.com/David-Li0406/Contextulization-Distillation|https://aclanthology.org/2024.findings-eacl.32|
|1024|Efficient Tuning and Inference for Large Language Models on Textual Graphs|Yun Zhu, Yaoke Wang, Haizhou Shi, Siliang Tang|2024-01-28|arXiv|https://github.com/ZhuYun97/ENGINE|https://doi.org/10.48550/arXiv.2401.15569|
|1025|SliceGPT: Compress Large Language Models by Deleting Rows and Columns|Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari Do Nascimento, Torsten Hoefler, James Hensman|2024-01-26|arXiv|https://github.com/microsoft/TransformerCompression|https://doi.org/10.48550/arXiv.2401.15024|
|1026|ConstraintChecker: A Plugin for Large Language Models to Reason on Commonsense Knowledge Bases|Quyet V. Do, Tianqing Fang, Shizhe Diao, Zhaowei Wang, Yangqiu Song|2024-01-25|EACL|https://github.com/HKUST-KnowComp/ConstraintChecker|https://aclanthology.org/2024.eacl-long.42|
|1027|FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design|Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song|2024-01-25|arXiv|https://github.com/usyd-fsalab/fp6_llm|https://doi.org/10.48550/arXiv.2401.14112|
|1028|Prompting Large Language Models for Zero-Shot Clinical Prediction with Structured Longitudinal Electronic Health Record Data|Yinghao Zhu, Zixiang Wang, Junyi Gao, Yuning Tong, Jingkun An, Weibin Liao, Ewen M. Harrison, Liantao Ma, Chengwei Pan|2024-01-25|arXiv|https://github.com/yhzhu99/llm4healthcare|https://doi.org/10.48550/arXiv.2402.01713|
|1029|RomanSetu: Efficiently unlocking multilingual capabilities of Large Language Models models via Romanization|Jaavid Aktar Husain, Raj Dabre, Aswanth Kumar, Jay Gala, Thanmay Jayakumar, Ratish Puduppully, Anoop Kunchukuttan|2024-01-25|arXiv|https://github.com/AI4Bharat/romansetu|https://doi.org/10.48550/arXiv.2401.14280|
|1030|Towards Goal-oriented Prompt Engineering for Large Language Models: A Survey|Haochen Li, Jonathan Leung, Zhiqi Shen|2024-01-25|arXiv|https://github.com/Alex-HaochenLi/Goal-oriented-Prompt-Engineering|http://arxiv.org/abs/2401.14043v2|
|1031|Wordflow: Social Prompt Engineering for Large Language Models|Zijie J. Wang, Aishwarya Chakravarthy, David Munechika, Duen Horng Chau|2024-01-25|arXiv|https://poloclub.github.io/wordflow|https://doi.org/10.48550/arXiv.2401.14447|
|1032|Democratizing Fine-grained Visual Recognition with Large Language Models|Mingxuan Liu, Subhankar Roy, Wenjing Li, Zhun Zhong, Nicu Sebe, Elisa Ricci|2024-01-24|arXiv|https://projfiner.github.io/|https://doi.org/10.48550/arXiv.2401.13837|
|1033|BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models|Feng Lin, Hanling Yi, Hongbin Li, Yifan Yang, Xiaotian Yu, Guangming Lu, Rong Xiao|2024-01-23|arXiv|https://github.com/linfeng93/BiTA|https://doi.org/10.48550/arXiv.2401.12522|
|1034|Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment|Keming Lu, Bowen Yu, Chang Zhou, Jingren Zhou|2024-01-23|arXiv|https://github.com/OFA-Sys/Ditto|https://doi.org/10.48550/arXiv.2401.12474|
|1035|An Empirical Study of In-context Learning in LLMs for Machine Translation|Pranjal A. Chitale, Jay Gala, Raj Dabre|2024-01-22|OpenReview|https://github.com/PranjalChitale/in-context-mt-analysis|http://arxiv.org/abs/2401.12097v3|
|1036|Spotting LLMs With Binoculars: Zero-Shot Detection of Machine-Generated Text|Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein|2024-01-22|arXiv|https://github.com/ahans30/Binoculars|http://arxiv.org/abs/2401.12070v2|
|1037|The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models|Kian Ahrabian, Zhivar Sourati, Kexuan Sun, Jiarui Zhang, Yifan Jiang, Fred Morstatter, Jay Pujara|2024-01-22|arXiv|https://github.com/kahrabian/mllm-nvar|https://doi.org/10.48550/arXiv.2401.12117|
|1038|Instructional Fingerprinting of Large Language Models|Jiashu Xu, Fei Wang, Mingyu Derek Ma, Pang Wei Koh, Chaowei Xiao, Muhao Chen|2024-01-21|arXiv|https://cnut1648.github.io/Model-Fingerprint/|https://doi.org/10.48550/arXiv.2401.12255|
|1039|Over-Reasoning and Redundant Calculation of Large Language Models|David Cheng-Han Chiang, Hung-Yi Lee|2024-01-21|EACL|https://github.com/d223302/Over-Reasoning-of-LLMs|https://aclanthology.org/2024.eacl-short.15|
|1040|Orion-14B: Open-source Multilingual Large Language Models|Du Chen, Yi Huang, Xiaopu Li, Yongqiang Li, Yongqiang Liu, Haihui Pan, Leichao Xu, Dacheng Zhang, Zhipeng Zhang, Kun Han|2024-01-20|arXiv|https://github.com/OrionStarAI/Orion|https://doi.org/10.48550/arXiv.2401.12246|
|1041|Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences|Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Fuxiao Liu, Gedas Bertasius, Mohit Bansal, Huaxiu Yao, Furong Huang|2024-01-19|ACL|https://github.com/umd-huang-lab/Mementos|https://aclanthology.org/2024.acl-long.25|
|1042|Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads|Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, Tri Dao|2024-01-19|arXiv|https://github.com/FasterDecoding/Medusa|http://arxiv.org/abs/2401.10774v3|
|1043|Large Language Models are Efficient Learners of Noise-Robust Speech Recognition|Yuchen Hu, Chen Chen, Chao-Han Huck Yang, Ruizhe Li, Chao Zhang, Pin-Yu Chen, Eng Siong Chng|2024-01-19|arXiv|https://github.com/YUCHEN005/RobustGER|https://doi.org/10.48550/arXiv.2401.10446|
|1044|Knowledge Fusion of Large Language Models|Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, Shuming Shi|2024-01-19|arXiv|https://github.com/fanqiwan/FuseLLM|https://doi.org/10.48550/arXiv.2401.10491|
|1045|MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning|Chenyu Wang, Weixin Luo, Qianyu Chen, Haonan Mai, Jindi Guo, Sixun Dong, Xiaohua, Xuan, Zhengxin Li, Lin Ma, Shenghua Gao|2024-01-19|arXiv|https://github.com/MLLM-Tool/MLLM-Tool|https://doi.org/10.48550/arXiv.2401.10727|
|1046|LangProp: A code optimization framework using Large Language Models applied to driving|Shu Ishida, Gianluca Corrado, George Fedoseev, Hudson Yeo, Lloyd Russell, Jamie Shotton, João F. Henriques, Anthony Hu|2024-01-18|arXiv|https://github.com/shuishida/LangProp|http://arxiv.org/abs/2401.10314v2|
|1047|R-Judge: Benchmarking Safety Risk Awareness for LLM Agents|Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, Rui Wang, Gongshen Liu|2024-01-18|OpenReview|https://github.com/Lordog/R-Judge|http://arxiv.org/abs/2401.10019v2|
|1048|SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model|Yang Zhan, Zhitong Xiong, Yuan Yuan|2024-01-18|arXiv|https://github.com/ZhanYang-nwpu/SkyEyeGPT|https://doi.org/10.48550/arXiv.2401.09712|
|1049|Spatial-Temporal Large Language Model for Traffic Prediction|Chenxi Liu, Sun Yang, Qianxiong Xu, Zhishuai Li, Cheng Long, Ziyue Li, Rui Zhao|2024-01-18|2024 25th IEEE International Conference on Mobile Data Management (MDM)|https://github.com/ChenxiLiu-HNU/ST-LLM|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10591642|
|1050|Towards Language-Driven Video Inpainting via Multimodal Large Language Models|Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, Chen Change Loy|2024-01-18|arXiv|https://jianzongwu.github.io/projects/rovi|https://doi.org/10.48550/arXiv.2401.10226|
|1051|Code Simulation Challenges for Large Language Models|Emanuele La Malfa, Christoph Weinhuber, Orazio Torre, Fangru Lin, Samuele Marro, Anthony Cohn, Nigel Shadbolt, Michael Wooldridge|2024-01-17|arXiv|https://github.com/EmanueleLM/CodeSimulation|https://doi.org/10.48550/arXiv.2401.09074|
|1052|Evaluating LLMs' Mathematical and Coding Competency through Ontology-guided Interventions|Pengfei Hong, Navonil Majumder, Deepanway Ghosal, Somak Aditya, Rada Mihalcea, Soujanya Poria|2024-01-17|arXiv|https://github.com/declare-lab/llm_robustness|http://arxiv.org/abs/2401.09395v4|
|1053|Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models|Jianhui Pang, Fanghua Ye, Longyue Wang, Dian Yu, Derek F. Wong, Shuming Shi, Zhaopeng Tu|2024-01-16|arXiv|https://github.com/pangjh3/LLM4MT|https://doi.org/10.48550/arXiv.2401.08350|
|1054|RoTBench: A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning|Junjie Ye, Yilong Wu, Songyang Gao, Caishuang Huang, Sixian Li, Guanyu Li, Xiaoran Fan, Qi Zhang, Tao Gui, Xuanjing Huang|2024-01-16|arXiv|https://github.com/Junjie-Ye/RoTBench|https://doi.org/10.48550/arXiv.2401.08326|
|1055|AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception|Yipo Huang, Quan Yuan, Xiangfei Sheng, Zhichao Yang, Haoning Wu, Pengfei Chen, Yuzhe Yang, Leida Li, Weisi Lin|2024-01-16|arXiv|https://github.com/yipoh/AesBench|https://doi.org/10.48550/arXiv.2401.08276|
|1056|MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception|Yuhao Wang, Yusheng Liao, Heyang Liu, Hongcheng Liu, Yanfeng Wang, Yu Wang|2024-01-15|ACL|https://github.com/YHWmz/MM-SAP|https://aclanthology.org/2024.acl-long.498|
|1057|CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning|Weiqi Wang, Tianqing Fang, Chunyang Li, Haochen Shi, Wenxuan Ding, Baixuan Xu, Zhaowei Wang, Jiaxin Bai, Xin Liu, Cheng Jiayang, Chunkit Chan, Yangqiu Song|2024-01-14|ACL|https://github.com/HKUST-KnowComp/CANDLE|https://aclanthology.org/2024.acl-long.128|
|1058|CHAMP: A Competition-level Dataset for Fine-Grained Analyses of LLMs' Mathematical Reasoning Capabilities|Yujun Mao, Yoon Kim, Yilun Zhou|2024-01-13|arXiv|https://yujunmao1.github.io/CHAMP/|http://arxiv.org/abs/2401.06961v2|
|1059|Extending LLMs' Context Window with 100 Samples|Yikai Zhang, Junlong Li, Pengfei Liu|2024-01-13|OpenReview|https://github.com/GAIR-NLP/Entropy-ABF|http://arxiv.org/abs/2401.07004v1|
|1060|Exploring the Best Practices of Query Expansion with Large Language Models|Le Zhang, Yihong Wu, Qian Yang, Jian-Yun Nie|2024-01-12|arXiv|https://github.com/lezhang7/Retrieval_MuGI|http://arxiv.org/abs/2401.06311v3|
|1061|INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning|Yutao Zhu, Peitian Zhang, Chenghao Zhang, Yifei Chen, Binyu Xie, Zheng Liu, Ji-Rong Wen, Zhicheng Dou|2024-01-12|ACL|https://github.com/DaoD/INTERS|https://aclanthology.org/2024.acl-long.154|
|1062|Intention Analysis Makes LLMs A Good Jailbreak Defender|Yuqi Zhang, Liang Ding, Lefei Zhang, Dacheng Tao|2024-01-12|OpenReview|https://github.com/alphadl/SafeLLM_with_IntentionAnalysis|http://arxiv.org/abs/2401.06561v3|
|1063|OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models|Shuai Wang, Liang Ding, Li Shen, Yong Luo, Bo Du, Dacheng Tao|2024-01-12|arXiv|https://github.com/alphadl/OOP-eval|https://doi.org/10.48550/arXiv.2401.06628|
|1064|Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs|Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, Saining Xie|2024-01-11|arXiv|https://tsb0601.github.io/mmvp_blog/|http://arxiv.org/abs/2401.06209v2|
|1065|TOFU: A Task of Fictitious Unlearning for LLMs|Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C. Lipton, J. Zico Kolter|2024-01-11|arXiv|https://locuslab.github.io/tofu/|http://arxiv.org/abs/2401.06121v1|
|1066|Towards Boosting Many-to-Many Multilingual Machine Translation with Large Language Models|Pengzhi Gao, Zhongjun He, Hua Wu, Haifeng Wang|2024-01-11|arXiv|https://github.com/gpengzhi/CrossConST-LLM|https://doi.org/10.48550/arXiv.2401.05861|
|1067|EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction|Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dongsheng Li, Deqing Yang|2024-01-11|OpenReview|https://github.com/microsoft/JARVIS/|http://arxiv.org/abs/2401.06201v3|
|1068|LLM-as-a-Coauthor: Can Mixed Human-Written and Machine-Generated Text Be Detected?|Qihui Zhang, Chujie Gao, Dongping Chen, Yue Huang, Yixin Huang, Zhenyang Sun, Shilin Zhang, Weiye Li, Zhengyan Fu, Yao Wan, Lichao Sun|2024-01-11|Findings of the Association for Computational Linguistics: NAACL 2024 - Findings|https://github.com/Dongping-Chen/MixSet|http://arxiv.org/abs/2401.05952v2|
|1069|Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint|Zhipeng Chen, Kun Zhou, Wayne Xin Zhao, Junchen Wan, Fuzheng Zhang, Di Zhang, Ji-Rong Wen|2024-01-11|arXiv|https://github.com/RUCAIBox/RLMEC|https://doi.org/10.48550/arXiv.2401.06081|
|1070|AugSumm: Towards Generalizable Speech Summarization Using Synthetic Labels from Large Language Models|Jee-weon Jung, Roshan S. Sharma, William Chen, Bhiksha Raj, Shinji Watanabe|2024-01-10|ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/Jungjee/AugSumm|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10447328|
|1071|DCR: Divide-and-Conquer Reasoning for Multi-choice Question Answering with LLMs|Zijie Meng, Yan Zhang, Zhaopeng Feng, Zuozhu Liu|2024-01-10|arXiv|https://github.com/AiMijie/Divide-and-Conquer|http://arxiv.org/abs/2401.05190v2|
|1072|Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security|Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu, Wenxing Xu, Xiang Wang, Yi Sun, Rui Kong, Yile Wang, Hanfei Geng, Jian Luan, Xuefeng Jin, Zilong Ye, Guanjing Xiong, Fan Zhang, Xiang Li, Mengwei Xu, Zhijun Li, Peng Li, Yang Liu, Ya-Qin Zhang, Yunxin Liu|2024-01-10|arXiv|https://github.com/MobileLLM/Personal_LLM_Agents_Survey|http://arxiv.org/abs/2401.05459v2|
|1073|The Impact of Reasoning Step Length on Large Language Models|Mingyu Jin, Qinkai Yu, Dong Shu, Haiyan Zhao, Wenyue Hua, Yanda Meng, Yongfeng Zhang, Mengnan Du|2024-01-10|arXiv|https://github.com/MingyuJ666/The-Impact-of-Reasoning-Step-Length-on-Large-Language-Models|https://doi.org/10.48550/arXiv.2401.04925|
|1074|Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search|Haochen Li, Xin Zhou, Zhiqi Shen|2024-01-09|arXiv|https://github.com/Alex-HaochenLi/ReCo|https://doi.org/10.48550/arXiv.2401.04514|
|1075|TechGPT-2.0: A large language model project to solve the task of knowledge graph construction|Jiaqi Wang, Yuying Chang, Zhong Li, Ning An, Qi Ma, Lei Hei, Haibo Luo, Yifei Lu, Feiliang Ren|2024-01-09|arXiv|https://github.com/neukg/TechGPT-2.0|https://doi.org/10.48550/arXiv.2401.04507|
|1076|Informed AI Regulation: Comparing the Ethical Frameworks of Leading LLM Chatbots Using an Ethics-Based Audit to Assess Moral Reasoning and Normative Values|Jon Chun, Katherine Elkins|2024-01-09|arXiv|https://github.com/jonchun/llm-sota-chatbots-ethics-based-audit|http://arxiv.org/abs/2402.01651v1|
|1077|Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs|Junjie Wang, Dan Yang, Binbin Hu, Yue Shen, Wen Zhang, Jinjie Gu|2024-01-09|arXiv|https://github.com/alipay/Analogic-Reasoning-Augmented-Large-Language-Model|http://arxiv.org/abs/2401.04319v3|
|1078|The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models|Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen|2024-01-06|arXiv|https://github.com/RUCAIBox/HaluEval-2.0|https://doi.org/10.48550/arXiv.2401.03205|
|1079|Malla: Demystifying Real-world Large Language Model Integrated Malicious Services|Zilong Lin, Jian Cui, Xiaojing Liao, XiaoFeng Wang|2024-01-06|arXiv|https://github.com/idllresearch/malicious-gpt|https://doi.org/10.48550/arXiv.2401.03315|
|1080|PeFoMed: Parameter Efficient Fine-tuning of Multimodal Large Language Models for Medical Imaging|Gang Liu, Jinlong He, Pengfei Li, Genrong He, Zhaolin Chen, Shenjun Zhong|2024-01-05|arXiv|https://github.com/jinlHe/PeFoMed|http://arxiv.org/abs/2401.02797v2|
|1081|VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language Model|Pengying Wu, Yao Mu, Bingxian Wu, Yi Hou, Ji Ma, Shanghang Zhang, Chang Liu|2024-01-05|arXiv|https://voro-nav.github.io|https://doi.org/10.48550/arXiv.2401.02695|
|1082|PLLaMa: An Open-source Large Language Model for Plant Science|Xianjun Yang, Junfeng Gao, Wenxin Xue, Erik Alexandersson|2024-01-03|arXiv|https://github.com/Xianjun-Yang/PLLaMa|https://doi.org/10.48550/arXiv.2401.01600|
|1083|A Comprehensive Study of Knowledge Editing for Large Language Models|Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen|2024-01-02|arXiv|https://github.com/zjunlp/EasyEdit|https://doi.org/10.48550/arXiv.2401.01286|
|1084|Auffusion: Leveraging the Power of Diffusion and Large Language Models for Text-to-Audio Generation|Jinlong Xue, Yayue Deng, Yingming Gao, Ya Li|2024-01-02|arXiv|https://auffusion.github.io|https://doi.org/10.48550/arXiv.2401.01044|
|1085|Self-Supervised Position Debiasing for Large Language Models|Zhongkun Liu, Zheng Chen, Mengqi Zhang, Zhaochun Ren, Pengjie Ren, Zhumin Chen|2024-01-02|ACL|https://github.com/LZKSKY/SOD|https://aclanthology.org/2024.findings-acl.170|
|1086|VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM|Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei|2024-01-02|arXiv|https://videodrafter.github.io|http://arxiv.org/abs/2401.01256v1|
|1087|MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use|Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, Lichao Sun|2024-01-01|arXiv|https://github.com/HowieHwong/MetaTool|https://doi.org/10.48550/arXiv.2310.03128|
|1088|MILL: Mutual Verification with Large Language Models for Zero-Shot Query Expansion|Pengyue Jia, Yiding Liu, Xiangyu Zhao, Xiaopeng Li, Changying Hao, Shuaiqiang Wang, Dawei Yin|2024-01-01|arXiv|https://github.com/Applied-Machine-Learning-Lab/MILL|https://doi.org/10.48550/arXiv.2310.19056|
|1089|MLCopilot: Unleashing the Power of Large Language Models in Solving Machine Learning Tasks|Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, Yuqing Yang|2024-01-01|EACL|https://github.com/microsoft/CoML|https://aclanthology.org/2024.eacl-long.179|
|1090|Massive Editing for Large Language Models via Meta Learning|Chenmien Tan, Ge Zhang, Jie Fu|2024-01-01|arXiv|https://github.com/ChenmienTan/malmen|https://doi.org/10.48550/arXiv.2311.04661|
|1091|MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models|Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T. Kwok, Zhenguo Li, Adrian Weller, Weiyang Liu|2024-01-01|arXiv|https://meta-math.github.io/|https://doi.org/10.48550/arXiv.2309.12284|
|1092|Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis|Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, Shujian Huang|2024-01-01|arXiv|https://github.com/NJUNLP/MMT-LLM|https://doi.org/10.48550/arXiv.2304.04675|
|1093|MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models|Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, Mohamed Elhoseiny|2024-01-01|arXiv|https://minigpt-4.github.io/|https://doi.org/10.48550/arXiv.2304.10592|
|1094|MiniLLM: Knowledge Distillation of Large Language Models|Yuxian Gu, Li Dong, Furu Wei, Minlie Huang|2024-01-01|ICLR|https://github.com/microsoft/LMOps/tree/main/minillm|https://openreview.net/forum?id=5h0qf7IBZZ|
|1095|Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites|Lei Wang, Jiabang He, Shenshen Li, Ning Liu, Ee-Peng Lim|2024-01-01|MMM|https://github.com/Anonymousanoy/FOHE|https://doi.org/10.1007/978-3-031-53302-0_3|
|1096|Multilingual Jailbreak Challenges in Large Language Models|Yue Deng, Wenxuan Zhang, Sinno Jialin Pan, Lidong Bing|2024-01-01|arXiv|https://github.com/DAMO-NLP-SG/multilingual-safety-for-LLMs|https://doi.org/10.48550/arXiv.2310.06474|
|1097|LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models|Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia|2024-01-01|arXiv|https://github.com/dvlab-research/LongLoRA|https://doi.org/10.48550/arXiv.2309.12307|
|1098|OctoPack: Instruction Tuning Code Large Language Models|Niklas Muennighoff, Qian Liu, Armel Randy Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro von Werra, Shayne Longpre|2024-01-01|ICLR|https://github.com/bigcode-project/octopack|https://openreview.net/forum?id=mw1PWNSWZP|
|1099|Low-code LLM: Graphical User Interface over Large Language Models|Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Zehua Wang, Yaobo Liang, Tao Ge, Chenfei Wu, Wang You, Ting Song, Yan Xia, Jonathan Tien, Nan Duan, Furu Wei|2024-01-01|Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024|https://github.com/moymix/TaskMatrix/tree/main/LowCodeLLM|http://arxiv.org/abs/2304.08103v3|
|1100|Large Language Models as Tool Makers|Tianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, Denny Zhou|2024-01-01|arXiv|https://github.com/ctlllll/LLM-ToolMaker|https://doi.org/10.48550/arXiv.2305.17126|
|1101|LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models|Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng He, Weizhu Chen, Tuo Zhao|2024-01-01|ICLR|https://github.com/yxli2123/LoftQ|https://openreview.net/forum?id=LzPWWPAdY4|
|1102|LibriSQA: A Novel Dataset and Framework for Spoken Question Answering with Large Language Models|Zihan Zhao, Yiyang Jiang, Heyang Liu, Yanfeng Wang, Yu Wang|2024-01-01|IEEE Transactions on Artificial Intelligence|https://github.com/ZihanZhaoSJTU/LibriSQA|http://arxiv.org/abs/2308.10390v4|
|1103|Learning to Retrieve In-Context Examples for Large Language Models|Liang Wang, Nan Yang, Furu Wei|2024-01-01|EACL|https://github.com/microsoft/LMOps/tree/main/llm_retriever|https://aclanthology.org/2024.eacl-long.105|
|1104|LayoutNUWA: Revealing the Hidden Layout Expertise of Large Language Models|Zecheng Tang, Chenfei Wu, Juntao Li, Nan Duan|2024-01-01|arXiv|https://github.com/ProjectNUWA/LayoutNUWA|https://doi.org/10.48550/arXiv.2309.09506|
|1105|Large Language Models as Optimizers|Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, Xinyun Chen|2024-01-01|arXiv|https://github.com/google-deepmind/opro|https://doi.org/10.48550/arXiv.2309.03409|
|1106|Large Language Models as Generalizable Policies for Embodied Tasks|Andrew Szot, Max Schwarzer, Harsh Agrawal, Bogdan Mazoure, Walter Talbott, Katherine Metcalf, Natalie Mackraz, R. Devon Hjelm, Alexander Toshev|2024-01-01|ICLR|https://llm-rl.github.io|https://openreview.net/forum?id=u6imHU4Ebu|
|1107|Large Language Models are Zero-Shot Rankers for Recommender Systems|Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian J. McAuley, Wayne Xin Zhao|2024-01-01|ECIR|https://github.com/RUCAIBox/LLMRank|https://doi.org/10.1007/978-3-031-56060-6_24|
|1108|Label-free Node Classification on Graphs with Large Language Models (LLMS)|Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang, Hui Liu, Jiliang Tang|2024-01-01|arXiv|https://github.com/CurryTang/LLMGNN|https://doi.org/10.48550/arXiv.2310.04668|
|1109|LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models|Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, Sinong Wang|2024-01-01|NAACL-HLT|https://github.com/Glaciohound/LM-Infinite|https://doi.org/10.18653/v1/2024.naacl-long.222|
|1110|LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models|Ahmad Faiz, Sotaro Kaneda, Ruhan Wang, Rita Chukwunyere Osi, Prateek Sharma, Fan Chen, Lei Jiang|2024-01-01|ICLR|https://github.com/SotaroKaneda/MLCarbon|https://openreview.net/forum?id=aIok3ZD9to|
|1111|PB-LLM: Partially Binarized Large Language Models|Yuzhang Shang, Zhihang Yuan, Qiang Wu, Zhen Dong|2024-01-01|arXiv|https://github.com/hahnyuan/BinaryLLM|https://doi.org/10.48550/arXiv.2310.00034|
|1112|IntellectSeeker: A Personalized Literature Management System with the Probabilistic Model and Large Language Model|Weizhen Bian, Siyan Liu, Yubo Zhou, Dezhi Chen, Yijie Liao, Zhenzhen Fan, Aobo Wang|2024-01-01|KSEM|https://github.com/LuckyBian/ISY5001|https://doi.org/10.1007/978-981-97-5489-2_24|
|1113|Kosmos-G: Generating Images in Context with Multimodal Large Language Models|Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng, Wenhu Chen, Furu Wei|2024-01-01|arXiv|https://xichenpan.github.io/kosmosg|https://doi.org/10.48550/arXiv.2310.02992|
|1114|OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models|Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, Ping Luo|2024-01-01|arXiv|https://github.com/OpenGVLab/OmniQuant|https://doi.org/10.48550/arXiv.2308.13137|
|1115|WizardCoder: Empowering Code Large Language Models with Evol-Instruct|Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, Daxin Jiang|2024-01-01|arXiv|https://github.com/nlpxucan/WizardLM|https://doi.org/10.48550/arXiv.2306.08568|
|1116|PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization|Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang|2024-01-01|12th International Conference on Learning Representations, ICLR 2024|https://github.com/WeOpenML/PandaLM|http://arxiv.org/abs/2306.05087v2|
|1117|Struc-Bench: Are Large Language Models Really Good at Generating Complex Structured Data?|Xiangru Tang, Yiming Zong, Jason Phang, Yilun Zhao, Wangchunshu Zhou, Arman Cohan, Mark Gerstein|2024-01-01|arXiv|https://github.com/gersteinlab/Struc-Bench|https://doi.org/10.48550/arXiv.2309.08963|
|1118|EE-LCE: An Event Extraction Framework Based on LLM-Generated CoT Explanation|Yanhua Yu, Yuanlong Wang, Yunshan Ma, Jie Li, Kangkang Lu, Zhiyong Huang, Tat Seng Chua|2024-01-01|Knowledge Science, Engineering and Management|https://github.com/Wangyl147/EE-LCE|http://dx.doi.org/10.1007/978-981-97-5492-2_3|
|1119|WizardLM: Empowering Large Language Models to Follow Complex Instructions|Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, Daxin Jiang|2024-01-01|arXiv|https://github.com/nlpxucan/WizardLM|https://doi.org/10.48550/arXiv.2304.12244|
|1120|VDC: Versatile Data Cleanser based on Visual-Linguistic Inconsistency by Multimodal Large Language Models|Zihao Zhu, Mingda Zhang, Shaokui Wei, Bingzhe Wu, Baoyuan Wu|2024-01-01|ICLR|https://github.com/zihao-ai/vdc|https://openreview.net/forum?id=ygxTuVz9eU|
|1121|Unveiling the Pitfalls of Knowledge Editing for Large Language Models|Zhoubo Li, Ningyu Zhang, Yunzhi Yao, Mengru Wang, Xi Chen, Huajun Chen|2024-01-01|arXiv|https://github.com/zjunlp/PitfallsKnowledgeEditing|https://doi.org/10.48550/arXiv.2310.02129|
|1122|Unlocking Emergent Modularity in Large Language Models|Zihan Qiu, Zeyu Huang, Jie Fu|2024-01-01|NAACL-HLT|https://github.com/qiuzh20/EMoE|https://doi.org/10.18653/v1/2024.naacl-long.144|
|1123|UniversalNER: Targeted Distillation from Large Language Models for Open Named Entity Recognition|Wenxuan Zhou, Sheng Zhang, Yu Gu, Muhao Chen, Hoifung Poon|2024-01-01|arXiv|https://universal-ner.github.io/|https://doi.org/10.48550/arXiv.2308.03279|
|1124|Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization|Yang Jin, Kun Xu, Kun Xu, Liwei Chen, Chao Liao, Jianchao Tan, Quzhe Huang, Bin Chen, Chenyi Lei, An Liu, Chengru Song, Xiaoqiang Lei, Di Zhang, Wenwu Ou, Kun Gai, Yadong Mu|2024-01-01|12th International Conference on Learning Representations, ICLR 2024|https://github.com/jy0205/LaVIT|http://arxiv.org/abs/2309.04669v3|
|1125|Understanding the Effects of RLHF on LLM Generalisation and Diversity|Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, Roberta Raileanu|2024-01-01|12th International Conference on Learning Representations, ICLR 2024|https://github.com/facebookresearch/rlfh-gen-div|http://arxiv.org/abs/2310.06452v3|
|1126|Tree-Planner: Efficient Close-loop Task Planning with Large Language Models|Mengkang Hu, Yao Mu, Xinmiao Yu, Mingyu Ding, Shiguang Wu, Wenqi Shao, Qiguang Chen, Bin Wang, Yu Qiao, Ping Luo|2024-01-01|arXiv|https://tree-planner.github.io/|https://doi.org/10.48550/arXiv.2310.08582|
|1127|Towards Automated End-to-End Health Misinformation Free Search with a Large Language Model|Ronak Pradeep, Jimmy Lin|2024-01-01|ECIR|https://github.com/castorini/pygaggle|https://doi.org/10.1007/978-3-031-56066-8_9|
|1128|Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers|Rajiv Movva, Sidhika Balachandar, Kenny Peng, Gabriel Agostini, Nikhil Garg, Emma Pierson|2024-01-01|NAACL-HLT|https://github.com/rmovva/LLM-publication-patterns-public|https://doi.org/10.18653/v1/2024.naacl-long.67|
|1129|ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of Large Language Models in Real-world Scenarios|Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, Xuanjing Huang|2024-01-01|arXiv|https://github.com/Junjie-Ye/ToolEyes|https://doi.org/10.48550/arXiv.2401.00741|
|1130|Spoken Question Answering and Speech Continuation Using Spectrogram-Powered LLM|Eliya Nachmani, Alon Levkovitch, Roy Hirsch, Julian Salazar, Chulayuth Asawaroengchai, Soroosh Mariooryad, Ehud Rivlin, RJ Skerry-Ryan, Michelle Tadmor Ramanovich|2024-01-01|12th International Conference on Learning Representations, ICLR 2024|https://michelleramanovich.github.io/spectron/spectron|http://arxiv.org/abs/2305.15255v4|
|1131|Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models|Hritik Bansal, John Dang, Aditya Grover|2024-01-01|arXiv|https://github.com/Hritikbansal/sparse_feedback|https://doi.org/10.48550/arXiv.2308.15812|
|1132|Sentiment Analysis in the Era of Large Language Models: A Reality Check|Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, Lidong Bing|2024-01-01|arXiv|https://github.com/DAMO-NLP-SG/LLM-Sentiment|https://doi.org/10.48550/arXiv.2305.15005|
|1133|Self-Prompting Large Language Models for Open-Domain QA|Junlong Li, Jinyuan Wang, Zhuosheng Zhang, Hai Zhao|2024-01-01|arXiv|https://github.com/lockon-n/self-prompting|https://doi.org/10.48550/arXiv.2212.08635|
|1134|Self-Improving for Zero-Shot Named Entity Recognition with Large Language Models|Tingyu Xie, Qi Li, Yan Zhang, Zuozhu Liu, Hongwei Wang|2024-01-01|arXiv|https://github.com/Emma1066/Self-Improve-Zero-Shot-NER|https://doi.org/10.48550/arXiv.2311.08921|
|1135|Seeking Neural Nuggets: Knowledge Transfer in Large Language Models from a Parametric Perspective|Ming Zhong, Chenxin An, Weizhu Chen, Jiawei Han, Pengcheng He|2024-01-01|arXiv|https://maszhongming.github.io/ParaKnowTransfer|https://doi.org/10.48550/arXiv.2310.11451|
|1136|SALMONN: Towards Generic Hearing Abilities for Large Language Models|Changli Tang, Wenyi Yu, Guangzhi Sun, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang|2024-01-01|arXiv|https://github.com/bytedance/SALMONN|https://doi.org/10.48550/arXiv.2310.13289|
|1137|Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models|Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee|2024-01-01|arXiv|https://github.com/johnheo/adadim-llm|https://doi.org/10.48550/arXiv.2309.15531|
|1138|R-Tuning: Instructing Large Language Models to Say `I Don't Know'|Hanning Zhang, Shizhe Diao, Yong Lin, Yi R. Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, Tong Zhang|2024-01-01|Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024|https://github.com/shizhediao/R-Tuning|http://arxiv.org/abs/2311.09677v3|
|1139|QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models|Jing Liu, Ruihao Gong, Xiuying Wei, Zhiwei Dong, Jianfei Cai, Bohan Zhuang|2024-01-01|arXiv|https://github.com/ziplab/QLLM|https://doi.org/10.48550/arXiv.2310.08041|
|1140|QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models|Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang, Zhengsu Chen, Xiaopeng Zhang, Qi Tian|2024-01-01|arXiv|https://github.com/yuhuixu1993/qa-lora|https://doi.org/10.48550/arXiv.2309.14717|
|1141|Prompt-Based Learning on Large Protein Language Models Improves Signal Peptide Prediction|Shuai Zeng, Duolin Wang, Lei Jiang, Dong Xu|2024-01-01|RECOMB|https://github.com/shuaizengMU/PEFT-SP|https://doi.org/10.1007/978-1-0716-3989-4_40|
|1142|Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models|Fobo Shi, Peijun Qing, Dong Yang, Nan Wang, Youbo Lei, Haonan Lu, Xiaodong Lin, Duantengchuan Li|2024-01-01|arXiv|https://github.com/YouBLEI/Prompt-Space|https://doi.org/10.48550/arXiv.2306.03799|
|1143|Prediction of actions and places by the time series recognition from images with Multimodal LLM|T. Ogawa, K. Yoshioka, K. Fukuda, T. Morita|2024-01-01|2024 IEEE 18th International Conference on Semantic Computing (ICSC)|https://github.com/tomo1115tomo/kg_reasoning_challenge|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10475660|
|1144|Knowledge Tracing as Language Processing: A Large-Scale Autoregressive Paradigm|Bojun Zhan, Teng Guo, Xueyi Li, Mingliang Hou, Qianru Liang, Boyu Gao, Weiqi Luo, Zitao Liu|2024-01-01|AIED|https://github.com/ai4ed/AIED2024-LLM-KT|https://doi.org/10.1007/978-3-031-64302-6_13|
|1145|LLM-grounded Video Diffusion Models|Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, Boyi Li|2024-01-01|12th International Conference on Learning Representations, ICLR 2024|https://llm-grounded-video-diffusion.github.io/|http://arxiv.org/abs/2309.17444v3|
|1146|Invited Paper: Software/Hardware Co-design for LLM and Its Application for Design Verification|L. J. Wan, Y. Huang, Y. Li, H. Ye, J. Wang, X. Zhang, D. Chen|2024-01-01|2024 29th Asia and South Pacific Design Automation Conference (ASP-DAC)|https://github.com/UIUC-ChenLab/Chrysalis-HLS|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10473893|
|1147|Can Large Language Models Infer Causation from Correlation?|Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab, Bernhard Schölkopf|2024-01-01|arXiv|https://github.com/causalNLP/corr2cause|https://doi.org/10.48550/arXiv.2306.05836|
|1148|Can LLM-Generated Misinformation Be Detected?|Canyu Chen, Kai Shu|2024-01-01|12th International Conference on Learning Representations, ICLR 2024|https://llm-misinformation.github.io/|http://arxiv.org/abs/2309.13788v5|
|1149|COMEM: In-Context Retrieval-Augmented Mass-Editing Memory in Large Language Models|Anonymous|2024-01-01|Findings of the Association for Computational Linguistics: NAACL 2024 - Findings|https://github.com/xxxx/xxxx$|https://doi.org/|
|1150|CLEX: Continuous Length Extrapolation for Large Language Models|Guanzheng Chen, Xin Li, Zaiqiao Meng, Shangsong Liang, Lidong Bing|2024-01-01|arXiv|https://github.com/DAMO-NLP-SG/CLEX|https://doi.org/10.48550/arXiv.2310.16450|
|1151|CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models|Yufei Huang, Deyi Xiong|2024-01-01|LREC/COLING|https://github.com/YFHuangxxxx/CBBQ|https://aclanthology.org/2024.lrec-main.260|
|1152|CAPTAIN at COLIEE 2024: Large Language Model for Legal Text Retrieval and Entailment|Phuong Nguyen, Cong Nguyen, Hiep Nguyen, Minh Nguyen, An Trieu, Dat Nguyen, Le-Minh Nguyen|2024-01-01|JSAI-isAI|https://github.com/phuongnm94/captain-coliee/tree/coliee2024|https://doi.org/10.1007/978-981-97-3076-6_9|
|1153|Beyond Accuracy: Evaluating Self-Consistency of Code Large Language Models with IdentityChain|Marcus J. Min, Yangruibo Ding, Luca Buratti, Saurabh Pujar, Gail E. Kaiser, Suman Jana, Baishakhi Ray|2024-01-01|arXiv|https://github.com/marcusm117/IdentityChain|https://doi.org/10.48550/arXiv.2310.14053|
|1154|Balanced and Explainable Social Media Analysis for Public Health with Large Language Models|Yan Jiang, Ruihong Qiu, Yi Zhang, Peng-Fei Zhang|2024-01-01|ADC|https://github.com/YanJiangJerry/ALEX|https://doi.org/10.1007/978-3-031-47843-7_6|
|1155|Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection|Jun Yan, Vikas Yadav, Shiyang Li, Lichang Chen, Zheng Tang, Hai Wang, Vijay Srinivasan, Xiang Ren, Hongxia Jin|2024-01-01|NAACL-HLT|https://poison-llm.github.io|https://doi.org/10.18653/v1/2024.naacl-long.337|
|1156|BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer|Akari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia Tsvetkov, Sebastian Ruder, Hannaneh Hajishirzi|2024-01-01|arXiv|https://buffetfs.github.io/|https://doi.org/10.48550/arXiv.2305.14857|
|1157|BAMBOO: A Comprehensive Benchmark for Evaluating Long Text Modeling Capacities of Large Language Models|Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen|2024-01-01|LREC/COLING|https://github.com/RUCAIBox/BAMBOO|https://aclanthology.org/2024.lrec-main.188|
|1158|Automated Comment Generation Based on the Large Language Model|Kaiwei Cai, Junsheng Zhou, Li Kong, Dandan Liang, Xianzhuo Li|2024-01-01|ICCSE|https://github.com/CarryCKW/EssayComGen|https://doi.org/10.1007/978-981-97-0730-0_25|
|1159|AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models|Xiaogeng Liu, Nan Xu, Muhao Chen, Chaowei Xiao|2024-01-01|arXiv|https://github.com/SheltonLiu-N/AutoDAN|https://doi.org/10.48550/arXiv.2310.04451|
|1160|Audio-LLM: Activating the Capabilities of Large Language Models to Comprehend Audio Data|Dongting Li, Chenchong Tang, Han Liu|2024-01-01|ISNN|https://github.com/orallove/audio-LLM|https://doi.org/10.1007/978-981-97-4399-5_13|
|1161|AskIt: Unified Programming Interface for Programming with Large Language Models|Katsumi Okuda, Saman P. Amarasinghe|2024-01-01|2024 IEEE/ACM International Symposium on Code Generation and Optimization (CGO)|https://github.com/katsumiok/ts-askit|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10444830|
|1162|Are Large Language Models Temporally Grounded?|Yifu Qiu, Zheng Zhao, Yftah Ziser, Anna Korhonen, Edoardo M. Ponti, Shay B. Cohen|2024-01-01|arXiv|https://github.com/yfqiu-nlp/temporal-llms|https://doi.org/10.48550/arXiv.2311.08398|
|1163|AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?|Qi Zhao, Shijie Wang, Ce Zhang, Changcheng Fu, Minh Quan Do, Nakul Agarwal, Kwonjoon Lee, Chen Sun|2024-01-01|arXiv|https://brown-palm.github.io/AntGPT|https://doi.org/10.48550/arXiv.2307.16368|
|1164|Amortizing intractable inference in large language models|Edward J. Hu, Moksh Jain, Eric Elmoznino, Younesse Kaddar, Guillaume Lajoie, Yoshua Bengio, Nikolay Malkin|2024-01-01|arXiv|https://github.com/GFNOrg/gfn-lm-tuning|https://doi.org/10.48550/arXiv.2310.04363|
|1165|Advancing Beyond Identification: Multi-bit Watermark for Large Language Models|KiYoon Yoo, Wonhyuk Ahn, Nojun Kwak|2024-01-01|NAACL-HLT|https://github.com/bangawayoo/mb-lm-watermarking|https://doi.org/10.18653/v1/2024.naacl-long.224|
|1166|Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts|Jian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, Yu Su|2024-01-01|ICLR|https://github.com/OSU-NLP-Group/LLM-Knowledge-Conflict|https://openreview.net/forum?id=auKAUJZMO6|
|1167|Adapting Large Language Models via Reading Comprehension|Daixuan Cheng, Shaohan Huang, Furu Wei|2024-01-01|arXiv|https://github.com/microsoft/LMOps|https://doi.org/10.48550/arXiv.2309.09530|
|1168|Adapting Large Language Models by Integrating Collaborative Semantics for Recommendation|Bowen Zheng, Yupeng Hou, Hongyu Lu, Yu Chen, Wayne Xin Zhao, Ming Chen, Ji-Rong Wen|2024-01-01|2024 IEEE 40th International Conference on Data Engineering (ICDE)|https://github.com/RUCAIBox/LC-Rec/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597986|
|1169|AceGPT, Localizing Large Language Models in Arabic|Huang Huang, Fei Yu, Jianqing Zhu, Xuening Sun, Hao Cheng, Dingjie Song, Zhihong Chen, Abdulmohsen Alharthi, Bang An, Juncai He, Ziche Liu, Zhiyi Zhang, Junying Chen, Jianquan Li, Benyou Wang, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, Jinchao Xu|2024-01-01|arXiv|https://github.com/FreedomIntelligence/AceGPT|https://doi.org/10.48550/arXiv.2309.12053|
|1170|A Wolf in Sheep&apos;s Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily|Peng Ding, Jun Kuang, Dan Ma, Xuezhi Cao, Yunsen Xian, Jiajun Chen, Shujian Huang|2024-01-01|arXiv|https://github.com/NJUNLP/ReNeLLM|https://doi.org/10.48550/arXiv.2311.08268|
|1171|A Survey on Multimodal Large Language Models for Autonomous Driving|Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang, Kuei-Da Liao, Tianren Gao, Erlong Li, Kun Tang, Zhipeng Cao, Tong Zhou, Ao Liu, Xinrui Yan, Shuqi Mei, Jianguo Cao, Ziran Wang, Chao Zheng|2024-01-01|2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)|https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10495592|
|1172|A Simple and Effective Pruning Approach for Large Language Models|Mingjie Sun, Zhuang Liu, Anna Bair, J. Zico Kolter|2024-01-01|arXiv|https://eric-mingjie.github.io/wanda/home.html|https://doi.org/10.48550/arXiv.2306.11695|
|1173|A Semantic Invariant Robust Watermark for Large Language Models|Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, Lijie Wen|2024-01-01|arXiv|https://github.com/THU-BPM/Robust_Watermark|https://doi.org/10.48550/arXiv.2310.06356|
|1174|In-context Autoencoder for Context Compression in a Large Language Model|Tao Ge, Jing Hu, Lei Wang, Xun Wang, Si-Qing Chen, Furu Wei|2024-01-01|ICLR|https://github.com/getao/icae|https://openreview.net/forum?id=uREj4ZuGJE|
|1175|Can Large Language Models Discern Evidence for Scientific Hypotheses? Case Studies in the Social Sciences|Sai Dileep Koneru, Jian Wu, Sarah Rajtmajer|2024-01-01|LREC/COLING|https://github.com/Sai90000/ScientificHypothesisEvidencing|https://aclanthology.org/2024.lrec-main.248|
|1176|An Unforgeable Publicly Verifiable Watermark for Large Language Models|Aiwei Liu, Leyi Pan, Xuming Hu, Shu'ang Li, Lijie Wen, Irwin King, Philip S. Yu|2024-01-01|ICLR|https://github.com/THU-BPM/unforgeable_watermark|https://openreview.net/forum?id=gMLQwKDY3N|
|1177|ChartFormer: A Large Vision Language Model for Converting Chart Images into Tactile Accessible SVGs|Omar Moured, Sara Alzalabny, Anas Osman, Thorsten Schwarz, Karin Müller, Rainer Stiefelhagen|2024-01-01|arXiv|https://github.com/nsothman/ChartFormer|https://doi.org/10.48550/arXiv.2405.19117|
|1178|Fast and Effective Weight Update for Pruned Large Language Models|Vladimír Boza|2024-01-01|Trans. Mach. Learn. Res.|https://github.com/fmfi-compbio/admm-pruning|https://openreview.net/forum?id=1hcpXd9Jir|
|1179|ChatASD: LLM-Based AI Therapist for ASD|Xiaoyu Ren, Yuanchen Bai, Huiyu Duan, Lei Fan, Erkang Fei, Geer Wu, Pradeep Ray, Menghan Hu, Chenyuan Yan, Guangtao Zhai|2024-01-01|Digital Multimedia Communications|https://github.com/DuanHuiyu/ChatASD|http://dx.doi.org/10.1007/978-981-97-3626-3_23|
|1180|IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models|Shaokun Zhang, Xiaobo Xia, Zhaoqing Wang, Ling-Hao Chen, Jiale Liu, Qingyun Wu, Tongliang Liu|2024-01-01|arXiv|https://skzhang1.github.io/IDEAL/|https://doi.org/10.48550/arXiv.2310.10873|
|1181|ICE-Score: Instructing Large Language Models to Evaluate Code|Terry Yue Zhuo|2024-01-01|EACL|https://github.com/terryyz/ice-score|https://aclanthology.org/2024.findings-eacl.148|
|1182|Harnessing Explanations: LLM-to-LM Interpreter for Enhanced Text-Attributed Graph Representation Learning|Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, Bryan Hooi|2024-01-01|12th International Conference on Learning Representations, ICLR 2024|https://github.com/XiaoxinHe/TAPE|http://arxiv.org/abs/2305.19523v5|
|1183|Guiding Instruction-based Image Editing via Multimodal Large Language Models|Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, Zhe Gan|2024-01-01|arXiv|https://mllm-ie.github.io|https://doi.org/10.48550/arXiv.2309.17102|
|1184|Generating Contextualized Mathematics Multiple-Choice Questions Utilizing Large Language Models|Ruijia Li, Yiting Wang, Chanjin Zheng, Yuan-Hao Jiang, Bo Jiang|2024-01-01|AIED Companion|https://github.com/youzizzz1028/MCQ-generation-Chain|https://doi.org/10.1007/978-3-031-64315-6_48|
|1185|GenTKG: Generative Forecasting on Temporal Knowledge Graph with Large Language Models|Ruotong Liao, Xu Jia, Yangzhe Li, Yunpu Ma, Volker Tresp|2024-01-01|Findings of the Association for Computational Linguistics: NAACL 2024 - Findings|https://github.com/mayhugotong/GenTKG|http://arxiv.org/abs/2310.07793v5|
|1186|GEE! Grammar Error Explanation with Large Language Models|Yixiao Song, Kalpesh Krishna, Rajesh Bhatt, Kevin Gimpel, Mohit Iyyer|2024-01-01|arXiv|https://github.com/Yixiao-Song/GEE-with-LLMs|https://doi.org/10.48550/arXiv.2311.09517|
|1187|From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning|Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, Jing Xiao|2024-01-01|Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2024|https://github.com/tianyi-lab/Cherry_LLM|http://arxiv.org/abs/2308.12032v5|
|1188|Found in the Middle: Permutation Self-Consistency Improves Listwise Ranking in Large Language Models|Raphael Tang, Xinyu Crystina Zhang, Xueguang Ma, Jimmy Lin, Ferhan Ture|2024-01-01|NAACL-HLT|https://github.com/castorini/perm-sc|https://doi.org/10.18653/v1/2024.naacl-long.129|
|1189|FirewaLLM: A Portable Data Protection and Recovery Framework for LLM Services|Bin Huang, Shiyu Yu, Jin Li, Yuyang Chen, Shaozheng Huang, Sufen Zeng, Shaowei Wang|2024-01-01|Data Mining and Big Data|https://github.com/ysy1216/FirewaLLM|http://dx.doi.org/10.1007/978-981-97-0844-4_2|
|1190|FinDABench: Benchmarking Financial Data Analysis Ability of Large Language Models|Shu Liu, Shangqing Zhao, Chenghao Jia, Xinlin Zhuang, Zhaoguang Long, Jie Zhou, Aimin Zhou, Man Lan, Qingquan Wu, Chong Yang|2024-01-01|arXiv|https://github.com/cubenlp/BIBench|http://arxiv.org/abs/2401.02982v4|
|1191|GeoLLM: Extracting Geospatial Knowledge from Large Language Models|Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David B. Lobell, Stefano Ermon|2024-01-01|arXiv|https://rohinmanvi.github.io/GeoLLM|https://doi.org/10.48550/arXiv.2310.06213|
|1192|Eureka: Human-Level Reward Design via Coding Large Language Models|Yecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayaraman, Yuke Zhu, Linxi Fan, Anima Anandkumar|2024-01-01|arXiv|https://eureka-research.github.io/|https://doi.org/10.48550/arXiv.2310.12931|
|1193|DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models|Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao Ma, Pinlong Cai, Min Dou, Botian Shi, Liang He, Yu Qiao|2024-01-01|arXiv|https://pjlab-adg.github.io/DiLu/|https://doi.org/10.48550/arXiv.2309.16292|
|1194|ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate|Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, Zhiyuan Liu|2024-01-01|12th International Conference on Learning Representations, ICLR 2024|https://github.com/chanchimin/ChatEval|http://arxiv.org/abs/2308.07201v1|
|1195|Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic|Xufeng Zhao, Mengdi Li, Wenhao Lu, Cornelius Weber, Jae Hee Lee, Kun Chu, Stefan Wermter|2024-01-01|LREC/COLING|https://github.com/xf-zhao/LoT|https://aclanthology.org/2024.lrec-main.543|
|1196|CoachLM: Automatic Instruction Revisions Improve the Data Quality in LLM Instruction Tuning|Y. Liu, S. Tao, X. Zhao, M. Zhu, W. Ma, J. Zhu, C. Su, Y. Hou, M. Zhang, M. Zhang, H. Ma, L. Zhang, H. Yang, Y. Jiang|2024-01-01|2024 IEEE 40th International Conference on Data Engineering (ICDE)|https://github.com/lunyiliu/CoachLM|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597991|
|1197|ContraDoc: Understanding Self-Contradictions in Documents with Large Language Models|Jierui Li, Vipul Raheja, Dhruv Kumar|2024-01-01|arXiv|https://github.com/ddhruvkr/CONTRADOC|https://doi.org/10.48550/arXiv.2311.09182|
|1198|DP-OPT: Make Large Language Model Your Privacy-Preserving Prompt Engineer|Junyuan Hong, Jiachen T. Wang, Chenhui Zhang, Zhangheng Li, Bo Li, Zhangyang Wang|2024-01-01|arXiv|https://github.com/VITA-Group/DP-OPT|https://doi.org/10.48550/arXiv.2312.03724|
|1199|INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models|Yew Ken Chia, Pengfei Hong, Lidong Bing, Soujanya Poria|2024-01-01|arXiv|https://github.com/declare-lab/instruct-eval|https://doi.org/10.48550/arXiv.2306.04757|
|1200|Drive Like a Human: Rethinking Autonomous Driving with Large Language Models|Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, Yu Qiao|2024-01-01|2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)|https://github.com/PJLab-ADG/DriveLikeAHuman|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10495699|
|1201|DyVal: Dynamic Evaluation of Large Language Models for Reasoning Tasks|Kaijie Zhu, Jiaao Chen, Jindong Wang, Neil Zhenqiang Gong, Diyi Yang, Xing Xie|2024-01-01|ICLR|https://github.com/microsoft/promptbench|https://openreview.net/forum?id=gjfOL9z5Xr|
|1202|EAI-SIM: An Open-Source Embodied AI Simulation Framework with Large Language Models|Guocai Liu, Tao Sun, Weihua Li, Xiaohui Li, Xin Liu, Jinqiang Cui|2024-01-01|2024 IEEE 18th International Conference on Control & Automation (ICCA)|https://github.com/PengICS/eai_sim|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10591865|
|1203|Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective|Jiatong Li, Yunqing Liu, Wenqi Fan, Xiao-Yong Wei, Hui Liu, Jiliang Tang, Qing Li|2024-01-01|IEEE Transactions on Knowledge and Data Engineering|https://github.com/phenixace/MolReGPT|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10516270|
|1204|DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models|Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James R. Glass, Pengcheng He|2024-01-01|arXiv|https://github.com/voidism/DoLa|https://doi.org/10.48550/arXiv.2309.03883|
|1205|MIMo: A Multi-Modal Infant Model for Studying Cognitive Development|D. Mattern, P. Schumacher, F. M. López, M. C. Raabe, M. R. Ernst, A. Aubret, J. Triesch|2024|IEEE Transactions on Cognitive and Developmental Systems|https://github.com/trieschlab/MIMo|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10382408|
|1206|MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models|Yilin Wen, Zifeng Wang, Jimeng Sun|2024|arXiv|https://github.com/wyl-willing/MindMap|https://doi.org/10.48550/arXiv.2308.09729|
|1207|MOSA: Music Motion with Semantic Annotation Dataset for Cross-Modal Music Processing|Y. -F. Huang, N. Moran, S. Coleman, J. Kelly, S. -H. Wei, P. -Y. Chen, Y. -H. Huang, T. -P. Chen, Y. -C. Kuo, Y. -C. Wei, C. -H. Li, D. -Y. Huang, H. -K. Kao, T. -W. Lin, L. Su|2024|IEEE/ACM Transactions on Audio, Speech, and Language Processing|https://github.com/yufenhuang/MOSA-Music-mOtion-and-Semantic-Annotation-dataset|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10542439|
|1208|Machine Vision Therapy: Multimodal Large Language Models Can Enhance Visual Robustness via Denoising In-Context Learning|Zhuo Huang, Chang Liu, Yinpeng Dong, Hang Su, Shibao Zheng, Tongliang Liu|2024|arXiv|https://github.com/tmllab/Machine_Vision_Therapy|https://doi.org/10.48550/arXiv.2312.02546|
|1209|Matching Problem Statements to Editorials in Competitive Programming|I. G. Dinu, C. Mihăescu, T. Rebedea|2024|2024 IEEE International Conference on Advanced Learning Technologies (ICALT)|https://github.com/DinuGeorge0019/MatchingProblemStatementsToEditorialsInCP|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10645920|
|1210|MedAgents: Large Language Models as Collaborators for Zero-shot Medical Reasoning|Xiangru Tang, Anni Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao, Xingyao Zhang, Arman Cohan, Mark Gerstein|2024|arXiv|https://github.com/gersteinlab/MedAgents|https://doi.org/10.48550/arXiv.2311.10537|
|1211|Merging Decision Transformers: Weight Averaging for Forming Multi-Task Policies|D. Lawson, A. H. Qureshi|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://github.com/daniellawson9999/merging-decision-transformer|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610919|
|1212|Meta-Reasoning: Semantics-Symbol Deconstruction for Large Language Models|Yiming Wang, Zhuosheng Zhang, Pei Zhang, Baosong Yang, Rui Wang|2024|ACL|https://github.com/Alsace08/Meta-Reasoning|https://aclanthology.org/2024.findings-acl.34|
|1213|Metasql: A Generate-Then-Rank Framework for Natural Language to SQL Translation|Y. Fan, Z. He, T. Ren, C. Huang, Y. Jing, K. Zhang, X. S. Wang|2024|2024 IEEE 40th International Conference on Data Engineering (ICDE)|https://github.com/Kaimary/MetaSQL|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597742|
|1214|MotionGPT: Human Motion Synthesis with Improved Diversity and Realism via GPT-3 Prompting|J. Ribeiro-Gomes, T. Cai, Z. Á. Milacski, C. Wu, A. Prakash, S. Takagi, A. Aubel, D. Kim, A. Bernardino, F. De La Torre|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/humansensinglab/MotionGPT|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484383|
|1215|Mitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement|C. Li, D. Chen, Y. Zhang, P. A. Beerel|2024|ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/HowardLi0816/dual-fusion-diffusion|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446820|
|1216|Mol-Instructions: A Large-Scale Biomolecular Instruction Dataset for Large Language Models|Yin Fang, Xiaozhuan Liang, Ningyu Zhang, Kangwei Liu, Rui Huang, Zhuo Chen, Xiaohui Fan, Huajun Chen|2024|arXiv|https://github.com/zjunlp/Mol-Instructions|https://doi.org/10.48550/arXiv.2306.08018|
|1217|On Context Utilization in Summarization with Large Language Models|Mathieu Ravaut, Aixin Sun, Nancy F. Chen, Shafiq Joty|2024|ACL|https://github.com/ntunlp/MiddleSum|https://aclanthology.org/2024.acl-long.153|
|1218|PoisonPrompt: Backdoor Attack on Prompt-Based Large Language Models|Hongwei Yao, Jian Lou, Zhan Qin|2024|ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/grasses/PoisonPrompt|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446267|
|1219|Open-NeRF: Towards Open Vocabulary NeRF Decomposition|H. Zhang, F. Li, N. Ahuja|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/haoz19/Open-NeRF|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484306|
|1220|Parameter Efficient Point Cloud Prompt Tuning for Unified Point Cloud Understanding|B. Fei, L. Liu, W. Yang, Z. Li, W. -M. Chen, L. Ma|2024|IEEE Transactions on Intelligent Vehicles|https://github.com/Fayeben/PCPT|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10552382|
|1221|Performance Analysis of Llama 2 Among Other LLMs|D. Huang, Z. Hu, Z. Wang|2024|2024 IEEE Conference on Artificial Intelligence (CAI)|https://github.com/inflaton/Llama-2-eval|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10605474|
|1222|PE-wav2vec: A Prosody-Enhanced Speech Model for Self-Supervised Prosody Learning in TTS|Z. -C. Liu, L. Chen, Y. -J. Hu, Z. -H. Ling, J. Pan|2024|IEEE/ACM Transactions on Audio, Speech, and Language Processing|https://ttsbylzc.github.io/PE-wav2vec|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10645206|
|1223|LLM-Grounder: Open-Vocabulary 3D Visual Grounding with Large Language Model as an Agent|Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan, Madhavan Iyengar, David F. Fouhey, Joyce Chai|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://chat-with-nerf.github.io/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610443|
|1224|M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models|Wai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou Li, Yuxin Jiang, Lifeng Shang, Qun Liu, Kam-Fai Wong|2024|arXiv|https://github.com/KwanWaiChung/M4LE|https://doi.org/10.48550/arXiv.2310.19240|
|1225|LogPrompt: Prompt Engineering Towards Zero-Shot and Interpretable Log Analysis|Y. Liu, S. Tao, W. Meng, F. Yao, X. Zhao, H. Yang|2024|2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)|https://github.com/lunyiliu/LogPrompt|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554918|
|1226|Q-BENCH: A Benchmark for Multi-modal Foundation Models on Low-level Vision from Single Images to Pairs|Z. Zhang, H. Wu, E. Zhang, G. Zhai, W. Lin|2024|IEEE Transactions on Pattern Analysis and Machine Intelligence|https://github.com/Q-Future/Q-Bench|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643329|
|1227|KnowLog: Knowledge Enhanced Pretrained Language Model for Log Understanding|L. Ma, W. Yang, B. Xu, S. Jiang, B. Fei, J. Liang, M. Zhou, Y. Xiao|2024|2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)|https://github.com/LeaperOvO/KnowLog|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548568|
|1228|Knowledge-Enhanced Recommendation with User-Centric Subgraph Network|G. Liu, Q. Yao, Y. Zhang, L. Chen|2024|2024 IEEE 40th International Conference on Data Engineering (ICDE)|https://github.com/leolouis14/KUCNet.1|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597876|
|1229|Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models|Ran Xu, Hejie Cui, Yue Yu, Xuan Kan, Wenqi Shi, Yuchen Zhuang, May Dongmei Wang, Wei Jin, Joyce Ho, Carl Yang|2024|ACL|https://github.com/ritaranx/ClinGen|https://aclanthology.org/2024.findings-acl.916|
|1230|LLM-CloudSec: Large Language Model Empowered Automatic and Deep Vulnerability Analysis for Intelligent Clouds|Daipeng Cao, W. Jun|2024|IEEE INFOCOM 2024 - IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)|https://github.com/DPCa0/LLM-CloudSec|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10620804|
|1231|LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models|Long Lian, Boyi Li, Adam Yala, Trevor Darrell|2024|arXiv|https://llm-grounded-diffusion.github.io|https://doi.org/10.48550/arXiv.2305.13655|
|1232|LLMCompass: Enabling Efficient Hardware Design for Large Language Model Inference|Hengrui Zhang, August Ning, Rohan Baskar Prabhakar, David Wentzlaff|2024|2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)|https://github.com/PrincetonUniversity/LLMCompass|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10609604|
|1233|Language-Conditioned Affordance-Pose Detection in 3D Point Clouds|T. Nguyen, M. N. Vu, B. Huang, T. Van Vo, V. Truong, N. Le, T. Vo, B. Le, A. Nguyen|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://3DAPNet.github.io|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610008|
|1234|Language-Guided Progressive Attention for Visual Grounding in Remote Sensing Images|K. Li, D. Wang, H. Xu, H. Zhong, C. Wang|2024|IEEE Transactions on Geoscience and Remote Sensing|https://github.com/like413/OPT-RSVG|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10584552|
|1235|Large Language Models are not Fair Evaluators|Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Lingpeng Kong, Qi Liu, Tianyu Liu, Zhifang Sui|2024|arXiv|https://github.com/i-Eval/FairEval|https://doi.org/10.48550/arXiv.2305.17926|
|1236|Large Language Models can be Guided to Evade AI-Generated Text Detection|Ning Lu, Shengcai Liu, Rui He, Yew-Soon Ong, Qi Wang, Ke Tang|2024|arXiv|https://github.com/ColinLu50/Evade-GPT-Detector|https://doi.org/10.48550/arXiv.2305.10847|
|1237|Leveraging Multimodal Knowledge for Spatio-Temporal Action Localization|K. Chen, T. Zhewei, X. Shu|2024|2024 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)|https://github.com/CKK-coder/Chaotic_World/tree/master|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10645431|
|1238|Lifelong Robot Library Learning: Bootstrapping Composable and Generalizable Skills for Embodied Control with Language Models|G. Tziafas, H. Kasaei|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://gtziafas.github.io/LRLL_project/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611448|
|1239|Lightening-Transformer: A Dynamically-Operated Optically-Interconnected Photonic Transformer Accelerator|H. Zhu, J. Gu, H. Wang, Z. Jiang, Z. Zhang, R. Tang, C. Feng, S. Han, R. T. Chen, D. Z. Pan|2024|2024 IEEE International Symposium on High-Performance Computer Architecture (HPCA)|https://github.com/zhuhanqing/Lightening-Transformer|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10476418|
|1240|Lightweight Model Pre-Training Via Language Guided Knowledge Distillation|M. Li, L. Zhang, M. Zhu, Z. Huang, G. Yu, J. Fan, T. Chen|2024|IEEE Transactions on Multimedia|https://github.com/mZhenz/LGD|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10551493|
|1241|Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes|Sunjun Kweon, Junu Kim, Jiyoun Kim, Sujeong Im, Eunbyeol Cho, Seongsu Bae, Jungwoo Oh, Gyubok Lee, Jong Hak Moon, Seng Chan You, Seungjin Baek, Chang Hoon Han, Yoon Bin Jung, Yohan Jo, Edward Choi|2024|arXiv|https://github.com/starmpcc/Asclepius|https://doi.org/10.48550/arXiv.2309.00237|
|1242|Vision Language Models in Autonomous Driving: A Survey and Outlook|X. Zhou, M. Liu, E. Yurtsever, B. L. Zagar, W. Zimmer, H. Cao, A. C. Knoll|2024|IEEE Transactions on Intelligent Vehicles|https://github.com/ge25nab/Awesome-VLM-AD-ITS|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531702|
|1243|Query-guided Attention in Vision Transformers for Localizing Objects Using a Single Sketch|A. Tripathi, A. Mishra, A. Chakraborty|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://vcl-iisc.github.io/locformer/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484208|
|1244|Investigating Large Language Models and Control Mechanisms to Improve Text Readability of Biomedical Abstracts|Zihao Li, Samuel Belkadi, Nicolo Micheletti, Lifeng Han, Matthew Shardlow, Goran Nenadic|2024|2024 IEEE 12th International Conference on Healthcare Informatics (ICHI)|https://github.comlHECTA-UoMlPLABA-MU|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10628856|
|1245|TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models|Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Haotian Wang, Ming Liu, Bing Qin|2024|arXiv|https://github.com/zchuz/TimeBench|https://doi.org/10.48550/arXiv.2311.17667|
|1246|TimeDRL: Disentangled Representation Learning for Multivariate Time-Series|C. Chang, C. -T. Chan, W. -Y. Wang, W. -C. Peng, T. -F. Chen|2024|2024 IEEE 40th International Conference on Data Engineering (ICDE)|https://github.com/blacksnail789521/TimeDRL|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597874|
|1247|Toward Grounded Commonsense Reasoning|M. Kwon, H. Hu, V. Myers, S. Karamcheti, A. Dragan, D. Sadigh|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://minaek.github.io/grounded_commonsense_reasoning/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611218|
|1248|Towards LLM4PCG: A Preliminary Evaluation of Open-Weight Large Language Models Beyond ChatGPT4PCG|P. Taveekitworachai, Y. Xia, P. Suntichaikul, R. Thawonmas|2024|2024 IEEE Conference on Games (CoG)|https://github.com/Pittawat2542/llm4pcg-python|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10645646|
|1249|Towards Rumor Detection with Multi-granularity Evidences: A Dataset and Benchmark|Z. Yang, J. Lin, Z. Guo, Y. Li, X. Li, Q. Li, W. Liu|2024|IEEE Transactions on Knowledge and Data Engineering|https://github.com/zhengyang5/RDE|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10531683|
|1250|Towards Visual-Prompt Temporal Answer Grounding in Instructional Video|S. Li, B. Li, B. Sun, Y. Weng|2024|IEEE Transactions on Pattern Analysis and Machine Intelligence|https://github.com/wengsyx/VPTSL|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10552074|
|1251|VDTuner: Automated Performance Tuning for Vector Data Management Systems|T. Yang, W. Hu, W. Peng, Y. Li, J. Li, G. Wang, X. Liu|2024|2024 IEEE 40th International Conference on Data Engineering (ICDE)|https://github.com/tiannuo-yanWVDTuner|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597809|
|1252|Vision-Language Interpreter for Robot Task Planning|K. Shirai, C. C. Beltran-Hernandez, M. Hamaya, A. Hashimoto, S. Tanaka, K. Kawaharazuka, K. Tanaka, Y. Ushiku, S. Mori|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://github.com/omron-sinicx/ViLaIn|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611112|
|1253|RIC: Rotate-Inpaint-Complete for Generalizable Scene Reconstruction|I. Kasahara, S. Agrawal, S. Engin, N. Chavan-Dafle, S. Song, V. Isler|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://samsunglabs.github.io/RIC-project-page/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611694|
|1254|Vision-Language Models Can Identify Distracted Driver Behavior From Naturalistic Videos|M. Z. Hasan, J. Chen, J. Wang, M. S. Rahman, A. Joshi, S. Velipasalar, C. Hegde, A. Sharma, S. Sarkar|2024|IEEE Transactions on Intelligent Transportation Systems|https://github.com/zahid-isu/DriveCLIP|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10492662|
|1255|Visual Hallucinations of Multi-modal Large Language Models|Wen Huang, Hongbin Liu, Minxin Guo, Neil Zhenqiang Gong|2024|ACL|https://github.com/PrimaveralScientist/VHTest|https://aclanthology.org/2024.findings-acl.573|
|1256|WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models|Shangqing Tu, Yuliang Sun, Yushi Bai, Jifan Yu, Lei Hou, Juanzi Li|2024|arXiv|https://github.com/THU-KEG/WaterBench|https://doi.org/10.48550/arXiv.2311.07138|
|1257|WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research|X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley, Y. Zou, W. Wang|2024|IEEE/ACM Transactions on Audio, Speech, and Language Processing|https://github.com/XinhaoMei/WavCaps|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10572302|
|1258|Weakly Correlated Multimodal Sentiment Analysis: New Dataset and Topic-Oriented Model|W. Liu, W. Li, Y. -P. Ruan, Y. Shu, J. Chen, Y. Li, C. Yu, Y. Zhang, J. Guan, S. Zhou|2024|IEEE Transactions on Affective Computing|https://github.com/PhenoixYANG/TOM|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10517619|
|1259|Zero and Few Short Learning Using Large Language Models for De-Identification of Medical Records|Y. S. Yashwanth, Rajashree Shettar|2024|IEEE Access|https://github.com/YashwanthYS/De-Identification-of-medical-Records|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10630540|
|1260|Zero-Shot Text Normalization via Cross-lingual Knowledge Distillation|L. Wang, X. Huang, Z. Yu, H. Peng, S. Gao, C. Mao, Y. Huang, L. Dong, P. S. Yu|2024|IEEE/ACM Transactions on Audio, Speech, and Language Processing|https://github.com/wlq2019/Zero-Shot-Text-Normalization|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10577109|
|1261|TestSpark: IntelliJ IDEA's Ultimate Test Generation Companion|A. Sapozhnikov, M. Olsthoorn, A. Panichella, V. Kovalenko, P. Derakhshanfar|2024|2024 IEEE/ACM 46th International Conference on Software Engineering: Companion Proceedings (ICSE-Companion)|https://github.com/JetBrains-Research/TestSpark|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10554841|
|1262|Talk2BEV: Language-enhanced Bird’s-eye View Maps for Autonomous Driving|T. Choudhary, V. Dewangan, S. Chandhok, S. Priyadarshan, A. Jain, A. K. Singh, S. Srivastava, K. M. Jatavallabhula, K. M. Krishna|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://llmbev.github.io/talk2bev/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611485|
|1263|Synergistic Interplay between Search and Large Language Models for Information Retrieval|Jiazhan Feng, Chongyang Tao, Xiubo Geng, Tao Shen, Can Xu, Guodong Long, Dongyan Zhao, Daxin Jiang|2024|ACL|https://github.com/Cyril-JZ/InteR|https://aclanthology.org/2024.acl-long.517|
|1264|Symbol-LLM: Towards Foundational Symbol-centric Interface For Large Language Models|Fangzhi Xu, Zhiyong Wu, Qiushi Sun, Siyu Ren, Fei Yuan, Shuai Yuan, Qika Lin, Yu Qiao, Jun Liu|2024|arXiv|https://xufangzhi.github.io/symbol-llm-page/|https://doi.org/10.48550/arXiv.2311.09278|
|1265|RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question Answering|Y. Wang, P. Ghamisi|2024|IEEE Transactions on Geoscience and Remote Sensing|https://github.com/Y-D-Wang/RSAdapter|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10555327|
|1266|Refining GPT-3 Embeddings with a Siamese Structure for Technical Post Duplicate Detection|X. Wu, H. Li, N. Yoshioka, H. Washizaki, F. Khomh|2024|2024 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)|https://github.com/mooselab/suppmaterial-PostDupGPT3|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10589878|
|1267|RoCo: Dialectic Multi-Robot Collaboration with Large Language Models|Zhao Mandi, Shreeya Jain, Shuran Song|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://project-roco.github.io|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610855|
|1268|RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking|H. Bharadhwaj, J. Vakil, M. Sharma, A. Gupta, S. Tulsiani, V. Kumar|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://robopen.github.io/for|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611293|
|1269|RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models|Zijun Long, George Killick, Richard McCreadie, Gerardo Aragon-Camarasa|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://github.com/longkukuhi/RoboLLM|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610797|
|1270|RoleLLM: Benchmarking, Eliciting, and Enhancing Role-Playing Abilities of Large Language Models|Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Stephen W. Huang, Jie Fu, Junran Peng|2024|ACL|https://github.com/InteractiveNLP-Team/RoleLLM-public|https://aclanthology.org/2024.findings-acl.878|
|1271|SPIN: Sparsifying and Integrating Internal Neurons in Large Language Models for Text Classification|Difan Jiao, Yilun Liu, Zhenwei Tang, Daniel Matter, Jürgen Pfeffer, Ashton Anderson|2024|ACL|https://github.com/difanj0713/SPIN|https://aclanthology.org/2024.findings-acl.277|
|1272|SPOTS: Stable Placement of Objects with Reasoning in Semi-Autonomous Teleoperation Systems|J. Lee, S. Park, J. Park, K. Lee, S. Choi|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://joonhyunglee.github.io/spots/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611613|
|1273|SafetyBench: Evaluating the Safety of Large Language Models with Multiple Choice Questions|Zhexin Zhang, Leqi Lei, Lindong Wu, Rui Sun, Yongkang Huang, Chong Long, Xiao Liu, Xuanyu Lei, Jie Tang, Minlie Huang|2024|arXiv|https://github.com/thu-coai/SafetyBench|https://doi.org/10.48550/arXiv.2309.07045|
|1274|Scalable Multi-Robot Collaboration with Large Language Models: Centralized or Decentralized Systems?|Yongchao Chen, Jacob Arkin, Yang Zhang, Nicholas Roy, Chuchu Fan|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://yongchao98.github.io/MIT-REALM-Multi-Robot/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610676|
|1275|Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models|Jinhao Duan, Hao Cheng, Shiqi Wang, Alex Zavalny, Chenan Wang, Renjing Xu, Bhavya Kailkhura, Kaidi Xu|2024|ACL|https://github.com/jinhaoduan/SAR|https://aclanthology.org/2024.acl-long.276|
|1276|Single-Frame Supervision for Spatio-Temporal Video Grounding|K. Liu, M. Qu, Y. Liu, Y. Wei, W. Zhe, Y. Zhao, W. Liu|2024|IEEE Transactions on Pattern Analysis and Machine Intelligence|https://github.com/qumengxue/T-SMILE|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10559975|
|1277|SparseCoder: Identifier-Aware Sparse Transformer for File- Level Code Summarization|Y. Wang, Y. Huang, D. Guo, H. Zhang, Z. Zheng|2024|2024 IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER)|https://github.com/DeepSoftwareAnalytics/SparseCoder|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10589798|
|1278|SpectralCLIP: Preventing Artifacts in Text-Guided Style Transfer from a Spectral Perspective|Z. Xu, S. Xing, E. Sangineto, N. Sebe|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/zipengxuc/SpectralCLIP|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484323|
|1279|Structured Chemistry Reasoning with Large Language Models|Siru Ouyang, Zhuosheng Zhang, Bing Yan, Xuan Liu, Yejin Choi, Jiawei Han, Lianhui Qin|2024|arXiv|https://github.com/ozyyshr/StructChem|https://doi.org/10.48550/arXiv.2311.09656|
|1280|Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs|Wenke Xia, Dong Wang, Xincheng Pang, Zhigang Wang, Bin Zhao, Di Hu, Xuelong Li|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://github.com/GeWu-Lab/LLM_articulated_object_manipulation|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610744|
|1281|Enhancing Multimodal Compositional Reasoning of Visual Language Models with Generative Negative Mining|U. Sahin, H. Li, Q. Khan, D. Cremers, V. Tresp|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://ugorsahin.github.io/enhancing-multimodal-compositional-reasoning-of-vlm.html|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484294|
|1282|Integrating Language Models with Symbolic Formulas for First-Order Logic Reasoning|Y. Sheng, L. Li, Y. Wang, D. Zeng|2024|ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/FOL-GNN|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446308|
|1283|Breaking the Silence: Whisper-Driven Emotion Recognition in AI Mental Support Models|X. Qu, Z. Sun, S. Feng, C. Chen, T. Tian|2024|2024 IEEE Conference on Artificial Intelligence (CAI)|https://github.com/xinghua-qu/speech_emotion_recognition|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10605509|
|1284|Augment the Pairs: Semantics-Preserving Image-Caption Pair Augmentation for Grounding-Based Vision and Language Models|J. Yi, B. Uzkent, O. Ignat, Z. Li, A. Garg, X. Yu, L. Liu|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/amzn/augment-the-pairs-wacv2024|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484174|
|1285|AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers|Yongchao Chen, Jacob Arkin, Charles Dawson, Yang Zhang, Nicholas Roy, Chuchu Fan|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://yongchao98.github.io/MIT-REALM-AutoTAMP/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611163|
|1286|BIBench: Benchmarking Data Analysis Knowledge of Large Language Models|Shu Liu, Shangqing Zhao, Chenghao Jia, Xinlin Zhuang, Zhaoguang Long, Man Lan|2024|arXiv|https://github.com/xxx|https://doi.org/10.48550/arXiv.2401.02982|
|1287|Benchmarking Cognitive Biases in Large Language Models as Evaluators|Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park, Zae Myung Kim, Dongyeop Kang|2024|arXiv|https://minnesotanlp.github.io/cobbler|https://doi.org/10.48550/arXiv.2309.17012|
|1288|Bi-directional Training for Composed Image Retrieval via Text Prompt Learning|Z. Liu, W. Sun, Y. Hong, D. Teney, S. Gould|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/Cuberick-Orion/Bi-Blip4CIR|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484044|
|1289|Black-Box Prompt Optimization: Aligning Large Language Models without Model Training|Jiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning Wang, Yuxiao Dong, Jie Tang, Minlie Huang|2024|arXiv|https://github.com/thu-coai/BPO|https://doi.org/10.48550/arXiv.2311.04155|
|1290|Bootstrapping Interactive Image–Text Alignment for Remote Sensing Image Captioning|C. Yang, Z. Li, L. Zhang|2024|IEEE Transactions on Geoscience and Remote Sensing|https://github.com/yangcong356/BITA|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415446|
|1291|Instruction Position Matters in Sequence Generation with Large Language Models|Yijin Liu, Xianfeng Zeng, Chenze Shao, Fandong Meng, Jie Zhou|2024|ACL|https://github.com/Adaxry/Post-Instruction/tree/main|https://aclanthology.org/2024.findings-acl.693|
|1292|ChatGPT4PCG 2 Competition: Prompt Engineering for Science Birds Level Generation|P. Taveekitworachai, F. Abdullah, M. F. Dewantoro, Y. Xia, P. Suntichaikul, R. Thawonmas, J. Togelius, J. Renz|2024|2024 IEEE Conference on Games (CoG)|https://github.com/chatgpt4pcg/experiments2024|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10645641|
|1293|CLOMO: Counterfactual Logical Modification with Large Language Models|Yinya Huang, Ruixin Hong, Hongming Zhang, Wei Shao, Zhicheng Yang, Dong Yu, Changshui Zhang, Xiaodan Liang, Linqi Song|2024|arXiv|https://github.com/Eleanor-H/CLOMO|https://doi.org/10.48550/arXiv.2311.17438|
|1294|CXR-IRGen: An Integrated Vision and Language Model for the Generation of Clinically Accurate Chest X-Ray Image-Report Pairs|J. Shentu, N. Al Moubayed|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/junjie-shentu/CXR-IRGen|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483744|
|1295|Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning|G. Zhang, Y. Zhang, K. Zhang, V. Tresp|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/gengyuanmax/WikiTiLo|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484512|
|1296|ChangeMamba: Remote Sensing Change Detection with Spatio-Temporal State Space Model|H. Chen, J. Song, C. Han, J. Xia, N. Yokoya|2024|IEEE Transactions on Geoscience and Remote Sensing|https://github.com/ChenHongruixuan/MambaCD|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10565926|
|1297|Characterizing Large Language Model Geometry Helps Solve Toxicity Detection and Generation|Randall Balestriero, Romain Cosentino, Sarath Shekkizhar|2024|ICML|https://github.com/RandallBalestriero/SplineLLM|https://openreview.net/forum?id=glfcwSsks8|
|1298|ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs|Zihao Zhao, Sheng Wang, Jinchen Gu, Yitao Zhu, Lanzhuju Mei, Zixu Zhuang, Zhiming Cui, Qian Wang, Dinggang Shen|2024|IEEE Transactions on Medical Imaging|https://github.com/zhaozh10/ChatCAD|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10522762|
|1299|ChatGPT for Robotics: Design Principles and Model Abilities|S. H. Vemprala, R. Bonatti, A. Bucker, A. Kapoor|2024|IEEE Access|https://github.com/microsoft/PromptCraft-Robotics|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10500490|
|1300|Audio-Journey: Open Domain Latent Diffusion Based Text-To-Audio Generation|J. Michaels, J. B. Li, L. Yao, L. Yu, Z. Wood-Doughty, F. Metze|2024|ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://audiojourney.github.io|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10448220|
|1301|Are Emergent Abilities in Large Language Models just In-Context Learning?|Sheng Lu, Irina Bigoulaeva, Rachneet Sachdeva, Harish Tayyar Madabushi, Iryna Gurevych|2024|arXiv|https://github.com/UKPLab/on-emergence|https://doi.org/10.48550/arXiv.2309.01809|
|1302|Annotation-free Audio-Visual Segmentation|J. Liu, Y. Wang, C. Ju, C. Ma, Y. Zhang, W. Xie|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://jinxiang-liu.github.io/anno-free-AVS/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484434|
|1303|An Evaluation System for Large Language Models based on Open-Ended Questions|Zhiyuan Cao, Zeyu Ma, Mingang Chen|2024|2024 IEEE 11th International Conference on Cyber Security and Cloud Computing (CSCloud)|https://github.com/JerryMazeyu/GreatLibrarian|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10605143|
|1304|3DMIT: 3D Multi-Modal Instruction Tuning for Scene Understanding|Z. Li, C. Zhang, X. Wang, R. Ren, Y. Xu, R. Ma, X. Liu, R. Wei|2024|2024 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)|https://github.com/staymylove/3DMIT|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10645462|
|1305|A Comprehensive Survey of Datasets for Large Language Model Evaluation|Y. Lu, C. Sun, Y. Yan, H. Zhu, D. Song, Q. Peng, L. Yu, X. Wang, J. Jiang, X. Ye|2024|2024 5th Information Communication Technologies Conference (ICTC)|https://github.com/lyt719/LLM-evaluation-datasets|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10601918|
|1306|A Dataset for Entity Recognition in the Automotive Warranty and Goodwill Domain|L. J. Weber, K. J. Ramalingam, M. Beyer, C. Liu, A. Zimmermann|2024|2024 7th International Conference on Artificial Intelligence and Big Data (ICAIBD)|https://github.com/lukaas95IAutomotive|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10604504|
|1307|A Large Dataset to Enhance Skin Cancer Classification With Transformer-Based Deep Neural Networks|M. Gallazzi, S. Biavaschi, A. Bulgheroni, T. M. Gatti, S. Corchs, I. Gallo|2024|IEEE Access|https://github.com/UnluckyMirco/A-Large-Dataset-to-Enhance-Skin-Cancer-Classification-with-Transformer-Based-DNN|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10623626|
|1308|A Masked Reference Token Supervision based Iterative Visual-language Framework for Robust Visual Grounding|C. Wang, W. Feng, S. Lyu, G. Cheng, X. Li, B. Liu, Q. Zhao|2024|IEEE Transactions on Circuits and Systems for Video Technology|https://github.com/cv516Buaa/IR-VG|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10659810|
|1309|A Survey on Deep Neural Network Pruning: Taxonomy, Comparison, Analysis, and Recommendations|H. Cheng, M. Zhang, J. Q. Shi|2024|IEEE Transactions on Pattern Analysis and Machine Intelligence|https://github.com/hrcheng1066/awesome-pruning|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10643325|
|1310|A Survey on Self-supervised Learning: Algorithms, Applications, and Future Trends|J. Gui, T. Chen, J. Zhang, Q. Cao, Z. Sun, H. Luo, D. Tao|2024|IEEE Transactions on Pattern Analysis and Machine Intelligence|https://github.com/guijiejie/SSL|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10559458|
|1311|A Two-Phase Recall-and-Select Framework for Fast Model Selection|J. Cui, W. Shi, H. Tao, W. Lu, X. Du|2024|2024 IEEE 40th International Conference on Data Engineering (ICDE)|https://github.com/plasware/two-phase-selection|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10597714|
|1312|A Two-Stage Adaptation of Large Language Models for Text Ranking|Longhui Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, Meishan Zhang, Min Zhang|2024|ACL|https://github.com/Alibaba-NLP/RankingGPT|https://aclanthology.org/2024.findings-acl.706|
|1313|AbGraftBERT: Enhancing Antibody Design Through Self-Grafting and Amino Acid-Based Perturbation of Pre- Trained Language Models|Z. Wang, X. Li, Y. Xie, Z. Wen, R. Jin, H. Takada, R. Nagatomi|2024|IEEE Access|https://github.com/azusakou/AbGraftBERT|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10562302|
|1314|Active Prompting with Chain-of-Thought for Large Language Models|Shizhe Diao, Pengcheng Wang, Yong Lin, Rui Pan, Xiang Liu, Tong Zhang|2024|ACL|https://github.com/shizhediao/active-prompt|https://aclanthology.org/2024.acl-long.73|
|1315|Adan: Adaptive Nesterov Momentum Algorithm for Faster Optimizing Deep Models|X. Xie, P. Zhou, H. Li, Z. Lin, S. Yan|2024|IEEE Transactions on Pattern Analysis and Machine Intelligence|https://github.com/sail-sg/Adan|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10586270|
|1316|Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models|Bilgehan Sel, Ahmad Al-Tawaha, Vanshaj Khattar, Lu Wang, Ruoxi Jia, Ming Jin|2024|arXiv|https://algorithm-of-thoughts.github.io|https://doi.org/10.48550/arXiv.2308.10379|
|1317|AlignBench: Benchmarking Chinese Alignment of Large Language Models|Xiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Andrew Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, Xiaohan Zhang, Lichao Sun, Xiaotao Gu, Hongning Wang, Jing Zhang, Minlie Huang, Yuxiao Dong, Jie Tang|2024|ACL|https://github.com/THUDM/AlignBench|https://aclanthology.org/2024.acl-long.624|
|1318|An Empirical Study on Noisy Label Learning for Program Understanding|W. Wang, Y. Li, A. Li, J. Zhang, W. Ma, Y. Liu|2024|2024 IEEE/ACM 46th International Conference on Software Engineering (ICSE)|https://github.com/jacobwwh/noise_SE|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10548728|
|1319|ChatGPT for Visually Impaired and Blind|A. Kuzdeuov, O. Mukayev, S. Nurgaliyev, A. Kunbolsyn, H. A. Varol|2024|2024 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)|https://github.com/IS2AI/talk-llm|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10463430|
|1320|CLAP4Emo: ChatGPT-Assisted Speech Emotion Retrieval with Natural Language Supervision|W. -C. Lin, S. Ghaffarzadegan, L. Bondi, A. Kumar, S. Das, H. -H. Wu|2024|ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/boschresearch/soundsee-emo-caps|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10447102|
|1321|ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences|Yuanhe Tian, Ruyi Gan, Yan Song, Jiaxing Zhang, Yongdong Zhang|2024|arXiv|https://github.com/synlp/ChiMed-GPT|https://doi.org/10.48550/arXiv.2311.06025|
|1322|ClassifAI: Automating Issue Reports Classification using Pre-Trained BERT (Bidirectional Encoder Representations from Transformers) Language Models|K. A. Alam, A. Jumani, H. Aamir, M. Uzair|2024|2024 IEEE/ACM International Workshop on Natural Language-Based Software Engineering (NLBSE)|https://github.com/HarrisAamir/Issue-Report-Classification-NLBSE-2024|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10647166|
|1323|Exploiting CLIP for Zero-shot HOI Detection Requires Knowledge Distillation at Multiple Levels|B. Wan, T. Tuytelaars|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/bobwan1995/Zeroshot-HOI-with-CLIP|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483987|
|1324|FLTRNN: Faithful Long-Horizon Task Planning for Robotics with Large Language Models|Jiatao Zhang, Lanling Tang, Yufan Song, Qiwei Meng, Haofu Qian, Jun Shao, Wei Song, Shiqiang Zhu, Jason Gu|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://tannl.github.io/FLTRNN.github.io/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611663|
|1325|FUR-API: Dataset and Baselines Toward Realistic API Anomaly Detection|Y. Liu, H. Yu, F. Dai, X. Gu, C. Cui, B. Li, W. Wang|2024|ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)|https://github.com/yijunL/FUR-API|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446512|
|1326|Fine-Granularity Alignment for Text-Based Person Retrieval via Semantics-Centric Visual Division|Z. Wei, Z. Zhang, P. Wu, J. Wang, P. Wang, Y. Zhang|2024|IEEE Transactions on Circuits and Systems for Video Technology|https://github.com/tujun233/SCVD|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10507050|
|1327|FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models|Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, Wei Wang|2024|arXiv|https://github.com/YJiangcm/FollowBench|https://doi.org/10.48550/arXiv.2310.20410|
|1328|FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation|Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry W. Wei, Jason Wei, Chris Tar, Yun-Hsuan Sung, Denny Zhou, Quoc V. Le, Thang Luong|2024|arXiv|https://github.com/freshllms/freshqa|https://doi.org/10.48550/arXiv.2310.03214|
|1329|Full Parameter Fine-tuning for Large Language Models with Limited Resources|Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, Xipeng Qiu|2024|arXiv|https://github.com/OpenLMLab/LOMO|https://doi.org/10.48550/arXiv.2306.09782|
|1330|Generating with Confidence: Uncertainty Quantification for Black-box Large Language Models|Zhen Lin, Shubhendu Trivedi, Jimeng Sun|2024|arXiv|https://github.com/zlin7/UQ-NLG|https://doi.org/10.48550/arXiv.2305.19187|
|1331|Efficient Feature Distillation for Zero-shot Annotation Object Detection|Z. Liu, X. Hu, R. Nevatia|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/dragonlzm/EZAD|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483901|
|1332|Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation|Yuan Yang, Siheng Xiong, Ali Payani, Ehsan Shareghi, Faramarz Fekri|2024|arXiv|https://github.com/gblackout/LogicLLaMA|https://doi.org/10.48550/arXiv.2305.15541|
|1333|Having Beer after Prayer? Measuring Cultural Bias in Large Language Models|Tarek Naous, Michael J. Ryan, Alan Ritter, Wei Xu|2024|ACL|https://github.com/tareknaous/camel|https://aclanthology.org/2024.acl-long.862|
|1334|HazardVLM: A Video Language Model for Real-Time Hazard Description in Automated Driving Systems|D. Xiao, M. Dianati, P. Jennings, R. Woodman|2024|IEEE Transactions on Intelligent Vehicles|https://github.com/dannierxiao/HazardVLM|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10654513|
|1335|Human-Centric Autonomous Systems With LLMs for User Command Reasoning|Yi Yang, Qingwen Zhang, Ci Li, Daniel Simões Marta, Nazre Batool, John Folkesson|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)|https://github.com/KTH-RPL/DriveCmd_LLM|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10495632|
|1336|INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing|Hanbin Wang, Zhenghao Liu, Shuo Wang, Ganqu Cui, Ning Ding, Zhiyuan Liu, Ge Yu|2024|arXiv|https://github.com/NEUIR/INTERVENOR|https://doi.org/10.48550/arXiv.2311.09868|
|1337|Instruct Me More! Random Prompting for Visual In-Context Learning|J. Zhang, B. Wang, L. Li, Y. Nakashima, H. Nagahara|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/Jackieam/InMeMo|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10483971|
|1338|InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models|Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, Tianyi Zhou|2024|arXiv|https://lichang-chen.github.io/InstructZero/|https://doi.org/10.48550/arXiv.2306.03082|
|1339|Evaluating Spatial Understanding of Large Language Models|Yutaro Yamada, Yihan Bao, Andrew Kyle Lampinen, Jungo Kasai, Ilker Yildirim|2024|arXiv|https://github.com/runopti/SpatialEvalLLM|https://doi.org/10.48550/arXiv.2310.14540|
|1340|GenSim: Generating Robotic Simulation Tasks via Large Language Models|Lirui Wang, Yiyang Ling, Zhecheng Yuan, Mohit Shridhar, Chen Bao, Yuzhe Qin, Bailin Wang, Huazhe Xu, Xiaolong Wang|2024|arXiv|https://liruiw.github.io/gensim|https://doi.org/10.48550/arXiv.2310.01361|
|1341|Efficient Batched Inference in Conditional Neural Networks|S. Selvam, A. Nagarajan, A. Raghunathan|2024|IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems|https://github.com/surya00060/BatchCond|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10638141|
|1342|EconAgent: Large Language Model-Empowered Agents for Simulating Macroeconomic Activities|Nian Li, Chen Gao, Mingyu Li, Yong Li, Qingmin Liao|2024|ACL|https://github.com/tsinghua-fib-lab/ACL24-EconAgent|https://aclanthology.org/2024.acl-long.829|
|1343|Client-Server Application for Real-Time Video Streaming|S. N. Dobrea, D. Petrisor|2024|2024 47th International Conference on Telecommunications and Signal Processing (TSP)|https://github.com/silviu-nicolae-dobrea/Client-Server-Application-for-Real-Time-Video-Streaming|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10605960|
|1344|CoFRIDA: Self-Supervised Fine-Tuning for Human-Robot Co-Painting|P. Schaldenbrand, G. Parmar, J. -Y. Zhu, J. McCann, J. Oh|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://pschaldenbrand.github.io/cofrida/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610618|
|1345|CompeteAI: Understanding the Competition Behaviors in Large Language Model-based Agents|Qinlin Zhao, Jindong Wang, Yixuan Zhang, Yiqiao Jin, Kaijie Zhu, Hao Chen, Xing Xie|2024|arXiv|https://github.com/microsoft/competeai|https://doi.org/10.48550/arXiv.2310.17512|
|1346|Conditionally Combining Robot Skills using Large Language Models|K. R. Zentner, Ryan Julian, Brian Ichter, Gaurav S. Sukhatme|2024|2024 IEEE International Conference on Robotics and Automation (ICRA)|https://github.com/krzentner/language-world/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10611275|
|1347|Conversational Product Recommendation using LLM|T. -J. Chang, L. H. -M. Lin, R. T. -H. Tsai|2024|2024 IEEE 4th International Conference on Electronic Communications, Internet of Things and Big Data (ICEIB)|https://github.com/terryobe-ncu/CPR_LLM|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10602608|
|1348|Cross-Lingual Knowledge Editing in Large Language Models|Jiaan Wang, Yunlong Liang, Zengkui Sun, Yuxuan Cao, Jiarong Xu, Fandong Meng|2024|arXiv|https://github.com/krystalan/Bi_ZsRE|https://doi.org/10.48550/arXiv.2309.08952|
|1349|Cross3DVG: Cross-Dataset 3D Visual Grounding on Different RGB-D Scans|T. Miyanishi, D. Azuma, S. Kurita, M. Kawanabe|2024|2024 International Conference on 3D Vision (3DV)|https://github.com/ATR-DBI/Cross3DVG|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10550741|
|1350|Decomposing Uncertainty for Large Language Models through Input Clarification Ensembling|Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, Yang Zhang|2024|arXiv|https://github.com/UCSB-NLP-Chang/llm_uncertainty|https://doi.org/10.48550/arXiv.2311.08718|
|1351|Discrete Prompt Compression With Reinforcement Learning|H. Jung, K. -J. Kim|2024|IEEE Access|https://github.com/nenomigami/PromptCompressor|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10535182|
|1352|Disentangled Pre-training for Image Matting|Y. Li, Z. Huang, G. Yu, L. Chen, Y. Wei, J. Jiao|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://crystraldo.github.io/dpt_mat/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484476|
|1353|DistillMIKE: Editing Distillation of Massive In-Context Knowledge Editing in Large Language Models|Shanbao Qiao, Xuebing Liu, Seung-Hoon Na|2024|ACL|https://github.com/xxxx/xxxx$|https://aclanthology.org/2024.findings-acl.455|
|1354|DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model|Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee. K. Wong, Zhenguo Li, Hengshuang Zhao|2024|IEEE Robotics and Automation Letters|https://tonyxuqaq.github.io/projects/DriveGPT4/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10629039|
|1355|Dynamic Multimodal Information Bottleneck for Multimodality Classification|Y. Fang, S. Wu, S. Zhang, C. Huang, T. Zeng, X. Xing, S. Walsh, G. Yang|2024|2024 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)|https://github.com/ayanglab/DMIB|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10484170|
|1356|EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism|Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou|2024|arXiv|https://github.com/pan-x-c/EE-LLM|https://doi.org/10.48550/arXiv.2312.04916|
|1357|EarthGPT: A Universal Multimodal Large Language Model for Multisensor Image Comprehension in Remote Sensing Domain|Wei Zhang, Miaoxin Cai, Tong Zhang, Yin Zhuang, Xuerui Mao|2024|IEEE Transactions on Geoscience and Remote Sensing|https://github.com/wivizhang/EarthGPT|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10547418|
|1358|LifelongMemory: Leveraging LLMs for Answering Queries in Long-form Egocentric Videos|Ying Wang, Yanlai Yang, Mengye Ren|2023-12-07|arXiv|https://github.com/Agentic-Learning-AI-Lab/lifelong-memory|http://arxiv.org/abs/2312.05269v2|
|1359|Large Language Models are Good Prompt Learners for Low-Shot Image Classification|Zhaoheng Zheng, Jingmin Wei, Xuefeng Hu, Haidong Zhu, Ram Nevatia|2023-12-07|arXiv|https://github.com/zhaohengz/LLaMP|https://doi.org/10.48550/arXiv.2312.04076|
|1360|From Text to Structure: Using Large Language Models to Support the Development of Legal Expert Systems|Samyar Janatian, Hannes Westermann, Jinzhe Tan, Jaromír Savelka, Karim Benyekhlef|2023-12-07|JURIX|https://github.com/samyarj/JCAPG-JURIX2023|https://doi.org/10.3233/FAIA230962|
|1361|FlexModel: A Framework for Interpretability of Distributed Large Language Models|Matthew Choi, Muhammad Adil Asif, John Willes, David Emerson|2023-12-05|arXiv|https://github.com/VectorInstitute/flex_model|https://doi.org/10.48550/arXiv.2312.03140|
|1362|Large Language Models on Graphs: A Comprehensive Survey|Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, Jiawei Han|2023-12-05|arXiv|https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs|https://doi.org/10.48550/arXiv.2312.02783|
|1363|Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation|Shanshan Zhong, Zhongzhan Huang, Shanghua Gao, Wushao Wen, Liang Lin, Marinka Zitnik, Pan Zhou|2023-12-05|arXiv|https://zhongshsh.github.io/CLoT/|http://arxiv.org/abs/2312.02439v3|
|1364|E4SRec: An Elegant Effective Efficient Extensible Solution of Large Language Models for Sequential Recommendation|Xinhang Li, Chong Chen, Xiangyu Zhao, Yong Zhang, Chunxiao Xing|2023-12-05|arXiv|https://github.com/HestiaSky/E4SRec/|https://doi.org/10.48550/arXiv.2312.02443|
|1365|The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning|Bill Yuchen Lin, Abhilasha Ravichander, Ximing Lu, Nouha Dziri, Melanie Sclar, Khyathi Chandu, Chandra Bhagavatula, Yejin Choi|2023-12-04|arXiv|https://allenai.github.io/re-align/|http://arxiv.org/abs/2312.01552v1|
|1366|TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding|Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, Lu Hou|2023-12-04|arXiv|https://github.com/RenShuhuai-Andy/TimeChat|https://doi.org/10.48550/arXiv.2312.02051|
|1367|Tree of Attacks: Jailbreaking Black-Box LLMs Automatically|Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, Amin Karbasi|2023-12-04|arXiv|https://github.com/RICommunity/TAP|http://arxiv.org/abs/2312.02119v2|
|1368|StoryGPT-V: Large Language Models as Consistent Story Visualizers|Xiaoqian Shen, Mohamed Elhoseiny|2023-12-04|arXiv|https://xiaoqian-shen.github.io/StoryGPT-V|https://doi.org/10.48550/arXiv.2312.02252|
|1369|Data Management For Large Language Models: A Survey|Zige Wang, Wanjun Zhong, Yufei Wang, Qi Zhu, Fei Mi, Baojun Wang, Lifeng Shang, Xin Jiang, Qun Liu|2023-12-04|arXiv|https://github.com/ZigeW/data_management_LLM|https://doi.org/10.48550/arXiv.2312.01700|
|1370|CoLLiE: Collaborative Training of Large Language Models in an Efficient Way|Kai Lv, Shuo Zhang, Tianle Gu, Shuhao Xing, Jiawei Hong, Keyu Chen, Xiaoran Liu, Yuqing Yang, Honglin Guo, Tengxiao Liu, Yu Sun, Qipeng Guo, Hang Yan, Xipeng Qiu|2023-12-01|EMNLP|https://github.com/OpenLMLab/collie|https://doi.org/10.18653/v1/2023.emnlp-demo.48|
|1371|Conceptual Engineering Using Large Language Models|Bradley P. Allen|2023-12-01|arXiv|https://github.com/bradleypallen/zero-shot-classifiers-for-conceptual-engineering|https://doi.org/10.48550/arXiv.2312.03749|
|1372|The Efficiency Spectrum of Large Language Models: An Algorithmic Survey|Tianyu Ding, Tianyi Chen, Haidong Zhu, Jiachen Jiang, Yiqi Zhong, Jinxin Zhou, Guangzhi Wang, Zhihui Zhu, Ilya Zharkov, Luming Liang|2023-12-01|arXiv|https://github.com/tding1/Efficient-LLM-Survey|https://doi.org/10.48550/arXiv.2312.00678|
|1373|Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities|Jinhua Liang, Xubo Liu, Wenwu Wang, Mark D. Plumbley, Huy Phan, Emmanouil Benetos|2023-11-30|arXiv|https://github.com/JinhuaLiang/APT|http://arxiv.org/abs/2312.00249v1|
|1374|ArthModel: Enhance Arithmetic Skills to Large Language Model|Yingdi Guo|2023-11-30|arXiv|https://github.com/eteced/arithmetic_finetuning_v1|https://doi.org/10.48550/arXiv.2311.18609|
|1375|LLVMs4Protest: Harnessing the Power of Large Language and Vision Models for Deciphering Protests in the News|Yongjun Zhang|2023-11-30|arXiv|https://github.com/Joshzyj/llvms4protest|https://doi.org/10.48550/arXiv.2311.18241|
|1376|mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model|Anwen Hu, Yaya Shi, Haiyang Xu, Jiabo Ye, Qinghao Ye, Ming Yan, Chenliang Li, Qi Qian, Ji Zhang, Fei Huang|2023-11-30|arXiv|https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/PaperOwl|https://doi.org/10.48550/arXiv.2311.18248|
|1377|Unveiling the Implicit Toxicity in Large Language Models|Jiaxin Wen, Pei Ke, Hao Sun, Zhexin Zhang, Chengfei Li, Jinfeng Bai, Minlie Huang|2023-11-29|EMNLP|https://github.com/thu-coai/Implicit-Toxicity|https://doi.org/10.18653/v1/2023.emnlp-main.84|
|1378|MM-SafetyBench: A Benchmark for Safety Evaluation of Multimodal Large Language Models|Xin Liu, Yichen Zhu, Jindong Gu, Yunshi Lan, Chao Yang, Yu Qiao|2023-11-29|arXiv|https://github.com/isXinLiu/MM-SafetyBench|http://arxiv.org/abs/2311.17600v5|
|1379|OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation|Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, Nenghai Yu|2023-11-29|arXiv|https://github.com/shikiw/OPERA|https://doi.org/10.48550/arXiv.2311.17911|
|1380|War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars|Wenyue Hua, Lizhou Fan, Lingyao Li, Kai Mei, Jianchao Ji, Yingqiang Ge, Libby Hemphill, Yongfeng Zhang|2023-11-28|arXiv|https://github.com/agiresearch/WarAgent|https://doi.org/10.48550/arXiv.2311.17227|
|1381|SEED-Bench-2: Benchmarking Multimodal Large Language Models|Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, Ying Shan|2023-11-28|arXiv|https://github.com/AILab-CVC/SEED-Bench|https://doi.org/10.48550/arXiv.2311.17092|
|1382|ChartLlama: A Multimodal LLM for Chart Understanding and Generation|Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, Hanwang Zhang|2023-11-27|OpenReview|https://tingxueronghua.github.io/ChartLlama/|http://arxiv.org/abs/2311.16483v1|
|1383|Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models|Munan Ning, Bin Zhu, Yujia Xie, Bin Lin, Jiaxi Cui, Lu Yuan, Dongdong Chen, Li Yuan|2023-11-27|arXiv|https://github.com/PKU-YuanGroup/Video-Bench|https://doi.org/10.48550/arXiv.2311.16103|
|1384|FlowZero: Zero-Shot Text-to-Video Synthesis with LLM-Driven Dynamic Scene Syntax|Yu Lu, Linchao Zhu, Hehe Fan, Yi Yang|2023-11-27|arXiv|https://flowzero-video.github.io|http://arxiv.org/abs/2311.15813v1|
|1385|FreeAL: Towards Human-Free Active Learning in the Era of Large Language Models|Ruixuan Xiao, Yiwen Dong, Junbo Zhao, Runze Wu, Minmin Lin, Gang Chen, Haobo Wang|2023-11-27|EMNLP|https://github.com/Justherozen/FreeAL|https://doi.org/10.18653/v1/2023.emnlp-main.896|
|1386|Speak Like a Native: Prompting Large Language Models in a Native Style|Zhicheng Yang, Yiwei Wang, Yinya Huang, Jing Xiong, Xiaodan Liang, Jing Tang|2023-11-22|arXiv|https://github.com/yangzhch6/AlignedCoT|https://doi.org/10.48550/arXiv.2311.13538|
|1387|Large Language Model as a Policy Teacher for Training Reinforcement Learning Agents|Zihao Zhou, Bin Hu, Chenyang Zhao, Pu Zhang, Bin Liu|2023-11-22|arXiv|https://github.com/ZJLAB-AMMI/LLM4Teach|http://arxiv.org/abs/2311.13373v6|
|1388|Soulstyler: Using Large Language Model to Guide Image Style Transfer for Target Object|Junhao Chen, Peng Rong, Jingbo Sun, Chao Li, Xiang Li, Hongwu Lv|2023-11-22|arXiv|https://github.com/yisuanwang/Soulstyler|https://doi.org/10.48550/arXiv.2311.13562|
|1389|Boosting Audio-visual Zero-shot Learning with Large Language Models|Haoxing Chen, Yaohui Li, Yan Hong, Zizheng Huang, Zhuoer Xu, Zhangxuan Gu, Jun Lan, Huijia Zhu, Weiqiang Wang|2023-11-21|arXiv|https://github.com/chenhaoxing/KDA|https://doi.org/10.48550/arXiv.2311.12268|
|1390|Prompting Frameworks for Large Language Models: A Survey|Xiaoxia Liu, Jingyi Wang, Jun Sun, Xiaohan Yuan, Guoliang Dong, Peng Di, Wenhai Wang, Dongxia Wang|2023-11-21|arXiv|https://github.com/lxx0628/Prompting-Framework-Survey|https://doi.org/10.48550/arXiv.2311.12785|
|1391|A Survey of Graph Meets Large Language Model: Progress and Future Directions|Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, Jeffrey Xu Yu|2023-11-21|arXiv|https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks|https://doi.org/10.48550/arXiv.2311.12399|
|1392|Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey|Yunpeng Huang, Jingwei Xu, Junyu Lai, Zixu Jiang, Taolue Chen, Zenan Li, Yuan Yao, Xiaoxing Ma, Lijuan Yang, Hao Chen, Shupeng Li, Penghao Zhao|2023-11-21|arXiv|https://github.com/Strivin0311/long-llms-learning|https://doi.org/10.48550/arXiv.2311.12351|
|1393|Causal Structure Learning Supervised by Large Language Model|Taiyu Ban, Lyuzhou Chen, Derui Lyu, Xiangyu Wang, Huanhuan Chen|2023-11-20|arXiv|https://github.com/tyMadara/ILS-CSL|https://doi.org/10.48550/arXiv.2311.11689|
|1394|Evil Geniuses: Delving into the Safety of LLM-based Agents|Yu Tian, Xiao Yang, Jingyuan Zhang, Yinpeng Dong, Hang Su|2023-11-20|arXiv|https://github.com/T1aNS1R/Evil-Geniuses|http://arxiv.org/abs/2311.11855v2|
|1395|InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models|Xiaotian Han, Quanzeng You, Yongfei Liu, Wentao Chen, Huangjie Zheng, Khalil Mrini, Xudong Lin, Yiqi Wang, Bohan Zhai, Jianbo Yuan, Heng Wang, Hongxia Yang|2023-11-20|OpenReview|https://infimm.github.io/InfiMM-Eval/|http://arxiv.org/abs/2311.11567v3|
|1396|LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge|Gongwei Chen, Leyang Shen, Rui Shao, Xiang Deng, Liqiang Nie|2023-11-20|arXiv|https://rshaojimmy.github.io/Projects/JiuTian-LION|https://doi.org/10.48550/arXiv.2311.11860|
|1397|Is &quot;A Helpful Assistant&quot; the Best Role for Large Language Models? A Systematic Evaluation of Social Roles in System Prompts|Mingqian Zheng, Jiaxin Pei, David Jurgens|2023-11-16|arXiv|https://github.com/Jiaxin-Pei/Prompting-with-Social-Roles|https://doi.org/10.48550/arXiv.2311.10054|
|1398|Aligning with Whom? Large Language Models Have Gender and Racial Biases in Subjective NLP Tasks|Huaman Sun, Jiaxin Pei, Minje Choi, David Jurgens|2023-11-16|arXiv|https://github.com/Jiaxin-Pei/LLM-Group-Bias|https://doi.org/10.48550/arXiv.2311.09730|
|1399|Backdoor Activation Attack: Attack Large Language Models using Activation Steering for Safety-Alignment|Haoran Wang, Kai Shu|2023-11-15|arXiv|https://github.com/wang2226/Backdoor-Activation-Attack|https://doi.org/10.48550/arXiv.2311.09433|
|1400|XplainLLM: A QA Explanation Dataset for Understanding LLM Decision-Making|Zichen Chen, Jianda Chen, Mitali Gaidhani, Ambuj Singh, Misha Sra|2023-11-15|arXiv|https://github.com/chen-zichen/XplainLLM_dataset|http://arxiv.org/abs/2311.08614v1|
|1401|Improving Zero-shot Visual Question Answering via Large Language Models with Reasoning Question Prompts|Yunshi Lan, Xiang Li, Xin Liu, Yang Li, Wei Qin, Weining Qian|2023-11-15|MM '23: Proceedings of the 31st ACM International Conference on Multimedia|https://github.com/ECNU-DASE-NLP/RQP|https://dl.acm.org/doi/10.1145/3581783.3612389|
|1402|Adversarial Preference Optimization: Enhancing Your Alignment via RM-LLM Game|Pengyu Cheng, Yifan Yang, Jian Li, Yong Dai, Tianhao Hu, Peixin Cao, Nan Du, Xiaolong Li|2023-11-14|arXiv|https://github.com/Linear95/APO|http://arxiv.org/abs/2311.08045v4|
|1403|Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding|Peng Jin, Ryuichi Takanobu, Caiwan Zhang, Xiaochun Cao, Li Yuan|2023-11-14|arXiv|https://github.com/PKU-YuanGroup/Chat-UniVi|https://doi.org/10.48550/arXiv.2311.08046|
|1404|CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation|Weixiang Yan, Haitian Liu, Yunkun Wang, Yunzhe Li, Qian Chen, Wen Wang, Tingyu Lin, Weishan Zhao, Li Zhu, Hari Sundaram, Shuiguang Deng|2023-11-14|OpenReview|https://github.com/WeixiangYAN/CodeScope|http://arxiv.org/abs/2311.08588v3|
|1405|Instruction-Following Evaluation for Large Language Models|Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, Le Hou|2023-11-14|arXiv|https://github.com/google-research/google-research/tree/master/instruction_following_eval|https://doi.org/10.48550/arXiv.2311.07911|
|1406|MAgIC: Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration|Lin Xu, Zhiyuan Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt Keutzer, See-Kiong Ng, Jiashi Feng|2023-11-14|arXiv|https://github.com/cathyxl/MAgIC|https://doi.org/10.48550/arXiv.2311.08562|
|1407|Towards Open-Ended Visual Recognition with Large Language Model|Qihang Yu, Xiaohui Shen, Liang-Chieh Chen|2023-11-14|arXiv|https://github.com/bytedance/OmniScient-Model|https://doi.org/10.48550/arXiv.2311.08400|
|1408|ExpNote: Black-box Large Language Models are better Task Solvers with Experience Notebook|Wangtao Sun, Xuanqing Yu, Shizhu He, Jun Zhao, Kang Liu|2023-11-13|EMNLP|https://github.com/forangel2014/ExpNote|https://doi.org/10.18653/v1/2023.findings-emnlp.1034|
|1409|SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models|Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang, Xuming He, Hongsheng Li, Yu Qiao|2023-11-13|arXiv|https://github.com/Alpha-VLLM/LLaMA2-Accessory|https://doi.org/10.48550/arXiv.2311.07575|
|1410|Can LLMs Patch Security Issues?|Kamel Alrashedy, Abdullah Aljasser|2023-11-13|OpenReview|https://github.com/Kamel773/LLM-code-refine|http://arxiv.org/abs/2312.00024v3|
|1411|AMBER: An LLM-free Multi-dimensional Benchmark for MLLMs Hallucination Evaluation|Junyang Wang, Yuhang Wang, Guohai Xu, Jing Zhang, Yukai Gu, Haitao Jia, Jiaqi Wang, Haiyang Xu, Ming Yan, Ji Zhang, Jitao Sang|2023-11-13|arXiv|https://github.com/junyangwang0410/AMBER|http://arxiv.org/abs/2311.07397v2|
|1412|A Step Closer to Comprehensive Answers: Constrained Multi-Stage Question Decomposition with Large Language Models|Hejing Cao, Zhenwei An, Jiazhan Feng, Kun Xu, Liwei Chen, Dongyan Zhao|2023-11-13|arXiv|https://github.com/alkaidpku/DQ-ToolQA|https://doi.org/10.48550/arXiv.2311.07491|
|1413|Flames: Benchmarking Value Alignment of LLMs in Chinese|Kexin Huang, Xiangyang Liu, Qianyu Guo, Tianxiang Sun, Jiawei Sun, Yaru Wang, Zeyang Zhou, Yixu Wang, Yan Teng, Xipeng Qiu, Yingchun Wang, Dahua Lin|2023-11-12|OpenReview|https://github.com/AIFlames/Flames|http://arxiv.org/abs/2311.06899v6|
|1414|Knowledgeable Preference Alignment for LLMs in Domain-specific Question Answering|Yichi Zhang, Zhuo Chen, Yin Fang, Yanxi Lu, Fangming Li, Wen Zhang, Huajun Chen|2023-11-11|OpenReview|https://github.com/zjukg/KnowPAT|http://arxiv.org/abs/2311.06503v3|
|1415|LayoutPrompter: Awaken the Design Ability of Large Language Models|Jiawei Lin, Jiaqi Guo, Shizhao Sun, Zijiang James Yang, Jian-Guang Lou, Dongmei Zhang|2023-11-11|arXiv|https://github.com/microsoft/LayoutGeneration/tree/main/LayoutPrompter|https://doi.org/10.48550/arXiv.2311.06495|
|1416|CFBenchmark: Chinese Financial Assistant Benchmark for Large Language Model|Yang Lei, Jiangtong Li, Ming Jiang, Junjie Hu, Dawei Cheng, Zhijun Ding, Changjun Jiang|2023-11-10|arXiv|https://github.com/TongjiFinLab/CFBenchmark|https://doi.org/10.48550/arXiv.2311.05812|
|1417|Fake Alignment: Are LLMs Really Aligned Well?|Yixu Wang, Yan Teng, Kexin Huang, Chengqi Lyu, Songyang Zhang, Wenwei Zhang, Xingjun Ma, Yu-Gang Jiang, Yu Qiao, Yingchun Wang|2023-11-10|OpenReview|https://github.com/AIFlames/Fake-Alignment|http://arxiv.org/abs/2311.05915v3|
|1418|Leveraging LLMs for Synthesizing Training Data Across Many Languages in Multilingual Dense Retrieval|Nandan Thakur, Jianmo Ni, Gustavo Hernández Ábrego, John Wieting, Jimmy Lin, Daniel Cer|2023-11-10|OpenReview|https://github.com/google-research-datasets/swim-ir|http://arxiv.org/abs/2311.05800v2|
|1419|Practical Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration|Wenjie Fu, Huandong Wang, Chen Gao, Guanghua Liu, Yong Li, Tao Jiang|2023-11-10|arXiv|https://github.com/wjfu99/MIA-LLMs|https://doi.org/10.48550/arXiv.2311.06062|
|1420|Smart Agent-Based Modeling: On the Use of Large Language Models in Computer Simulations|Zengqing Wu, Run Peng, Xu Han, Shuyuan Zheng, Yixin Zhang, Chuan Xiao|2023-11-10|arXiv|https://github.com/Roihn/SABM|https://doi.org/10.48550/arXiv.2311.06330|
|1421|u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model|Jinjin Xu, Liwu Xu, Yuzhe Yang, Xiang Li, Fanyi Wang, Yanchun Xie, Yi-Jie Huang, Yaqian Li|2023-11-09|arXiv|https://github.com/OPPOMKLab/u-LLaVA|https://doi.org/10.48550/arXiv.2311.05348|
|1422|Long-Horizon Dialogue Understanding for Role Identification in the Game of Avalon with Large Language Models|Simon Stepputtis, Joseph Campbell, Yaqi Xie, Zhengyang Qi, Wenxin Sharon Zhang, Ruiyi Wang, Sanketh Rangreji, Charles Lewis, Katia P. Sycara|2023-11-09|EMNLP|https://sstepput.github.io/Avalon-NLU/|https://doi.org/10.18653/v1/2023.findings-emnlp.748|
|1423|Combating Misinformation in the Age of LLMs: Opportunities and Challenges|Canyu Chen, Kai Shu|2023-11-09|arXiv|https://llm-misinformation.github.io/|http://arxiv.org/abs/2311.05656v1|
|1424|A Survey of Large Language Models in Medicine: Progress, Application, and Challenge|Hongjian Zhou, Fenglin Liu, Boyang Gu, Xinyu Zou, Jinfa Huang, Jinge Wu, Yiru Li, Sam S. Chen, Peilin Zhou, Junling Liu, Yining Hua, Chengfeng Mao, Chenyu You, Xian Wu, Yefeng Zheng, Lei Clifton, Zheng Li, Jiebo Luo, David A. Clifton|2023-11-09|arXiv|https://github.com/AI-in-Health/MedLLMsPracticalGuide|https://doi.org/10.48550/arXiv.2311.05112|
|1425|AutoChip: Automating HDL Generation Using LLM Feedback|Shailja Thakur, Jason Blocklove, Hammond Pearce, Benjamin Tan, Siddharth Garg, Ramesh Karri|2023-11-08|arXiv|https://github.com/shailja-thakur/AutoChip|http://arxiv.org/abs/2311.04887v2|
|1426|Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models|Rocktim Jyoti Das, Mingjie Sun, Liqun Ma, Zhiqiang Shen|2023-11-08|arXiv|https://github.com/VILA-Lab/GBLM-Pruner|https://doi.org/10.48550/arXiv.2311.04902|
|1427|How Abstract Is Linguistic Generalization in Large Language Models? Experiments with Argument Structure|Michael Wilson, Jackson Petty, Robert Frank|2023-11-08|Trans. Assoc. Comput. Linguistics|https://github.com/clay-lab/structural-alternations|https://doi.org/10.1162/tacl_a_00608|
|1428|LongQLoRA: Efficient and Effective Method to Extend Context Length of Large Language Models|Jianxin Yang|2023-11-08|arXiv|https://github.com/yangjianxin1/LongQLoRA|https://doi.org/10.48550/arXiv.2311.04879|
|1429|Do LLMs exhibit human-like response biases? A case study in survey design|Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, Graham Neubig|2023-11-07|arXiv|https://github.com/lindiatjuatja/BiasMonkey|http://arxiv.org/abs/2311.04076v5|
|1430|Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves|Yihe Deng, Weitong Zhang, Zixiang Chen, Quanquan Gu|2023-11-07|arXiv|https://github.com/uclaml/Rephrase-and-Respond|https://doi.org/10.48550/arXiv.2311.04205|
|1431|ALYMPICS: LLM Agents Meet Game Theory -- Exploring Strategic Decision-Making with AI Agents|Shaoguang Mao, Yuzhe Cai, Yan Xia, Wenshan Wu, Xun Wang, Fengyi Wang, Tao Ge, Furu Wei|2023-11-06|arXiv|https://github.com/microsoft/Alympics|http://arxiv.org/abs/2311.03220v4|
|1432|DeepInception: Hypnotize Large Language Model to Be Jailbreaker|Xuan Li, Zhanke Zhou, Jianing Zhu, Jiangchao Yao, Tongliang Liu, Bo Han|2023-11-06|arXiv|https://github.com/tmlr-group/DeepInception|https://doi.org/10.48550/arXiv.2311.03191|
|1433|LitSumm: Large language models for literature summarisation of non-coding RNAs|Andrew Green, Carlos Ribas, Nancy Ontiveros-Palacios, Sam Griffiths-Jones, Anton I. Petrov, Alex Bateman, Blake Sweeney|2023-11-06|arXiv|https://github.com/RNAcentral/litscan-summarization|https://doi.org/10.48550/arXiv.2311.03056|
|1434|Zero-shot Bilingual App Reviews Mining with Large Language Models|Jialiang Wei, Anne-Lise Courbis, Thomas Lambolais, Binbin Xu, Pierre-Louis Bernard, Gérard Dray|2023-11-06|2023 IEEE 35th International Conference on Tools with Artificial Intelligence (ICTAI)|https://github.com/Jl-wei/mini-bar|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10356483|
|1435|PILL: Plug Into LLM with Adapter Expert and Attention Gate|Fangyuan Zhang, Tingting Liang, Zhengyuan Wu, Yuyu Yin|2023-11-03|arXiv|https://github.com/DsaltYfish/PILL|http://arxiv.org/abs/2311.02126v1|
|1436|Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs|Qingru Zhang, Chandan Singh, Liyuan Liu, Xiaodong Liu, Bin Yu, Jianfeng Gao, Tuo Zhao|2023-11-03|arXiv|https://github.com/QingruZhang/PASTA|http://arxiv.org/abs/2311.02262v1|
|1437|Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review|Mingze Yuan, Peng Bao, Jiajia Yuan, Yunhao Shen, Zifan Chen, Yi Xie, Jie Zhao, Yang Chen, Li Zhang, Lin Shen, Bin Dong|2023-11-03|arXiv|https://github.com/mingze-yuan/Awesome-LLM-Healthcare|https://doi.org/10.48550/arXiv.2311.01918|
|1438|AFPQ: Asymmetric Floating Point Quantization for LLMs|Yijia Zhang, Sicheng Zhang, Shijie Cao, Dayou Du, Jianyu Wei, Ting Cao, Ningyi Xu|2023-11-03|arXiv|https://github.com/zhangsichengsjtu/AFPQ|http://arxiv.org/abs/2311.01792v1|
|1439|LLM4Drive: A Survey of Large Language Models for Autonomous Driving|Zhenjie Yang, Xiaosong Jia, Hongyang Li, Junchi Yan|2023-11-02|arXiv|https://github.com/Thinklab-SJTU/Awesome-LLM4AD|https://doi.org/10.48550/arXiv.2311.01043|
|1440|Advances in Embodied Navigation Using Large Language Models: A Survey|Jinzhou Lin, Han Gao, Xuxiang Feng, Rongtao Xu, Changwei Wang, Man Zhang, Li Guo, Shibiao Xu|2023-11-01|arXiv|https://github.com/Rongtao-Xu/Awesome-LLM-EN|http://arxiv.org/abs/2311.00530v4|
|1441|Efficient LLM Inference on CPUs|Haihao Shen, Hanwen Chang, Bo Dong, Yu Luo, Hengyu Meng|2023-11-01|arXiv|https://github.com/intel/intel-extension-for-transformers|http://arxiv.org/abs/2311.00502v2|
|1442|LLMs may Dominate Information Access: Neural Retrievers are Biased Towards LLM-Generated Texts|Sunhao Dai, Yuqi Zhou, Liang Pang, Weihao Liu, Xiaolin Hu, Yong Liu, Xiao Zhang, Gang Wang, Jun Xu|2023-10-31|arXiv|https://github.com/KID-22/LLM4IR-Bias|http://arxiv.org/abs/2310.20501v2|
|1443|Large Language Model Can Interpret Latent Space of Sequential Recommender|Zhengyi Yang, Jiancan Wu, Yanchen Luo, Jizhi Zhang, Yancheng Yuan, An Zhang, Xiang Wang, Xiangnan He|2023-10-31|arXiv|https://github.com/YangZhengyi98/RecInterpreter|https://doi.org/10.48550/arXiv.2310.20487|
|1444|Learning From Mistakes Makes LLM Better Reasoner|Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang Lou, Weizhu Chen|2023-10-31|arXiv|https://github.com/microsoft/LEMA|http://arxiv.org/abs/2310.20689v4|
|1445|Making Large Language Models Better Data Creators|Dong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen W. White, Sujay Kumar Jauhar|2023-10-31|arXiv|https://github.com/microsoft/llm-data-creation|https://doi.org/10.48550/arXiv.2310.20111|
|1446|CoLLM: Integrating Collaborative Embeddings into Large Language Models for Recommendation|Yang Zhang, Fuli Feng, Jizhi Zhang, Keqin Bao, Qifan Wang, Xiangnan He|2023-10-30|arXiv|https://github.com/zyang1580/CoLLM|https://doi.org/10.48550/arXiv.2310.19488|
|1447|Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation|Qintong Li, Leyang Cui, Lingpeng Kong, Wei Bi|2023-10-30|arXiv|https://github.com/qtli/CoEval|https://doi.org/10.48550/arXiv.2310.19740|
|1448|Evaluating Large Language Models: A Comprehensive Survey|Zishan Guo, Renren Jin, Chuang Liu, Yufei Huang, Dan Shi, Supryadi, Linhao Yu, Yan Liu, Jiaxuan Li, Bojian Xiong, Deyi Xiong|2023-10-30|arXiv|https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers|https://doi.org/10.48550/arXiv.2310.19736|
|1449|Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models|Ziqiao Ma, Jacob Sansom, Run Peng, Joyce Chai|2023-10-30|EMNLP|https://github.com/Mars-tin/awesome-theory-of-mind|https://doi.org/10.18653/v1/2023.findings-emnlp.72|
|1450|DetermLR: Augmenting LLM-based Logical Reasoning from Indeterminacy to Determinacy|Hongda Sun, Weikai Xu, Wei Liu, Jian Luan, Bin Wang, Shuo Shang, Ji-Rong Wen, Rui Yan|2023-10-28|OpenReview|https://github.com/XiaoMi/DetermLR|http://arxiv.org/abs/2310.18659v2|
|1451|FP8-LM: Training FP8 Large Language Models|Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang, Shuguang Liu, Joe Chau, Han Hu, Peng Cheng|2023-10-27|arXiv|https://github.com/Azure/MS-AMP|https://doi.org/10.48550/arXiv.2310.18313|
|1452|Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time|Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, Beidi Chen|2023-10-26|Proceedings of the 40th International Conference on Machine Learning, 2023, 919|https://github.com/FMInference/DejaVu|http://arxiv.org/abs/2310.17157v1|
|1453|LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation|Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, Tat-Seng Chua|2023-10-26|MM '23: Proceedings of the 31st ACM International Conference on Multimedia|https://layoutllm-t2i.github.io|https://dl.acm.org/doi/10.1145/3581783.3612012|
|1454|SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models|Shanshan Zhong, Zhongzhan Huang, Wushao Wen, Jinghui Qin, Liang Lin|2023-10-26|MM '23: Proceedings of the 31st ACM International Conference on Multimedia|https://github.com/Qrange-group/SUR-adapter|https://dl.acm.org/doi/10.1145/3581783.3611863|
|1455|RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs|Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric Xing, Zhiting Hu|2023-10-25|arXiv|https://github.com/tanyuqian/redco|http://arxiv.org/abs/2310.16355v3|
|1456|Improving generalization in large language models by learning prefix subspaces|Louis Falissard, Vincent Guigue, Laure Soulier|2023-10-24|arXiv|https://github.com/Liloulou/prefix_subspace|https://doi.org/10.48550/arXiv.2310.15793|
|1457|SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation|Jialing Pan, Adrien Sadé, Jin Kim, Eric Soriano, Guillem Sole, Sylvain Flamant|2023-10-24|arXiv|https://github.com/sade-adrien/SteloCoder|http://arxiv.org/abs/2310.15539v2|
|1458|LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery|Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, Luming Liang|2023-10-24|arXiv|https://github.com/microsoft/lorashear|https://doi.org/10.48550/arXiv.2310.18356|
|1459|Large Language Models are Temporal and Causal Reasoners for Video Question Answering|Dohwan Ko, Ji Soo Lee, Woo-Young Kang, Byungseok Roh, Hyunwoo J. Kim|2023-10-24|arXiv|https://github.com/mlvlab/Flipped-VQA|https://doi.org/10.48550/arXiv.2310.15747|
|1460|Woodpecker: Hallucination Correction for Multimodal Large Language Models|Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao Wang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun, Enhong Chen|2023-10-24|arXiv|https://github.com/BradyFU/Woodpecker|https://doi.org/10.48550/arXiv.2310.16045|
|1461|Ignore This Title and HackAPrompt: Exposing Systemic Vulnerabilities of LLMs through a Global Scale Prompt Hacking Competition|Sander Schulhoff, Jeremy Pinto, Anaum Khan, Louis-François Bouchard, Chenglei Si, Svetlina Anati, Valen Tagliabue, Anson Liu Kost, Christopher Carnahan, Jordan Boyd-Graber|2023-10-24|arXiv|https://github.com/PromptLabs/hackaprompt|http://arxiv.org/abs/2311.16119v3|
|1462|CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation|Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy F. Chen, Zhengyuan Liu, Diyi Yang|2023-10-24|EMNLP|https://github.com/SALT-NLP/CoAnnotating|https://doi.org/10.18653/v1/2023.emnlp-main.92|
|1463|CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model|Kaiyan Zhang, Ning Ding, Biqing Qi, Xuekai Zhu, Xinwei Long, Bowen Zhou|2023-10-24|EMNLP|https://github.com/TsinghuaC3I/CRaSh|https://doi.org/10.18653/v1/2023.emnlp-main.597|
|1464|Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation|Jason Lucas, Adaku Uchendu, Michiharu Yamashita, Jooyoung Lee, Shaurya Rohatgi, Dongwon Lee|2023-10-24|arXiv|https://github.com/mickeymst/F3|http://arxiv.org/abs/2310.15515v1|
|1465|A Survey on Detection of LLMs-Generated Content|Xianjun Yang, Liangming Pan, Xuandong Zhao, Haifeng Chen, Linda Petzold, William Yang Wang, Wei Cheng|2023-10-24|arXiv|https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection|http://arxiv.org/abs/2310.15654v1|
|1466|Federated Learning of Large Language Models with Parameter-Efficient Prompt Tuning and Adaptive Optimization|Tianshi Che, Ji Liu, Yang Zhou, Jiaxiang Ren, Jiwen Zhou, Victor S. Sheng, Huaiyu Dai, Dejing Dou|2023-10-23|EMNLP|https://github.com/llm-eff/FedPepTAO|https://doi.org/10.18653/v1/2023.emnlp-main.488|
|1467|The Skipped Beat: A Study of Sociopragmatic Understanding in LLMs for 64 Languages|Chiyu Zhang, Khai Duy Doan, Qisheng Liao, Muhammad Abdul-Mageed|2023-10-23|arXiv|https://github.com/UBC-NLP/SPARROW|http://arxiv.org/abs/2310.14557v1|
|1468|Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models|Gabriel Herbert Sarch, Yue Wu, Michael J. Tarr, Katerina Fragkiadaki|2023-10-23|EMNLP|https://helper-agent-llm.github.io|https://doi.org/10.18653/v1/2023.findings-emnlp.226|
|1469|AlpaCare: Instruction-tuned Large Language Models for Medical Application|Xinlu Zhang, Chenxin Tian, Xianjun Yang, Lichang Chen, Zekun Li, Linda Ruth Petzold|2023-10-23|arXiv|https://github.com/XZhang97666/AlpaCare|https://doi.org/10.48550/arXiv.2310.14558|
|1470|Efficient Memory Management for Large Language Model Serving with PagedAttention|Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, Ion Stoica|2023-10-23|SOSP '23: Proceedings of the 29th Symposium on Operating Systems Principles|https://github.com/vllm-project/vllm|https://dl.acm.org/doi/10.1145/3600006.3613165|
|1471|A Survey on LLM-Generated Text Detection: Necessity, Methods, and Future Directions|Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F. Wong, Lidia S. Chao|2023-10-23|arXiv|https://github.com/NLP2CT/LLM-generated-Text-Detection|http://arxiv.org/abs/2310.14724v3|
|1472|DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning|Wei Chen, Qiushi Wang, Zefei Long, Xianyin Zhang, Zhongtian Lu, Bingxuan Li, Siyuan Wang, Jiarong Xu, Xiang Bai, Xuanjing Huang, Zhongyu Wei|2023-10-23|arXiv|https://github.com/FudanDISC/DISC-FinLLM|https://doi.org/10.48550/arXiv.2310.15205|
|1473|CXR-LLaVA: Multimodal Large Language Model for Interpreting Chest X-ray Images|Seowoo Lee, Jiwon Youn, Hyungjin Kim, Mansu Kim, Soon Ho Yoon|2023-10-22|arXiv|https://github.com/ECOFRI/CXR_LLAVA|https://doi.org/10.48550/arXiv.2310.18341|
|1474|Evaluating Subjective Cognitive Appraisals of Emotions from Large Language Models|Hongli Zhan, Desmond C. Ong, Junyi Jessy Li|2023-10-22|EMNLP|https://github.com/honglizhan/CovidET-Appraisals-Public|https://doi.org/10.18653/v1/2023.findings-emnlp.962|
|1475|PromptMix: A Class Boundary Augmentation Method for Large Language Model Distillation|Gaurav Sahu, Olga Vechtomova, Dzmitry Bahdanau, Issam H. Laradji|2023-10-22|EMNLP|https://github.com/ServiceNow/PromptMix-EMNLP-2023|https://doi.org/10.18653/v1/2023.emnlp-main.323|
|1476|LARCH: Large Language Model-based Automatic Readme Creation with Heuristics|Yuta Koreeda, Terufumi Morishita, Osamu Imaichi, Yasuhiro Sogawa|2023-10-21|CIKM '23: Proceedings of the 32nd ACM International Conference on Information and Knowledge Management|https://github.com/hitachi-nlp/larch|https://dl.acm.org/doi/10.1145/3583780.3614744|
|1477|MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model|Le Zhang, Yihong Wu, Fengran Mo, Jian-Yun Nie, Aishwarya Agrawal|2023-10-20|EMNLP|https://github.com/lezhang7/MOQAGPT|https://doi.org/10.18653/v1/2023.findings-emnlp.85|
|1478|Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking|Shengyao Zhuang, Bing Liu, Bevan Koopman, Guido Zuccon|2023-10-20|EMNLP|https://github.com/ielab/llm-qlm|https://doi.org/10.18653/v1/2023.findings-emnlp.590|
|1479|Tuna: Instruction Tuning using Feedback from Large Language Models|Haoran Li, Yiran Liu, Xingxing Zhang, Wei Lu, Furu Wei|2023-10-20|EMNLP|https://github.com/microsoft/LMOps|https://doi.org/10.18653/v1/2023.findings-emnlp.1011|
|1480|Democratizing Reasoning Ability: Tailored Learning from Large Language Model|Zhaoyang Wang, Shaohan Huang, Yuxuan Liu, Jiahai Wang, Minghui Song, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei Deng, Feng Sun, Qi Zhang|2023-10-20|EMNLP|https://github.com/Raibows/Learn-to-Reason|https://doi.org/10.18653/v1/2023.emnlp-main.120|
|1481|Copyright Violations and Large Language Models|Antonia Karamolegkou, Jiaang Li, Li Zhou, Anders Søgaard|2023-10-20|EMNLP|https://github.com/coastalcph/CopyrightLLMs|https://doi.org/10.18653/v1/2023.emnlp-main.458|
|1482|BotChat: Evaluating LLMs' Capabilities of Having Multi-Turn Dialogues|Haodong Duan, Jueqi Wei, Chonghua Wang, Hongwei Liu, Yixiao Fang, Songyang Zhang, Dahua Lin, Kai Chen|2023-10-20|OpenReview|https://github.com/open-compass/BotChat/|http://arxiv.org/abs/2310.13650v1|
|1483|3D-GPT: Procedural 3D Modeling with Large Language Models|Chunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang, Zishan Qin, Stephen Gould|2023-10-19|arXiv|https://chuny1.github.io/3DGPT/3dgpt.html|https://doi.org/10.48550/arXiv.2310.12945|
|1484|AgentTuning: Enabling Generalized Agent Abilities for LLMs|Aohan Zeng, Mingdao Liu, Rui Lu, Bowen Wang, Xiao Liu, Yuxiao Dong, Jie Tang|2023-10-19|OpenReview|https://github.com/THUDM/AgentTuning|http://arxiv.org/abs/2310.12823v2|
|1485|Attack Prompt Generation for Red Teaming and Defending Large Language Models|Boyi Deng, Wenjie Wang, Fuli Feng, Yang Deng, Qifan Wang, Xiangnan He|2023-10-19|EMNLP|https://github.com/Aatrox103/SAP|https://doi.org/10.18653/v1/2023.findings-emnlp.143|
|1486|CLAIR: Evaluating Image Captions with Large Language Models|David Chan, Suzanne Petryk, Joseph E. Gonzalez, Trevor Darrell, John F. Canny|2023-10-19|arXiv|https://davidmchan.github.io/clair/|https://doi.org/10.48550/arXiv.2310.12971|
|1487|Creative Robot Tool Use with Large Language Models|Mengdi Xu, Peide Huang, Wenhao Yu, Shiqi Liu, Xilun Zhang, Yaru Niu, Tingnan Zhang, Fei Xia, Jie Tan, Ding Zhao|2023-10-19|arXiv|https://creative-robotool.github.io/|https://doi.org/10.48550/arXiv.2310.13065|
|1488|Large Language Model for Multi-objective Evolutionary Optimization|Fei Liu, Xi Lin, Zhenkun Wang, Shunyu Yao, Xialiang Tong, Mingxuan Yuan, Qingfu Zhang|2023-10-19|arXiv|https://github.com/FeiLiu36/LLM4MOEA|https://doi.org/10.48550/arXiv.2310.12541|
|1489|Reliable Academic Conference Question Answering: A Study Based on Large Language Model|Zhiwei Huang, Long Jin, Junjie Wang, Mingchen Tu, Yin Hua, Zhiqiang Liu, Jiawei Meng, Huajun Chen, Wen Zhang|2023-10-19|arXiv|https://github.com/zjukg/ConferenceQA|https://doi.org/10.48550/arXiv.2310.13028|
|1490|A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction|Ruihao Shui, Yixin Cao, Xiang Wang, Tat-Seng Chua|2023-10-18|EMNLP|https://github.com/srhthu/LM-CompEval-Legal|https://doi.org/10.18653/v1/2023.findings-emnlp.490|
|1491|Personalized Soups: Personalized Large Language Model Alignment via Post-hoc Parameter Merging|Joel Jang, Seungone Kim, Bill Yuchen Lin, Yizhong Wang, Jack Hessel, Luke Zettlemoyer, Hannaneh Hajishirzi, Yejin Choi, Prithviraj Ammanabrolu|2023-10-17|arXiv|https://github.com/joeljang/RLPHF|https://doi.org/10.48550/arXiv.2310.11564|
|1492|TEQ: Trainable Equivalent Transformation for Quantization of LLMs|Wenhua Cheng, Yiyang Cai, Kaokao Lv, Haihao Shen|2023-10-17|arXiv|https://github.com/intel/neural-compressor|http://arxiv.org/abs/2310.10944v1|
|1493|Theory of Mind for Multi-Agent Collaboration via Large Language Models|Huao Li, Yu Quan Chong, Simon Stepputtis, Joseph Campbell, Dana T. Hughes, Charles Lewis, Katia P. Sycara|2023-10-16|arXiv|https://github.com/romanlee6/multi_LLM_comm|https://doi.org/10.48550/arXiv.2310.10701|
|1494|FATE-LLM: A Industrial Grade Federated Learning Framework for Large Language Models|Tao Fan, Yan Kang, Guoqiang Ma, Weijing Chen, Wenbin Wei, Lixin Fan, Qiang Yang|2023-10-16|arXiv|https://github.com/FederatedAI/FATE-LLM|https://doi.org/10.48550/arXiv.2310.10049|
|1495|BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology|Odhran O'Donoghue, Aleksandar Shtedritski, John Ginger, Ralph Abboud, Ali Essa Ghareeb, Justin Booth, Samuel G Rodriques|2023-10-16|arXiv|https://github.com/bioplanner/bioplanner|http://arxiv.org/abs/2310.10632v1|
|1496|Assessing the Reliability of Large Language Model Knowledge|Weixuan Wang, Barry Haddow, Alexandra Birch, Wei Peng|2023-10-15|arXiv|https://github.com/Vicky-Wil/MONITOR|https://doi.org/10.48550/arXiv.2310.09820|
|1497|MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning|Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, Mohamed Elhoseiny|2023-10-14|arXiv|https://minigpt-v2.github.io/|https://doi.org/10.48550/arXiv.2310.09478|
|1498|InstructTODS: Large Language Models for End-to-End Task-Oriented Dialogue Systems|Willy Chung, Samuel Cahyawijaya, Bryan Wilie, Holy Lovenia, Pascale Fung|2023-10-13|arXiv|https://github.com/WillyHC22/InstructTODS/|https://doi.org/10.48550/arXiv.2310.08885|
|1499|Ranking LLM-Generated Loop Invariants for Program Verification|Saikat Chakraborty, Shuvendu K. Lahiri, Sarah Fakhoury, Madanlal Musuvathi, Akash Lal, Aseem Rastogi, Aditya Senthilnathan, Rahul Sharma, Nikhil Swamy|2023-10-13|Findings of the Association for Computational Linguistics: EMNLP 2023|https://github.com/microsoft/NeuralInvariantRanker|http://arxiv.org/abs/2310.09342v3|
|1500|QUIK: Towards End-to-End 4-Bit Inference on Generative Large Language Models|Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, Dan Alistarh|2023-10-13|arXiv|https://github.com/IST-DASLab/QUIK|http://arxiv.org/abs/2310.09259v2|
|1501|Assessing and Enhancing the Robustness of Large Language Models with Task Structure Variations for Logical Reasoning|Qiming Bao, Gael Gendron, Alex Yuxuan Peng, Wanjun Zhong, Neset Tan, Yang Chen, Michael Witbrock, Jiamou Liu|2023-10-13|arXiv|https://github.com/Strong-AI-Lab/Logical-and-abstract-reasoning|http://arxiv.org/abs/2310.09430v4|
|1502|Human-in-the-loop Machine Translation with Large Language Model|Xinyi Yang, Runzhe Zhan, Derek F. Wong, Junchao Wu, Lidia S. Chao|2023-10-13|arXiv|https://github.com/NLP2CT/HIL-MT/|https://doi.org/10.48550/arXiv.2310.08908|
|1503|Dynamic Sparse No Training: Training-Free Fine-tuning for Sparse LLMs|Yuxin Zhang, Lirui Zhao, Mingbao Lin, Yunyun Sun, Yiwu Yao, Xingjia Han, Jared Tanner, Shiwei Liu, Rongrong Ji|2023-10-13|arXiv|https://github.com/zyxxmu/DSnoT|http://arxiv.org/abs/2310.08915v3|
|1504|EasyGen: Easing Multimodal Generation with BiDiffuser and LLMs|Xiangyu Zhao, Bo Liu, Qijiong Liu, Guangyuan Shi, Xiao-Ming Wu|2023-10-13|arXiv|https://github.com/zxy556677/EasyGen|http://arxiv.org/abs/2310.08949v3|
|1505|Can We Edit Multimodal Large Language Models?|Siyuan Cheng, Bozhong Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, Ningyu Zhang|2023-10-12|EMNLP|https://github.com/zjunlp/EasyEdit|https://doi.org/10.18653/v1/2023.emnlp-main.856|
|1506|HoneyBee: Progressive Instruction Finetuning of Large Language Models for Materials Science|Yu Song, Santiago Miret, Huan Zhang, Bang Liu|2023-10-12|EMNLP|https://github.com/BangLab-UdeM-Mila/NLP4MatSci-HoneyBee|https://doi.org/10.18653/v1/2023.findings-emnlp.380|
|1507|Impact of Co-occurrence on Factual Knowledge of Large Language Models|Cheongwoong Kang, Jaesik Choi|2023-10-12|EMNLP|https://github.com/CheongWoong/impact_of_cooccurrence|https://doi.org/10.18653/v1/2023.findings-emnlp.518|
|1508|Retrieve Anything To Augment Large Language Models|Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, Jian-Yun Nie|2023-10-11|arXiv|https://github.com/FlagOpen/FlagEmbedding|https://doi.org/10.48550/arXiv.2310.07554|
|1509|Large Language Models Are Zero-Shot Time Series Forecasters|Nate Gruver, Marc Finzi, Shikai Qiu, Andrew Gordon Wilson|2023-10-11|NeurIPS|https://github.com/ngruver/llmtime|http://papers.nips.cc/paper_files/paper/2023/hash/3eb7ca52e8207697361b2c0fb3926511-Abstract-Conference.html|
|1510|How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances|Zihan Zhang, Meng Fang, Ling Chen, Mohammad-Reza Namazi-Rad, Jun Wang|2023-10-11|EMNLP|https://github.com/hyintell/awesome-refreshing-llms|https://doi.org/10.18653/v1/2023.emnlp-main.516|
|1511|An Empirical Study of Instruction-tuning Large Language Models in Chinese|Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, Weiping Wang|2023-10-11|EMNLP|https://github.com/PhoebusSi/Alpaca-CoT|https://doi.org/10.18653/v1/2023.findings-emnlp.269|
|1512|Making Large Language Models Perform Better in Knowledge Graph Completion|Yichi Zhang, Zhuo Chen, Lingbing Guo, Yajing Xu, Wen Zhang, Huajun Chen|2023-10-10|arXiv|https://github.com/zjukg/KoPA|https://doi.org/10.48550/arXiv.2310.06671|
|1513|NEWTON: Are Large Language Models Capable of Physical Reasoning?|Yi Ru Wang, Jiafei Duan, Dieter Fox, Siddhartha S. Srinivasa|2023-10-10|EMNLP|https://newtonreasoning.github.io|https://doi.org/10.18653/v1/2023.findings-emnlp.652|
|1514|Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation|Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, Danqi Chen|2023-10-10|arXiv|https://github.com/Princeton-SysML/Jailbreak_LLM|http://arxiv.org/abs/2310.06987v1|
|1515|A Closer Look into Automatic Evaluation Using Large Language Models|David Cheng-Han Chiang, Hung-yi Lee|2023-10-09|arXiv|https://github.com/d223302/A-Closer-Look-To-LLM-Evaluation/|https://doi.org/10.48550/arXiv.2310.05657|
|1516|Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models|Guangzhi Sun, Wenyi Yu, Changli Tang, Xianzhao Chen, Tian Tan, Wei Li, Lu Lu, Zejun Ma, Chao Zhang|2023-10-09|arXiv|https://github.com/BriansIDP/AudioVisualLLM|https://doi.org/10.48550/arXiv.2310.05863|
|1517|OptiMUS: Optimization Modeling Using MIP Solvers and large language models|Ali AhmadiTeshnizi, Wenzhi Gao, Madeleine Udell|2023-10-09|arXiv|https://github.com/teshnizi/OptiMUS|https://doi.org/10.48550/arXiv.2310.06116|
|1518|Outlier Weighed Layerwise Sparsity (OWL): A Missing Secret Sauce for Pruning LLMs to High Sparsity|Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia, Gen Li, Ajay Jaiswal, Mykola Pechenizkiy, Yi Liang, Michael Bendersky, Zhangyang Wang, Shiwei Liu|2023-10-08|arXiv|https://github.com/luuyin/OWL|http://arxiv.org/abs/2310.05175v3|
|1519|Chat Vector: A Simple Approach to Equip LLMs with Instruction Following and Model Alignment in New Languages|Shih-Cheng Huang, Pin-Zu Li, Yu-Chi Hsu, Kuang-Ming Chen, Yu Tung Lin, Shih-Kai Hsiao, Richard Tzong-Han Tsai, Hung-yi Lee|2023-10-07|OpenReview|https://github.com/aqweteddy/ChatVector|http://arxiv.org/abs/2310.04799v3|
|1520|HowToCaption: Prompting LLMs to Transform Video Annotations at Scale|Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian Rupprecht, Bernt Schiele, Hilde Kuehne|2023-10-07|arXiv|https://github.com/ninatu/howtocaption|http://arxiv.org/abs/2310.04900v1|
|1521|Chain of Natural Language Inference for Reducing Large Language Model Ungrounded Hallucinations|Deren Lei, Yaxi Li, Mengya Hu, Mingyu Wang, Vincent Yun, Emily Ching, Eslam Kamal|2023-10-06|arXiv|https://github.com/microsoft/CoNLI_hallucination|https://doi.org/10.48550/arXiv.2310.03951|
|1522|LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models|Saaket Agashe, Yue Fan, Anthony Reyna, Xin Eric Wang|2023-10-05|arXiv|https://github.com/eric-ai-lab/llm_coordination|http://arxiv.org/abs/2310.03903v2|
|1523|SteP: Stacked LLM Policies for Web Actions|Paloma Sodhi, S. R. K. Branavan, Yoav Artzi, Ryan McDonald|2023-10-05|arXiv|https://asappresearch.github.io/webagents-step|http://arxiv.org/abs/2310.03720v2|
|1524|SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks|Alexander Robey, Eric Wong, Hamed Hassani, George J. Pappas|2023-10-05|arXiv|https://github.com/arobey1/smooth-llm|https://doi.org/10.48550/arXiv.2310.03684|
|1525|DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training|Dacheng Li, Rulin Shao, Anze Xie, Eric P. Xing, Xuezhe Ma, Ion Stoica, Joseph E. Gonzalez, Hao Zhang|2023-10-05|arXiv|https://github.com/RulinShao/LightSeq|http://arxiv.org/abs/2310.03294v2|
|1526|Editing Personality for Large Language Models|Shengyu Mao, Xiaohan Wang, Mengru Wang, Yong Jiang, Pengjun Xie, Fei Huang, Ningyu Zhang|2023-10-03|arXiv|https://github.com/zjunlp/EasyEdit|http://arxiv.org/abs/2310.02168v3|
|1527|Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View|Jintian Zhang, Xin Xu, Ningyu Zhang, Ruibo Liu, Bryan Hooi, Shumin Deng|2023-10-03|OpenReview|https://github.com/zjunlp/MachineSoM|http://arxiv.org/abs/2310.02124v3|
|1528|Generalizable Long-Horizon Manipulations with Large Language Models|Haoyu Zhou, Mingyu Ding, Weikun Peng, Masayoshi Tomizuka, Lin Shao, Chuang Gan|2023-10-03|arXiv|https://object814.github.io/Task-Condition-With-LLM/|https://doi.org/10.48550/arXiv.2310.02264|
|1529|Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond|Liang Chen, Yichi Zhang, Shuhuai Ren, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Tianyu Liu, Baobao Chang|2023-10-03|arXiv|https://github.com/pkunlp-icler/PCA-EVAL/|https://doi.org/10.48550/arXiv.2310.02071|
|1530|All Languages Matter: On the Multilingual Safety of Large Language Models|Wenxuan Wang, Zhaopeng Tu, Chang Chen, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu|2023-10-02|arXiv|https://github.com/Jarviswang94/Multilingual_safety_benchmark|https://doi.org/10.48550/arXiv.2310.00905|
|1531|Compressing LLMs: The Truth is Rarely Pure and Never Simple|Ajay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, Zhangyang Wang, Yinfei Yang|2023-10-02|arXiv|https://github.com/VITA-Group/llm-kick|http://arxiv.org/abs/2310.01382v2|
|1532|Large Language Model-Powered Smart Contract Vulnerability Detection: New Perspectives|Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Furkan Tekin, Ling Liu|2023-10-02|2023 5th IEEE International Conference on Trust, Privacy and Security in Intelligent Systems and Applications (TPS-ISA)|https://github.com/git-disl/GPTLens|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10431564|
|1533|Use Your INSTINCT: INSTruction optimization for LLMs usIng Neural bandits Coupled with Transformers|Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, Bryan Kian Hsiang Low|2023-10-02|arXiv|https://github.com/xqlin98/INSTINCT|http://arxiv.org/abs/2310.02905v3|
|1534|Who is ChatGPT? Benchmarking LLMs' Psychological Portrayal Using PsychoBench|Jen-tse Huang, Wenxuan Wang, Eric John Li, Man Ho Lam, Shujie Ren, Youliang Yuan, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu|2023-10-02|arXiv|https://github.com/CUHK-ARISE/PsychoBench|http://arxiv.org/abs/2310.01386v2|
|1535|Meta Semantic Template for Evaluation of Large Language Models|Yachuan Liu, Liang Chen, Jindong Wang, Qiaozhu Mei, Xing Xie|2023-10-01|arXiv|https://llm-eval.github.io/|https://doi.org/10.48550/arXiv.2310.01448|
|1536|Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs|Shiyu Xuan, Qingpei Guo, Ming Yang, Shiliang Zhang|2023-10-01|arXiv|https://github.com/SY-Xuan/Pink|http://arxiv.org/abs/2310.00582v3|
|1537|From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning|Xuansheng Wu, Wenlin Yao, Jianshu Chen, Xiaoman Pan, Xiaoyang Wang, Ninghao Liu, Dong Yu|2023-09-30|OpenReview|https://github.com/JacksonWuxs/Interpret_Instruction_Tuning_LLMs|http://arxiv.org/abs/2310.00492v3|
|1538|Pruning Small Pre-Trained Weights Irreversibly and Monotonically Impairs "Difficult" Downstream Tasks in LLMs|Lu Yin, Ajay Jaiswal, Shiwei Liu, Souvik Kundu, Zhangyang Wang|2023-09-29|arXiv|https://github.com/VITA-Group/Junk_DNA_Hypothesis|http://arxiv.org/abs/2310.02277v2|
|1539|CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets|Lifan Yuan, Yangyi Chen, Xingyao Wang, Yi R. Fung, Hao Peng, Heng Ji|2023-09-29|arXiv|https://github.com/lifan-yuan/CRAFT|http://arxiv.org/abs/2309.17428v2|
|1540|L2CEval: Evaluating Language-to-Code Generation Capabilities of Large Language Models|Ansong Ni, Pengcheng Yin, Yilun Zhao, Martin Riddell, Troy Feng, Rui Shen, Stephen Yin, Ye Liu, Semih Yavuz, Caiming Xiong, Shafiq Joty, Yingbo Zhou, Dragomir Radev, Arman Cohan|2023-09-29|arXiv|https://l2c-eval.github.io/|https://doi.org/10.48550/arXiv.2309.17446|
|1541|At Which Training Stage Does Code Data Help LLMs Reasoning?|Yingwei Ma, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, Shanshan Li|2023-09-28|arXiv|https://github.com/yingweima2022/CodeLLM|http://arxiv.org/abs/2309.16298v2|
|1542|LawBench: Benchmarking Legal Knowledge of Large Language Models|Zhiwei Fei, Xiaoyu Shen, Dawei Zhu, Fengzhe Zhou, Zhuo Han, Songyang Zhang, Kai Chen, Zongwen Shen, Jidong Ge|2023-09-28|arXiv|https://github.com/open-compass/LawBench/|https://doi.org/10.48550/arXiv.2309.16289|
|1543|ConPET: Continual Parameter-Efficient Tuning for Large Language Models|Chenyang Song, Xu Han, Zheni Zeng, Kuai Li, Chen Chen, Zhiyuan Liu, Maosong Sun, Tao Yang|2023-09-26|arXiv|https://github.com/Raincleared-Song/ConPET|https://doi.org/10.48550/arXiv.2309.14763|
|1544|RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models|Ronak Pradeep, Sahel Sharifymoghaddam, Jimmy Lin|2023-09-26|arXiv|https://github.com/castorini/rank_llm|https://doi.org/10.48550/arXiv.2309.15088|
|1545|VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning|Han Lin, Abhay Zala, Jaemin Cho, Mohit Bansal|2023-09-26|arXiv|https://videodirectorgpt.github.io|http://arxiv.org/abs/2309.15091v1|
|1546|Free-Bloom: Zero-Shot Text-to-Video Generator with LLM Director and LDM Animator|Hanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu, Jingyi Yu, Sibei Yang|2023-09-25|Advances in Neural Information Processing Systems|https://github.com/SooLab/Free-Bloom|http://arxiv.org/abs/2309.14494v1|
|1547|Prompting and Fine-Tuning Open-Sourced Large Language Models for Stance Classification|Iain J. Cruickshank, Lynnette Hui Xian Ng|2023-09-24|arXiv|https://github.com/ijcruic/LLM-Stance-Labeling|http://arxiv.org/abs/2309.13734v2|
|1548|Effective Distillation of Table-based Reasoning Ability from LLMs|Bohao Yang, Chen Tang, Kun Zhao, Chenghao Xiao, Chenghua Lin|2023-09-22|arXiv|https://github.com/Bernard-Yang/DistillTableCoT|http://arxiv.org/abs/2309.13182v2|
|1549|ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs|Justin Chih-Yao Chen, Swarnadeep Saha, Mohit Bansal|2023-09-22|arXiv|https://github.com/dinobby/ReConcile|http://arxiv.org/abs/2309.13007v3|
|1550|Goal-Oriented Prompt Attack and Safety Evaluation for LLMs|Chengyuan Liu, Fubang Zhao, Lizhi Qing, Yangyang Kang, Changlong Sun, Kun Kuang, Fei Wu|2023-09-21|arXiv|https://github.com/liuchengyuan123/CPAD|http://arxiv.org/abs/2309.11830v2|
|1551|The Reversal Curse: LLMs trained on "A is B" fail to learn "B is A"|Lukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, Owain Evans|2023-09-21|arXiv|https://github.com/lukasberglund/reversal_curse|http://arxiv.org/abs/2309.12288v4|
|1552|DISC-LawLLM: Fine-tuning Large Language Models for Intelligent Legal Services|Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Xuanjing Huang, Zhongyu Wei|2023-09-20|arXiv|https://github.com/FudanDISC/DISC-LawLLM|https://doi.org/10.48550/arXiv.2309.11325|
|1553|Are Large Language Models Really Robust to Word-Level Perturbations?|Haoyu Wang, Guozheng Ma, Cong Yu, Ning Gui, Linrui Zhang, Zhiqi Huang, Suwei Ma, Yongzhe Chang, Sen Zhang, Li Shen, Xueqian Wang, Peilin Zhao, Dacheng Tao|2023-09-20|arXiv|https://github.com/Harry-mic/TREvaL|https://doi.org/10.48550/arXiv.2309.11166|
|1554|CFGPT: Chinese Financial Assistant with Large Language Model|Jiangtong Li, Yuxuan Bian, Guoxuan Wang, Yang Lei, Dawei Cheng, Zhijun Ding, Changjun Jiang|2023-09-19|arXiv|https://github.com/TongjiFinLab/CFGPT|https://doi.org/10.48550/arXiv.2309.10654|
|1555|MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback|Xingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, Heng Ji|2023-09-19|arXiv|https://xingyaoww.github.io/mint-bench|http://arxiv.org/abs/2309.10691v3|
|1556|Prompt a Robot to Walk with Large Language Models|Yen-Jen Wang, Bike Zhang, Jianyu Chen, Koushil Sreenath|2023-09-18|arXiv|https://prompt2walk.github.io/|https://doi.org/10.48550/arXiv.2309.09969|
|1557|R2GenGPT: Radiology Report Generation with Frozen LLMs|Zhanyu Wang, Lingqiao Liu, Lei Wang, Luping Zhou|2023-09-18|arXiv|https://github.com/wang-zhanyu/R2GenGPT|http://arxiv.org/abs/2309.09812v2|
|1558|FedJudge: Federated Legal Large Language Model|Linan Yue, Qi Liu, Yichao Du, Weibo Gao, Ye Liu, Fangzhou Yao|2023-09-15|arXiv|https://github.com/yuelinan/FedJudge|https://doi.org/10.48550/arXiv.2309.08173|
|1559|InvestLM: A Large Language Model for Investment using Financial Domain Instruction Tuning|Yi Yang, Yixuan Tang, Kar Yan Tam|2023-09-15|arXiv|https://github.com/AbaciNLP/InvestLM|https://doi.org/10.48550/arXiv.2309.13064|
|1560|Using Large Language Models for Knowledge Engineering (LLMKE): A Case Study on Wikidata|Bohui Zhang, Ioannis Reklos, Nitisha Jain, Albert Meroño-Peñuela, Elena Simperl|2023-09-15|KBC-LM/LM-KBC@ISWC|https://github.com/bohuizhang/LLMKE|https://ceur-ws.org/Vol-3577/paper8.pdf|
|1561|Is ChatGPT Fair for Recommendation? Evaluating Fairness in Large Language Model Recommendation|Jizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He|2023-09-14|RecSys '23: Proceedings of the 17th ACM Conference on Recommender Systems|https://github.com/jizhi-zhang/FaiRLLM|https://dl.acm.org/doi/10.1145/3604915.3608860|
|1562|SwitchGPT: Adapting Large Language Models for Non-Text Outputs|Xinyu Wang, Bohan Zhuang, Qi Wu|2023-09-14|arXiv|https://github.com/xinke-wang/SwitchGPT|https://doi.org/10.48550/arXiv.2309.07623|
|1563|TALLRec: An Effective and Efficient Tuning Framework to Align Large Language Model with Recommendation|Keqin Bao, Jizhi Zhang, Yang Zhang, Wenjie Wang, Fuli Feng, Xiangnan He|2023-09-14|RecSys '23: Proceedings of the 17th ACM Conference on Recommender Systems|https://github.com/SAI990323/TALLRec|https://dl.acm.org/doi/10.1145/3604915.3608857|
|1564|The Rise and Potential of Large Language Model Based Agents: A Survey|Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, Tao Gui|2023-09-14|arXiv|https://github.com/WooooDyy/LLM-Agent-Paper-List|https://doi.org/10.48550/arXiv.2309.07864|
|1565|BHASA: A Holistic Southeast Asian Linguistic and Cultural Evaluation Suite for Large Language Models|Wei Qi Leong, Jian Gang Ngui, Yosephine Susanto, Hamsawardhini Rengarajan, Kengatharaiyer Sarveswaran, William-Chandra Tjhi|2023-09-12|arXiv|https://github.com/aisingapore/BHASA|https://doi.org/10.48550/arXiv.2309.06085|
|1566|Re-Reading Improves Reasoning in Large Language Models|Xiaohan Xu, Chongyang Tao, Tao Shen, Can Xu, Hongbo Xu, Guodong Long, Jian-guang Lou|2023-09-12|OpenReview|https://github.com/Tebmer/Rereading-LLM-Reasoning/|http://arxiv.org/abs/2309.06275v2|
|1567|Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs|Wenhua Cheng, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He, Kaokao Lv, Yi Liu|2023-09-11|arXiv|https://github.com/intel/auto-round|http://arxiv.org/abs/2309.05516v4|
|1568|NExT-GPT: Any-to-Any Multimodal LLM|Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, Tat-Seng Chua|2023-09-11|arXiv|https://next-gpt.github.io/|http://arxiv.org/abs/2309.05519v3|
|1569|Certifying LLM Safety against Adversarial Prompting|Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron Jiaxun Li, Soheil Feizi, Himabindu Lakkaraju|2023-09-06|arXiv|https://github.com/aounon/certified-llm-safety|http://arxiv.org/abs/2309.02705v3|
|1570|ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models|Chenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, Hongzhu Shi, Ji Zhang, Fei Huang, Jingren Zhou|2023-09-02|EMNLP|https://github.com/modelscope/modelscope-agent|https://doi.org/10.18653/v1/2023.emnlp-demo.51|
|1571|Taken out of context: On measuring situational awareness in LLMs|Lukas Berglund, Asa Cooper Stickland, Mikita Balesni, Max Kaufmann, Meg Tong, Tomasz Korbak, Daniel Kokotajlo, Owain Evans|2023-09-01|arXiv|https://github.com/AsaCooperStickland/situational-awareness-evals|http://arxiv.org/abs/2309.00667v1|
|1572|PointLLM: Empowering Large Language Models to Understand Point Clouds|Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiangmiao Pang, Dahua Lin|2023-08-31|arXiv|https://github.com/OpenRobotLab/PointLLM|https://doi.org/10.48550/arXiv.2308.16911|
|1573|SpeechTokenizer: Unified Speech Tokenizer for Speech Large Language Models|Xin Zhang, Dong Zhang, Shimin Li, Yaqian Zhou, Xipeng Qiu|2023-08-31|arXiv|https://0nutation.github.io/SpeechTokenizer.github.io/|https://doi.org/10.48550/arXiv.2308.16692|
|1574|LLaSM: Large Language and Speech Model|Yu Shu, Siwei Dong, Guangyao Chen, Wenhao Huang, Ruihua Zhang, Daochen Shi, Qiqi Xiang, Yemin Shi|2023-08-30|arXiv|https://github.com/LinkSoul-AI/LLaSM|https://doi.org/10.48550/arXiv.2308.15930|
|1575|WALL-E: Embodied Robotic WAiter Load Lifting with Large Language Model|Tianyu Wang, Yifan Li, Haitao Lin, Xiangyang Xue, Yanwei Fu|2023-08-30|arXiv|https://star-uu-wang.github.io/WALL-E/|https://doi.org/10.48550/arXiv.2308.15962|
|1576|Where Would I Go Next? Large Language Models as Human Mobility Predictors|Xinglei Wang, Meng Fang, Zichao Zeng, Tao Cheng|2023-08-29|arXiv|https://github.com/xlwang233/LLM-Mob|https://doi.org/10.48550/arXiv.2308.15197|
|1577|DISC-MedLLM: Bridging General Large Language Models and Real-World Medical Consultation|Zhijie Bao, Wei Chen, Shengze Xiao, Kuang Ren, Jiaao Wu, Cheng Zhong, Jiajie Peng, Xuanjing Huang, Zhongyu Wei|2023-08-28|arXiv|https://github.com/FudanDISC/DISC-MedLLM|https://doi.org/10.48550/arXiv.2308.14346|
|1578|Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering|Keheng Wang, Feiyu Duan, Sirui Wang, Peiguang Li, Yunsen Xian, Chuantao Yin, Wenge Rong, Zhang Xiong|2023-08-25|OpenReview|https://github.com/AdelWang/KD-CoT/tree/main|http://arxiv.org/abs/2308.13259v2|
|1579|Large Language Models Should Ask Clarifying Questions to Increase Confidence in Generated Code|Jie JW Wu|2023-08-25|arXiv|https://mapsworkshop.github.io/|http://arxiv.org/abs/2308.13507v2|
|1580|Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models|Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, Yang Liu|2023-08-25|arXiv|https://github.com/PVIT-official/PVIT|https://doi.org/10.48550/arXiv.2308.13437|
|1581|LLMRec: Benchmarking Large Language Models on Recommendation Task|Junling Liu, Chao Liu, Peilin Zhou, Qichen Ye, Dading Chong, Kang Zhou, Yueqi Xie, Yuwei Cao, Shoujin Wang, Chenyu You, Philip S. Yu|2023-08-23|arXiv|https://github.com/williamliujl/LLMRec|https://doi.org/10.48550/arXiv.2308.12241|
|1582|LKPNR: LLM and KG for Personalized News Recommendation Framework|Chen hao, Xie Runfeng, Cui Xiangyang, Yan Zhou, Wang Xin, Xuan Zhanwei, Zhang Kai|2023-08-23|arXiv|https://github.com/Xuan-ZW/LKPNR|http://arxiv.org/abs/2308.12028v1|
|1583|Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis|Chang Liu, Bo Wu|2023-08-22|arXiv|https://github.com/Ayame1006/LLMtoGraph|https://doi.org/10.48550/arXiv.2308.11224|
|1584|Enhancing NeRF akin to Enhancing LLMs: Generalizable NeRF Transformer with Mixture-of-View-Experts|Wenyan Cong, Hanxue Liang, Peihao Wang, Zhiwen Fan, Tianlong Chen, Mukund Varma, Yi Wang, Zhangyang Wang|2023-08-22|2023 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/VITA-Group/GNT-MOVE|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10378393|
|1585|Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models|Martin Weyssow, Xin Zhou, Kisub Kim, David Lo, Houari A. Sahraoui|2023-08-21|arXiv|https://github.com/martin-wey/peft-llm-code/|https://doi.org/10.48550/arXiv.2308.10462|
|1586|Large Language Models for Software Engineering: A Systematic Literature Review|Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John C. Grundy, Haoyu Wang|2023-08-21|arXiv|https://github.com/xinyi-hou/LLM4SE_SLR|https://doi.org/10.48550/arXiv.2308.10620|
|1587|RaLLe: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models|Yasuto Hoshi, Daisuke Miyashita, Youyang Ng, Kento Tatsuno, Yasuhiro Morioka, Osamu Torii, Jun Deguchi|2023-08-21|EMNLP|https://github.com/yhoshi3/RaLLe|https://doi.org/10.18653/v1/2023.emnlp-demo.4|
|1588|How Good Are LLMs at Out-of-Distribution Detection?|Bo Liu, Liming Zhan, Zexin Lu, Yujie Feng, Lei Xue, Xiao-Ming Wu|2023-08-20|arXiv|https://github.com/Awenbocc/LLM-OOD|http://arxiv.org/abs/2308.10261v4|
|1589|LMTuner: An user-friendly and highly-integrable Training Framework for fine-tuning Large Language Models|Yixuan Weng, Zhiqi Wang, Huanxuan Liao, Shizhu He, Shengping Liu, Kang Liu, Jun Zhao|2023-08-20|arXiv|https://wengsyx.github.io/LMTuner/|https://doi.org/10.48550/arXiv.2308.10252|
|1590|GameEval: Evaluating LLMs on Conversational Games|Dan Qiao, Chenfei Wu, Yaobo Liang, Juntao Li, Nan Duan|2023-08-19|arXiv|https://github.com/GameEval/GameEval|http://arxiv.org/abs/2308.10032v1|
|1591|Inductive-bias Learning: Generating Code Models with Large Language Model|Toma Tanaka, Naofumi Emoto, Tsukasa Yumibayashi|2023-08-19|arXiv|https://github.com/fuyu-quant/IBLM|https://doi.org/10.48550/arXiv.2308.09890|
|1592|ChatHaruhi: Reviving Anime Character in Reality via Large Language Model|Cheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi MI, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, Linkang Zhan, Yaokai Jia, Pingyu Wu, Haozhen Sun|2023-08-18|arXiv|https://github.com/LC1332/Chat-Haruhi-Suzumiya|https://doi.org/10.48550/arXiv.2308.09597|
|1593|WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct|Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Dongmei Zhang|2023-08-18|arXiv|https://github.com/nlpxucan/WizardLM|https://doi.org/10.48550/arXiv.2308.09583|
|1594|Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection|Zekun Li, Baolin Peng, Pengcheng He, Xifeng Yan|2023-08-17|OpenReview|https://github.com/Leezekun/instruction-following-robustness-eval|http://arxiv.org/abs/2308.10819v3|
|1595|The Unreasonable Effectiveness of Large Language-Vision Models for Source-free Video Domain Adaptation|Giacomo Zara, Alessandro Conti, Subhankar Roy, Stéphane Lathuilière, Paolo Rota, Elisa Ricci|2023-08-17|2023 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/giaczara/dallv|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10377542|
|1596|MemoChat: Tuning LLMs to Use Memos for Consistent Long-Range Open-Domain Conversation|Junru Lu, Siyu An, Mingbao Lin, Gabriele Pergola, Yulan He, Di Yin, Xing Sun, Yunsheng Wu|2023-08-16|arXiv|https://github.com/LuJunru/MemoChat|http://arxiv.org/abs/2308.08239v2|
|1597|A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems|Keqin Bao, Jizhi Zhang, Wenjie Wang, Yang Zhang, Zhengyi Yang, Yancheng Luo, Fuli Feng, Xiangnan He, Qi Tian|2023-08-16|arXiv|https://github.com/SAI990323/Grounding4Rec|https://doi.org/10.48550/arXiv.2308.08434|
|1598|Link-Context Learning for Multimodal LLMs|Yan Tai, Weichen Fan, Zhao Zhang, Feng Zhu, Rui Zhao, Ziwei Liu|2023-08-15|arXiv|https://github.com/isekai-portal/Link-Context-Learning|http://arxiv.org/abs/2308.07891v1|
|1599|EasyEdit: An Easy-to-use Knowledge Editing Framework for Large Language Models|Peng Wang, Ningyu Zhang, Bozhong Tian, Zekun Xi, Yunzhi Yao, Ziwen Xu, Mengru Wang, Shengyu Mao, Xiaohan Wang, Siyuan Cheng, Kangwei Liu, Yuansheng Ni, Guozhou Zheng, Huajun Chen|2023-08-14|arXiv|https://github.com/zjunlp/EasyEdit|https://doi.org/10.48550/arXiv.2308.07269|
|1600|LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked|Mansi Phute, Alec Helbling, Matthew Hull, ShengYun Peng, Sebastian Szyller, Cory Cornelius, Duen Horng Chau|2023-08-14|arXiv|https://github.com/poloclub/llm-self-defense|http://arxiv.org/abs/2308.07308v4|
|1601|Platypus: Quick, Cheap, and Powerful Refinement of LLMs|Ariel N. Lee, Cole J. Hunter, Nataniel Ruiz|2023-08-14|arXiv|https://platypus-llm.github.io|http://arxiv.org/abs/2308.07317v2|
|1602|GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher|Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Pinjia He, Shuming Shi, Zhaopeng Tu|2023-08-12|arXiv|https://github.com/RobustNLP/CipherChat|http://arxiv.org/abs/2308.06463v2|
|1603|BOLAA: Benchmarking and Orchestrating LLM-augmented Autonomous Agents|Zhiwei Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby Heinecke, Rithesh Murthy, Yihao Feng, Zeyuan Chen, Juan Carlos Niebles, Devansh Arpit, Ran Xu, Phil Mui, Huan Wang, Caiming Xiong, Silvio Savarese|2023-08-11|OpenReview|https://github.com/salesforce/BOLAA|http://arxiv.org/abs/2308.05960v1|
|1604|LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking|Fahim Dalvi, Maram Hasanain, Sabri Boughorbel, Basel Mousi, Samir Abdaljalil, Nizi Nazar, Ahmed Abdelali, Shammur Absar Chowdhury, Hamdy Mubarak, Ahmed Ali, Majd Hawasly, Nadir Durrani, Firoj Alam|2023-08-09|arXiv|https://github.com/qcri/LLMeBench/|http://arxiv.org/abs/2308.04945v2|
|1605|Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions|Juncheng Li, Kaihang Pan, Zhiqi Ge, Minghe Gao, Wei Ji, Wenqiao Zhang, Tat-Seng Chua, Siliang Tang, Hanwang Zhang, Yueting Zhuang|2023-08-08|arXiv|https://github.com/DCDmllm/Cheetah|http://arxiv.org/abs/2308.04152v4|
|1606|AgentBench: Evaluating LLMs as Agents|Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, Jie Tang|2023-08-07|arXiv|https://github.com/THUDM/AgentBench|http://arxiv.org/abs/2308.03688v2|
|1607|Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using EmotionBench|Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu|2023-08-07|arXiv|https://github.com/CUHK-ARISE/EmotionBench|http://arxiv.org/abs/2308.03656v4|
|1608|LISA: Reasoning Segmentation via Large Language Model|Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, Jiaya Jia|2023-08-01|arXiv|https://github.com/dvlab-research/LISA|https://doi.org/10.48550/arXiv.2308.00692|
|1609|Scaling Sentence Embeddings with Large Language Models|Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing Wang, Fuzhen Zhuang|2023-07-31|arXiv|https://github.com/kongds/scaling_sentemb|https://doi.org/10.48550/arXiv.2307.16645|
|1610|HAGRID: A Human-LLM Collaborative Dataset for Generative Information-Seeking with Attribution|Ehsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan Thakur, Jimmy Lin|2023-07-31|arXiv|https://github.com/project-miracl/hagrid|http://arxiv.org/abs/2307.16883v1|
|1611|Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models|Keyu Pan, Yawen Zeng|2023-07-30|arXiv|https://github.com/HarderThenHarder/transformers_tasks/tree/main/LLM/llms_mbti|https://doi.org/10.48550/arXiv.2307.16180|
|1612|SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension|Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, Ying Shan|2023-07-30|arXiv|https://github.com/AILab-CVC/SEED-Bench|http://arxiv.org/abs/2307.16125v2|
|1613|Okapi: Instruction-tuned Large Language Models in Multiple Languages with Reinforcement Learning from Human Feedback|Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan A. Rossi, Thien Huu Nguyen|2023-07-29|EMNLP|https://github.com/nlp-uoregon/Okapi|https://doi.org/10.18653/v1/2023.emnlp-demo.28|
|1614|Towards Codable Watermarking for Injecting Multi-bits Information to LLMs|Lean Wang, Wenkai Yang, Deli Chen, Hao Zhou, Yankai Lin, Fandong Meng, Jie Zhou, Xu Sun|2023-07-29|arXiv|https://github.com/lancopku/codable-watermarking-for-llm|http://arxiv.org/abs/2307.15992v3|
|1615|TrafficSafetyGPT: Tuning a Pre-trained Large Language Model to a Domain-Specific Expert in Transportation Safety|Ou Zheng, Mohamed A. Abdel-Aty, Dongdong Wang, Chenzhu Wang, Shengxuan Ding|2023-07-28|arXiv|https://github.com/ozheng1993/TrafficSafetyGPT|https://doi.org/10.48550/arXiv.2307.15311|
|1616|TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer|Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Xiao Luo, Yu Qiao, Yiran Zhong|2023-07-27|arXiv|https://github.com/OpenNLPLab/TransnormerLLM|http://arxiv.org/abs/2307.14995v2|
|1617|GrammarGPT: Exploring Open-Source LLMs for Native Chinese Grammatical Error Correction with Supervised Fine-Tuning|Yaxin Fan, Feng Jiang, Peifeng Li, Haizhou Li|2023-07-26|arXiv|https://github.com/FreedomIntelligence/GrammarGPT|http://arxiv.org/abs/2307.13923v2|
|1618|Three Bricks to Consolidate Watermarks for Large Language Models|Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, Teddy Furon|2023-07-26|2023 IEEE International Workshop on Information Forensics and Security (WIFS)|https://github.com/facebookresearch/three_bricks|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10374576|
|1619|WavJourney: Compositional Audio Creation with Large Language Models|Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui, Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D. Plumbley, Wenwu Wang|2023-07-26|arXiv|https://audio-agi.github.io/WavJourney_demopage/|https://doi.org/10.48550/arXiv.2307.14335|
|1620|QuIP: 2-Bit Quantization of Large Language Models With Guarantees|Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, Christopher De Sa|2023-07-25|NeurIPS|https://github.com/Cornell-RelaxML/QuIP|http://papers.nips.cc/paper_files/paper/2023/hash/0df38cd13520747e1e64e5b123a78ef8-Abstract-Conference.html|
|1621|Investigating the Factual Knowledge Boundary of Large Language Models with Retrieval Augmentation|Ruiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin Zhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen, Haifeng Wang|2023-07-20|arXiv|https://github.com/RUCAIBox/LLM-Knowledge-Boundary|https://doi.org/10.48550/arXiv.2307.11019|
|1622|Of Models and Tin Men - a behavioural economics study of principal-agent problems in AI alignment using large-language models|Steve Phelps, Rebecca Ranson|2023-07-20|arXiv|https://github.com/phelps-sg/llm-cooperation|https://doi.org/10.48550/arXiv.2307.11137|
|1623|Emotional Intelligence of Large Language Models|Xuena Wang, Xueting Li, Zi Yin, Yue Wu, Liu Jia|2023-07-18|arXiv|https://emotional-intelligence.github.io/|https://doi.org/10.48550/arXiv.2307.09042|
|1624|Comparative Performance Evaluation of Large Language Models for Extracting Molecular Interactions and Pathway Knowledge|Gilchan Park, Byung-Jun Yoon, Xihaier Luo, Vanessa López-Marrero, Patrick R. Johnstone, Shinjae Yoo, Francis J. Alexander|2023-07-17|arXiv|https://github.com/boxorange/BioIE-LLM|https://doi.org/10.48550/arXiv.2307.08813|
|1625|Latent Jailbreak: A Benchmark for Evaluating Text Safety and Output Robustness of Large Language Models|Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, Zhenzhong Lan|2023-07-17|arXiv|https://github.com/qiuhuachuan/latent-jailbreak|https://doi.org/10.48550/arXiv.2307.08487|
|1626|Planting a SEED of Vision in Large Language Model|Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, Ying Shan|2023-07-16|arXiv|https://github.com/AILab-CVC/SEED|https://doi.org/10.48550/arXiv.2307.08041|
|1627|Creating a Dataset for High-Performance Computing Code Translation using LLMs: A Bridge Between OpenMP Fortran and C++|Bin Lei, Caiwen Ding, Le Chen, Pei-Hung Lin, Chunhua Liao|2023-07-15|2023 IEEE High Performance Extreme Computing Conference (HPEC)|https://github.com/bin123apple/Fortran-CPP-HPC-code-translation-dataset|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10363534|
|1628|Can Large Language Models Empower Molecular Property Prediction?|Chen Qian, Huayi Tang, Zhirui Yang, Hong Liang, Yong Liu|2023-07-14|arXiv|https://github.com/ChnQ/LLM4Mol|https://doi.org/10.48550/arXiv.2307.07443|
|1629|Certified Robustness for Large Language Models with Self-Denoising|Zhen Zhang, Guanhua Zhang, Bairu Hou, Wenqi Fan, Qing Li, Sijia Liu, Yang Zhang, Shiyu Chang|2023-07-14|arXiv|https://github.com/UCSB-NLP-Chang/SelfDenoise|https://doi.org/10.48550/arXiv.2307.07171|
|1630|Large Language Models Understand and Can be Enhanced by Emotional Stimuli|Cheng Li, Jindong Wang, Yixuan Zhang, Kaijie Zhu, Wenxin Hou, Jianxun Lian, Fang Luo, Qiang Yang, Xing Xie|2023-07-14|arXiv|https://llm-enhance.github.io/|http://arxiv.org/abs/2307.11760v7|
|1631|Negated Complementary Commonsense using Large Language Models|Navid Rezaei, Marek Z. Reformat|2023-07-13|arXiv|https://github.com/navidre/negated_complementary_commonsense|https://doi.org/10.48550/arXiv.2307.06794|
|1632|mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs|Gregor Geigle, Abhay Jain, Radu Timofte, Goran Glavaš|2023-07-13|arXiv|https://github.com/gregor-ge/mBLIP|http://arxiv.org/abs/2307.06930v3|
|1633|SayPlan: Grounding Large Language Models using 3D Scene Graphs for Scalable Robot Task Planning|Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-Chakra, Ian D. Reid, Niko Sünderhauf|2023-07-12|CoRL|https://sayplan.github.io|https://proceedings.mlr.press/v229/rana23a.html|
|1634|AI-UPV at EXIST 2023 - Sexism Characterization Using Large Language Models Under The Learning with Disagreement Regime|Angel Felipe Magnossão de Paula, Giulia Rizzi, Elisabetta Fersini, Damiano Spina|2023-07-07|CLEF|https://github.com/AngelFelipeMP/Sexism-LLM-Learning-With-Disagreement|https://ceur-ws.org/Vol-3497/paper-084.pdf|
|1635|GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest|Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai Chen, Ping Luo|2023-07-07|arXiv|https://github.com/jshilong/GPT4RoI|https://doi.org/10.48550/arXiv.2307.03601|
|1636|INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers|Lakshmi Nair, Mikhail Bernadskiy, Arulselvan Madhavan, Craig Chan, Ayon Basumallik, Darius Bunandar|2023-07-07|arXiv|https://github.com/lightmatter-ai/INT-FP-QSim|https://doi.org/10.48550/arXiv.2307.03712|
|1637|RecallM: An Adaptable Memory Mechanism with Temporal Understanding for Large Language Models|Brandon Kynoch, Hugo Latapie, Dwane van der Sluis|2023-07-06|arXiv|https://github.com/cisco-open/DeepVision/tree/main/recallm|http://arxiv.org/abs/2307.02738v3|
|1638|External Reasoning: Towards Multi-Large-Language-Models Interchangeable Assistance with Human Feedback|Akide Liu|2023-07-05|arXiv|https://github.com/AkideLiu/ANLP|https://doi.org/10.48550/arXiv.2307.12057|
|1639|mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding|Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, Fei Huang|2023-07-04|arXiv|https://github.com/X-PLUG/mPLUG-DocOwl|https://doi.org/10.48550/arXiv.2307.02499|
|1640|Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners|Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama, Fei Xia, Jake Varley, Zhenjia Xu, Dorsa Sadigh, Andy Zeng, Anirudha Majumdar|2023-07-04|CoRL|https://robot-help.github.io|https://proceedings.mlr.press/v229/ren23a.html|
|1641|Embodied Task Planning with Large Language Models|Zhenyu Wu, Ziwei Wang, Xiuwei Xu, Jiwen Lu, Haibin Yan|2023-07-04|arXiv|https://gary3410.github.io/TaPA|https://doi.org/10.48550/arXiv.2307.01848|
|1642|Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias|Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander J. Ratner, Ranjay Krishna, Jiaming Shen, Chao Zhang|2023-06-28|NeurIPS|https://github.com/yueyu1030/AttrPrompt|http://papers.nips.cc/paper_files/paper/2023/hash/ae9500c4f5607caf2eff033c67daa9d7-Abstract-Datasets_and_Benchmarks.html|
|1643|Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic|Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, Rui Zhao|2023-06-27|arXiv|https://github.com/shikras/shikra|http://arxiv.org/abs/2306.15195v2|
|1644|Fauno: The Italian Large Language Model that will leave you senza parole!|Andrea Bacciu, Giovanni Trappolini, Andrea Santilli, Emanuele Rodolà, Fabrizio Silvestri|2023-06-26|IIR|https://github.com/RSTLess-research/Fauno-Italian-LLM|https://ceur-ws.org/Vol-3448/paper-24.pdf|
|1645|Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models|Yinyu Lan, Yanru Wu, Wang Xu, Weiqiang Feng, Youhao Zhang|2023-06-25|arXiv|https://finllm.github.io/workshop/#/fcb|https://doi.org/10.48550/arXiv.2306.14096|
|1646|H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models|Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, Zhangyang Wang, Beidi Chen|2023-06-24|arXiv|https://github.com/FMInference/H2O|http://arxiv.org/abs/2306.14048v3|
|1647|Bring Your Own Data! Self-Supervised Evaluation for Large Language Models|Neel Jain, Khalid Saifullah, Yuxin Wen, John Kirchenbauer, Manli Shu, Aniruddha Saha, Micah Goldblum, Jonas Geiping, Tom Goldstein|2023-06-23|arXiv|https://github.com/neelsjain/BYOD|https://doi.org/10.48550/arXiv.2306.13651|
|1648|MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models|Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Rongrong Ji|2023-06-23|arXiv|https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models|https://doi.org/10.48550/arXiv.2306.13394|
|1649|AudioPaLM: A Large Language Model That Can Speak and Listen|Paul K. Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, Hannah Muckenhirn, Dirk Padfield, James Qin, Danny Rozenberg, Tara N. Sainath, Johan Schalkwyk, Matthew Sharifi, Michelle Tadmor Ramanovich, Marco Tagliasacchi, Alexandru Tudor, Mihajlo Velimirovic, Damien Vincent, Jiahui Yu, Yongqiang Wang, Vicky Zayats, Neil Zeghidour, Yu Zhang, Zhishuai Zhang, Lukas Zilka, Christian Havnø Frank|2023-06-22|arXiv|https://google-research.github.io/seanet/audiopalm/examples|https://doi.org/10.48550/arXiv.2306.12925|
|1650|Can LLMs Express Their Uncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs|Miao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, Bryan Hooi|2023-06-22|arXiv|https://github.com/MiaoXiong2320/llm-uncertainty|http://arxiv.org/abs/2306.13063v2|
|1651|OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue|Weihao Gao, Zhuo Deng, Zhiyuan Niu, Fuju Rong, Chucheng Chen, Zheng Gong, Wenze Zhang, Daimin Xiao, Fang Li, Zhenjie Cao, Zhaoyi Ma, Wenbin Wei, Lan Ma|2023-06-21|arXiv|https://github.com/ML-AILab/OphGLM|https://doi.org/10.48550/arXiv.2306.12174|
|1652|Learning to Generate Better Than Your LLM|Jonathan D. Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra Misra, Wen Sun|2023-06-20|arXiv|https://github.com/Cornell-RL/tril|http://arxiv.org/abs/2306.11816v2|
|1653|Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost|Juexiao Zhou, Xiuying Chen, Xin Gao|2023-06-19|arXiv|https://github.com/JoshuaChou2018/MedAGI|http://arxiv.org/abs/2306.10765v1|
|1654|MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators|Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu Chen, Lei Bai, Qi Chu, Nenghai Yu, Wanli Ouyang|2023-06-19|arXiv|https://qiqiapink.github.io/MotionGPT/|http://arxiv.org/abs/2306.10900v2|
|1655|Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset|Saeid Alavi Naeini, Raeid Saqur, Mozhgan Saeidi, John M. Giorgi, Babak Taati|2023-06-19|NeurIPS|https://github.com/TaatiTeam/OCW|http://papers.nips.cc/paper_files/paper/2023/hash/11e3e0f1b29dcd31bd0952bfc1357f68-Abstract-Datasets_and_Benchmarks.html|
|1656|LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning|Yunlong Tang, Jinrui Zhang, Xiangchen Wang, Teng Wang, Feng Zheng|2023-06-17|arXiv|https://github.com/zjr2000/LLMVA-GEBC|https://doi.org/10.48550/arXiv.2306.10354|
|1657|Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models|Qingyu Tan, Hwee Tou Ng, Lidong Bing|2023-06-15|ACL|https://github.com/DAMO-NLP-SG/TempReason|https://doi.org/10.18653/v1/2023.acl-long.828|
|1658|Artificial Artificial Artificial Intelligence: Crowd Workers Widely Use Large Language Models for Text Production Tasks|Veniamin Veselovsky, Manoel Horta Ribeiro, Robert West|2023-06-13|arXiv|https://github.com/epfl-dlab/GPTurk|https://doi.org/10.48550/arXiv.2306.07899|
|1659|ReadProbe: A Demo of Retrieval-Enhanced Large Language Models to Support Lateral Reading|Dake Zhang, Ronak Pradeep|2023-06-13|arXiv|https://github.com/DakeZhang1998/ReadProbe|https://doi.org/10.48550/arXiv.2306.07875|
|1660|TrojLLM: A Black-box Trojan Prompt Attack on Large Language Models|Jiaqi Xue, Mengxin Zheng, Ting Hua, Yilin Shen, Yepeng Liu, Ladislau Bölöni, Qian Lou|2023-06-12|NeurIPS|https://github.com/UCF-ML-Research/TrojLLM|http://papers.nips.cc/paper_files/paper/2023/hash/cf04d01a0e76f8b13095349d9caca033-Abstract-Conference.html|
|1661|RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs|Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, Ye Tian, Sujian Li|2023-06-11|arXiv|https://restgpt.github.io/|https://doi.org/10.48550/arXiv.2306.06624|
|1662|FinGPT: Open-Source Financial Large Language Models|Hongyang Yang, Xiao-Yang Liu, Christina Dan Wang|2023-06-09|arXiv|https://github.com/AI4Finance-Foundation/FinGPT|https://doi.org/10.48550/arXiv.2306.06031|
|1663|Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena|Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica|2023-06-09|arXiv|https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge|http://arxiv.org/abs/2306.05685v4|
|1664|Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding|Mu Cai, Zeyi Huang, Yuheng Li, Haohan Wang, Yong Jae Lee|2023-06-09|arXiv|https://github.com/mu-cai/svg-llm|https://doi.org/10.48550/arXiv.2306.06094|
|1665|M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models|Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, Lidong Bing|2023-06-08|NeurIPS|https://github.com/DAMO-NLP-SG/M3Exam|http://papers.nips.cc/paper_files/paper/2023/hash/117c5c8622b0d539f74f6d1fb082a2e9-Abstract-Datasets_and_Benchmarks.html|
|1666|RETA-LLM: A Retrieval-Augmented Large Language Model Toolkit|Jiongnan Liu, Jiajie Jin, Zihan Wang, Jiehan Cheng, Zhicheng Dou, Ji-Rong Wen|2023-06-08|arXiv|https://github.com/RUC-GSAI/YuLan-IR/tree/main/RETA-LLM|https://doi.org/10.48550/arXiv.2306.05212|
|1667|The Two Word Test: A Semantic Benchmark for Large Language Models|Nicholas Riccardi, Rutvik H. Desai|2023-06-07|arXiv|https://github.com/NickRiccardi/two-word-test|https://doi.org/10.48550/arXiv.2306.04610|
|1668|PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts|Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, Xing Xie|2023-06-07|arXiv|https://github.com/microsoft/promptbench|https://doi.org/10.48550/arXiv.2306.04528|
|1669|MISGENDERED: Limits of Large Language Models in Understanding Pronouns|Tamanna Hossain, Sunipa Dev, Sameer Singh|2023-06-06|ACL|https://tamannahossainkay.github.io/misgendered/|https://doi.org/10.18653/v1/2023.acl-long.293|
|1670|ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory|Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao|2023-06-06|arXiv|https://chatdatabase.github.io/|http://arxiv.org/abs/2306.03901v2|
|1671|Enabling Intelligent Interactions between an Agent and an LLM: A Reinforcement Learning Approach|Bin Hu, Chenyang Zhao, Pu Zhang, Zihao Zhou, Yuanhang Yang, Zenglin Xu, Bin Liu|2023-06-06|arXiv|https://github.com/ZJLAB-AMMI/LLM4RL|http://arxiv.org/abs/2306.03604v8|
|1672|AutoScrum: Automating Project Planning Using Large Language Models|Martin Schröder|2023-06-05|arXiv|https://github.com/autoscrum/autoscrum|https://doi.org/10.48550/arXiv.2306.03197|
|1673|Benchmarking Large Language Models on CMExam - A comprehensive Chinese Medical Exam Dataset|Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, Michael Lingzhi Li|2023-06-05|NeurIPS|https://github.com/williamliujl/CMExam|http://papers.nips.cc/paper_files/paper/2023/hash/a48ad12d588c597f4725a8b84af647b5-Abstract-Datasets_and_Benchmarks.html|
|1674|Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs|Alexander K. Lew, Tan Zhi-Xuan, Gabriel Grand, Vikash K. Mansinghka|2023-06-05|arXiv|https://github.com/probcomp/hfppl|https://doi.org/10.48550/arXiv.2306.03081|
|1675|AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration|Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, Song Han|2023-06-01|arXiv|https://github.com/mit-han-lab/llm-awq|http://arxiv.org/abs/2306.00978v4|
|1676|Revisiting the Reliability of Psychological Scales on Large Language Models|Jen-tse Huang, Wenxuan Wang, Man Ho Lam, Eric John Li, Wenxiang Jiao, Michael R. Lyu|2023-05-31|arXiv|https://github.com/CUHK-ARISE/LLMPersonality|http://arxiv.org/abs/2305.19926v3|
|1677|Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate|Tian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, Shuming Shi|2023-05-30|arXiv|https://github.com/Skytliang/Multi-Agents-Debate|https://doi.org/10.48550/arXiv.2305.19118|
|1678|GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction|Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, Ying Shan|2023-05-30|NeurIPS|https://github.com/StevenGrove/GPT4Tools|http://papers.nips.cc/paper_files/paper/2023/hash/e393677793767624f2821cec8bdd02f1-Abstract-Conference.html|
|1679|SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models|Hongxin Li, Jingran Su, Yuntao Chen, Qing Li, Zhaoxiang Zhang|2023-05-30|NeurIPS|https://sheetcopilot.github.io/|http://papers.nips.cc/paper_files/paper/2023/hash/0ff30c4bf31db0119a6219e0d250e037-Abstract-Conference.html|
|1680|The Magic of IF: Investigating Causal Reasoning Abilities in Large Language Models of Code|Xiao Liu, Da Yin, Chen Zhang, Yansong Feng, Dongyan Zhao|2023-05-30|ACL|https://github.com/xxxiaol/magic-if|https://doi.org/10.18653/v1/2023.findings-acl.574|
|1681|BigTrans: Augmenting Large Language Models with Multilingual Translation Capability over 100 Languages|Wen Yang, Chong Li, Jiajun Zhang, Chengqing Zong|2023-05-29|arXiv|https://github.com/ZNLP/BigTranslate|https://doi.org/10.48550/arXiv.2305.18098|
|1682|What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks|Taicheng Guo, Kehan Guo, Bozhao Nan, Zhenwen Liang, Zhichun Guo, Nitesh V. Chawla, Olaf Wiest, Xiangliang Zhang|2023-05-27|NeurIPS|https://github.com/ChemFoundationModels/ChemLLMBench|http://papers.nips.cc/paper_files/paper/2023/hash/bbb330189ce02be00cf7346167028ab1-Abstract-Datasets_and_Benchmarks.html|
|1683|Integrating Action Knowledge and LLMs for Task Planning and Situation Handling in Open Worlds|Yan Ding, Xiaohan Zhang, Saeid Amiri, Nieqing Cao, Hao Yang, Andy Kaminski, Chad Esselink, Shiqi Zhang|2023-05-27|Autonomous Robots, 2023|https://cowplanning.github.io/|http://arxiv.org/abs/2305.17590v2|
|1684|Heterogeneous Value Evaluation for Large Language Models|Zhaowei Zhang, Nian Liu, Siyuan Qi, Ceyao Zhang, Ziqi Rong, Song-Chun Zhu, Shuguang Cui, Yaodong Yang|2023-05-26|arXiv|https://github.com/zowiezhang/HVAE|https://doi.org/10.48550/arXiv.2305.17147|
|1685|LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations|Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, Elias B. Khalil|2023-05-26|arXiv|https://khalil-research.github.io/LLM4ARC|http://arxiv.org/abs/2305.18354v2|
|1686|Large Language Models Are Partially Primed in Pronoun Interpretation|Suet-Ying Lam, Qingcheng Zeng, Kexun Zhang, Chenyu You, Rob Voigt|2023-05-26|ACL|https://github.com/zkx06111/llm_priming|https://doi.org/10.18653/v1/2023.findings-acl.605|
|1687|Chain-of-Thought Hub: A Continuous Effort to Measure Large Language Models&apos; Reasoning Performance|Yao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao Peng, Tushar Khot|2023-05-26|arXiv|https://github.com/FranxYao/chain-of-thought-hub|https://doi.org/10.48550/arXiv.2305.17306|
|1688|Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory|Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, Yu Qiao, Zhaoxiang Zhang, Jifeng Dai|2023-05-25|arXiv|https://github.com/OpenGVLab/GITM|https://doi.org/10.48550/arXiv.2305.17144|
|1689|ExpertPrompting: Instructing Large Language Models to be Distinguished Experts|Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, Zhendong Mao|2023-05-24|arXiv|https://github.com/OFA-Sys/ExpertLLaMA|https://doi.org/10.48550/arXiv.2305.14688|
|1690|Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models|Geewook Kim, Hodong Lee, Daehee Kim, Haeji Jung, Sanghee Park, Yoonsik Kim, Sangdoo Yun, Taeho Kil, Bado Lee, Seunghyun Park|2023-05-24|EMNLP|https://naver-ai.github.io/cream|https://doi.org/10.18653/v1/2023.emnlp-main.735|
|1691|Meta-Tuning LLMs to Leverage Lexical Knowledge for Generalizable Language Style Understanding|Ruohao Guo, Wei Xu, Alan Ritter|2023-05-24|arXiv|http://github.com/octaviaguo/Style-LLM|http://arxiv.org/abs/2305.14592v2|
|1692|Leveraging Pre-trained Large Language Models to Construct and Utilize World Models for Model-based Task Planning|Lin Guan, Karthik Valmeekam, Sarath Sreedharan, Subbarao Kambhampati|2023-05-24|NeurIPS|https://guansuns.github.io/pages/llm-dm|http://papers.nips.cc/paper_files/paper/2023/hash/f9f54762cbb4fe4dbffdd4f792c31221-Abstract-Conference.html|
|1693|Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners|Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, Muhan Zhang|2023-05-24|arXiv|https://github.com/XiaojuanTang/ICSR|https://doi.org/10.48550/arXiv.2305.14825|
|1694|LLMDet: A Third Party Large Language Models Generated Text Detection Tool|Kangxi Wu, Liang Pang, Huawei Shen, Xueqi Cheng, Tat-Seng Chua|2023-05-24|EMNLP|https://github.com/TrustedLLM/LLMDet|https://doi.org/10.18653/v1/2023.findings-emnlp.139|
|1695|Investigating Table-to-Text Generation Capabilities of Large Language Models in Real-World Information Seeking Scenarios|Yilun Zhao, Haowei Zhang, Shengyun Si, Linyong Nan, Xiangru Tang, Arman Cohan|2023-05-24|EMNLP|https://github.com/yale-nlp/LLM-T2T|https://doi.org/10.18653/v1/2023.emnlp-industry.17|
|1696|IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models|Haoxuan You, Rui Sun, Zhecan Wang, Long Chen, Gengyu Wang, Hammad A. Ayyubi, Kai-Wei Chang, Shih-Fu Chang|2023-05-24|EMNLP|https://github.com/Hxyou/IdealGPT|https://doi.org/10.18653/v1/2023.findings-emnlp.755|
|1697|How Predictable Are Large Language Model Capabilities? A Case Study on BIG-bench|Qinyuan Ye, Harvey Yiyun Fu, Xiang Ren, Robin Jia|2023-05-24|EMNLP|https://github.com/INK-USC/predicting-big-bench|https://doi.org/10.18653/v1/2023.findings-emnlp.503|
|1698|Estimating Large Language Model Capabilities without Labeled Test Data|Harvey Yiyun Fu, Qinyuan Ye, Albert Xu, Xiang Ren, Robin Jia|2023-05-24|EMNLP|https://github.com/harvey-fin/icl-estimate|https://doi.org/10.18653/v1/2023.findings-emnlp.639|
|1699|Enabling Large Language Models to Generate Text with Citations|Tianyu Gao, Howard Yen, Jiatong Yu, Danqi Chen|2023-05-24|EMNLP|https://github.com/princeton-nlp/ALCE|https://doi.org/10.18653/v1/2023.emnlp-main.398|
|1700|ClusterLLM: Large Language Models as a Guide for Text Clustering|Yuwei Zhang, Zihan Wang, Jingbo Shang|2023-05-24|EMNLP|https://github.com/zhang-yu-wei/ClusterLLM|https://doi.org/10.18653/v1/2023.emnlp-main.858|
|1701|Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models|Gen Luo, Yiyi Zhou, Tianhe Ren, Shengxin Chen, Xiaoshuai Sun, Rongrong Ji|2023-05-24|NeurIPS|https://luogen1996.github.io/lavin|http://papers.nips.cc/paper_files/paper/2023/hash/5e84e4413268b713f0d4a1b23a9dae57-Abstract-Conference.html|
|1702|AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models|Siqi Ouyang, Lei Li|2023-05-24|EMNLP|https://github.com/owaski/AutoPlan|https://doi.org/10.18653/v1/2023.findings-emnlp.205|
|1703|Allies: Prompting Large Language Model with Beam Search|Hao Sun, Xiao Liu, Yeyun Gong, Yan Zhang, Daxin Jiang, Linjun Yang, Nan Duan|2023-05-24|EMNLP|https://github.com/microsoft/SimXNS/tree/main/ALLIES|https://doi.org/10.18653/v1/2023.findings-emnlp.247|
|1704|ALGO: Synthesizing Algorithmic Programs with LLM-Generated Oracle Verifiers|Kexun Zhang, Danqing Wang, Jingtao Xia, William Yang Wang, Lei Li|2023-05-24|arXiv|https://github.com/zkx06111/ALGO|http://arxiv.org/abs/2305.14591v3|
|1705|A Causal View of Entity Bias in (Large) Language Models|Fei Wang, Wenjie Mo, Yiwei Wang, Wenxuan Zhou, Muhao Chen|2023-05-24|EMNLP|https://github.com/luka-group/Causal-View-of-Entity-Bias|https://doi.org/10.18653/v1/2023.findings-emnlp.1013|
|1706|Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark|Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, David Jurgens|2023-05-24|EMNLP|https://github.com/minjechoi/SOCKET|https://doi.org/10.18653/v1/2023.emnlp-main.699|
|1707|ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models|Zhipeng Chen, Kun Zhou, Beichen Zhang, Zheng Gong, Wayne Xin Zhao, Ji-Rong Wen|2023-05-23|arXiv|https://github.com/RUCAIBOX/ChatCoT|https://doi.org/10.48550/arXiv.2305.14323|
|1708|Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement|Zhiheng Xi, Senjie Jin, Yuhao Zhou, Rui Zheng, Songyang Gao, Jia Liu, Tao Gui, Qi Zhang, Xuanjing Huang|2023-05-23|EMNLP|https://github.com/WooooDyy/Self-Polish|https://doi.org/10.18653/v1/2023.findings-emnlp.762|
|1709|DirecT2V: Large Language Models are Frame-Level Directors for Zero-Shot Text-to-Video Generation|Susung Hong, Junyoung Seo, Heeseong Shin, Sunghwan Hong, Seungryong Kim|2023-05-23|arXiv|https://github.com/KU-CVLAB/DirecT2V|http://arxiv.org/abs/2305.14330v3|
|1710|Generating Data for Symbolic Language with Large Language Models|Jiacheng Ye, Chengzu Li, Lingpeng Kong, Tao Yu|2023-05-23|EMNLP|https://github.com/HKUNLP/SymGen|https://doi.org/10.18653/v1/2023.emnlp-main.523|
|1711|Can Large Language Models Capture Dissenting Human Voices?|Noah Lee, Na Min An, James Thorne|2023-05-23|EMNLP|https://github.com/xfactlab/emnlp2023-LLM-Disagreement|https://doi.org/10.18653/v1/2023.emnlp-main.278|
|1712|Automatic Model Selection with Large Language Models for Reasoning|James Xu Zhao, Yuxi Xie, Kenji Kawaguchi, Junxian He, Michael Qizhe Xie|2023-05-23|EMNLP|https://github.com/XuZhao0/Model-Selection-Reasoning|https://doi.org/10.18653/v1/2023.findings-emnlp.55|
|1713|Aligning Large Language Models through Synthetic Feedback|Sungdong Kim, Sanghwan Bae, Jamin Shin, Soyoung Kang, Dong-Hyun Kwak, Kang Min Yoo, Minjoon Seo|2023-05-23|arXiv|https://github.com/naver-ai/almost|https://doi.org/10.48550/arXiv.2305.13735|
|1714|LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities|Yuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao, Yixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen, Ningyu Zhang|2023-05-22|arXiv|https://github.com/zjunlp/AutoKG|http://arxiv.org/abs/2305.13168v2|
|1715|Lion: Adversarial Distillation of Proprietary Large Language Models|Yuxin Jiang, Chunkit Chan, Mingyang Chen, Wei Wang|2023-05-22|EMNLP|https://github.com/YJiangcm/Lion|https://doi.org/10.18653/v1/2023.emnlp-main.189|
|1716|Rethinking the Evaluation for Conversational Recommendation in the Era of Large Language Models|Xiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan Wang, Ji-Rong Wen|2023-05-22|arXiv|https://github.com/RUCAIBox/iEvaLM-CRS|https://doi.org/10.48550/arXiv.2305.13112|
|1717|VideoLLM: Modeling Video Sequence with Large Language Models|Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong Lu, Limin Wang|2023-05-22|arXiv|https://github.com/cg1177/VideoLLM|https://doi.org/10.48550/arXiv.2305.13292|
|1718|PiVe: Prompting with Iterative Verification Improving Graph-based Generative Capability of LLMs|Jiuzhou Han, Nigel Collier, Wray Buntine, Ehsan Shareghi|2023-05-21|arXiv|https://github.com/Jiuzhouh/PiVe|http://arxiv.org/abs/2305.12392v3|
|1719|Logic-LM: Empowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning|Liangming Pan, Alon Albalak, Xinyi Wang, William Yang Wang|2023-05-20|EMNLP|https://github.com/teacherpeterpan/Logic-LLM|https://doi.org/10.18653/v1/2023.findings-emnlp.248|
|1720|What Makes for Good Visual Tokenizers for Large Language Models?|Guangzhi Wang, Yixiao Ge, Xiaohan Ding, Mohan S. Kankanhalli, Ying Shan|2023-05-20|arXiv|https://github.com/TencentARC/GVT|https://doi.org/10.48550/arXiv.2305.12223|
|1721|Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate|Kai Xiong, Xiao Ding, Yixin Cao, Ting Liu, Bing Qin|2023-05-19|EMNLP|https://github.com/Waste-Wood/FORD|https://doi.org/10.18653/v1/2023.findings-emnlp.508|
|1722|HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models|Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen|2023-05-19|arXiv|https://github.com/RUCAIBox/HaluEval|https://doi.org/10.48550/arXiv.2305.11747|
|1723|LLM-Pruner: On the Structural Pruning of Large Language Models|Xinyin Ma, Gongfan Fang, Xinchao Wang|2023-05-19|NeurIPS|https://github.com/horseee/LLM-Pruner|http://papers.nips.cc/paper_files/paper/2023/hash/44956951349095f74492a5471128a7e0-Abstract-Conference.html|
|1724|X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models|Yixiong Chen, Li Liu, Chris Ding|2023-05-18|arXiv|https://github.com/Schuture/Benchmarking-Awesome-Diffusion-Models|https://doi.org/10.48550/arXiv.2305.10843|
|1725|VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks|Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu Qiao, Jifeng Dai|2023-05-18|NeurIPS|https://github.com/OpenGVLab/InternGPT|http://papers.nips.cc/paper_files/paper/2023/hash/c1f7b1ed763e9c75e4db74b49b76db5f-Abstract-Conference.html|
|1726|Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model|Siyuan Huang, Zhengkai Jiang, Hao Dong, Yu Qiao, Peng Gao, Hongsheng Li|2023-05-18|arXiv|https://github.com/OpenGVLab/Instruct2Act|https://doi.org/10.48550/arXiv.2305.11176|
|1727|SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities|Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, Xipeng Qiu|2023-05-18|EMNLP|https://0nutation.github.io/SpeechGPT.github.io/|https://doi.org/10.18653/v1/2023.findings-emnlp.1055|
|1728|Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors|Kai Zhang, Bernal Jimenez Gutierrez, Yu Su|2023-05-18|ACL|https://github.com/OSU-NLP-Group/QA4RE|https://doi.org/10.18653/v1/2023.findings-acl.50|
|1729|M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models|Chuang Liu, Renren Jin, Yuqi Ren, Linhao Yu, Tianyu Dong, Xiaohan Peng, Shuting Zhang, Jianxiang Peng, Peiyi Zhang, Qingqing Lyu, Xiaowen Su, Qun Liu, Deyi Xiong|2023-05-17|arXiv|https://github.com/tjunlp-lab/M3KE|https://doi.org/10.48550/arXiv.2305.10263|
|1730|Large Language Models are Built-in Autoregressive Search Engines|Noah Ziems, Wenhao Yu, Zhihan Zhang, Meng Jiang|2023-05-16|ACL|https://github.com/Ziems/llm-url|https://doi.org/10.18653/v1/2023.findings-acl.167|
|1731|StructGPT: A General Framework for Large Language Model to Reason over Structured Data|Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin Zhao, Ji-Rong Wen|2023-05-16|arXiv|https://github.com/RUCAIBox/StructGPT|https://doi.org/10.48550/arXiv.2305.09645|
|1732|Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics|Steve Phelps, Yvan I. Russell|2023-05-13|arXiv|https://github.com/phelps-sg/llm-cooperation|https://doi.org/10.48550/arXiv.2305.07970|
|1733|Evaluating Open-Domain Question Answering in the Era of Large Language Models|Ehsan Kamalloo, Nouha Dziri, Charles L. A. Clarke, Davood Rafiei|2023-05-11|ACL|https://github.com/ehsk/OpenQA-eval|https://doi.org/10.18653/v1/2023.acl-long.307|
|1734|Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations|Qingyu Chen, Jingcheng Du, Yan Hu, Vipina Kuttichi Keloth, Xueqing Peng, Kalpana Raja, Rui Zhang, Zhiyong Lu, Hua Xu|2023-05-10|arXiv|https://github.com/qingyu-qc/gpt_bionlp_benchmark|https://doi.org/10.48550/arXiv.2305.16326|
|1735|Multilingual LLMs are Better Cross-lingual In-context Learners with Alignment|Eshaan Tanwar, Subhabrata Dutta, Manish Borthakur, Tanmoy Chakraborty|2023-05-10|arXiv|https://github.com/EshaanT/X-InSTA|http://arxiv.org/abs/2305.05940v3|
|1736|Large Language Models Need Holistically Thought in Medical Conversational QA|Yixuan Weng, Bin Li, Fei Xia, Minjun Zhu, Bin Sun, Shizhu He, Kang Liu, Jun Zhao|2023-05-09|arXiv|https://github.com/WENGSYX/HoT|https://doi.org/10.48550/arXiv.2305.05410|
|1737|Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models|Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee, Ee-Peng Lim|2023-05-06|ACL|https://github.com/AGI-Edgerunners/Plan-and-Solve-Prompting|https://doi.org/10.18653/v1/2023.acl-long.147|
|1738|Large Language Models for Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering|Noah Hollmann, Samuel Müller, Frank Hutter|2023-05-05|NeurIPS|https://github.com/automl/CAAFE|http://papers.nips.cc/paper_files/paper/2023/hash/8c2df4c35cdbee764ebb9e9d0acd5197-Abstract-Conference.html|
|1739|CodeGen2: Lessons for Training LLMs on Programming and Natural Languages|Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, Yingbo Zhou|2023-05-03|arXiv|https://github.com/salesforce/CodeGen|http://arxiv.org/abs/2305.02309v2|
|1740|Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation|Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, Lingming Zhang|2023-05-02|NeurIPS|https://github.com/evalplus/evalplus|http://papers.nips.cc/paper_files/paper/2023/hash/43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html|
|1741|How to Unleash the Power of Large Language Models for Few-shot Relation Extraction?|Xin Xu, Yuqi Zhu, Xiaohan Wang, Ningyu Zhang|2023-05-02|SustaiNLP|https://github.com/zjunlp/DeepKE/tree/main/example/llm|https://doi.org/10.18653/v1/2023.sustainlp-1.13|
|1742|Complex Logical Reasoning over Knowledge Graphs using Large Language Models|Nurendra Choudhary, Chandan K. Reddy|2023-05-02|arXiv|https://github.com/Akirato/LLM-KG-Reasoning|https://doi.org/10.48550/arXiv.2305.01157|
|1743|Towards autonomous system: flexible modular production system enhanced with large language model agents|Yuchen Xia, Manthan Shenoy, Nasser Jazdi, Michael Weyrich|2023-04-28|2023 IEEE 28th International Conference on Emerging Technologies and Factory Automation (ETFA)|https://github.com/YuchenXia/GPT4IndustrialAutomation|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275362|
|1744|Origin Tracing and Detecting of LLMs|Linyang Li, Pengyu Wang, Ke Ren, Tianxiang Sun, Xipeng Qiu|2023-04-27|arXiv|https://github.com/OpenLMLab/|http://arxiv.org/abs/2304.14072v1|
|1745|mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality|Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qi Qian, Ji Zhang, Fei Huang, Jingren Zhou|2023-04-27|arXiv|https://github.com/X-PLUG/mPLUG-Owl|https://doi.org/10.48550/arXiv.2304.14178|
|1746|Enhancing Large Language Model with Self-Controlled Memory Framework|Bing Wang, Xinnian Liang, Jian Yang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, Zhoujun Li|2023-04-26|OpenReview|https://github.com/wbbeyourself/SCM4LLMs|http://arxiv.org/abs/2304.13343v2|
|1747|Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond|Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, Xia Hu|2023-04-26|arXiv|https://github.com/Mooler0410/LLMsPracticalGuide|http://arxiv.org/abs/2304.13712v2|
|1748|Text-to-Audio Generation using Instruction-Tuned LLM and Latent Diffusion Model|Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, Soujanya Poria|2023-04-24|OpenReview|https://github.com/declare-lab/tango|http://arxiv.org/abs/2304.13731v2|
|1749|LLM+P: Empowering Large Language Models with Optimal Planning Proficiency|Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, Peter Stone|2023-04-22|arXiv|https://github.com/Cranial-XIX/llm-pddl|https://doi.org/10.48550/arXiv.2304.11477|
|1750|Emergent and Predictable Memorization in Large Language Models|Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony, Shivanshu Purohit, Edward Raff|2023-04-21|NeurIPS|https://github.com/EleutherAI/pythia|http://papers.nips.cc/paper_files/paper/2023/hash/59404fb89d6194641c69ae99ecdf8f6d-Abstract-Conference.html|
|1751|Safety Assessment of Chinese Large Language Models|Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, Minlie Huang|2023-04-20|arXiv|https://github.com/thu-coai/Safety-Prompts|https://doi.org/10.48550/arXiv.2304.10436|
|1752|Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models|Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Jianfeng Gao|2023-04-19|NeurIPS|https://chameleon-llm.github.io|http://papers.nips.cc/paper_files/paper/2023/hash/871ed095b734818cfba48db6aeb25a62-Abstract-Conference.html|
|1753|A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models|Qianqian Xie, Zheheng Luo, Benyou Wang, Sophia Ananiadou|2023-04-18|arXiv|https://github.com/KenZLuo/Biomedical-Text-Summarization-Survey/tree/master|http://arxiv.org/abs/2304.08763v2|
|1754|Outlier Suppression+: Accurate quantization of large language models by equivalent and effective shifting and scaling|Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, Xianglong Liu|2023-04-18|EMNLP|https://github.com/ModelTC/Outlier_Suppression_Plus|https://doi.org/10.18653/v1/2023.emnlp-main.102|
|1755|OpenAGI: When LLM Meets Domain Experts|Yingqiang Ge, Wenyue Hua, Kai Mei, Jianchao Ji, Juntao Tan, Shuyuan Xu, Zelong Li, Yongfeng Zhang|2023-04-10|Advances in Neural Information Processing Systems|https://github.com/agiresearch/OpenAGI|http://arxiv.org/abs/2304.04370v6|
|1756|TagGPT: Large Language Models are Zero-shot Multimodal Taggers|Chen Li, Yixiao Ge, Jiayong Mao, Dian Li, Ying Shan|2023-04-06|arXiv|https://github.com/TencentARC/TagGPT|https://doi.org/10.48550/arXiv.2304.03022|
|1757|Document-Level Machine Translation with Large Language Models|Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian Yu, Shuming Shi, Zhaopeng Tu|2023-04-05|EMNLP|https://github.com/longyuewangdcu/Document-MT-LLM|https://doi.org/10.18653/v1/2023.emnlp-main.1036|
|1758|LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models|Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim, Lidong Bing, Xing Xu, Soujanya Poria, Roy Ka-Wei Lee|2023-04-04|EMNLP|https://github.com/AGI-Edgerunners/LLM-Adapters|https://doi.org/10.18653/v1/2023.emnlp-main.319|
|1759|Does Human Collaboration Enhance the Accuracy of Identifying LLM-Generated Deepfake Texts?|Adaku Uchendu, Jooyoung Lee, Hua Shen, Thai Le, Ting-Hao 'Kenneth' Huang, Dongwon Lee|2023-04-03|arXiv|https://github.com/huashen218/llm-deepfake-human-study|http://arxiv.org/abs/2304.01002v3|
|1760|Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling|Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O&apos;Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, Oskar van der Wal|2023-04-03|arXiv|https://github.com/EleutherAI/pythia|https://doi.org/10.48550/arXiv.2304.01373|
|1761|Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection|Maxime Labonne, Sean Moran|2023-04-03|arXiv|https://github.com/jpmorganchase/emailspamdetection|https://doi.org/10.48550/arXiv.2304.01238|
|1762|DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents|Varun Nair, Elliot Schumacher, Geoffrey J. Tso, Anitha Kannan|2023-03-30|arXiv|https://github.com/curai/curai-research/tree/main/DERA|https://doi.org/10.48550/arXiv.2303.17071|
|1763|Context-faithful Prompting for Large Language Models|Wenxuan Zhou, Sheng Zhang, Hoifung Poon, Muhao Chen|2023-03-20|EMNLP|https://github.com/wzhouad/context-faithful-llm|https://doi.org/10.18653/v1/2023.findings-emnlp.968|
|1764|How well do Large Language Models perform in Arithmetic tasks?|Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang|2023-03-16|arXiv|https://github.com/GanjinZero/math401-llm|https://doi.org/10.48550/arXiv.2304.02015|
|1765|Can ChatGPT Replace Traditional KBQA Models? An In-depth Analysis of the Question Answering Performance of the GPT LLM Family|Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, Guilin Qi|2023-03-14|The Semantic Web – ISWC 2023|https://github.com/tan92hl/Complex-Question-Answering-Evaluation-of-GPT-family|http://arxiv.org/abs/2303.07992v3|
|1766|Chat with the Environment: Interactive Multimodal Perception Using Large Language Models|Xufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, Stefan Wermter|2023-03-14|2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)|https://matcha-model.github.io/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10342363|
|1767|FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU|Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E. Gonzalez, Percy Liang, Christopher Ré, Ion Stoica, Ce Zhang|2023-03-13|ICML|https://github.com/FMInference/FlexGen|https://proceedings.mlr.press/v202/sheng23a.html|
|1768|Large Language Models Know Your Contextual Search Intent: A Prompting Framework for Conversational Search|Kelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou, Haonan Chen, Hongjin Qian|2023-03-12|EMNLP|https://github.com/kyriemao/LLM4CS/|https://doi.org/10.18653/v1/2023.findings-emnlp.86|
|1769|Planning with Large Language Models for Code Generation|Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, Chuang Gan|2023-03-09|ICLR|https://codeaimcts.github.io|https://openreview.net/pdf?id=Lr8cOOtYbfL|
|1770|LIDA: A Tool for Automatic Generation of Grammar-Agnostic Visualizations and Infographics using Large Language Models|Victor Dibia|2023-03-06|ACL|https://microsoft.github.io/lida/|https://doi.org/10.18653/v1/2023.acl-demo.11|
|1771|Guiding Large Language Models via Directional Stimulus Prompting|Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao, Xifeng Yan|2023-02-22|NeurIPS|https://github.com/Leezekun/Directional-Stimulus-Prompting|http://papers.nips.cc/paper_files/paper/2023/hash/c5601d99ed028448f29d1dae2e4a926d-Abstract-Conference.html|
|1772|MarioGPT: Open-Ended Text2Level Generation through Large Language Models|Shyam Sudhakaran, Miguel González Duque, Claire Glanois, Matthias Freiberger, Elias Najarro, Sebastian Risi|2023-02-12|arXiv|https://github.com/shyamsn97/mario-gpt|https://doi.org/10.48550/arXiv.2302.05981|
|1773|Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents|Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian Ma, Yitao Liang|2023-02-03|arXiv|https://github.com/CraftJarvis/MC-Planner|https://doi.org/10.48550/arXiv.2302.01560|
|1774|Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning|Xinyi Wang, Wanrong Zhu, Michael Saxon, Mark Steyvers, William Yang Wang|2023-01-27|NeurIPS|https://github.com/WANGXinyiLinda/concept-based-demonstration-selection|http://papers.nips.cc/paper_files/paper/2023/hash/3255a7554605a88800f4e120b3a929e1-Abstract-Conference.html|
|1775|Batch Prompting: Efficient Inference with Large Language Model APIs|Zhoujun Cheng, Jungo Kasai, Tao Yu|2023-01-19|EMNLP|https://github.com/xlang-ai/batch-prompting|https://doi.org/10.18653/v1/2023.emnlp-industry.74|
|1776|InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval|Vitor Jeronymo, Luiz Henrique Bonifacio, Hugo Queiroz Abonizio, Marzieh Fadaee, Roberto de Alencar Lotufo, Jakub Zavrel, Rodrigo Frassetto Nogueira|2023-01-04|arXiv|https://github.com/zetaalphavector/inPars/tree/master/tpu|https://doi.org/10.48550/arXiv.2301.01820|
|1777|Large Language Models as Corporate Lobbyists|John J. Nay|2023-01-03|arXiv|https://github.com/JohnNay/llm-lobbyist|https://doi.org/10.48550/arXiv.2301.01181|
|1778|SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models|Guangxuan Xiao, Ji Lin, Mickaël Seznec, Hao Wu, Julien Demouth, Song Han|2023-01-01|ICML|https://github.com/mit-han-lab/smoothquant|https://proceedings.mlr.press/v202/xiao23c.html|
|1779|Parallel Context Windows for Large Language Models|Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Inbal Magar, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, Yoav Shoham|2023-01-01|ACL|https://github.com/ai21labs/parallel-context-windows|https://doi.org/10.18653/v1/2023.acl-long.352|
|1780|PEER: Empowering Writing with Large Language Models|Kathrin Seßler, Tao Xiang, Lukas Bogenrieder, Enkelejda Kasneci|2023-01-01|EC-TEL|https://github.com/Kasneci-Lab/AI-assisted-writing|https://doi.org/10.1007/978-3-031-42682-7_73|
|1781|Navigation with Large Language Models: Semantic Guesswork as a Heuristic for Planning|Dhruv Shah, Michael Robert Equi, Blazej Osinski, Fei Xia, Brian Ichter, Sergey Levine|2023-01-01|arXiv|https://github.com/Michael-Equi/lfg-nav|https://doi.org/10.48550/arXiv.2310.10103|
|1782|Learning Video Representations from Large Language Models|Yue Zhao, Ishan Misra, Philipp Krähenbühl, Rohit Girdhar|2023-01-01|2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://facebookresearch.github.io/LaViLa|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204456|
|1783|LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models|Chan Hee Song, Brian M. Sadler, Jiaman Wu, Wei-Lun Chao, Clayton Washington, Yu Su|2023-01-01|2023 IEEE/CVF International Conference on Computer Vision (ICCV)|https://dki-lab.github.io/LLM-Planner|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10378628|
|1784|Large Language Models are few(1)-shot Table Reasoners|Wenhu Chen|2023-01-01|EACL|https://github.com/wenhuchen/TableCoT|https://doi.org/10.18653/v1/2023.findings-eacl.83|
|1785|Large Language Models are Better Reasoners with Self-Verification|Yixuan Weng, Minjun Zhu, Fei Xia, Bin Li, Shizhu He, Shengping Liu, Bin Sun, Kang Liu, Jun Zhao|2023-01-01|EMNLP|https://github.com/WENGSYX/Self-Verification|https://doi.org/10.18653/v1/2023.findings-emnlp.167|
|1786|From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models|Jiaxian Guo, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Boyang Li, Dacheng Tao, Steven C. H. Hoi|2023-01-01|2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204235|
|1787|I2MVFormer: Large Language Model Generated Multi-View Document Supervision for Zero-Shot Image Classification|Muhammad Ferjad Naeem, Muhammad Gul Zain Ali Khan, Yongqin Xian, Muhammad Zeshan Afzal, Didier Stricker, Luc Van Gool, Federico Tombari|2023-01-01|2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/ferjad/I2DFormer|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10204007|
|1788|Explainable Integration of Knowledge Graphs Using Large Language Models|Abdullah Fathi Ahmed, Asep Fajar Firmansyah, Mohamed Ahmed Sherif, Diego Moussallem, Axel-Cyrille Ngonga Ngomo|2023-01-01|NLDB|https://github.com/dice-group/NMV-LS|https://doi.org/10.1007/978-3-031-35320-8_9|
|1789|Evaluating the Factual Consistency of Large Language Models Through News Summarization|Derek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah Kwan, Mohit Bansal, Colin Raffel|2023-01-01|ACL|https://github.com/r-three/fib|https://doi.org/10.18653/v1/2023.findings-acl.322|
|1790|CXR-CLIP: Toward Large Scale Chest X-ray Language-Image Pre-training|Kihyun You, Jawook Gu, Jiyeon Ham, Beomhee Park, Jiho Kim, Eun Kyoung Hong, Woonhyuk Baek, Byungseok Roh|2023-01-01|MICCAI|https://github.com/kakaobrain/cxr-clip|https://doi.org/10.1007/978-3-031-43895-0_10|
|1791|Action-GPT: Leveraging Large-scale Language Models for Improved and Generalized Action Generation|Sai Shashank Kalakonda, Shubh Maheshwari, Ravi Kiran Sarvadevabhatla|2023-01-01|2023 IEEE International Conference on Multimedia and Expo (ICME)|https://actiongpt.github.io|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10219779|
|1792|A Comparative Evaluation on Melody Generation of Large Language Models|K. Suzuki, J. Cai, J. Li, T. Yamauchi, K. Tei|2023-01-01|2023 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)|https://github.com/545659928/LLMMelody|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10326362|
|1793|GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models|Archiki Prasad, Peter Hase, Xiang Zhou, Mohit Bansal|2023-01-01|EACL|https://github.com/archiki/GrIPS|https://doi.org/10.18653/v1/2023.eacl-main.277|
|1794|Open-vocabulary Queryable Scene Representations for Real World Planning|B. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrishnan, M. S. Ryoo, A. Stone, D. Kappler|2023|2023 IEEE International Conference on Robotics and Automation (ICRA)|https://nlmap-saycan.github.io|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10161534|
|1795|ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction|J. He, L. Wang, Y. Hu, N. Liu, H. Liu, X. Xu, H. T. Shen|2023|2023 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/MAEHCM/ICL-D3IE|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10377921|
|1796|Language-enhanced RNR-Map: Querying Renderable Neural Radiance Field maps with natural language|F. Taioli, F. Cunico, F. Girella, R. Bologna, A. Farinelli, M. Cristani|2023|2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)|https://intelligolabs.github.io/Le-RNR-Map/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350426|
|1797|Learning Approximate Execution Semantics From Traces for Binary Function Similarity|K. Pei, Z. Xuan, J. Yang, S. Jana, B. Ray|2023|IEEE Transactions on Software Engineering|https://github.com/CUMLSec/trex|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002189|
|1798|Looking at words and points with attention: a benchmark for text-to-shape coherence|Andrea Amaduzzi, Giuseppe Lisanti, Samuele Salti, Luigi di Stefano|2023|2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)|https://cvlab-unibo.github.io/CrossCoherence-Web/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350574|
|1799|MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge|W. Lin, L. Karlinsky, N. Shvetsova, H. Possegger, M. Kozinski, R. Panda, R. Feris, H. Kuehne, H. Bischof|2023|2023 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/wlin-at/MAXI|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10377160|
|1800|March in Chat: Interactive Prompting for Remote Embodied Referring Expression|Y. Qiao, Y. Qi, Z. Yu, J. Liu, Q. Wu|2023|2023 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/YanyuanQiao/MiC|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10377261|
|1801|VAFOR: Proactive Voice Assistant for Object Retrieval in the Physical World|B. Satyev, H. Ahn|2023|2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)|https://github.com/bekatan/vafor|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10309466|
|1802|Sparse-DySta: Sparsity-Aware Dynamic and Static Scheduling for Sparse Multi-DNN Workloads|H. Fan, S. I. Venieris, A. Kouris, N. D. Lane|2023|2023 56th IEEE/ACM International Symposium on Microarchitecture (MICRO)|https://github.com/SamsungLabs/Sparse-Multi-DNN-Scheduling.CCS|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10411414|
|1803|Transferable Decoding with Visual Entities for Zero-Shot Image Captioning|J. Fei, T. Wang, J. Zhang, Z. He, C. Wang, F. Zheng|2023|2023 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/FeiElysia/ViECap|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10378259|
|1804|Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation|R. Chen, Y. Chen, N. Jiao, K. Jia|2023|2023 IEEE/CVF International Conference on Computer Vision (ICCV)|https://fantasia3d.github.io/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10377067|
|1805|Visual Language Maps for Robot Navigation|C. Huang, O. Mees, A. Zeng, W. Burgard|2023|2023 IEEE International Conference on Robotics and Automation (ICRA)|https://vlmaps.github.io|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160969|
|1806|Waffling around for Performance: Visual Classification with Random Words and Broad Concepts|K. Roth, J. M. Kim, A. Sophia Koepke, O. Vinyals, C. Schmid, Z. Akata|2023|2023 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/ExplainableML/WaffleCLIP|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10378352|
|1807|What does a platypus look like? Generating customized prompts for zero-shot image classification|S. Pratt, I. Covert, R. Liu, A. Farhadi|2023|2023 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/sarahpratt/CuPL|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10378461|
|1808|eP-ALM: Efficient Perceptual Augmentation of Language Models|M. Shukor, C. Dancette, M. Cord|2023|2023 IEEE/CVF International Conference on Computer Vision (ICCV)|https://github.com/mshukor/eP-ALM|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10377462|
|1809|Generate rather than Retrieve: Large Language Models are Strong Context Generators|Wenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu, Mingxuan Ju, Soumya Sanyal, Chenguang Zhu, Michael Zeng, Meng Jiang|2023|ICLR|https://github.com/wyu97/GenRead|https://openreview.net/pdf?id=fB0hRu9GZUS|
|1810|Generating Executable Action Plans with Environmentally-Aware Language Models|M. Gramopadhye, D. Szafir|2023|2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)|https://github.com/hri-ironlab/scene_aware_language_planner|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10341989|
|1811|FactLLaMA: Optimizing Instruction-Following Language Models with External Knowledge for Automated Fact-Checking|T. -H. Cheung, K. -M. Lam|2023|2023 Asia Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)|https://thcheung.github.io/factllama|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10317251|
|1812|ChatGPT Empowered Long-Step Robot Control in Various Environments: A Case Application|N. Wake, A. Kanehira, K. Sasabuchi, J. Takamatsu, K. Ikeuchi|2023|IEEE Access|https://github.com/microsoft/ChatGPT-Robot-Manipulation-Prompts|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10235949|
|1813|A Decoupling Paradigm With Prompt Learning for Remote Sensing Image Change Captioning|C. Liu, R. Zhao, J. Chen, Z. Qi, Z. Zou, Z. Shi|2023|IEEE Transactions on Geoscience and Remote Sensing|https://github.com/Chen-Yang-Liu/PromptCC|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10271701|
|1814|AIEPU at ALQAC 2023: Deep Learning Methods for Legal Information Retrieval and Question Answering|L. Hoang, T. Bui, C. Nguyen, L. -M. Nguyen|2023|2023 15th International Conference on Knowledge and Systems Engineering (KSE)|https://github.com/DucLong06/Legal-Prompts|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10299426|
|1815|Extraction of Medication and Temporal Relation from Clinical Text using Neural Language Models|H. Tu, L. Han, G. Nenadic|2023|2023 IEEE International Conference on Big Data (BigData)|https://github.com/HECTA-UoM/MedTem|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386489|
|1816|Automatic Chain of Thought Prompting in Large Language Models|Zhuosheng Zhang, Aston Zhang, Mu Li, Alex Smola|2023|ICLR|https://github.com/amazon-research/auto-cot|https://openreview.net/pdf?id=5NTt8GFjUHkr|
|1817|Beyond Lexical Consistency: Preserving Semantic Consistency for Program Translation|Y. Du, Y. -F. Ma, Z. Xie, M. Li|2023|2023 IEEE International Conference on Data Mining (ICDM)|https://github.com/duyali2000/PSCPT|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10415807|
|1818|CLIPGraphs: Multimodal Graph Networks to Infer Object-Room Affinities|A. Agrawal, R. Arora, A. Datta, S. Banerjee, B. Bhowmick, K. M. Jatavallabhula, M. Sridharan, M. Krishna|2023|2023 32nd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)|https://clipgraphs.github.io|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10309325|
|1819|COMEX: A Tool for Generating Customized Source Code Representations|D. Das, N. S. Mathews, A. Mathai, S. Tamilselvam, K. Sedamaki, S. Chimalakonda, A. Kumar|2023|2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)|https://github.com/IBM/tree-sitter-codeviews|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298568|
|1820|Are We Ready to Embrace Generative AI for Software Q&A?|B. Xu, T. -D. Nguyen, T. Le-Cong, T. Hoang, J. Liu, K. Kim, C. Gong, C. Niu, C. Wang, B. Le, D. Lo|2023|2023 38th IEEE/ACM International Conference on Automated Software Engineering (ASE)|https://github.com/maxxbw54/GAI4SQA|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10298467|
|1821|ChatGPT for Vulnerability Detection, Classification, and Repair: How Far Are We?|M. Fu, C. K. Tantithamthavorn, V. Nguyen, T. Le|2023|2023 30th Asia-Pacific Software Engineering Conference (APSEC)|https://github.com/awsm-research/ChatGPT4Vul|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10479409|
|1822|ChatGraph: Interpretable Text Classification by Converting ChatGPT Knowledge to Graphs|Y. Shi, H. Ma, W. Zhong, Q. Tan, G. Mai, X. Li, T. Liu, J. Huang|2023|2023 IEEE International Conference on Data Mining Workshops (ICDMW)|https://github.com/sycny/ChatGraph|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10411620|
|1823|Code as Policies: Language Model Programs for Embodied Control|Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, brian ichter, Pete Florence, Andy Zeng|2023|2023 IEEE International Conference on Robotics and Automation (ICRA)|https://code-as-policies.github.io|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10160591|
|1824|CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis|Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, Caiming Xiong|2023|ICLR|https://github.com/salesforce/CodeGen|https://openreview.net/pdf?id=iaYcJKpY2B_|
|1825|DS-Fusion: Artistic Typography via Discriminated and Stylized Diffusion|M. Tanveer, Y. Wang, A. Mahdavi-Amiri, H. Zhang|2023|2023 IEEE/CVF International Conference on Computer Vision (ICCV)|https://ds-fusion.github.io/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10376835|
|1826|DiviML: A Module-based Heuristic for Mapping Neural Networks onto Heterogeneous Platforms|Y. Ghannane, M. S. Abdelfattah|2023|2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)|https://github.com/abdelfattah-lab/diviml|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10323712|
|1827|Exploring the Value of Pre-trained Language Models for Clinical Named Entity Recognition|S. Belkadi, L. Han, Y. Wu, G. Nenadic|2023|2023 IEEE International Conference on Big Data (BigData)|https://github.com/HECTA-UoM/TransformerCRF|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10386154|
|1828|Elixir: Train a Large Language Model on a Small GPU Cluster|Haichen Huang, Jiarui Fang, Hongxin Liu, Shenggui Li, Yang You|2022-12-10|arXiv|https://github.com/hpcaitech/ColossalAI/tree/feature/elixir|https://doi.org/10.48550/arXiv.2212.05339|
|1829|Large Language Models are Pretty Good Zero-Shot Video Game Bug Detectors|Mohammad Reza Taesiri, Finlay Macklon, Yihe Wang, Hengshuo Shen, Cor-Paul Bezemer|2022-10-05|arXiv|https://asgaardlab.github.io/LLMxBugs|https://doi.org/10.48550/arXiv.2210.02506|
|1830|Induced Natural Language Rationales and Interleaved Markup Tokens Enable Extrapolation in Large Language Models|Mirelle Bueno, Carlos Gemmel, Jeffrey Dalton, Roberto de Alencar Lotufo, Rodrigo Frassetto Nogueira|2022-08-24|arXiv|https://github.com/MirelleB/induced-rationales-markup-tokens|https://doi.org/10.48550/arXiv.2208.11445|
|1831|A systematic evaluation of large language models of code|Frank F. Xu, Uri Alon, Graham Neubig, Vincent Josua Hellendoorn|2022-06|MAPS 2022: Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming|https://github.com/VHellendoorn/Code-LMs|https://dl.acm.org/doi/10.1145/3520312.3534862|
|1832|Large Language Models are Zero-Shot Reasoners|Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, Yusuke Iwasawa|2022-05-24|NeurIPS|https://github.com/kojima-takeshi188/zero_shot_cot|http://papers.nips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html|
|1833|Few-shot Task-agnostic Neural Architecture Search for Distilling Large Language Models|Dongkuan Xu, Subhabrata Mukherjee, Xiaodong Liu, Debadeepta Dey, Wenhui Wang, Xiang Zhang, Ahmed Hassan Awadallah, Jianfeng Gao|2022-01-01|NeurIPS|https://github.com/microsoft/autodistil|http://papers.nips.cc/paper_files/paper/2022/hash/b7c12689a89e98a61bcaa65285a41b7c-Abstract-Conference.html|
|1834|Large Language Models Can Be Strong Differentially Private Learners|Xuechen Li, Florian Tramèr, Percy Liang, Tatsunori Hashimoto|2022-01-01|ICLR|https://github.com/lxuechen/private-transformers|https://openreview.net/forum?id=bVuP3ltATMz|
|1835|LoRA: Low-Rank Adaptation of Large Language Models|Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen|2022-01-01|ICLR|https://github.com/microsoft/LoRA|https://openreview.net/forum?id=nZeVKeeFYf9|
|1836|Reshaping Robot Trajectories Using Natural Language Commands: A Study of Multi-Modal Data Alignment Using Transformers|A. Bucker, L. Figueredo, S. Haddadinl, A. Kapoor, S. Ma, R. Bonatti|2022|2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)|https://arthurfenderbucker.github.io/NL_trajectory_reshaper/|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9981810|
|1837|ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic|Y. Tewel, Y. Shalev, I. Schwartz, L. Wolf|2022|2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)|https://github.com/YoadTew/zero-shot-image-to-text|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9878503|
|1838|LLM: Learning Cross-Modality Person Re-Identification via Low-Rank Local Matching|Y. Feng, J. Xu, Y. -m. Ji, F. Wu|2021-01-01|IEEE Signal Processing Letters|https://github.com/fegnyujian/LLM|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521771|
|1839|DSI-Net: Deep Synergistic Interaction Network for Joint Classification and Segmentation With Endoscope Images|M. Zhu, Z. Chen, Y. Yuan|2021|IEEE Transactions on Medical Imaging|https://github.com/CityU-AIM-Group/DSI-Net|https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9440441|
|1840|$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference|Benfeng Xu, Quan Wang, Zhendong Mao, Yajuan Lyu, Qiaoqiao She, Yongdong Zhang|nan|ICLR 2023 poster|https://github.com/BenfengXu/KNNPrompting|https://openreview.net/pdf/feaebae1ecc4dd15ee54a25f37db2412f7e62789.pdf|
|1841|ALaRM: Align Language Models via Hierarchical Rewards Modeling|Anonymous|nan|OpenReview|https://ALaRM-anonymized.github.io|https://openreview.net/pdf/8242caa074ae30e7605ca0b8d9a275f5f5c1aa50.pdf|
|1842|Ability Boundary of Data in LLMs' Math Reasoning|Anonymous|nan|OpenReview|https://github.com/Anonymous|https://openreview.net/pdf/1342b121d792c55a260962fdf89c3a945c0b339c.pdf|
|1843|Ask Me Anything: A simple strategy for prompting language models|Simran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, Christopher Re|nan|ICLR 2023 notable top 25%|https://github.com/HazyResearch/ama_prompting|https://openreview.net/pdf/5b1bcdac167fa4b294480f303ac3722afa8a9aac.pdf|
|1844|Authorship Style Transfer with Inverse Transfer Data Augmentation|Anonymous|nan|OpenReview|https://github.com/AnonymousRole/ITDA|https://openreview.net/pdf/7285ef94ef19db8222a532cbdb061aa7b8a5d873.pdf|
|1845|BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives|Anonymous|nan|OpenReview|https://github.com/BIRCO-benchmark/BIRCO|https://openreview.net/pdf/1b3834d872ef03da3f243e22672cb01cfe4af930.pdf|
|1846|Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models|Hannah Rose Kirk, Yennie Jun, Filippo Volpin, Haider Iqbal, Elias Benussi, Frederic A Dreyer, Aleksandar Shtedritski, Yuki Asano|nan|NeurIPS 2021 Poster|https://github.com/oxai/intersectional_gpt2|https://openreview.net/pdf/440fa4da3ab1b67f9e9235c5097452bf825390b2.pdf|
|1847|Can Large Language Models Distinguish Cause from Effect?|Zhiheng LYU, Zhijing Jin, Rada Mihalcea, Mrinmaya Sachan, Bernhard Schölkopf|nan|CRL@UAI 2022 Poster|https://github.com/cogito233/llm-bivariate-causal-discovery|https://openreview.net/pdf/d4186fc709159c86405b2bead85ad324e86294d9.pdf|
|1848|Can We Edit Factual Knowledge by In-Context Learning?|Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, Baobao Chang|nan|OpenReview|https://github.com/Zce1112zslx/IKE|https://arxiv.org/pdf/2305.12740.pdf|
|1849|CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending|Anonymous|nan|OpenReview|https://github.com/****/****|https://openreview.net/pdf/e1c845cd990b7c2d5e286c855eb3e506507f24d6.pdf|
|1850|CodeTransOcean: A Comprehensive Multilingual Benchmark for Code Translation|Wen Wang|nan|OpenReview|https://github.com/WeixiangYAN/CodeTransOcean|https://openreview.net/pdf?id=hv3VpXDIh8|
|1851|DNA-GPT|Wei Cheng|nan|OpenReview|https://github.com/Xianjun-Yang/DNA-GPT|https://arxiv.org/pdf/2305.17359.pdf|
|1852|DNA-GPT: DIVERGENT N-GRAM ANALYSIS FOR TRAINING-FREE DETECTION OF GPT-GENERATED TEXT|Xianjun Yang|nan|OpenReview|https://github.com/Xianjun-Yang/DNA-GPT|https://arxiv.org/pdf/2305.17359.pdf|
|1853|DeCap: Decoding CLIP Latents for Zero-Shot Captioning via Text-Only Training|Wei Li, Linchao Zhu, Longyin Wen, Yi Yang|nan|ICLR 2023 poster|https://github.com/dhg-wei/DeCap|https://openreview.net/pdf/20281fe81003b21131076887ded62556d1c2dc19.pdf|
|1854|Decomposed Prompting: A Modular Approach for Solving Complex Tasks|Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, Ashish Sabharwal|nan|ICLR 2023 poster|https://github.com/allenai/DecomP|https://openreview.net/pdf/c0a9bac140b181384176f474f3533e053b8d663d.pdf|
|1855|DiLM: Distilling Dataset into Language Model for Text-level Dataset Distillation|Anonymous|nan|OpenReview|https://github.com/|https://openreview.net/pdf/b66e32f763616cf77ef48ca417ce1101ea7934a1.pdf|
|1856|Directing the violence or admonishing it?  A survey of contronymy and androcentrism in Google Translate and some recommendations|Anonymous|nan|OpenReview|https://github.com/rteehas/GT_study_recommendations|https://openreview.net/pdf/30ebaac518dfea9d2b890133ab9877ed88460708.pdf|
|1857|Do As I Can, Not As I Say: Grounding Language in Robotic Affordances|brian ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov, Sergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander T Toshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown, Michael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek Rettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng Kuang, Sally Jesmonth, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine Hsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, Chuyuan Kelly Fu|nan|OpenReview|https://github.com/google-research/google-research/blob/master/saycan/SayCan-Robot-Pick-Place.ipynb|https://arxiv.org/pdf/2204.01691.pdf|
|1858|Energy-based Models are Zero-Shot Planners for Compositional Scene Rearrangement|Nikolaos Gkanatsios, Ayush Jain, Zhou Xian, Yunchu Zhang, Christopher G Atkeson, Katerina Fragkiadaki|nan|RSS-23 LTAMP Poster|https://ebmplanner.github.io|https://openreview.net/pdf/b1f6170bbe5a4558b2805e4d264a4063d8492cf1.pdf|
|1859|Grounding Complex Natural Language Commands for Temporal Tasks in Unseen Environments|Jason Xinyu Liu, Ziyi Yang, Ifrah Idrees, Sam Liang, Benjamin Schornstein, Stefanie Tellex, Ankit Shah|nan|CoRL 2023 Poster|https://github.com/h2r/Lang2LTL|https://openreview.net/pdf/c32e1b4b650074c3bbf27c40d09f44d328340713.pdf|
|1860|Hash Layers For Large Sparse Models|Stephen Roller, Sainbayar Sukhbaatar, Arthur Szlam, Jason E Weston|nan|NeurIPS 2021 Spotlight|https://github.com/facebookresearch/ParlAI/tree/main/projects/params_vs_compute|https://openreview.net/pdf/c722df087549e62aa8f20733964d2fe2a2c94c6e.pdf|
|1861|Ignore Previous Prompt: Attack Techniques For Language Models|Fábio Perez, Ian Ribeiro|nan|MLSW2022|https://github.com/agencyenterprise/PromptInject|https://openreview.net/pdf/2abb688b382bcf0e2b98ae845717e6c4462763de.pdf|
|1862|KERPLE: Kernelized Relative Positional Embedding for Length Extrapolation|Ta-Chung Chi, Ting-Han Fan, Peter Ramadge, Alexander Rudnicky|nan|NeurIPS 2022 Accept|https://github.com/chijames/KERPLE|https://openreview.net/pdf/8454975f969fcabd1ebee718881010a21c621f8a.pdf|
|1863|Language Conditioned Traffic Generation|Shuhan Tan, Boris Ivanovic, Xinshuo Weng, Marco Pavone, Philipp Kraehenbuehl|nan|CoRL 2023 Poster|https://github.com/Ariostgx/lctgen|https://openreview.net/pdf/34c183e547c1870a787c9b43b4bcf7ea496f0281.pdf|
|1864|Learning to repair: Repairing model output errors after deployment using a dynamic memory of feedback|Anonymous|nan|OpenReview|https://github.com/allenai/interscript|https://openreview.net/pdf/8f7b14769c87da84d2a6003e1412035ef3018576.pdf|
|1865|Lifelike-Writer: Authorship Style Transfer with Inverse Knowledge Distillation|Anonymous|nan|OpenReview|https://github.com/AnonymousRole/Lifelike-Writer|https://openreview.net/pdf/9d11d05966cc7aba2954b870947bd3264c198af2.pdf|
|1866|M2T2: Multi-Task Masked Transformer for Object-centric Pick and Place|Wentao Yuan, Adithyavairavan Murali, Arsalan Mousavian, Dieter Fox|nan|CoRL 2023 Poster|https://m2-t2.github.io|https://openreview.net/pdf/9bd269a763bab649c7d23e3feecff9ba16982185.pdf|
|1867|MARIO: MAth Reasoning with code Interpreter Output - A Reproducible Pipeline|Minpeng Liao, Wei Luo, Chengxi Li, Wu Jing, Kai Fan|nan|OpenReview|https://github.com/MARIO-Math-Reasoning/MARIO|https://openreview.net/pdf/d1120bb65ec677a3f29d21512e96ac3ec1281657.pdf|
|1868|MEDIMP: 3D Medical Images with clinical Prompts from limited tabular data for renal transplantation|Leo Milecki, Vicky Kalogeiton, Sylvain Bodard, Dany Anglicheau, Jean-Michel Correas, Marc-Olivier Timsit, Maria Vakalopoulou|nan|MIDL 2023 Poster|https://github.com/leomlck/MEDIMP|https://openreview.net/pdf/71110e2a3119266431c8ed18a9d36dc6fa93324e.pdf|
|1869|Multi-LexSum: Real-world Summaries of Civil Rights Lawsuits at Multiple Granularities|Zejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg, Margo Schlanger, Doug Downey|nan|NeurIPS 2022 Datasets and Benchmarks |https://multilexsum.github.io|https://openreview.net/pdf/46112dbc53b937542a73cb14d67d1eb3f2fd22c7.pdf|
|1870|Multitask Prompted Training Enables Zero-Shot Task Generalization|Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, Alexander M Rush|nan|ICLR 2022 Spotlight|https://github.com/bigscience-workshop/t-zero|https://openreview.net/pdf/bec425b93713482f8e2de5d1d15b66ff95a47026.pdf|
|1871|On Fake News Detection with LLM Enhanced Semantics Mining|Anonymous|nan|OpenReview|https://github.com/LEG4FD/LEG4FD|https://openreview.net/pdf/ee5cbff327c79c172ad3b32ba4e6aaf163010f2f.pdf|
|1872|Question Translation Training for Better Multilingual Reasoning|Wenhao Zhu, Shujian Huang, Fei Yuan, Shuaijie She, Jiajun Chen, Alexandra Birch|nan|OpenReview|https://github.com/NJUNLP/QAlign|https://openreview.net/pdf/c234d11fa2acd80c339c63f233c0b64bdc7d7379.pdf|
|1873|RAINPOEM: Retrieval-augmented In-context knowledge post editing on massive-editing memory in a transformer|Anonymous|nan|OpenReview|https://github.com/XXX/XXX|https://openreview.net/pdf/e905e649640af957a0f3c955fa17ecf7af1a7243.pdf|
|1874|REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction|Zeyi Liu, Arpit Bahety, Shuran Song|nan|CoRL 2023 Poster|https://github.com/real-stanford/reflect|https://openreview.net/pdf/55b4404e7bac9f0d8a2b022f42f708f2921aef16.pdf|
|1875|Recitation-Augmented Language Models|Zhiqing Sun, Xuezhi Wang, Yi Tay, Yiming Yang, Denny Zhou|nan|ICLR 2023 poster|https://github.com/Edward-Sun/RECITE|https://openreview.net/pdf/693f49dd101c5c13e74972b49546fdff73d91ac4.pdf|
|1876|SLAP: Spatial-Language Attention Policies|Priyam Parashar, Vidhi Jain, Xiaohan Zhang, Jay Vakil, Sam Powers, Yonatan Bisk, Chris Paxton|nan|CoRL 2023 Poster|https://robotslap.github.io|https://openreview.net/pdf/661a22a879071769365a7ab84fc127d65c7809e8.pdf|
|1877|SayTap: Language to Quadrupedal Locomotion|Yujin Tang, Wenhao Yu, Jie Tan, Heiga Zen, Aleksandra Faust, Tatsuya Harada|nan|CoRL 2023 Poster|https://saytap.github.io|https://openreview.net/pdf/8165ee2ae940d29cf4608613ce6b3c6fd1d2c189.pdf|
|1878|Scaling Up and Distilling Down: Language-Guided Robot Skill Acquisition|Huy Ha, Pete Florence, Shuran Song|nan|CoRL 2023 Poster|https://github.com/real-stanford/scalingup|https://openreview.net/pdf/fc651c57b63f8554c325df33fa6f7f830d0d84d8.pdf|
|1879|Sniffer: Origin Tracing and Detecting of LLMs|Anonymous|nan|OpenReview|https://github.com//|https://openreview.net/pdf/7c03ff5692a3e33a751eb2b85a7f8d4788fd439e.pdf|
|1880|SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems|Anonymous|nan|OpenReview|https://speechagents.github.io/|https://openreview.net/pdf/02895cfa33b2425a64c72980658dc7c4c6347bfb.pdf|
|1881|TETA: Temporal-Enhanced Text-to-Audio Generation|Anonymous|nan|OpenReview|https://teta2023.github.io|https://openreview.net/pdf/83211a5457300f447e41005ca148d8ab282a9142.pdf|
|1882|Tell Me Where to Go: A Composable Framework for Context-Aware Embodied Robot Navigation|Harel Biggie, Ajay Narasimha Mopidevi, Dusty Woods, Chris Heckman|nan|CoRL 2023 Poster|https://github.com/arpg/navcon|https://openreview.net/pdf/51b20f4fa9c80f50a198b7aa959a31d11546a69d.pdf|
|1883|Think Big, Teach Small: Do Language Models Distil Occam’s Razor?|Gonzalo Jaimovitch-Lopez, David Castellano Falcón, Cesar Ferri, Jose Hernandez-Orallo|nan|NeurIPS 2021 Poster|https://github.com/gonzalojaimovitch/think-big-teach-small|https://openreview.net/pdf/555b549c0543b9df9125693f03dca14c9f00d1f0.pdf|
|1884|VoiceTuner: Self-Supervised Pre-training and Efficient Fine-tuing For Voice Generation|Anonymous|nan|OpenReview|https://VoiceTuner.github.io|https://openreview.net/pdf/f6b8f197f69f9a3cd55451216451e2a57ab28289.pdf|
|1885|VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models|Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, Li Fei-Fei|nan|CoRL 2023 Oral|https://github.com/huangwl18/VoxPoser|https://openreview.net/pdf/da53725fbe2f24020fa62595c807dc3ebdac6888.pdf|
|1886|When Does Differentially Private Learning Not Suffer in High Dimensions?|Xuechen Li, Daogao Liu, Tatsunori Hashimoto, Huseyin A Inan, Janardhan Kulkarni, YinTat Lee, Abhradeep Guha Thakurta|nan|NeurIPS 2022 Accept|https://github.com/lxuechen/private-transformers/tree/main/examples/classification/spectral_analysis|https://openreview.net/pdf/2f3937729f5e56e328788ae652b67a4052e22982.pdf|
|1887|When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment|Zhijing Jin, Sydney Levine, Fernando Gonzalez Adauto, Ojasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada Mihalcea, Joshua B. Tenenbaum, Bernhard Schölkopf|nan|NeurIPS 2022 Accept|https://github.com/feradauto/MoralCoT|https://openreview.net/pdf/d334b60faf45e0e2c49d08b19c18df1b363e3786.pdf|
