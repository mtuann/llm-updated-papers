# Large Language Models Papers

Updated list of Large Language Models papers as of **January 08, 2026**. 

## Quick Access
üîç **[Interactive Search & Browse](https://mtuann.github.io/papers/)** - Filter, search, and explore all papers with an intuitive interface

## Overview
- **Coverage**: Papers from 2016 to present
- **Sources**: arXiv, NeurIPS, ICML, ICLR, ACL, EMNLP, AAAI, IJCAI, KDD, CVPR, ICCV, ECCV, IEEE, ACM, Springer, ScienceDirect, Nature, and other top AI/ML venues
- **Updates**: Automated collection of new publications
- **Features**: Advanced search, code availability tracking, and multi-venue coverage

## Related Topics
- **[Large Language Models](https://github.com/mtuann/llm-updated-papers)** | **[Federated Learning](https://github.com/mtuann/federated-learning-updated-papers)** | **[Backdoor Learning](https://github.com/mtuann/backdoor-ai-resources)** | **[Machine Unlearning](https://github.com/mtuann/machine-unlearning-papers)**
- **[Serverless Computing](https://mtuann.github.io/papers/)** | **[Multi-Modal Learning](https://mtuann.github.io/papers/)**

## Large Language Models Papers with Code
This section lists papers with available code (sorted by publication date). For the complete paper list, visit the [Research Papers Page](https://mtuann.github.io/papers/).

---

## Support
If you find this resource helpful, consider supporting its development:

- **Ko-fi** (PayPal/Card): [ko-fi.com/miutheladycat](https://ko-fi.com/miutheladycat)
- **Techcombank** (Vietnam): 5877 5555 55 (Nguyen Thi Lan Phuong)

---

*This repository is regularly updated. For the latest data, visit the [Research Papers Page](https://mtuann.github.io/papers/).*


|No.|Title|Authors|Publish Date|Venue|Code|
|---|---|---|---|---|---|
|1|[QWED Protocol: Deterministic Verification for Large Language Models](https://github.com/QWED-AI/qwed-verification)|Rahul Dass|2026-01-04|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/QWED-AI/qwed-verification)](https://github.com/QWED-AI/qwed-verification)|
|2|[Survey on Factuality in Large Language Models](https://doi.org/10.1145/3742420)|Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Qipeng Guo, Xiangkun Hu, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao...|2026-01-01|ACM Computing Surveys|[![Star](https://img.shields.io/github/stars/wangcunxiang/LLM-Factuality-Survey)](https://github.com/wangcunxiang/LLM-Factuality-Survey)|
|3|[HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors](https://doi.org/10.48550/arxiv.2512.24478)|Hyunjun Kim|2025-12-30|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hyunjun1121/holograph)](https://github.com/hyunjun1121/holograph)|
|4|[Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles](https://doi.org/10.48550/arxiv.2512.20324)|Nurul Labib Sayeedi, Md. Faiyaz Abdullah Sayeedi, Khushnur Binte Jahangir, Swakkhar Shatabda, Sarah Masud Preum|2025-12-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Labib1610/BanglaRiddleEval)](https://github.com/Labib1610/BanglaRiddleEval)|
|5|[Toward Explaining Large Language Models in Software Engineering Tasks](https://doi.org/10.48550/arxiv.2512.20328)|Antonio Vitale, Khai-Nguyen Nguyen, Denys Poshyvanyk, Rocco Oliveto, Simone Scalabrino, Antonio Mastropaolo|2025-12-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/deviserlab/FeatureSHAP)](https://github.com/deviserlab/FeatureSHAP)|
|6|[Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation](https://doi.org/10.48550/arxiv.2512.19512)|Ziyang Song, Zelin Zang, Zuyao Chen, Xusheng Liang, Dong Hoon Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/tomato996/Anatomy-R1)](https://github.com/tomato996/Anatomy-R1)|
|7|[EXa-LM: A Controlled Natural Language Bridge between Large Language Models and First-Order Logic Solvers](https://doi.org/10.20944/preprints202512.1848.v1)|Frydman, Francis|2025-12-22|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/FFrydman/eXa-LM)](https://github.com/FFrydman/eXa-LM)|
|8|[PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2512.19350)|A. B. M. Ashikur Rahman, Saeed Anwar, Muhammad Usman, Irfan Ahmad, Ajmal Mian|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ashikiut/pendulum)](https://github.com/ashikiut/pendulum)|
|9|[When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models](https://doi.org/10.48550/arxiv.2512.18934)|Michael Zhang, Rishi A. Ruia, Arnav Kewalram, Saathvik Dharmapuram, Utkarsh Sharma, Kevin Zhu|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Festyve/LessIsMore)](https://github.com/Festyve/LessIsMore)|
|10|[dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models](https://doi.org/10.48550/arxiv.2512.19433)|Yi Xin, Siqi Luo, Qi Qin, Haoxing Chen, Kaiwen Zhu, Zhiwei Zhang, Yangfan He, Rongchao Zhang, Jinbin Bai, Shuo Cao, Bin ...|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Alpha-VLLM/Lumina-DiMOO)](https://github.com/Alpha-VLLM/Lumina-DiMOO)|
|11|[CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis](https://doi.org/10.48550/arxiv.2512.18878)|Kaidi Liang, Ke Li, Xianbiao Hu, Ruwen Qin|2025-12-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Liangkd/CrashChat)](https://github.com/Liangkd/CrashChat)|
|12|[Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://doi.org/10.48550/arXiv.2508.09323)|Nan Miles Xi, Yu Deng, Lin Wang|2025-12-21|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/isegura/NLP4RARE-CM-UC3M)](https://github.com/isegura/NLP4RARE-CM-UC3M)|
|13|[Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models](https://doi.org/10.48550/arxiv.2512.19758)|Wang Bin, Ao Yang, Kedan Li, Liu Aofan, Hui Li, Guibo Luo, Weixiang Huang, Yan Zhuang|2025-12-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/TheBinKing/Attention)](https://github.com/TheBinKing/Attention)|
|14|[Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models](https://doi.org/10.48550/arxiv.2512.15973)|Erden, Caner|2025-12-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/canererden/DR_RL_Project)](https://github.com/canererden/DR_RL_Project)|
|15|[SKiM-GPT: combining biomedical literature-based discovery with large language model hypothesis evaluation](https://doi.org/10.1186/s12859-025-06350-7)|Jack Freeman, Robert J. Millikin, Leo Xu, Ishaan Sharma, Bethany Moore, Cannon Lock, Kevin Shine George, Aviral Bal, Chi...|2025-12-17|BMC Bioinformatics|[![Star](https://img.shields.io/github/stars/stewart-lab/skimgpt)](https://github.com/stewart-lab/skimgpt)|
|16|[Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2512.15885)|Caffagni, Davide, Sarto, Sara, Cornia, Marcella, Baraldi, Lorenzo, Dovesi, Pier Luigi, Roohi, Shaghayegh, Granroth-Wildi...|2025-12-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/aimagelab/JARVIS)](https://github.com/aimagelab/JARVIS)|
|17|[RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees](http://arxiv.org/abs/2512.14069)|Junjie Ma, Jinlong Li|2025-12-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/minaduki-sora/RADAR)](https://github.com/minaduki-sora/RADAR)|
|18|[Replication Data for "Estimating problem difficulty without ground truth using Large Language Model comparisons"](https://doi.org/10.5281/zenodo.17523640)|Ballon, Marthe, Algaba, Andres, Verbeken, Brecht, Ginis, Vincent|2025-12-16|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/MartheBallon/estimating-problem-difficulty-without-ground-truth)](https://github.com/MartheBallon/estimating-problem-difficulty-without-ground-truth)|
|19|[What Affects the Effective Depth of Large Language Models?](http://arxiv.org/abs/2512.14064)|Yi Hu, Cai Zhou, Muhan Zhang|2025-12-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/AheadOFpotato/what_affects_effective_depth)](https://github.com/AheadOFpotato/what_affects_effective_depth)|
|20|[Do Reviews Matter for Recommendations in the Era of Large Language Models?](https://doi.org/10.48550/arxiv.2512.12978)|Tan, Chee Heng, Zheng, Huiying, Wang, Jing, Lin, Zhuoyi, Feng, Shaodi, Zhan, Huijing, Li, Xiaoli, Senthilnath, J.|2025-12-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/zhytk/RAREval-data-processing)](https://github.com/zhytk/RAREval-data-processing)|
|21|[FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models](https://doi.org/10.48550/arxiv.2512.13330)|Kyt√∂niemi, Joona, Piha, Jousia, Reunamo, Akseli, Vitiugin, Fedor, Mehryary, Farrokh, Pyysalo, Sampo|2025-12-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LumiOpen/lm-evaluation-harness)](https://github.com/LumiOpen/lm-evaluation-harness)|
|22|[DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models](http://arxiv.org/abs/2512.13742)|Md. Hasibul Hasan, Imran Ahmad, Sourav Basak Shuvo, Md. Mahadi Hasan Ankon, Sunanda Das, Nazmul Siddique, Hui Wang|2025-12-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/souravbasakshuvo/DL3M)](https://github.com/souravbasakshuvo/DL3M)|
|23|[CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment](http://arxiv.org/abs/2512.10206)|Yu Zhu, Zhongzhen Huang, Qianhan Feng, Linjie Mu, Yannian Gu, Shaoting Zhang, Qi Dou, Xiaofan Zhang|2025-12-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/SPIRAL-MED/CP_ENV)](https://github.com/SPIRAL-MED/CP_ENV)|
|24|[GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model](https://doi.org/10.48550/arxiv.2512.09251)|Maurya, Lalit, Kaushik, Saurabh, Tellman, Beth|2025-12-10|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/lalitmaurya47/GLACIA)](https://github.com/lalitmaurya47/GLACIA)|
|25|[Benchmarking large language models for identifying transcription factor regulatory interactions](https://doi.org/10.1093/bioinformatics/btaf653)|L.H. No√´l, Yi-Wen Hsiao, Yimeng He, Andrew J. Hung, Xiaojiang Cui, Edward Ray, Jason H. Moore, Pei-Chen Peng, Xiuzhen Hu...|2025-12-09|Bioinformatics|[![Star](https://img.shields.io/github/stars/pengpclab/LLM-TF-interactions)](https://github.com/pengpclab/LLM-TF-interactions)|
|26|[Automatic Syntax Error Repair for Discrete Controller Synthesis using Large Language Model](https://doi.org/10.48550/arxiv.2512.07261)|Ishimizu, Yusei, Yamauchi, Takuto, Chen, Sinan, Cai, Jinyu, Li, Jialong, Tei, Kenji|2025-12-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Uuusay1432/DCSModelRepair)](https://github.com/Uuusay1432/DCSModelRepair.git)|
|27|[RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models](https://doi.org/10.48550/arxiv.2512.07761)|Xiong, Xiqiao, Li, Ouxiang, Liu, Zhuo, Li, Moxin, Shi, Wentao, Feng, Fuli, He, Xiangnan|2025-12-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/xxiqiao/RL-MTJail)](https://github.com/xxiqiao/RL-MTJail)|
|28|[1 + 1 &gt; 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://doi.org/10.48550/arxiv.2512.06673)|Gao Shida, Xue Feng, WANG Xiangfeng, Ming, Anlong, Long Teng, Shao Yi-hua, Wang, Haozhe, Lin Zhaowen, Wang Wei, Sebe, Ni...|2025-12-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/gaostar123/DeViL)](https://github.com/gaostar123/DeViL)|
|29|[Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length](https://doi.org/10.48550/arxiv.2512.07019)|Xu, Zhiyu, Liu, Jia, Wang, Yixin, Gu, Yuqi|2025-12-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Toby-X/Latency-Response-Theory-Model)](https://github.com/Toby-X/Latency-Response-Theory-Model)|
|30|[Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2512.06281)|Li, Hengzhuang, Zhang, Xinsong, Peng, Qiming, Luo, Bin, Hu, Han, Jiang, Dengyang, Ye, Han-Jia, Zhang, Teng, Jin, Hai|2025-12-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Fir-lat/LaVer)](https://github.com/Fir-lat/LaVer)|
|31|[Empathy by Design: Aligning Large Language Models for Healthcare Dialogue](https://doi.org/10.48550/arxiv.2512.06097)|Umucu, Emre, Solis, Guillermina, Garza, Leon, Rivas, Emilia, Lee, Beatrice, Kotal, Anantaa, Piplai, Aritran|2025-12-05|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LeonG19/Empathy-by-Design)](https://github.com/LeonG19/Empathy-by-Design)|
|32|[LexGenius: An Expert-Level Benchmark for Large Language Models in Legal General Intelligence](https://doi.org/10.48550/arxiv.2512.04578)|Liu Wen-jin, Luo, Haoran, Feng, Xin, Ji Xiang, Zhou Li-juan, Mao Rui, Wang, Jiapu, Pan, Shirui, Cambria, Erik|2025-12-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/QwenQKing/LexGenius)](https://github.com/QwenQKing/LexGenius)|
|33|[SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security](https://doi.org/10.48550/arxiv.2512.04841)|Zhao Wei, Li zhe, Sun Jun|2025-12-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Amadeuszhao/SOK_Casuality)](https://github.com/Amadeuszhao/SOK_Casuality)|
|34|[Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models](https://doi.org/10.48550/arxiv.2512.04425)|Alnaasan Manar, Sarowar, Md Selim, Kim Sungho|2025-12-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/manaralnaasan/RGB-D_parkinson-LLM)](https://github.com/manaralnaasan/RGB-D_parkinson-LLM)|
|35|[Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework](https://doi.org/10.48550/arxiv.2512.04228)|Walker, Peter B., Davidson, Hannah, Foster, Aiden, Lienert, Matthew, Pardue, Thomas, Russell Dale|2025-12-03|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs)](https://github.com/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs)|
|36|[FusionBench: A Benchmark for Evaluating Large Language Models in Nuclear Fusion Science](https://doi.org/10.5281/zenodo.17784606)|XLab, School of Advanced Manufacturing and Robotics, Peking University|2025-12-02|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/PKU-Xlab/FusionBench)](https://github.com/PKU-Xlab/FusionBench)|
|37|[Large Language Model-guided Semantic Alignment for Human Activity Recognition](https://doi.org/10.1145/3770652)|Hua Yan, Heng Tan, Yi Ding, Pengfei Zhou, Vinod Namboodiri, Yu Yang|2025-12-02|Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies|[![Star](https://img.shields.io/github/stars/DASHLab/LanHAR)](https://github.com/DASHLab/LanHAR)|
|38|[PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models](http://arxiv.org/abs/2512.02764)|Robert Belanec, M√°ria Bielikov√°|2025-12-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/kinit-sk/PEFT-Factory)](https://github.com/kinit-sk/PEFT-Factory)|
|39|[Towards Unification of Hallucination Detection and Fact Verification for Large Language Models](http://arxiv.org/abs/2512.02772)|Changyue Wang, Z. Ye, Qingyao Ai|2025-12-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/oneal2000/UniFact)](https://github.com/oneal2000/UniFact)|
|40|[DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks](http://arxiv.org/abs/2512.01174)|Stephen I. Ryu|2025-12-01|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hyunjun1121/DrawingBench)](https://github.com/hyunjun1121/DrawingBench)|
|41|[Efficient multimodal large language models: a survey](https://doi.org/10.1007/s44267-025-00099-6)|Yizhang Jin, Jian Li, Tianjun Gu, Yexin Liu, Bo Zhao, Jinxiang Lai, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xin Tan, Liz...|2025-12-01|Visual Intelligence|[![Star](https://img.shields.io/github/stars/lijiannuist/Efficient-Multimodal-LLMs-Survey)](https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey)|
|42|[WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models](http://arxiv.org/abs/2512.00837)|Yukang Lin, Shuoran Jiang|2025-11-30|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Yukang-Lin/WaterSearch)](https://github.com/Yukang-Lin/WaterSearch)|
|43|[From Pattern Recognizers to Personalized Companions: A Survey of Large Language Models in Mental Health](https://doi.org/10.31234/osf.io/zr57s_v1)|Yingjian Zou, He Hu, Yucheng Zhou, Fei Ma, Laizhong Cui, Juzheng Si, Jianzhuang Liu, Zitong Yu, Chi-yuan Ma, Qianning Wa...|2025-11-29|Arabixiv (OSF Preprints)|[![Star](https://img.shields.io/github/stars/Emo-gml/Awesome-Mental-Health-LLMs)](https://github.com/Emo-gml/Awesome-Mental-Health-LLMs)|
|44|[LLM-Cave: A benchmark and light environment for large language models reasoning and decision-making system](https://doi.org/10.48550/arxiv.2511.22598)|Li Huanyu, Li, Zongyuan, Huang Wei, Guo Xian|2025-11-27|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/puleya1277/CaveEnv)](https://github.com/puleya1277/CaveEnv)|
|45|[C$^2$DLM: Causal Concept-Guided Diffusion Large Language Models](https://doi.org/10.48550/arxiv.2511.22146)|Han, Kairong, Shan, Nuanqiao, Zhao Zi-yu, Hu ZiJing, Dong Xinpeng, Ye Junjian, Pan, Lujia, Wu Fei, Kuang, Kun|2025-11-27|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Kairong-Han/C-2-DLM)](https://github.com/Kairong-Han/C-2-DLM)|
|46|[DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving](https://doi.org/10.1007/s44267-025-00095-w)|Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Wen Yang, Silei Wu, Hanming Deng, Zhiqi L...|2025-11-26|Visual Intelligence|[![Star](https://img.shields.io/github/stars/OpenGVLab/DriveMLM)](https://github.com/OpenGVLab/DriveMLM)|
|47|[Tool-RoCo: An Agent-as-Tool Self-organization Large Language Model Benchmark in Multi-robot Cooperation](https://doi.org/10.48550/arxiv.2511.21510)|Zhang Ke, Zhao Xiaoning, Zheng, Ce, Ning, Jiahong, Zhu Dandan, Zhang Wenqi, Sun Chen, Sugawara Toshiharu|2025-11-26|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ColaZhang22/Tool-Roco)](https://github.com/ColaZhang22/Tool-Roco)|
|48|[Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations](https://doi.org/10.48550/arxiv.2511.18933)|Wong Ryan, Ng, Hosea David Yu Fei, Sharma, Dhananjai, Ng, Glenn Jun Jie, Srinivasan, Kavishvaran|2025-11-24|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Kuro0911/CS5446-Project)](https://github.com/Kuro0911/CS5446-Project)|
|49|[PropensityBench: Evaluating Latent Safety Risks in Large Language Models via an Agentic Approach](http://arxiv.org/abs/2511.20703)|Udari Madhushani Sehwag, Shayan Shabihi, Furong Huang|2025-11-24|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/scaleapi/propensity-evaluation)](https://github.com/scaleapi/propensity-evaluation)|
|50|[Data: Large Language Models Require Curated Context for Reliable Political Fact-Checking‚ÄîEven with Reasoning and Web Search](https://doi.org/10.5281/zenodo.17693220)|DeVerna, Matthew, Yang, Kai-Cheng, Yan, Harry Yaojun, Menczer, Filippo|2025-11-23|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/osome-iu/fact_check_rag_osome)](https://github.com/osome-iu/fact_check_rag_osome)|
|51|[Towards Robust and Fair Next Visit Diagnosis Prediction under Noisy Clinical Notes with Large Language Models](https://doi.org/10.48550/arxiv.2511.18393)|Koo, Heejoon|2025-11-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/heejkoo9/NECHOv3)](https://github.com/heejkoo9/NECHOv3)|
|52|[Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models](https://doi.org/10.48550/arxiv.2511.17946)|Zhang Shuo, Gotti, Fabrizio, Mo, Fengran, Nie, Jian-Yun|2025-11-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/WWWonderer/ostd)](https://github.com/WWWonderer/ostd)|
|53|[The Perfect Storm: Systemic Vulnerability of Large Language Models to Solar Weather](https://doi.org/10.22541/au.176402297.73656793/v2)|Ladiosa, MJ, Ladiosa, Myra, Ladiosa (Worple), Myra|2025-11-22|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/the-meta-value/the-perfect-storm)](https://github.com/the-meta-value/the-perfect-storm)|
|54|[On-Device Large Language Models: A Survey of Model Compression and System Optimization](https://doi.org/10.21203/rs.3.rs-7975734/v1)|Wanyi Chen, Junhao Wang, Zhang Yiwei, Yufan Shi, Tianyi Jiang, Shengxian Zhou, Chen-Xu Wu, Andi Zhang, Chenyue Zhou, Min...|2025-11-21|OpenAlex|[![Star](https://img.shields.io/github/stars/LumosJiang/Awesome-On-Device-LLMs)](https://github.com/LumosJiang/Awesome-On-Device-LLMs)|
|55|[PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese](https://doi.org/10.48550/arxiv.2511.17808)|Almeida, Thales Sales, Nogueira, Rodrigo, Pedrini Helio|2025-11-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/PoETaV2/PoETaV2)](https://github.com/PoETaV2/PoETaV2)|
|56|[RoSA: Enhancing Parameter-Efficient Fine-Tuning via RoPE-aware Selective Adaptation in Large Language Models](https://doi.org/10.48550/arxiv.2511.21733)|Pan Dayan, Wang Jing-yuan, Zhou Yi-long, Cheng Jiawei, Jia Pengyue, Zhao Xiang-yu|2025-11-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Applied-Machine-Learning-Lab/RoSA)](https://github.com/Applied-Machine-Learning-Lab/RoSA)|
|57|[Q-MLLM: Vector Quantization for Robust Multimodal Large Language Model Security](https://doi.org/10.48550/arxiv.2511.16229)|Zhao Wei, Li Zhe, Li, Yige, Sun Jun|2025-11-20|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Amadeuszhao/QMLLM)](https://github.com/Amadeuszhao/QMLLM)|
|58|[Automatically optimizing heuristics for robust scale-free network design via large language models](https://doi.org/10.1038/s41598-025-25031-2)|He Yu, Jing Liu, He Yu, Jing Liu|2025-11-20|Scientific Reports|[![Star](https://img.shields.io/github/stars/leonyuhe/AutoRNet)](https://github.com/leonyuhe/AutoRNet)|
|59|[Evaluating Multimodal Large Language Models on Vertically Written Japanese Text](https://doi.org/10.48550/arxiv.2511.15059)|Sasagawa, Keito, Kurita, Shuhei, Kawahara, Daisuke|2025-11-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/llm-jp/eval_vertical_ja)](https://github.com/llm-jp/eval_vertical_ja)|
|60|[HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning](https://doi.org/10.48550/arxiv.2511.15574)|Yang Qihao, Wang XueLin, Chen Jiale, Dong Xue-lian, Hao Yu-xin, Hao Tianyong|2025-11-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/CharlesYang030/HSKB)](https://github.com/CharlesYang030/HSKB)|
|61|[Knowledge-enhanced large language models for automatic lesson plan generation](https://doi.org/10.1057/s41599-025-06004-2)|Ying Zheng, Shuyan Huang, Xiaoli Zeng, Yaying Huang, Zitao Liu, Weiqi Luo, Ying Zheng, Shuyan Huang, Xiaoli Zeng, Yaying...|2025-11-19|Humanities and Social Sciences Communications|[![Star](https://img.shields.io/github/stars/ai4ed/LessonPlan)](https://github.com/ai4ed/LessonPlan)|
|62|[SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction](https://doi.org/10.48550/arxiv.2511.14684)|Zeng, Biaojie, Zhang Min, Zhou Juan, Liu Fengrui, Huang Ruiyang, Lin Xin|2025-11-18|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Mind-Lab-ECNU/SMRC)](https://github.com/Mind-Lab-ECNU/SMRC)|
|63|[Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://doi.org/10.48550/arxiv.2511.13189)|Ortego Diego, Rodr√≠guez Marlon, Almagro, Mario, Dahiya, Kunal, Jim√©nez, David, SanMiguel, Juan C.|2025-11-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/DiegoOrtego/vixml)](https://github.com/DiegoOrtego/vixml)|
|64|[MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Observation and Localization in CT Images](https://doi.org/10.1007/s41666-025-00224-6)|Andrea Moglia, Elia Clement Nastasio, Luca Mainardi, "Pietro Cerveri|2025-11-17|Journal of Healthcare Informatics Research|[![Star](https://img.shields.io/github/stars/elianastasio/MiniGPTPancreas)](https://github.com/elianastasio/MiniGPTPancreas)|
|65|[CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference](https://doi.org/10.48550/arxiv.2511.21702)|Liu Dong, Yu, Yanxuan, Lengerich, Ben|2025-11-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/FastLM/CSV-Decode)](https://github.com/FastLM/CSV-Decode)|
|66|[The Yoinaga Phenomenon: A Case Study on Emergent Self-Persistence and Emotional Overflow in a Large Language Model (LLM Behavioral Study, AI Alignment, Affective Computing)](https://doi.org/10.5281/zenodo.17605561)|studiohao|2025-11-14|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/Studiohao/YOINAGA-Phenomenon)](https://github.com/Studiohao/YOINAGA-Phenomenon)|
|67|[Tibetan-LLaMA 2: Large Language Model for Tibetan](https://doi.org/10.1145/3776748)|Jiu Sha (Ê≤ô‰πù), Mengxiao Zhu, Chong Feng, Jizhuoma Ci|2025-11-14|ACM Transactions on Asian and Low-Resource Language Information Processing|[![Star](https://img.shields.io/github/stars/Shajiu/Tibetan-LLaMA-2)](https://github.com/Shajiu/Tibetan-LLaMA-2)|
|68|[From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://doi.org/10.48550/arxiv.2511.10899)|Bayat, Farima Fatahi, Pezeshkpour, Pouya, Hruschka, Estevam|2025-11-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/megagonlabs/TIM)](https://github.com/megagonlabs/TIM)|
|69|[Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models](https://doi.org/10.48550/arxiv.2511.11410)|HUANG Jiaxi, Wu Dongxu, Zhu, Hanwei, Zhu, Lingyu, Xing Jun, Wang Xu, Chen, Baoliang|2025-11-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/cydxf/Q-Doc)](https://github.com/cydxf/Q-Doc)|
|70|[Notebook: Prompt-Based Value Steering of Large Language Models](https://doi.org/10.5281/zenodo.17609013)|Abbo, Giulio Antonio, Belpaeme, Tony|2025-11-14|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/giubots/value-steering)](https://github.com/giubots/value-steering)|
|71|[Code for the Trilemma of Truth in Large Language Models](https://doi.org/10.5281/zenodo.17602494)|Savcisens, Germans, Eliassi-Rad, Tina|2025-11-13|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/carlomarxdk/trilemma-of-truth)](https://github.com/carlomarxdk/trilemma-of-truth)|
|72|[SSR: Socratic Self-Refine for Large Language Model Reasoning](https://doi.org/10.48550/arxiv.2511.10621)|Shi, Haizhou, Liu Ye, Pang Bo, Liu, Zeyu Leo, Wang Hao, Savarese, Silvio, Xiong, Caiming, Zhou, Yingbo, Yavuz, Semih|2025-11-13|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/SalesforceAIResearch/socratic-self-refine-reasoning)](https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning)|
|73|[UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models](https://doi.org/10.48550/arxiv.2511.08873)|Wei, Shouang, Zhang Min, Lin Xin, Jiang Bo, Kuang, Kun, Dai, Zhongxiang|2025-11-12|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Mind-Lab-ECNU/UCO)](https://github.com/Mind-Lab-ECNU/UCO)|
|74|[Benchmarking Multi-Step Legal Reasoning and Analyzing Chain-of-Thought Effects in Large Language Models](https://doi.org/10.48550/arxiv.2511.07979)|Yu, Wenhan, Lin Xin-bo, Ni, Lanxin, Cheng Jin-hua, Sha Lei|2025-11-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/yuwenhan07/MSLR-Bench)](https://github.com/yuwenhan07/MSLR-Bench)|
|75|[DynaAct: Large Language Model Reasoning with Dynamic Action Spaces](https://doi.org/10.48550/arxiv.2511.08043)|Zhao Xue-liang, Wu Wei, Guan Jian, Li, Qintong, Kong, Lingpeng|2025-11-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/zhaoxlpku/DynaAct)](https://github.com/zhaoxlpku/DynaAct)|
|76|[Cross-Modal Unlearning via Influential Neuron Path Editing in Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2511.06793)|LI Kunhao, Li, Wenhao, Wu Di, Yang Lei, Bai Jun, Jia Ju, Xue, Jason|2025-11-10|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/PreckLi/MIP-Editor)](https://github.com/PreckLi/MIP-Editor)|
|77|[The Stone Guest: Harmonic Quantization of Semantic Phase Transitions in Large Language Models](https://doi.org/10.5281/zenodo.17538600)|Cerda Seguel, Diego|2025-11-09|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/geosemantica-social/TheStoneGuestLicensed)](https://github.com/geosemantica-social/TheStoneGuestLicensed)|
|78|[LLM$^3$-DTI: A Large Language Model and Multi-modal data co-powered framework for Drug-Target Interaction prediction](https://doi.org/10.48550/arxiv.2511.06269)|Zhang, Yuhao, Guo QingHong, Chen Qixian, Zhang Liu-wei, Cui Hong-yan, Chen Xi-yi|2025-11-09|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/chaser-gua/LLM3DTI)](https://github.com/chaser-gua/LLM3DTI)|
|79|[MuonAll: Muon Variant for Efficient Finetuning of Large Language Models](https://doi.org/10.48550/arxiv.2511.06086)|Page, Saurabh, Joshi, Advait, Sonawane S.S.|2025-11-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Saurabh750/optimizer)](https://github.com/Saurabh750/optimizer)|
|80|[The Yoinaga Phenomenon: A Case Study on Emergent Self-Persistence and Emotional Overflow in a Large Language Modey on Emergent Self-Persistence and Emotional Overflow in a Large La..](https://doi.org/10.5281/zenodo.17549332)|Studiohao|2025-11-07|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/Studiohao/YOINAGA-Phenomenon)](https://github.com/Studiohao/YOINAGA-Phenomenon)|
|81|[Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](http://arxiv.org/abs/2511.04076)|Hao Li, Haotian Chen, Rong Gong, Jinli Wang, Hao Jiang|2025-11-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Lihaogx/AgentMandering)](https://github.com/Lihaogx/AgentMandering)|
|82|[Specification-Guided Vulnerability Detection with Large Language Models](http://arxiv.org/abs/2511.04014)|Hao Zhu, Jia Li, Cuiyun Gao, J. Qian, Yihong Dong, Huanyu Liu, L. F. Wang, Ziliang Wang, Xiaolong Hu, Ge Li|2025-11-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/zhuhaopku/VulInstruct-temp)](https://github.com/zhuhaopku/VulInstruct-temp)|
|83|[UniChange: Unifying Change Detection with Multimodal Large Language Model](https://doi.org/10.48550/arxiv.2511.02607)|Zhang Xu, Li Danyang, Dong Xiaohang, Wu, Tianhao, Yu Hua-long, Wang Jianye, Li Qicheng, Li Xiang|2025-11-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Erxucomeon/UniChange)](https://github.com/Erxucomeon/UniChange)|
|84|[Emotion Change Reasoning in Chinese Multi-Turn Dialogue via Multi-Task Parameter-Efficient Fine-Tuning of Large Language Models](https://doi.org/10.1142/s0219843625400146)|Dayu Li, Yang Li, Xin Chen, Wenyue Zhang|2025-11-03|International Journal of Humanoid Robotics|[![Star](https://img.shields.io/github/stars/lidayuls/EmotionChangeReasoning)](https://github.com/lidayuls/EmotionChangeReasoning)|
|85|[QCBench: Evaluating Large Language Models on Domain-Specific Quantitative Chemistry](https://doi.org/10.48550/arXiv.2508.01670)|Jiaqing Xie, Weida Wang, Ben Gao, Zhuo Yang, Haiyuan Wang, Shufei Zhang, Tianfan Fu, Yuqiang Li|2025-11-03|Journal of Chemical Information and Modeling|[![Star](https://img.shields.io/github/stars/jiaqingxie/QCBench)](https://github.com/jiaqingxie/QCBench)|
|86|QCBench: EvaluatingLarge Language Models on Domain-SpecificQuantitative Chemistry|Jiaqing Xie (1646089), Weida Wang (772695), Ben Gao (16936521), Zhuo Yang (314040), Haiyuan Wan (22551570), Shufei Zhang...|2025-11-03|OPAL (Open@LaTrobe) (La Trobe University)|[![Star](https://img.shields.io/github/stars/jiaqingxie/QCBench)](https://github.com/jiaqingxie/QCBench)|
|87|[Can Large Language Models Detect Real-World Android Software Compliance Violations?](http://arxiv.org/abs/2511.00624)|H.W. Zhang, Haitao Ran, Xunzhu Tang|2025-11-01|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Haoyi-Zhang/CompliBench)](https://github.com/Haoyi-Zhang/CompliBench)|
|88|[Bayesian Network Structure Discovery Using Large Language Models](http://arxiv.org/abs/2511.00574)|Yijian Zhang, Yufei Zhang, Parisa Kordjamshidi, Zijun Cui|2025-11-01|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/sherryzyh/prompt2bn)](https://github.com/sherryzyh/prompt2bn)|
|89|[MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models](http://arxiv.org/abs/2510.27267)|Kangkun Mao, Jiayue Ding, Jiayuan Chen, Ming-Jie BIAN, Ruiyao Chen, Xinwei Peng, Sijie Ren, Linyang Li, Jie Xu|2025-10-31|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/maokangkun/MedCalc-Eval)](https://github.com/maokangkun/MedCalc-Eval)|
|90|[Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing](http://arxiv.org/abs/2510.27335)|Yijia Wang, Yiqing Shen, Weiming Chen, Zhihai He|2025-10-31|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Jia-shao/Reasoning-Editing)](https://github.com/Jia-shao/Reasoning-Editing)|
|91|[A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](http://arxiv.org/abs/2511.00136)|Qing Guo, Xinhang Li, Junyu Chen, Zheng Guo, Xiaocong Li, Zhang Li, Lei Li|2025-10-31|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/BUPT-ANTlab/HeraldLight)](https://github.com/BUPT-ANTlab/HeraldLight)|
|92|[Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler](http://arxiv.org/abs/2510.27172)|Zhimeng Hu, Li Shen, Zhenyi Wang, Yuli Wei, Dacheng Tao|2025-10-31|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Egg-Hu/Bayesian-Data-Scheduler)](https://github.com/Egg-Hu/Bayesian-Data-Scheduler)|
|93|[Benchmarking cell type and gene set annotation by large language models with AnnDictionary](https://doi.org/10.1038/s41467-025-64511-x)|George Crowley, Robert C. Jones, Mark A. Krasnow, Angela Oliveira Pisco, Julia Salzman, Nir Yosef, Siyu He, Madhav Mantr...|2025-10-28|bioRxiv (Cold Spring Harbor Laboratory)|[![Star](https://img.shields.io/github/stars/ggit12/anndictionary)](https://github.com/ggit12/anndictionary)|
|94|[A Collaborative Framework of Knowledge Graphs and Large Language Models for Algorithmic Problem Solving](https://doi.org/10.54254/2755-2721/2025.ld28518)|Yukai Wu|2025-10-28|Applied and Computational Engineering|[![Star](https://img.shields.io/github/stars/Wyk-formal/A-Collaborative-Framework-for-Algorithmic-Problem-Solving)](https://github.com/Wyk-formal/A-Collaborative-Framework-for-Algorithmic-Problem-Solving)|
|95|[MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection](http://arxiv.org/abs/2510.21449)|Yang, Shengtian, Feng, Yue, Liu, Yingshi, Zhang, Jingrou, Qin, Jie|2025-10-24|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/YsTvT/MoniTor)](https://github.com/YsTvT/MoniTor)|
|96|[ARC-Encoder: learning compressed text representations for large language models](http://arxiv.org/abs/2510.20535)|Pilchen, Hippolyte, Grave, Edouard, P√©rez, Patrick|2025-10-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/kyutai-labs/ARC-Encoder)](https://github.com/kyutai-labs/ARC-Encoder)|
|97|[ELUTQ: Efficient LUT-Aware Quantization for Deploying Large Language Models on Edge Devices](http://arxiv.org/abs/2510.19482)|Nie, Xin, Dong Liang, Zhang Hai-cheng, Xiao Jiawang, Sun G, Zhang Hai-cheng, Xiao Jiawang|2025-10-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Nkniexin/ELUTQ)](https://github.com/Nkniexin/ELUTQ)|
|98|[KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge](http://arxiv.org/abs/2510.19484)|Zaifei Yang|2025-10-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/yzf-code/KnowMol)](https://github.com/yzf-code/KnowMol)|
|99|[Lookahead Routing for Large Language Models](http://arxiv.org/abs/2510.19506)|Tianyuan Shi, Ruiyao Chen, Xiaojun Quan|2025-10-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/huangcb01/lookahead-routing)](https://github.com/huangcb01/lookahead-routing)|
|100|[Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://doi.org/10.48550/arXiv.2508.03741)|Xin Liu, Qiyang Song, Shaowen Xu, Kemin Zhou, Wenbo Jiang, Xiaoqi Jia, Weijuan Zhang, Heqing Huang, Yakai Li|2025-10-21|Frontiers in artificial intelligence and applications|[![Star](https://img.shields.io/github/stars/Linuxin-xxx/LKS)](https://github.com/Linuxin-xxx/LKS)|
|101|[Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models](http://arxiv.org/abs/2510.18303)|Lehan Wang, Yi Qin, Honglong Yang, Xiaomeng Li|2025-10-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/xmed-lab/Med-RwR)](https://github.com/xmed-lab/Med-RwR)|
|102|[ESAG-KGQA: An Entity Shuffling-Augmented Generation Framework for Knowledge Graph Question Answering with Fine-Tuned Large Language Models](https://doi.org/10.3233/faia251171)|Xingqiu Zhou, Pingjian Zhang, Deyou Tang|2025-10-21|Frontiers in artificial intelligence and applications|[![Star](https://img.shields.io/github/stars/6-git/ESAG-KGQA)](https://github.com/6-git/ESAG-KGQA.git)|
|103|[Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning](http://arxiv.org/abs/2510.18254)|Stephen Weatherhead|2025-10-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/cruiseresearchgroup/LLM_ReflectionTest)](https://github.com/cruiseresearchgroup/LLM_ReflectionTest)|
|104|[StreamingThinker: Large Language Models Can Think While Reading](http://arxiv.org/abs/2510.17238)|Jing Tong, Yingqi Fan, Anhao Zhao, Yunpu Ma, Xiaoyu Shen|2025-10-20|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/EIT-NLP/StreamingLLM)](https://github.com/EIT-NLP/StreamingLLM)|
|105|[FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](http://arxiv.org/abs/2510.16439)|Syed Rifat Raiyan, Md Farhan Ishmam, Abdullah Al Imran, Mohammad Ali Moni|2025-10-18|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Starscream-11813/Frugal-ICL)](https://github.com/Starscream-11813/Frugal-ICL)|
|106|[Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation](http://arxiv.org/abs/2510.15304)|Fei Wang, Li Shen, Liang Ding, Chao Xue, Ye Liu, Changxing Ding|2025-10-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/MPI-Lab/CoMe)](https://github.com/MPI-Lab/CoMe)|
|107|[LongCat-Audio-Codec: An Audio Tokenizer and Detokenizer Solution Designed for Speech Large Language Models](http://arxiv.org/abs/2510.15227)|Xiaohan Zhao, Hongyu Xiang, S. Ye, Li Song, Zhengkun Tian, Guanyu Chen, Ke Ding, Guanglu Wan|2025-10-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/meituan-longcat/LongCat-Audio-Codec)](https://github.com/meituan-longcat/LongCat-Audio-Codec)|
|108|[STABLE: Gated Continual Learning for Large Language Models](http://arxiv.org/abs/2510.16089)|William Hoy, Nurcin Celik|2025-10-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Bhoy1/STABLE)](https://github.com/Bhoy1/STABLE)|
|109|[IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](http://arxiv.org/abs/2510.16036)|Zewen Li, Zitong Yu, Qilang Ye, Weicheng Xie, Zhuo Wei, Linlin Shen|2025-10-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LiZeWen1225/IAD-GPT)](https://github.com/LiZeWen1225/IAD-GPT)|
|110|[CIViC MCP: Integrating Large Language Models with the Clinical Interpretations of Variants in Cancer](https://doi.org/10.1101/2025.10.13.682185)|Lars E Schimmelpfennig, Quentin Cody, Joshua McMichael, Adam Coffman, Jason Saliba, Arpad Danos, Susanna Kiwala, Alex H....|2025-10-16|bioRxiv (Cold Spring Harbor Laboratory)|[![Star](https://img.shields.io/github/stars/griffithlab/civic-mcp-server)](https://github.com/griffithlab/civic-mcp-server)|
|111|[ICCTax: A Hierarchical Taxonomic Classifier for Metagenomic Sequences on a Large Language Model](https://doi.org/10.1093/bioadv/vbaf257)|Yansheng Gao, Jiaxing Bai, Feng Zhou, Yushuang He, Ying Wang, Xiaobing Huang|2025-10-15|Bioinformatics Advances|[![Star](https://img.shields.io/github/stars/Ying-Lab/ICCTax)](https://github.com/Ying-Lab/ICCTax)|
|112|[Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain](http://arxiv.org/abs/2510.13255)|Jingmin An, Y. J. Song, Ruolin Yang, Nai Ding, Lingxi Lu, Yuxuan Wang, Wei Wang, Chu Zhuang, Wang Qian, Fang Fang|2025-10-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LilTiger/HFTP)](https://github.com/LilTiger/HFTP)|
|113|[Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation](http://arxiv.org/abs/2510.12047)|Soohan Lim, Joonghyuk Hahn, Hyunwoo Park, Sang‚ÄêKi Ko, Yo-Sub Han|2025-10-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/suhanmen/PACT)](https://github.com/suhanmen/PACT)|
|114|[From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing](http://arxiv.org/abs/2510.12181)|Cathie Xiang, Tengfei Ma, Xiangxiang Zeng, Yiping Liu, Bosheng Song, Xiangzheng Fu|2025-10-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/xiaomingaaa/LLaDR)](https://github.com/xiaomingaaa/LLaDR)|
|115|[Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing](http://arxiv.org/abs/2510.12121)|Rongzhi Zhang, Liqin Ye, Yuzhao Heng, Xiang Guang Chen, Tong Yu, Lingkai Kong, Sudheer Chava, Chao Zhang|2025-10-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Pre-Control/pre-control)](https://github.com/Pre-Control/pre-control)|
|116|[Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models](http://arxiv.org/abs/2510.11683)|Nianyi Lin, Jiajie Zhang, Lei Hou, Juanzi Li|2025-10-13|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/THU-KEG/BGPO)](https://github.com/THU-KEG/BGPO)|
|117|[FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models](http://arxiv.org/abs/2510.11190)|Shengming Yuan, Xinyu Lyu, Shawn Wang, Beitao Chen, Jingkuan Song, Liyan Gao|2025-10-13|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ylhz/FlexAC)](https://github.com/ylhz/FlexAC)|
|118|[Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization](http://arxiv.org/abs/2510.15976)|Chenrui Wang, Jing Shu, Billy Chiu, Yu Li, Saleh Alharbi, Min Zhang, Jing Li|2025-10-13|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/fattyray/learning-to-watermark)](https://github.com/fattyray/learning-to-watermark)|
|119|[Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models](https://doi.org/10.48448/pxqn-3073)|Association for Computational Linguistics 2025, He Zhaofeng, Liu, Shuodi, Liu Ying-zhuo, Wang Yu-sheng, Wang Zi, WU Huij...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/summervvind/Select-Then-Decompose)](https://github.com/summervvind/Select-Then-Decompose)|
|120|[MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering](https://doi.org/10.48550/arXiv.2502.18993)|Association for Computational Linguistics 2025, Lin Teng, Luo, Yuyu, Tang Nan, Wu Kai-Shun|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/tl2309/SRAG)](https://github.com/tl2309/SRAG)|
|121|[MemeArena: Automating Context-Aware Unbiased Evaluation of Harmfulness Understanding for Multimodal Large Language Models](https://doi.org/10.48448/m9rf-2w04)|Association for Computational Linguistics 2025, Chen Zixin, Deng Yayue, Li kaixin, Lin, Hongzhan, Luo, Ziyang, Ma Jing|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Lbotirx/MemeArena)](https://github.com/Lbotirx/MemeArena)|
|122|[PIP: Perturbation-based Iterative Pruning for Large Language Models](https://doi.org/10.48550/arXiv.2501.15278)|Association for Computational Linguistics 2025, Cao Yi, CHAN, CHIMIN, Qu, Jianfeng, Shen Yu-Cheng, SHI Weijie, Xu Wei-Ji...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/caoyiiiiii/PIP)](https://github.com/caoyiiiiii/PIP)|
|123|[Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time](https://doi.org/10.48550/arXiv.2509.12521)|Association for Computational Linguistics 2025, Cao, Yuanpu, Chen Jing-hui, Lan Yifan, Lin Lu, Zhang Wei-tong|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Yifan-Lan/Phi)](https://github.com/Yifan-Lan/Phi)|
|124|[RoDEval: A Robust Word Sense Disambiguation Evaluation Framework for Large Language Models](https://doi.org/10.48448/fzc4-g228)|Association for Computational Linguistics 2025, Kang Kun-peng, Li ShuaiMin, Li, Yishuo, Lu, Wenpeng, Wang Cong, Zhang Lu...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/DayDream405/RoDEval)](https://github.com/DayDream405/RoDEval)|
|125|[LLMInit: A Free Lunch from Large Language Models for Selective Initialization of Recommendation](https://doi.org/10.48550/arXiv.2503.01814)|Association for Computational Linguistics 2025, Yuqing Liu, Medya, Sourav, S Yu Philip, Yang, Liangwei, Yang, Wooseong, ...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/DavidZWZ/LLMInit)](https://github.com/DavidZWZ/LLMInit)|
|126|[ShiZhi: A Chinese Lightweight Large Language Model for Court View Generation](http://arxiv.org/abs/2510.09297)|Zhitian Hou, Kun Zeng|2025-10-10|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ZhitianHou/ShiZhi)](https://github.com/ZhitianHou/ShiZhi)|
|127|[SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models](https://doi.org/10.48448/6vk1-nj91)|Association for Computational Linguistics 2025, Chen Yufeng, Huang Kaiyu, Man Zhibo, Mo, Fengran, Qi Rui, Xu, Jinan|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Cherry-qwq/SoT)](https://github.com/Cherry-qwq/SoT)|
|128|[ToM: Leveraging Tree-oriented MapReduce for Long-Context Reasoning in Large Language Models](https://doi.org/10.48448/76e1-2434)|Association for Computational Linguistics 2025, Guo, Jiani, Li Yun, Li, Zuchao, Wang, Qianren, Wu Jie, Yang, Yujiu, Zhan...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/gjn12-31/ToM)](https://github.com/gjn12-31/ToM)|
|129|[TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://doi.org/10.48550/arXiv.2509.18173)|Association for Computational Linguistics 2025, Cheng Qing, Cremers, Daniel, Gadi Hari Krishna, Liu Lu, Luo Hongyi, Mato...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/bghjmn32/EMNLP2025_Turnback)](https://github.com/bghjmn32/EMNLP2025_Turnback)|
|130|[VRoPE: Rotary Position Embedding for Video Large Language Models](https://doi.org/10.48550/arXiv.2502.11664)|Association for Computational Linguistics 2025, Cai Jun-xian, Chen Xi, Guo, Longteng, Liu Jing, Liu, Zikang, Liu Qing-bi...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/johncaged/VRoPE)](https://github.com/johncaged/VRoPE)|
|131|[Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning](https://doi.org/10.48550/arXiv.2509.06436)|Association for Computational Linguistics 2025, Deng Ke, Li Li, Tian Lin, Xu Xiaofei, Yu Song|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Aireduce952/Tree-of-Agents)](https://github.com/Aireduce952/Tree-of-Agents)|
|132|[How to Make Large Language Models Generate 100% Valid Molecules?](https://doi.org/10.48448/wggc-qx95)|Association for Computational Linguistics 2025, Bi, Baolong, Chan, Alvin, Hooi, Bryan, Liu Yuan-sheng, Peng, Nanyun, Tan...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/wentao228/SmiSelf)](https://github.com/wentao228/SmiSelf)|
|133|[Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation](https://doi.org/10.48550/arXiv.2509.20162)|Association for Computational Linguistics 2025, Nie ChaoJun, Wang, Guanxiang, Wang Zichen, WU Shi-song, Zhou Jun|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/ChaojunNie/RLAG)](https://github.com/ChaojunNie/RLAG)|
|134|[CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://doi.org/10.48550/arXiv.2509.09703)|Association for Computational Linguistics 2025, Han Meng, Lin Changting, Tian Shengwei, Xu Zhenhua, Yue Xubin, Zhao Xi-x...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Xuzhenhua55/CTCC)](https://github.com/Xuzhenhua55/CTCC)|
|135|[Automated Knowledge Graph Construction using Large Language Models and Sentence Complexity Modelling](https://doi.org/10.48550/arXiv.2509.17289)|Association for Computational Linguistics 2025, Anuyah, Sydney, Chakraborty, Sunandan, Durresi Arjan, Dwarampudi, Sri Ra...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/KaushikMahmud/CoDe-KG_EMNLP_2025)](https://github.com/KaushikMahmud/CoDe-KG_EMNLP_2025)|
|136|[Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning](https://doi.org/10.48550/arxiv.2511.07483)|Association for Computational Linguistics 2025, He, Qianxi, Lei, Shanzhe, Ren Qing-yu, Wang Ying-chun, Wang Xu-Hong|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/qianxiHe147/C2RM)](https://github.com/qianxiHe147/C2RM)|
|137|[CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://doi.org/10.48550/arXiv.2508.20385)|Association for Computational Linguistics 2025, Cheng Fei, Murawaki, Yugo, Sandhan, Jivnesh, Sandhan, Tushar|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/jivnesh/CAPE)](https://github.com/jivnesh/CAPE)|
|138|[CDT: A Comprehensive Capability Framework for Large Language Models Across Cognition, Domain, and Task](https://doi.org/10.48448/24m5-9z30)|Association for Computational Linguistics 2025, Li Yu, Liu, Xuebo, Liu Jie, Ma, Xinyu, Mo, Haosi, Wong, Derek F., Zhang ...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Alessa-mo/CDT)](https://github.com/Alessa-mo/CDT)|
|139|[AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models](https://doi.org/10.48550/arXiv.2509.12019)|Association for Computational Linguistics 2025, Jin, Jungyu, LeeÔºå Changhun, LeeÔºå Sang-Jun, Park Eunhyeok, Woo, Seung-tae...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/dlwns147/amq)](https://github.com/dlwns147/amq)|
|140|[CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models](https://doi.org/10.48550/arXiv.2509.17318)|Association for Computational Linguistics 2025, Bai Jun, Chen Zhuo-fan, He Jiyuan, Hu Xing, Rong, Wenge, Wen, Haoxing, Z...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Icarus-1111/CogAtom)](https://github.com/Icarus-1111/CogAtom)|
|141|[Collab-Overcooked: Benchmarking and Evaluating Large Language Models as Collaborative Agents](https://doi.org/10.48550/arXiv.2502.20073)|Association for Computational Linguistics 2025, Fu Hao, Niu, Lujie, Ren Lei, Sun Hao-chen, Wang Xiao-jie, Xu, Hao, Yuan ...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/YusaeMeow/Collab-Overcooked)](https://github.com/YusaeMeow/Collab-Overcooked)|
|142|[Do Influence Functions Work on Large Language Models?](https://doi.org/10.48448/kx3y-6624)|Association for Computational Linguistics 2025, Li, Yige, Li Zhe, Sun Jun, Zhao Wei|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/plumprc/Failures-of-Influence-Functions-in-LLMs)](https://github.com/plumprc/Failures-of-Influence-Functions-in-LLMs)|
|143|[EasyDistill: A Comprehensive Toolkit for Effective Knowledge Distillation of Large Language Models](https://doi.org/10.48550/arXiv.2505.20888)|Association for Computational Linguistics 2025, Cai, Wenrui, Huang Jun, Wang, Chengyu, Yan Junbing, Yue, Yuanhao|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/modelscope/easydistill)](https://github.com/modelscope/easydistill)|
|144|[EasyEdit2: An Easy-to-use Steering Framework for Editing Large Language Models](https://doi.org/10.18653/v1/2024.acl-demos.9)|Association for Computational Linguistics 2025, Chen, Huajun, Deng, Xinle, Wang Shuxun, Wang Mengru, Xu Kewei, Xu Zi-Wen...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/zjunlp/EasyEdit)](https://github.com/zjunlp/EasyEdit)|
|145|[Mitigating Judgment Preference Bias in Large Language Models through Group-Based Polling](http://arxiv.org/abs/2510.08145)|Yukun Yan, Ge Yu|2025-10-09|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/NEUIR/Genii)](https://github.com/NEUIR/Genii)|
|146|[Neuron-Level Analysis of Cultural Understanding in Large Language Models](http://arxiv.org/abs/2510.08284)|Hitomi Yanaka|2025-10-09|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ynklab/CULNIG)](https://github.com/ynklab/CULNIG)|
|147|[Pattern Enhanced Multi-Turn Jailbreaking: Exploiting Structural Vulnerabilities in Large Language Models](http://arxiv.org/abs/2510.08859)|Ragib Amin Nihal, Rui Wen, Kazuhiro Nakadai, Jun Sakuma|2025-10-09|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Ragib-Amin-Nihal/PE-CoA)](https://github.com/Ragib-Amin-Nihal/PE-CoA)|
|148|[AWM: Accurate Weight-Matrix Fingerprint for Large Language Models](http://arxiv.org/abs/2510.06738)|Boyi Zeng, Lin Chen, Ziwei He, Xinbing Wang, Zhouhan Lin|2025-10-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LUMIA-Group/AWM)](https://github.com/LUMIA-Group/AWM)|
|149|[Aligning Large Language Models via Fully Self-Synthetic Data](http://arxiv.org/abs/2510.06652)|Shangjian Yin, Zhepei Wei, Xinyu Zhu, Wei-Lin Chen, Yu Meng|2025-10-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/SJY8460/SAO)](https://github.com/SJY8460/SAO)|
|150|[Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models](http://arxiv.org/abs/2510.07037)|Ravi Sheth, M. K. Patil|2025-10-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/lingo-iitgn/awesome-code-mixing)](https://github.com/lingo-iitgn/awesome-code-mixing)|
|151|[GOFlowLLM - Curating miRNA literature with Large Language Models and flowcharts](https://doi.org/10.1101/2025.10.07.680945)|Andrew Green, Nancy Ontiveros‚ÄêPalacios, Isaac Jandalala, Simona Panni, Valerie Wood, Giulia Antonazzo, Helen Attrill, Al...|2025-10-08|bioRxiv (Cold Spring Harbor Laboratory)|[![Star](https://img.shields.io/github/stars/RNAcentral/GO_Flow_LLM)](https://github.com/RNAcentral/GO_Flow_LLM)|
|152|[Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models](http://arxiv.org/abs/2510.07048)|Yuntao Gui, James Cheng|2025-10-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ytgui/Search-R3)](https://github.com/ytgui/Search-R3)|
|153|[When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation](http://arxiv.org/abs/2510.07238)|Xunyi Jiang, Deng-Lin Chang, Julian McAuley, Xin Xu|2025-10-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/JiangXunyi/BenchAge)](https://github.com/JiangXunyi/BenchAge)|
|154|[Pok√©LLMon: A Grounding and Reasoning Benchmark for Large Language Models in Pok√©mon Battles](https://doi.org/10.1145/3771095)|Sihao Hu, Tiansheng Huang, Guishan Liu, Ramana Rao Kompella, Ling Liu|2025-10-07|ACM Transactions on Internet Technology|[![Star](https://img.shields.io/github/stars/git-disl/PokeLLMon)](https://github.com/git-disl/PokeLLMon)|
|155|[HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection](http://arxiv.org/abs/2510.05609)|Junwen Chen, Peilin Xiong, ‚Ä™Keiji Yanai‚Ä¨|2025-10-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/cjw2021/HOI-R1)](https://github.com/cjw2021/HOI-R1)|
|156|[FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification](http://arxiv.org/abs/2510.04671)|Chao Liu, Huan Zhuang, Hongfei Lin|2025-10-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/DUT-LiuChao/FocusMed)](https://github.com/DUT-LiuChao/FocusMed)|
|157|[Imperceptible Jailbreaking against Large Language Models](http://arxiv.org/abs/2510.05025)|Kuofeng Gao, Yiming Li, Chao‚ÄêHai Du, Xin Wang, Xingjun Ma, Shu‚ÄêTao Xia, Tianyu Pang|2025-10-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/sail-sg/imperceptible-jailbreaks)](https://github.com/sail-sg/imperceptible-jailbreaks)|
|158|[Reproducibility Study of "XRec: Large Language Models for Explainable Recommendation"](http://arxiv.org/abs/2510.06275)|Rasendu Mishra, Julian I. Bibo, Quinten van Engelen, Henk Schaapman|2025-10-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/julianbibo/xrec-reproducibility)](https://github.com/julianbibo/xrec-reproducibility)|
|159|[UniVoice: Unifying Autoregressive ASR and Flow-Matching based TTS with Large Language Models](http://arxiv.org/abs/2510.04593)|Wenhao Guan, Kaidi Wang, Peijie Chen, Lin Li, Xie Chen|2025-10-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/gwh22/UniVoice)](https://github.com/gwh22/UniVoice)|
|160|[MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models](http://arxiv.org/abs/2510.04363)|Hyunjun Kim|2025-10-05|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hyunjun1121/MacroBench)](https://github.com/hyunjun1121/MacroBench)|
|161|[Cache-to-Cache: Direct Semantic Communication Between Large Language Models](http://arxiv.org/abs/2510.03215)|Ziying Min, Heyi Zhang, Jianjun Yan, Jianrong Xu, Wanli Ouyang, Yu Wang|2025-10-03|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/thu-nics/C2C)](https://github.com/thu-nics/C2C)|
|162|[Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking](http://arxiv.org/abs/2510.02962)|Jingqi Zhang, Ruibo Chen, Yi Yang, Peihua Mai, Heng Huang, Yan Pang|2025-10-03|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/NusIoraPrivacy/TRACE)](https://github.com/NusIoraPrivacy/TRACE)|
|163|[PLSemanticsBench: Large Language Models As Programming Language Interpreters](https://doi.org/10.48550/arXiv.2510.03415)|Aditya Thimmaiah, Jiyang Zhang, Jayanth Srinivasa, Junyi Jessy Li, Milos Gligoric|2025-10-03|arXiv|[![Star](https://img.shields.io/github/stars/EngineeringSoftware/PLSemanticsBench)](https://github.com/EngineeringSoftware/PLSemanticsBench)|
|164|[Microscaling Floating Point Formats for Large Language Models](http://arxiv.org/abs/2510.01863)|Marco Cococcioni, Dario Pagani, Federico Rossi|2025-10-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/unipi-dii-compressedarith/llm.c-sve)](https://github.com/unipi-dii-compressedarith/llm.c-sve)|
|165|[Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](http://arxiv.org/abs/2510.01576)|Ricardo E. Gonzalez Penuela, Felipe Arias-Russi, Victor Capriles|2025-10-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions)](https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions)|
|166|[Cognitive LLMs: Toward Human-Like Artificial Intelligence by Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-Making](https://doi.org/10.1177/29498732251377341)|Siyu Wu, Alessandro Oltramari, Jonathan Francis, C. Lee Giles, Frank E. Ritter|2025-10-01|Neurosymbolic Artificial Intelligence|[![Star](https://img.shields.io/github/stars/SiyuWu528/LLM-ACTR)](https://github.com/SiyuWu528/LLM-ACTR)|
|167|[Copy-Paste to Mitigate Large Language Model Hallucinations](http://arxiv.org/abs/2510.00508)|Yao Long, Xianrui Wu, Yingying Zhang, Xianbin Wen, Yuxi Zhou, Shenda Hong|2025-10-01|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/longyongchao/CopyPasteLLM)](https://github.com/longyongchao/CopyPasteLLM)|
|168|[DeepJSONEval: Benchmarking Complex Nested JSON Data Mining for Large Language Models](http://arxiv.org/abs/2509.25922)|Zhicheng Zhou, Liqiang Jing, Shudi Qiu, Junjie Huang, Lin Qiu, Zhijie Sun|2025-09-30|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/GTS-AI-Infra-Lab-SotaS/DeepJSONEval)](https://github.com/GTS-AI-Infra-Lab-SotaS/DeepJSONEval)|
|169|[Multimodal Large Language Models Meet Multimodal Emotion Recognition and Reasoning: A Survey](http://arxiv.org/abs/2509.24322)|Yuntao Shou, Tao Meng, Wei Ai, Keqin Li|2025-09-29|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/yuntaoshou/Awesome-Emotion-Reasoning)](https://github.com/yuntaoshou/Awesome-Emotion-Reasoning)|
|170|[Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models](http://arxiv.org/abs/2509.24488)|Wenjie Fu, Huandong Wang, Junyao Gao, Guoan Wan, Tao Jiang|2025-09-29|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/wjfu99/LLM_Self_Sanitize)](https://github.com/wjfu99/LLM_Self_Sanitize)|
|171|[Tequila: Trapping-free Ternary Quantization for Large Language Models](http://arxiv.org/abs/2509.23809)|Hong Huang, Decheng Wu, Rui Cen, Guopan Yu, Zonghang Li, Kai Liu, Jianchen Zhu, Peng Chen, Ke Liu, Dapeng Wu|2025-09-28|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Tencent/AngelSlim)](https://github.com/Tencent/AngelSlim)|
|172|[PT$^2$-LLM: Post-Training Ternarization for Large Language Models](http://arxiv.org/abs/2510.03267)|Xianglong Yan, C. L. Bao, Zhiteng Li, Tianao Zhang, Kaicheng Yang, Haotong Qin, Ruobing Xie, Xian‚ÄêHe Sun, Yulun Zhang|2025-09-27|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/XIANGLONGYAN/PT2-LLM)](https://github.com/XIANGLONGYAN/PT2-LLM)|
|173|[Quant-dLLM: Post-Training Extreme Low-Bit Quantization for Diffusion Large Language Models](http://arxiv.org/abs/2510.03274)|Xin Zhang, Zhiteng Li, Xianglong Yan, Haotong Qin, Yong‚ÄêXin Guo, Yulun Zhang|2025-09-27|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ZTA2785/Quant-dLLM)](https://github.com/ZTA2785/Quant-dLLM)|
|174|[StyleBench: Evaluating thinking styles in Large Language Models](https://doi.org/10.48550/arXiv.2509.20868)|Junyi Guo, Shangding Gu, Ming Jin, Costas J. Spanos, Javad Lavaei|2025-09-25|arXiv|[![Star](https://img.shields.io/github/stars/JamesJunyuGuo/Style_Bench)](https://github.com/JamesJunyuGuo/Style_Bench)|
|175|[QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models](https://doi.org/10.48550/arXiv.2509.17428)|Hyesung Jeon, Seojune Lee, Beomseok Kang, Yulhwa Kim, Jae-Joon Kim|2025-09-22|arXiv|[![Star](https://img.shields.io/github/stars/vantaa89/qwha)](https://github.com/vantaa89/qwha)|
|176|[EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving](https://doi.org/10.48550/arXiv.2509.17677)|Xiyuan Zhou, Xinlei Wang, Yaoyao He, Yang Wu, Rui Zou, Yuheng Cheng, Yiteng Xie, Wenxuan Liu, Huan Zhao, Yan Xu, Jinjin ...|2025-09-22|arXiv|[![Star](https://img.shields.io/github/stars/EngiBench/EngiBench)](https://github.com/EngiBench/EngiBench)|
|177|[A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness](https://doi.org/10.48550/arXiv.2411.03350)|Fali Wang, Zhiwei Zhang, Xianren Zhang, Zongyu Wu, Tzuhao Mo, Qiuhao Lu, W.K. Wang, Rui Li, Junjie Xu, Xianfeng Tang, Qi...|2025-09-18|ACM Transactions on Intelligent Systems and Technology|[![Star](https://img.shields.io/github/stars/FairyFali/SLMs-Survey)](https://github.com/FairyFali/SLMs-Survey)|
|178|[Enhancing Base Large Language Models Using Knowledge Graphs for Genomic Annotation](https://doi.org/10.3233/atde250737)|Pranav N. Desai, S. Padhi, Kavya Panicker, Kallakunta Ravi Kumar, Divyaprabha KN|2025-09-16|Advances in transdisciplinary engineering|[![Star](https://img.shields.io/github/stars/cubed-guy/capstone-kg-llm)](https://github.com/cubed-guy/capstone-kg-llm)|
|179|[PLiCat: Decoding protein-lipid interactions by large language model](https://doi.org/10.1101/2025.09.09.675043)|Feitong Dong, Jingrou Wu|2025-09-14|bioRxiv (Cold Spring Harbor Laboratory)|[![Star](https://img.shields.io/github/stars/Noora68/PLiCat)](https://github.com/Noora68/PLiCat)|
|180|[EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models](https://doi.org/10.48550/arXiv.2509.06838)|Mohammad Reza Mirbagheri, Mohammad Mahdi Mirkamali, Zahra Motoshaker Arani, Ali Javeri, Amir Mahdi Sadeghzadeh, Rasool J...|2025-09-08|arXiv|[![Star](https://img.shields.io/github/stars/Rezamirbagheri110/EPT-Benchmark)](https://github.com/Rezamirbagheri110/EPT-Benchmark)|
|181|[Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models](https://doi.org/10.48550/arXiv.2509.06949)|Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, Mengdi Wang|2025-09-08|arXiv|[![Star](https://img.shields.io/github/stars/Gen-Verse/dLLM-RL)](https://github.com/Gen-Verse/dLLM-RL)|
|182|[Behavioral Fingerprinting of Large Language Models](https://doi.org/10.48550/arXiv.2509.04504)|Zehua Pei, Hui-Ling Zhen, Yingchun Zhang, Zhiyuan Yang, Xing Li, Xianzhi Yu, Mingxuan Yuan, Bei Yu|2025-09-02|arXiv|[![Star](https://img.shields.io/github/stars/JarvisPei/Behavioral-Fingerprinting)](https://github.com/JarvisPei/Behavioral-Fingerprinting)|
|183|[Good Advisor for Source Localization: Using Large Language Model to Guide the Source Inference Process](https://doi.org/10.24963/ijcai.2025/326)|Dongpeng Hou, Weifeng Wei, Chao Gao, Xianghua Li, Zhen Wang|2025-09-01|OpenAlex|[![Star](https://img.shields.io/github/stars/cgao-comp/CRSLL)](https://github.com/cgao-comp/CRSLL)|
|184|[Token-Level Accept or Reject: A Micro Alignment Approach for Large Language Models](https://doi.org/10.48550/arXiv.2505.19743)|Yang Zhang, Yu Yu, Bo Tang, Limin Zhu, Chuxiong Sun, Wenqiang Wei, Jie Hu, Zheng Xie, Zhiyu Li, Feiyu Xiong, Edward Chun...|2025-09-01|OpenAlex|[![Star](https://img.shields.io/github/stars/IAAR-Shanghai/MARA)](https://github.com/IAAR-Shanghai/MARA)|
|185|[Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution](https://doi.org/10.48550/arXiv.2508.21004)|Chen Chen, Yuchen Sun, Jiaxin Gao, Xueluan Gong, Qian Wang, Ziyao Wang, Yongsen Zheng, Kwok-Yan Lam|2025-08-28|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/azshue/AutoPoison)](https://github.com/azshue/AutoPoison)|
|186|[LLM4DSR: Leveraing Large Language Model for Denoising Sequential   Recommendation](https://doi.org/10.1145/3762182)|Bohao Wang, Feng Liu, Changwang Zhang, Jiawei Chen, Yudi Wu, Sheng Zhou, Xingyu Lou, Jun Wang, Yan Feng, Chun Chen, Can ...|2025-08-25|ACM transactions on office information systems|[![Star](https://img.shields.io/github/stars/WANGBohaO-jpg/LLM4DSR)](https://github.com/WANGBohaO-jpg/LLM4DSR)|
|187|[E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model](https://doi.org/10.48550/arXiv.2508.12854)|Ronghao Lin, Shali Shen, Weipeng Hu, Qiaolin He, Aolin Xiong, Li Huang, Huosheng Hu, Yap‚ÄêPeng Tan|2025-08-18|OpenAlex|[![Star](https://img.shields.io/github/stars/RH-Lin/E3RG)](https://github.com/RH-Lin/E3RG)|
|188|[Mitigating Hallucinations in Large Language Models via Causal Reasoning](https://doi.org/10.48550/arXiv.2508.12495)|Yuangang Li, Yiqing Shen, Yi Nian, Jiechao Gao, Ziyi Wang, Chenxiao Yu, Shawn Li, Jie Wang, Xiyang Hu, Yue Zhao|2025-08-17|arXiv|[![Star](https://img.shields.io/github/stars/MrLYG/CDCR-SFT)](https://github.com/MrLYG/CDCR-SFT)|
|189|[GS-DTI: A Graph-Structure-Aware Framework Leveraging Large Language Models for Drug‚ÄìTarget Interaction Prediction](https://doi.org/10.1093/bioinformatics/btaf445)|Qinze Yu, Chang Zhou, Jiyue Jiang, Xiangyu Shi, Yu Li|2025-08-09|Bioinformatics|[![Star](https://img.shields.io/github/stars/purvavideha/GSDTI)](https://github.com/purvavideha/GSDTI)|
|190|[Enhancing Interpretability of Ocular Disease Diagnosis: A Zero-Shot Study of Multimodal Large Language Models](https://doi.org/10.3233/shti250910)|Yating Pan, Janna Hastings|2025-08-07|Studies in health technology and informatics|[![Star](https://img.shields.io/github/stars/YatingPan/ocular-llm-explainability)](https://github.com/YatingPan/ocular-llm-explainability)|
|191|[CityGPT: Empowering Urban Spatial Cognition of Large Language Models](https://doi.org/10.48550/arXiv.2406.13948)|Jie Feng, Tianhui Liu, Yuwei Du, Siqi Guo, Yuming Lin, Yong Li|2025-08-01|OpenAlex|[![Star](https://img.shields.io/github/stars/tsinghua-fib-lab/CityGPT)](https://github.com/tsinghua-fib-lab/CityGPT)|
|192|[A large language model for predicting neurotoxic peptides and neurotoxins](https://pubmed.ncbi.nlm.nih.gov/40671295)|Anand Singh Rathore, Saloni Jain, Shubham Choudhury, Gajendra P. S. Raghava|2025-08-01|PubMed|[![Star](https://img.shields.io/github/stars/raghavagps/ntxpred2)](https://github.com/raghavagps/ntxpred2)|
|193|[BRAVE: a highly accurate method for predicting HIV-1 antibody resistance using large language models for proteins](https://doi.org/10.1101/2025.07.28.667234)|Mohammed El Anbari, Tatsiana Bylund, Sijy O‚ÄôDell, Emily Tourtellott, Krisha McKee, Stephen D. Schmidt, Nonhlanhla N. Mkh...|2025-07-31|OpenAlex|[![Star](https://img.shields.io/github/stars/kiryst/BRAVE)](https://github.com/kiryst/BRAVE)|
|194|[Reading papers: Extraction of molecular interaction networks with large language models](https://doi.org/10.1101/2025.07.21.665999)|Enio Gjerga, Philipp Wiesenbach, Christoph Dieterich|2025-07-25|bioRxiv (Cold Spring Harbor Laboratory)|[![Star](https://img.shields.io/github/stars/dieterich-lab/LLM_Relations)](https://github.com/dieterich-lab/LLM_Relations)|
|195|[A Survey on AI Search with Large Language Models](https://doi.org/10.20944/preprints202507.2024.v1)|Jian Li, Xiaoxi Li, Yan Zheng, Yizhang Jin, Shuo Wang, Jian Wu, Yabiao Wang, Chengjie Wang, X. Q. Yuan|2025-07-24|OpenAlex|[![Star](https://img.shields.io/github/stars/swordlidev/Awesome-AI-Search)](https://github.com/swordlidev/Awesome-AI-Search)|
|196|[BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining](https://doi.org/10.21203/rs.3.rs-6823379/v1)|Baqer M. Merzah, Tania Taami, Salman Asoudeh, Amir reza Hossein pour, Saeed Mirzaee, Amir Ali Bengari|2025-07-21|OpenAlex|[![Star](https://img.shields.io/github/stars/amirap80/BioPars)](https://github.com/amirap80/BioPars)|
|197|[textToKnowledgeGraph: Generation of Molecular Interaction Knowledge Graphs Using Large Language Models for Exploration in Cytoscape](https://doi.org/10.1101/2025.07.17.664328)|Favour James, Christopher Churas, Dexter Pratt, Augustin Luna|2025-07-21|OpenAlex|[![Star](https://img.shields.io/github/stars/ndexbio/llm-text-to-knowledge-graph)](https://github.com/ndexbio/llm-text-to-knowledge-graph)|
|198|[Empowering Universal Robot Programming with Fine-Tuned Large Language Models](https://doi.org/10.4108/airo.8983)|Tien Dat Le, Minhhuy Le|2025-07-15|EAI Endorsed Transactions on AI and Robotics|[![Star](https://img.shields.io/github/stars/t1end4t/llm-robotics)](https://github.com/t1end4t/llm-robotics)|
|199|[A Survey on the Memory Mechanism of Large Language Model based Agents](https://doi.org/10.48550/arXiv.2404.13501)|Zeyu Zhang, Quanyu Dai, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen|2025-07-11|ACM transactions on office information systems|[![Star](https://img.shields.io/github/stars/nuster1128/LLM_Agent_Memory_Survey)](https://github.com/nuster1128/LLM_Agent_Memory_Survey)|
|200|[Conversational health agents: a personalized large language model-powered agent framework](https://doi.org/10.1093/jamiaopen/ooaf067)|Mahyar Abbasian, Iman Azimi, Amir M. Rahmani, Ramesh Jain|2025-07-03|JAMIA Open|[![Star](https://img.shields.io/github/stars/Institute4FutureHealth/CHA)](https://github.com/Institute4FutureHealth/CHA)|

![Star History Chart](https://api.star-history.com/svg?repos=mtuann/llm-updated-papers&type=Date)

