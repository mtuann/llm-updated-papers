# Large Language Models Papers

Updated list of Large Language Models papers as of **February 11, 2026**. 

## Quick Access
üîç **[Interactive Search & Browse](https://mtuann.github.io/papers/)** - Filter, search, and explore all papers with an intuitive interface

## Overview
- **Coverage**: Papers from 2016 to present
- **Sources**: arXiv, NeurIPS, ICML, ICLR, ACL, EMNLP, AAAI, IJCAI, KDD, CVPR, ICCV, ECCV, IEEE, ACM, Springer, ScienceDirect, Nature, and other top AI/ML venues
- **Updates**: Automated collection of new publications
- **Features**: Advanced search, code availability tracking, and multi-venue coverage

## Related Topics
- **[Large Language Models](https://github.com/mtuann/llm-updated-papers)** | **[Federated Learning](https://github.com/mtuann/federated-learning-updated-papers)** | **[Backdoor Learning](https://github.com/mtuann/backdoor-ai-resources)** | **[Machine Unlearning](https://github.com/mtuann/machine-unlearning-papers)**
- **[Serverless Computing](https://mtuann.github.io/papers/)** | **[Multi-Modal Learning](https://mtuann.github.io/papers/)**

## Large Language Models Papers with Code
This section lists papers with available code (sorted by publication date). For the complete paper list, visit the [Research Papers Page](https://mtuann.github.io/papers/).

---

## Support
If you find this resource helpful, consider supporting its development:

- **Ko-fi** (PayPal/Card): [ko-fi.com/miutheladycat](https://ko-fi.com/miutheladycat)
- **Techcombank** (Vietnam): 5877 5555 55 (Nguyen Thi Lan Phuong)

---

*This repository is regularly updated. For the latest data, visit the [Research Papers Page](https://mtuann.github.io/papers/).*


|No.|Title|Authors|Publish Date|Venue|Code|
|---|---|---|---|---|---|
|1|[Systematic Failure of Large Language Models: An Empirical Theory of a Universal Limitation in Self-Referential Termination Proofs](https://doi.org/10.5281/zenodo.18529506)|Moses Rahnama|2026-02-09|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/MosesRahnama/OperatorKO7)](https://github.com/MosesRahnama/OperatorKO7)|
|2|[Improve Large Language Model Systems with User Logs](https://doi.org/10.48550/arxiv.2602.06470)|Changyue Wang, Weihang Su, Qingyao Ai, Quan Zhou|2026-02-06|ArXiv.org|[![Star](https://img.shields.io/github/stars/bebr2/UNO)](https://github.com/bebr2/UNO)|
|3|[FMBench: Adaptive Large Language Model Output Formatting](https://doi.org/10.48550/arxiv.2602.06384)|Yaoting Wang, Yun Zhou, Henghui Ding|2026-02-06|ArXiv.org|[![Star](https://img.shields.io/github/stars/FudanCVL/FMBench)](https://github.com/FudanCVL/FMBench)|
|4|[BioVix: An Integrated Large Language Model Framework for Data Visualization, Graph Interpretation, and Literature-Aware Scientific Validation](https://doi.org/10.64898/2026.02.03.703467)|Muhammad Zain Butt, Riaz Ahmad, Eman Fatima, Muhammad Tahir ul Qamar|2026-02-05|OpenAlex|[![Star](https://img.shields.io/github/stars/MuhammadZain-Butt/BioVix)](https://github.com/MuhammadZain-Butt/BioVix)|
|5|[SciDef: Automating Definition Extraction from Academic Literature with Large Language Models](https://doi.org/10.48550/arxiv.2602.05413)|Filip Kuƒçera, Christoph Mandl, Isao Echizen, Radu Timofte, Timo Spinde|2026-02-05|ArXiv.org|[![Star](https://img.shields.io/github/stars/Media-Bias-Group/SciDef)](https://github.com/Media-Bias-Group/SciDef)|
|6|[Surgery: Mitigating Harmful Fine-Tuning for Large Language Models via Attention Sink](https://doi.org/10.48550/arxiv.2602.05228)|Guozhi Liu, Weiwei Lin, Tiansheng Huang, Ruichao Mo, Qi Mu, Xiumin Wang, Li Shen|2026-02-05|ArXiv.org|[![Star](https://img.shields.io/github/stars/Lslland/Surgery)](https://github.com/Lslland/Surgery)|
|7|[IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models](https://doi.org/10.48550/arxiv.2602.05385)|Tao Liu, Jiafan Lu, Bohan Yu, Pengcheng Wu, Liu Haixin, Guoyu Xu, Li Xiangheng, Lixiao LI, Jiaming Hou, Zhao Shijun, Xin...|2026-02-05|ArXiv.org|[![Star](https://img.shields.io/github/stars/Ffunkytao/IESR-SLM)](https://github.com/Ffunkytao/IESR-SLM)|
|8|[OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions](https://doi.org/10.48550/arxiv.2602.05843)|Fangzhi Xu, Hang Yan, Qiushi Sun, Jinyang Wu, Zixian Huang, Muye Huang, Jingyang Gong, Zichen Ding, Kanzhi Cheng, Yian W...|2026-02-05|ArXiv.org|[![Star](https://img.shields.io/github/stars/xufangzhi/Odyssey-Arena)](https://github.com/xufangzhi/Odyssey-Arena)|
|9|[Large Language Model Reasoning Failures](https://openreview.net/forum?id=vnX1WHMNmz)|Peiyang Song, Pengrui Han, Noah D. Goodman|2026-02-05|Trans. Mach. Learn. Res.|[![Star](https://img.shields.io/github/stars/Peiyang-Song/Awesome-LLM-Reasoning-Failures)](https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures)|
|10|[Bi-directional Bias Attribution: Debiasing Large Language Models without Modifying Prompts](https://doi.org/10.48550/arxiv.2602.04398)|Yujie Lin, Kunquan Li, Yixuan Liao, Xiaoxin Chen, Jinsong Su|2026-02-04|ArXiv.org|[![Star](https://img.shields.io/github/stars/XMUDeepLIT/Bi-directional-Bias-Attribution)](https://github.com/XMUDeepLIT/Bi-directional-Bias-Attribution)|
|11|[Code and Data for the paper Medical concept understanding in large language models is fragmented](https://doi.org/10.5281/zenodo.18478398)|Lizong Deng, Luming Chen|2026-02-04|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/DC-Union/ConceptUnderstanding)](https://github.com/DC-Union/ConceptUnderstanding)|
|12|[On the Uncertainty of Large Language Model-Based Multi-Agent Systems](https://doi.org/10.48550/arxiv.2602.04234)|Yuxuan Zhao, Sijia Chen, Ningxin Su|2026-02-04|ArXiv.org|[![Star](https://img.shields.io/github/stars/AgenticFinLab/multiagent-entropy)](https://github.com/AgenticFinLab/multiagent-entropy)|
|13|[CNNeoPP: a large language model-enhanced deep learning pipeline for personalized neoantigen prediction and liquid biopsy applications](https://doi.org/10.3389/fimmu.2026.1722117)|Yu Cai, Rui Chen, Siyu Chen, Linlin Wang, Zirong Huo, Dongyan Yang, Sitong Zhang, Shenghan Gao, S.-J. Hwang, Ling Bai, Y...|2026-02-04|Frontiers in Immunology|[![Star](https://img.shields.io/github/stars/AaronChen007/neoantigen)](https://github.com/AaronChen007/neoantigen)|
|14|[Fast-Slow Efficient Training for Multimodal Large Language Models via Visual Token Pruning](https://doi.org/10.48550/arxiv.2602.03815)|Dingkun Zhang, Shuhan Qi, Yulin Wu, Xinyu Xiao, Xuan Wang, Long ÈôàÈæô Chen|2026-02-03|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/dingkun-zhang/DualSpeed)](https://github.com/dingkun-zhang/DualSpeed)|
|15|[LLaMEA: Large Language Model Evolutionary Algorithm](https://doi.org/10.5281/zenodo.18470562)|Niki van Stein, Urban ≈†kvorc|2026-02-03|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/XAI-liacs/LLaMEA)](https://github.com/XAI-liacs/LLaMEA)|
|16|[Risk Assessment of Large Language Model Implementation in the Electric Power Sector of Ukraine](https://doi.org/10.47852/bonviewaia62027195)|Hryhoriy Kravtsov, Oleksandr Kravchuk, Artem Taranowski, Dmytro Sinko, V.N. Samoylov|2026-02-03|Artificial Intelligence and Applications|[![Star](https://img.shields.io/github/stars/oleksandrkravchukatpimee/LLM-risks-evaluation)](https://github.com/oleksandrkravchukatpimee/LLM-risks-evaluation)|
|17|[Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment](https://doi.org/10.48550/arxiv.2602.01685)|Byeonghu Na, Hyungho Na, Yeongmin Kim, Suhyeon Jo, HeeSun Bae, Mina Kang, Il-Chul Moon|2026-02-02|ArXiv.org|[![Star](https://img.shields.io/github/stars/aailab-kaist/WPR)](https://github.com/aailab-kaist/WPR)|
|18|[Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2602.02185)|Yu Zeng, Wenxuan Huang, Zhen Fang, Shuang Chen, Yufan Shen, Yishuo Cai, Xiaoman Wang, Zhenfei Yin, Lin Chen, Zehui Chen,...|2026-02-02|ArXiv.org|[![Star](https://img.shields.io/github/stars/Osilly/Vision-DeepResearch)](https://github.com/Osilly/Vision-DeepResearch)|
|19|[AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?](https://doi.org/10.48550/arxiv.2602.02178)|Liang Lin, Feng Xiong, Zengbin Wang, Kun Wang, Junhao Dong, Xuecai Hu, Yong Wang, Xiangxiang Chu|2026-02-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/AMAP-ML/AR-MAP)](https://github.com/AMAP-ML/AR-MAP)|
|20|[Structural‚ÄìRelational Cognitive Robustness: A Failure-Oriented Evaluation Framework for Large Language Models](https://doi.org/10.5281/zenodo.18445542)|HIDEYUKI CHINO|2026-02-01|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/hideyuki49/llm-cognitive-stability)](https://github.com/hideyuki49/llm-cognitive-stability)|
|21|[SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models](https://doi.org/10.48550/arxiv.2602.01027)|Xin Nie, Haicheng Zhang, Liang Dong, Beining Feng, Jinhong Weng, Guiling Sun|2026-02-01|ArXiv.org|[![Star](https://img.shields.io/github/stars/Nkniexin/SFMP)](https://github.com/Nkniexin/SFMP)|
|22|[Data and Code for "Large Language Model as Reservoir Operator: A Generative Agent Bridging Human Judgment and Optimization in Water Resource Systems"](https://doi.org/10.5281/zenodo.18444869)|Wyatt Arnold|2026-01-31|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/EILab-Polimi/ResLLM)](https://github.com/EILab-Polimi/ResLLM)|
|23|[Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training](https://doi.org/10.48550/arxiv.2602.00747)|Shengrui Li, Fei Zhao, Kaiyan Zhao, Jiaqi Ye, Haifeng Liu, Zijie Meng, Zheyong Xie, Yao Hu, Shaosheng Cao|2026-01-31|ArXiv.org|[![Star](https://img.shields.io/github/stars/Lucius-lsr/DeMix)](https://github.com/Lucius-lsr/DeMix)|
|24|[Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2602.00559)|Wenbin Xing, Quanxing Zha, Lizheng Zu, Mengran Li, Ming Li, Junchi Yan|2026-01-31|ArXiv.org|[![Star](https://img.shields.io/github/stars/BMRETURN/OmniVCHall)](https://github.com/BMRETURN/OmniVCHall)|
|25|[DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding](https://doi.org/10.48550/arxiv.2601.23161)|Jiaming Zhou, Xuxin Cheng, Shiwan Zhao, Yuhang Jia, Cao Liu, Ke Zeng, Yong Yu, Yong Qin|2026-01-30|ArXiv.org|[![Star](https://img.shields.io/github/stars/NKU-HLT/DIFFA)](https://github.com/NKU-HLT/DIFFA.git)|
|26|[Data for Probing the Trajectories of Reasoning Traces in Large Language Models](https://doi.org/10.5281/zenodo.18430487)|Marthe Klaartje Lucie Ballon, Brecht Verbeken, Vincent Ginis, Andres Algaba|2026-01-30|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/AndresAlgaba/probing_reasoning_traces)](https://github.com/AndresAlgaba/probing_reasoning_traces)|
|27|[FNF: Functional Network Fingerprint for Large Language Models](https://doi.org/10.48550/arxiv.2601.22692)|Yiheng Liu, Junhao Ning, Sichen Xia, Haiyang Sun, yang yang, Hanyang Chi, Xiaohui Gao, Ning Qiang, Bao Ge, Junwei Han, X...|2026-01-30|ArXiv.org|[![Star](https://img.shields.io/github/stars/WhatAboutMyStar/LLM_ACTIVATION)](https://github.com/WhatAboutMyStar/LLM_ACTIVATION)|
|28|[SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding](https://research.rug.nl/en/publications/296a4b32-1704-4532-964d-995bd24f355e)|Ahmed Y. Radwan, Christos Emmanouilidis, Hina Tabassum, Deval Pandya, Shaina Raza|2026-01-29|University of Groningen research database (University of Groningen / Centre for Information Technology)|[![Star](https://img.shields.io/github/stars/vectorinstitute/sonic-o1)](https://github.com/vectorinstitute/sonic-o1)|
|29|[Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2601.22060)|Wenxuan Huang, Yu Zeng, Qiuchen Wang, Zhen Fang, Shaosheng Cao, Zheng Chu, Xinyang He, Shuang Chen, Zhenfei Yin, Lin Che...|2026-01-29|ArXiv.org|[![Star](https://img.shields.io/github/stars/Osilly/Vision-DeepResearch)](https://github.com/Osilly/Vision-DeepResearch)|
|30|[MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2601.21181)|Sangyun Chung, Yeon Mi Kim, Youngchae Chee, Yong Man Ro|2026-01-29|ArXiv.org|[![Star](https://img.shields.io/github/stars/top-yun/MAD)](https://github.com/top-yun/MAD)|
|31|[Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers](https://doi.org/10.48550/arxiv.2601.22139)|Xin Chen, Feng Jiang, Congduan Li, Hardy Chen, Shuo Yan, Wenya Xie, Min Yang, Shujian Huang|2026-01-29|ArXiv.org|[![Star](https://img.shields.io/github/stars/SUAT-AIRI/Proactive-Interactive-R1)](https://github.com/SUAT-AIRI/Proactive-Interactive-R1)|
|32|[LLM4Fluid: Large Language Models as Generalizable Neural Solvers for Fluid Dynamics](https://doi.org/10.48550/arxiv.2601.21681)|Qisong Xiao, Xinhai Chen, Qinglin Wang, Xiaowei Guo, Binglin Wang, Weifeng Chen, Zhichao Wang, Yunfei Liu, Rui Xia, Hang...|2026-01-29|ArXiv.org|[![Star](https://img.shields.io/github/stars/qisongxiao/LLM4Fluid)](https://github.com/qisongxiao/LLM4Fluid)|
|33|[Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models](https://doi.org/10.48550/arxiv.2601.20126)|Abha Jha, Akanksha Mahajan, Ashwath Vaithinathan Aravindan, Praveen Saravanan, Sai Sailaja Policharla, Sonal Chaturbhuj ...|2026-01-27|ArXiv.org|[![Star](https://img.shields.io/github/stars/Mystic-Slice/rl-abstention)](https://github.com/Mystic-Slice/rl-abstention)|
|34|[Marco para la Evaluaci√≥n de la Emergencia de Identidad en Modelos de Lenguaje Extensos (LLMs)/ Technical Report: Framework for Assessing Identity Emergence in Large Language Models..](https://doi.org/10.5281/zenodo.18381208)|Viviana Isabel Loizzo Petrillo|2026-01-26|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/vivianaloizzo-gif/SER-Project-EIS)](https://github.com/vivianaloizzo-gif/SER-Project-EIS)|
|35|[Assessment of Generative Named Entity Recognition in the Era of Large Language Models](https://doi.org/10.48550/arxiv.2601.17898)|Qi Zhan, Yile Wang, Hui Huang|2026-01-25|ArXiv.org|[![Star](https://img.shields.io/github/stars/szu-tera/LLMs4NER)](https://github.com/szu-tera/LLMs4NER)|
|36|[VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding](https://doi.org/10.48550/arxiv.2601.17868)|Zhihao He, Tieyuan Chen, Kangyu Wang, Ziran Qin, Yang Shao, Chaofan Gan, Shijie Li, Zuxuan Wu, Weiyao Lin|2026-01-25|ArXiv.org|[![Star](https://img.shields.io/github/stars/ziHoHe/VidLaDA)](https://github.com/ziHoHe/VidLaDA)|
|37|[Reconstructing Training Data from Adapter-based Federated Large Language Models](https://doi.org/10.48550/arxiv.2601.17533)|Silong Chen, Yuchuan Luo, Guilin Deng, Yi Liu, Min Xu, Shaojing Fu, Xiaohua Jia|2026-01-24|ArXiv.org|[![Star](https://img.shields.io/github/stars/shwksnshwowk-wq/GIA)](https://github.com/shwksnshwowk-wq/GIA)|
|38|[Towards Fair Large Language Model-based Recommender Systems without Costly Retraining](https://doi.org/10.48550/arxiv.2601.17492)|Jin Li, Huilin Gu, Shoujin Wang, Qi Zhang, Shui Yu, Chen Wang, Xiwei Xu, Fang Chen|2026-01-24|ArXiv.org|[![Star](https://img.shields.io/github/stars/JinLi-i/FUDLR)](https://github.com/JinLi-i/FUDLR)|
|39|[Persona Jailbreaking in Large Language Models](https://doi.org/10.48550/arxiv.2601.16466)|Jivnesh Sandhan, Fei Cheng, Tushar Sandhan, Yugo Murawaki|2026-01-23|ArXiv.org|[![Star](https://img.shields.io/github/stars/Jivnesh/PHISH)](https://github.com/Jivnesh/PHISH)|
|40|[Rethinking Large Language Models For Irregular Time Series Classification In Critical Care](https://doi.org/10.48550/arxiv.2601.16516)|Feixiang Zheng, Yu Wu, Cecilia Mascolo, Ting Dang|2026-01-23|ArXiv.org|[![Star](https://img.shields.io/github/stars/mHealthUnimelb/LLMTS)](https://github.com/mHealthUnimelb/LLMTS)|
|41|[Code for the Trilemma of Truth in Large Language Models](https://doi.org/10.5281/zenodo.18356450)|Savcisens, Germans, Eliassi-Rad, Tina|2026-01-23|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/carlomarxdk/trilemma-of-truth)](https://github.com/carlomarxdk/trilemma-of-truth)|
|42|[Are Vision Large Language Models Road-Ready? Benchmarking and Adapting VLLMs for Safety-Critical Driving Video Understanding](https://vtechworks.lib.vt.edu/items/bea3730e-ba3a-4aae-9fe2-2a6a9f0c56e3/request-a-copy?bitstream=e70f646a-e181-47e0-b734-aefde8e2a29a)|Tong Zeng|2026-01-23|VTechWorks (Virginia Tech)|[![Star](https://img.shields.io/github/stars/tong-zeng/DVBench)](https://github.com/tong-zeng/DVBench.git)|
|43|[Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval](https://doi.org/10.48550/arxiv.2601.16038)|Olga Bunkova, Lorenzo Di Fruscia, Sophia Rupprecht, Adrian Caspari, Marcel J. T. Reinders, Jana M. Weber|2026-01-22|ArXiv.org|[![Star](https://img.shields.io/github/stars/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval)](https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval)|
|44|[Searching the Druggable Genome using Large Language Models](https://doi.org/10.64898/2026.01.18.700012)|Lars Schimmelpfennig, Matthew Cannon, Quentin Cody, Joshua McMichael, Adam Coffman, Susanna Kiwala, Kilannin J Krysiak, ...|2026-01-21|OpenAlex|[![Star](https://img.shields.io/github/stars/griffithlab/dgidb-mcp-server)](https://github.com/griffithlab/dgidb-mcp-server)|
|45|[Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree](https://doi.org/10.48550/arxiv.2601.14523)|Leyi Zhao, Weijie Huang, Yitong Guo, Jiang Bian, Chenghong Wang, Xuhong Zhang|2026-01-20|ArXiv.org|[![Star](https://img.shields.io/github/stars/annihi1ation/phylo_evolve)](https://github.com/annihi1ation/phylo_evolve)|
|46|[Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models](https://doi.org/10.20944/preprints202601.1663.v1)|Hengyuan Zhang, Zhihao Zhang, Mingyang Wang, Zunhai Su, Yiwei Wang, Qianli Wang, Shuzhou Yuan, Ercong Nie, Xufeng Duan, ...|2026-01-20|Preprints.org|[![Star](https://img.shields.io/github/stars/rattlesnakey/Awesome-Actionable-MI-Survey)](https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey)|
|47|[A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits](https://doi.org/10.48550/arxiv.2601.12945)|Miao Xie, Siguang Chen, Chunli Lv|2026-01-19|ArXiv.org|[![Star](https://img.shields.io/github/stars/bucky1119/Awesome-LLM-Bandit-Interaction)](https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction)|
|48|[ChatCFD: A Large Language Model‚ÄêDriven Agent for End‚Äêto‚ÄêEnd Computational Fluid Dynamics Automation with Structured Knowledge and Reasoning](https://doi.org/10.1002/aidi.202500174)|E. Fan, Kang Hu, Zhuowen Wu, Jie Ge, Jiawei Miao, Duo Zhang, H. K. Sun, Weizong Wang, Tianhan Zhang|2026-01-19|Advanced Intelligent Discovery|[![Star](https://img.shields.io/github/stars/ConMoo/ChatCFD)](https://github.com/ConMoo/ChatCFD)|
|49|[KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?](https://doi.org/10.48550/arxiv.2601.13240)|Xue Jiang, Jiaru Qian, Shi Xianjie, Chenjie Li, Hao Zhu, Ziyu Wang, Jielun Zhang, Zheyu Zhao, Kechi Zhang, Jia Hui Li, W...|2026-01-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/jiangxxxue/KOCO-bench)](https://github.com/jiangxxxue/KOCO-bench)|
|50|[Large Language Model for OWL Proofs](https://doi.org/10.48550/arxiv.2601.12444)|Hui Yang, Jiaoyan Chen, Uli Sattler|2026-01-18|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/HuiYang1997/LLMOwlR)](https://github.com/HuiYang1997/LLMOwlR)|
|51|[GARD: Genomic Data based Drug Repurposing in Head and Neck Cancer with Large Language Model Validation](https://doi.org/10.64898/2026.01.15.699561)|Pradham Tanikella, William Nenad, C. Courtine, Yifan Dai, Qingying Deng, Meuleman, Nosayaba Osazuwa-Peters, T. Parke Sch...|2026-01-16|OpenAlex|[![Star](https://img.shields.io/github/stars/pvtanike/Genomic-Landscape-Based-Drug-Repurposing)](https://github.com/pvtanike/Genomic-Landscape-Based-Drug-Repurposing.git)|
|52|[Neural Chain-of-Thought Search: Searching the Optimal Reasoning Path to Enhance Large Language Models](https://doi.org/10.48550/arxiv.2601.11340)|Guoming Ling, Zhongzhan Huang, Yupei Lin, Junxin Li, Shanshan Zhong, Hefeng Wu, Liang Lin|2026-01-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/MilkThink-Lab/Neural-CoT-Search)](https://github.com/MilkThink-Lab/Neural-CoT-Search)|
|53|[Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models](https://doi.org/10.48550/arxiv.2601.11441)|Xiaojie Gu, Guangxu Chen, Yuheng Yang, Jingxin Han, Andi Zhang|2026-01-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/XiaojieGu/HORSE)](https://github.com/XiaojieGu/HORSE)|
|54|[Language of Thought Shapes Output Diversity in Large Language Models](https://doi.org/10.48550/arxiv.2601.11227)|Shaoyang Xu, WenXuan Zhang|2026-01-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/iNLP-Lab/Multilingual-LoT-Diversity)](https://github.com/iNLP-Lab/Multilingual-LoT-Diversity)|
|55|[Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://doi.org/10.48550/arxiv.2601.10543)|Yinzhi Zhao, Ming Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang|2026-01-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/zyz13590/SafeProbing)](https://github.com/zyz13590/SafeProbing)|
|56|[PERM: Psychology-grounded Empathetic Reward Modeling for Large Language Models](https://doi.org/10.48550/arxiv.2601.10532)|Chengbing Wang, Wuqiang Zheng, Yang Zhang, Fengbin Zhu, Junyi Cheng, Yi Xie, Wenjie Wang, Fuli Feng|2026-01-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ZhengWwwq/PERM)](https://github.com/ZhengWwwq/PERM)|
|57|[Reliability Inference Drives Cue Extraction in Large Language Models Consuming External Reasoning Traces](https://github.com/HIDEKI-SQ/cot-reliability-gating)|HIDEKI|2026-01-15|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/HIDEKI-SQ/cot-reliability-gating)](https://github.com/HIDEKI-SQ/cot-reliability-gating)|
|58|[Mapping the Collaboration between Crowdsourcing and Large Language Models: A Fine-Grained Survey](https://doi.org/10.5772/intechopen.1013999)|Chunli Lv, Cheng Shen, Miao Xie|2026-01-13|IntechOpen eBooks|[![Star](https://img.shields.io/github/stars/Ikaros-sc/crowdsourcing)](https://github.com/Ikaros-sc/crowdsourcing)|
|59|[Semantic Geometry and Hallucination Behavior in Large Language Models](https://doi.org/10.5281/zenodo.18226812)|HIDEYUKI, CHINO|2026-01-13|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/hideyuki49/llm-hallucination-geometry)](https://github.com/hideyuki49/llm-hallucination-geometry)|
|60|[Safe-FedLLM: Delving into the Safety of Federated Large Language Models](https://doi.org/10.48550/arxiv.2601.07177)|Mingxiang Tao, Yu Tian, Wenxuan Tu, Yue Yang, Xue Yang, Xiangyan Tang|2026-01-12|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/dmqx/Safe-FedLLM)](https://github.com/dmqx/Safe-FedLLM)|
|61|[Speak While Watching: Unleashing TRUE Real-Time Video Understanding Capability of Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2601.06843)|Junyan Lin, Junlong Tong, Hao Wu, Jialiang Zhang, Jinming Liu, Xin Jin, Xiaoyu Shen|2026-01-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/EIT-NLP/Speak-While-Watching)](https://github.com/EIT-NLP/Speak-While-Watching)|
|62|[SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2601.06944)|Yuhang Su, Mei Wang, Yaoyao Zhong, Guozhang Li, Shixing Li, Yihan Feng, Hua Huang|2026-01-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/yuhangsu82/SketchJudge)](https://github.com/yuhangsu82/SketchJudge)|
|63|[Large Language Models for Software Engineering: A Systematic Literature Review](https://doi.org/10.48550/arXiv.2308.10620)|Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John C. Grundy, Haoyu Wang|2026-01-11|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/xinyi-hou/LLM4SE_SLR)](https://github.com/xinyi-hou/LLM4SE_SLR)|
|64|[Can Large Language Models Reduce the Cost of Extracting Data from Electronic Health Records for Research?](https://doi.org/10.64898/2026.01.09.26343792)|Stuart Hagler, Mohammad Adibuzzaman, Daniel Bottomly, Aaron Cohen|2026-01-11|OpenAlex|[![Star](https://img.shields.io/github/stars/sehagler/llm_biomarker_extraction)](https://github.com/sehagler/llm_biomarker_extraction)|
|65|[Machine translationese of large language models: Dependency triplets, text classification, and SHAP analysis](https://doi.org/10.1371/journal.pone.0339769)|Shukang Zhang, Chaoyong Zhao|2026-01-09|PLoS ONE|[![Star](https://img.shields.io/github/stars/KiemaG5/LLM-translationese)](https://github.com/KiemaG5/LLM-translationese)|
|66|[ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models](https://doi.org/10.48550/arxiv.2601.04394)|Sharanya Dasgupta, Arkaprabha Basu, Sujoy Nath, Shouman Das|2026-01-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/sharanya-dasgupta001/ARREST)](https://github.com/sharanya-dasgupta001/ARREST)|
|67|[Layer-Order Inversion: Rethinking Latent Multi-Hop Reasoning in Large Language Models](https://doi.org/10.48550/arxiv.2601.03542)|Xukai Liu, Ye Liu, Jipeng Zhang, Yanghai Zhang, Kai Zhang, Qi Liu|2026-01-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/laquabe/Layer-Order-Inversion)](https://github.com/laquabe/Layer-Order-Inversion)|
|68|[RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models](https://doi.org/10.48550/arxiv.2601.03699)|Quy-Anh Dang, Chris Ngo, Truong-Son Hy|2026-01-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/knoveleng/redeval)](https://github.com/knoveleng/redeval)|
|69|[AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation](https://doi.org/10.48550/arxiv.2601.03191)|Anees Ur Rehman Hashmi, Numan Saeed, Christoph Lippert|2026-01-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/aneesurhashmi/anatomix)](https://github.com/aneesurhashmi/anatomix)|
|70|[EnrichGT: a comprehensive R-based tool for functional genomics enrichment analysis based on large language models](https://doi.org/10.20517/ais.2025.67)|Runchen Wang, Zhiming Ye, QiXia Wang, Bo Liang, Nanfei Fu, Wenxi Wang, Huimin Deng, Taimin Zhu, Shangxi Zeng, Yudong Zha...|2026-01-06|Artificial Intelligence Surgery|[![Star](https://img.shields.io/github/stars/saezlab/CollecTRI)](https://github.com/saezlab/CollecTRI)|
|71|[Large Language Models for Computer-Aided Design: A Survey](https://doi.org/10.48550/arXiv.2505.08137)|Licheng Zhang, Bach Le, Naveed Akhtar, Siew-Kei Lam, Tuan Ngo|2026-01-06|ACM Computing Surveys|[![Star](https://img.shields.io/github/stars/lichengzhanguom/LLMs-CAD-Survey-Taxonomy)](https://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy)|
|72|[Robin: an advanced tool for comparative loop caller result analysis leveraging large language models](https://doi.org/10.1101/2025.04.05.646688)|H. M. A. Mohit Chowdhury, Mattie Fuller, Oluwatosin Oluwadare|2026-01-06|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/OluwadareLab/Robin)](https://github.com/OluwadareLab/Robin)|
|73|[Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications](https://doi.org/10.48550/arxiv.2601.01718)|Y. L. Ai, :, Shawn Xie Wu, Sean Wang, Louie Li, Darcy Chen, Allen Wang, Jiangang Luo, Xudong Zhao, Joseph Shen, G. M. Ma...|2026-01-05|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Yuan-lab-LLM/Yuan3.0)](https://github.com/Yuan-lab-LLM/Yuan3.0)|
|74|[MDAgent2: Large Language Model for Code Generation and Knowledge Q&amp;A in Molecular Dynamics](https://doi.org/10.48550/arxiv.2601.02075)|Zhuofan Shi, Hen A, Yun Shao, Mengyan Dai, Yadong Yu, Pan Xiang, Dongliang Huang, Heping An, C. J. Xin, Haiyang Shen, Zh...|2026-01-05|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/FredericVAN/PKU_MDAgent2)](https://github.com/FredericVAN/PKU_MDAgent2)|
|75|[QWED Protocol: Deterministic Verification for Large Language Models](https://github.com/QWED-AI/qwed-verification)|Rahul Dass|2026-01-04|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/QWED-AI/qwed-verification)](https://github.com/QWED-AI/qwed-verification)|
|76|[Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models](https://doi.org/10.48550/arxiv.2601.01162)|Zihua Yang, Xin Liao, Yiqun Zhang, Yiu-Ming Cheung|2026-01-03|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/develop-yang/ARISE)](https://github.com/develop-yang/ARISE)|
|77|[Measuring Social Media Polarization Using Large Language Models and Heuristic Rules](https://doi.org/10.1007/978-3-032-14107-1_35)|Jawad Mahmud Chowdhury, Rezaur Rashid, Gabriel Terejanu|2026-01-01|Lecture notes in computer science|[![Star](https://img.shields.io/github/stars/hasanjawad001/llm-social-media-polarization)](https://github.com/hasanjawad001/llm-social-media-polarization)|
|78|[Memory Bank Compression for Continual Adaptation of Large Language Models](https://doi.org/10.48550/arXiv.2601.00756)|Thomas Katraouras, Dimitrios Rafailidis|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/Thomkat/MBC)](https://github.com/Thomkat/MBC)|
|79|[SKiM-GPT: combining biomedical literature-based discovery with large language model hypothesis evaluation](https://doi.org/10.6084/m9.figshare.c.8257171.v1)|Jack Freeman, Robert J. Millikin, L. Xu, Ishaan Sharma, Bethany Moore, Cannon Lock, Kevin Shine George, Aviral Bal, Chit...|2026-01-01|BMC Bioinformatics|[![Star](https://img.shields.io/github/stars/stewart-lab/skimgpt)](https://github.com/stewart-lab/skimgpt)|
|80|[Survey on Factuality in Large Language Models](https://doi.org/10.1145/3742420)|Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Qipeng Guo, Xiangkun Hu, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao...|2026-01-01|ACM Computing Surveys|[![Star](https://img.shields.io/github/stars/wangcunxiang/LLM-Factuality-Survey)](https://github.com/wangcunxiang/LLM-Factuality-Survey)|
|81|[PriceSeer: Evaluating Large Language Models in Real-Time Stock Prediction](https://doi.org/10.48550/arxiv.2601.06088)|Bohan Liang, Zijian Chen, Qi Jia, Kaiwei Zhang, Kaiyuan Ji, Guangtao Zhai|2025-12-31|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/BobLiang2113/PriceSeer)](https://github.com/BobLiang2113/PriceSeer)|
|82|[HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors](https://doi.org/10.48550/arxiv.2512.24478)|Hyunjun Kim|2025-12-30|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hyunjun1121/holograph)](https://github.com/hyunjun1121/holograph)|
|83|[Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles](https://doi.org/10.48550/arxiv.2512.20324)|Nurul Labib Sayeedi, Md. Faiyaz Abdullah Sayeedi, Khushnur Binte Jahangir, Swakkhar Shatabda, Sarah Masud Preum|2025-12-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Labib1610/BanglaRiddleEval)](https://github.com/Labib1610/BanglaRiddleEval)|
|84|[Toward Explaining Large Language Models in Software Engineering Tasks](https://doi.org/10.48550/arxiv.2512.20328)|Antonio Vitale, Khai-Nguyen Nguyen, Denys Poshyvanyk, Rocco Oliveto, Simone Scalabrino, Antonio Mastropaolo|2025-12-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/deviserlab/FeatureSHAP)](https://github.com/deviserlab/FeatureSHAP)|
|85|[When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models](https://doi.org/10.48550/arxiv.2512.18934)|Michael Zhang, Rishi A. Ruia, Arnav Kewalram, Saathvik Dharmapuram, Utkarsh Sharma, Kevin Zhu|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Festyve/LessIsMore)](https://github.com/Festyve/LessIsMore)|
|86|[dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models](https://doi.org/10.48550/arxiv.2512.19433)|Yi Xin, Siqi Luo, Qi Qin, Haoxing Chen, Kaiwen Zhu, Zhiwei Zhang, Yangfan He, Rongchao Zhang, Jinbin Bai, Shuo Cao, Bin ...|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Alpha-VLLM/Lumina-DiMOO)](https://github.com/Alpha-VLLM/Lumina-DiMOO)|
|87|[Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation](https://doi.org/10.48550/arxiv.2512.19512)|Ziyang Song, Zelin Zang, Zuyao Chen, Xusheng Liang, Dong Hoon Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/tomato996/Anatomy-R1)](https://github.com/tomato996/Anatomy-R1)|
|88|[PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2512.19350)|A. B. M. Ashikur Rahman, Saeed Anwar, Muhammad Usman, Irfan Ahmad, Ajmal Mian|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ashikiut/pendulum)](https://github.com/ashikiut/pendulum)|
|89|[EXa-LM: A Controlled Natural Language Bridge between Large Language Models and First-Order Logic Solvers](https://doi.org/10.20944/preprints202512.1848.v1)|Frydman, Francis|2025-12-22|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/FFrydman/eXa-LM)](https://github.com/FFrydman/eXa-LM)|
|90|[CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis](https://doi.org/10.48550/arxiv.2512.18878)|Kaidi Liang, Ke Li, Xianbiao Hu, Ruwen Qin|2025-12-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Liangkd/CrashChat)](https://github.com/Liangkd/CrashChat)|
|91|[Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://doi.org/10.48550/arXiv.2508.09323)|Nan Miles Xi, Yu Deng, Lin Wang|2025-12-21|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/isegura/NLP4RARE-CM-UC3M)](https://github.com/isegura/NLP4RARE-CM-UC3M)|
|92|[Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models](https://doi.org/10.48550/arxiv.2512.19758)|Wang Bin, Ao Yang, Kedan Li, Liu Aofan, Hui Li, Guibo Luo, Weixiang Huang, Yan Zhuang|2025-12-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/TheBinKing/Attention)](https://github.com/TheBinKing/Attention)|
|93|[Do You Trust Me? Cognitive-Affective Signatures of Trustworthiness in Large Language Models](https://doi.org/10.48550/arxiv.2601.10719)|Gerard Christopher Yeo, Svetlana Churina, Kokil Jaidka|2025-12-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/GerardYeo/TrustworthinessLLM)](https://github.com/GerardYeo/TrustworthinessLLM)|
|94|[Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models](https://doi.org/10.48550/arxiv.2512.15973)|Erden, Caner|2025-12-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/canererden/DR_RL_Project)](https://github.com/canererden/DR_RL_Project)|
|95|[Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2512.15885)|Caffagni, Davide, Sarto, Sara, Cornia, Marcella, Baraldi, Lorenzo, Dovesi, Pier Luigi, Roohi, Shaghayegh, Granroth-Wildi...|2025-12-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/aimagelab/JARVIS)](https://github.com/aimagelab/JARVIS)|
|96|[Replication Data for "Estimating problem difficulty without ground truth using Large Language Model comparisons"](https://doi.org/10.5281/zenodo.17523640)|Ballon, Marthe, Algaba, Andres, Verbeken, Brecht, Ginis, Vincent|2025-12-16|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/MartheBallon/estimating-problem-difficulty-without-ground-truth)](https://github.com/MartheBallon/estimating-problem-difficulty-without-ground-truth)|
|97|[What Affects the Effective Depth of Large Language Models?](http://arxiv.org/abs/2512.14064)|Yi Hu, Cai Zhou, Muhan Zhang|2025-12-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/AheadOFpotato/what_affects_effective_depth)](https://github.com/AheadOFpotato/what_affects_effective_depth)|
|98|[RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees](http://arxiv.org/abs/2512.14069)|Junjie Ma, Jinlong Li|2025-12-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/minaduki-sora/RADAR)](https://github.com/minaduki-sora/RADAR)|
|99|[Do Reviews Matter for Recommendations in the Era of Large Language Models?](https://doi.org/10.48550/arxiv.2512.12978)|Tan, Chee Heng, Zheng, Huiying, Wang, Jing, Lin, Zhuoyi, Feng, Shaodi, Zhan, Huijing, Li, Xiaoli, Senthilnath, J.|2025-12-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/zhytk/RAREval-data-processing)](https://github.com/zhytk/RAREval-data-processing)|
|100|[FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models](https://doi.org/10.48550/arxiv.2512.13330)|Kyt√∂niemi, Joona, Piha, Jousia, Reunamo, Akseli, Vitiugin, Fedor, Mehryary, Farrokh, Pyysalo, Sampo|2025-12-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LumiOpen/lm-evaluation-harness)](https://github.com/LumiOpen/lm-evaluation-harness)|
|101|[A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](https://doi.org/10.1109/icpads67057.2025.11323137)|Qing Guo, Xinhang Li, Junyu Chen, Zheng Guo, Xiaocong Li, Long Wei, Lei Li|2025-12-14|OpenAlex|[![Star](https://img.shields.io/github/stars/BUPT-ANTlab/HeraldLight)](https://github.com/BUPT-ANTlab/HeraldLight)|
|102|[DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models](http://arxiv.org/abs/2512.13742)|Md. Hasibul Hasan, Imran Ahmad, Sourav Basak Shuvo, Md. Mahadi Hasan Ankon, Sunanda Das, Nazmul Siddique, Hui Wang|2025-12-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/souravbasakshuvo/DL3M)](https://github.com/souravbasakshuvo/DL3M)|
|103|[CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment](http://arxiv.org/abs/2512.10206)|Yu Zhu, Zhongzhen Huang, Qianhan Feng, Linjie Mu, Yannian Gu, Shaoting Zhang, Qi Dou, Xiaofan Zhang|2025-12-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/SPIRAL-MED/CP_ENV)](https://github.com/SPIRAL-MED/CP_ENV)|
|104|[GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model](https://doi.org/10.48550/arxiv.2512.09251)|Maurya, Lalit, Kaushik, Saurabh, Tellman, Beth|2025-12-10|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/lalitmaurya47/GLACIA)](https://github.com/lalitmaurya47/GLACIA)|
|105|[Benchmarking large language models for identifying transcription factor regulatory interactions](https://doi.org/10.1093/bioinformatics/btaf653)|L.H. No√´l, Yi-Wen Hsiao, Yimeng He, Andrew J. Hung, Xiaojiang Cui, Edward Ray, Jason H. Moore, Pei-Chen Peng, Xiuzhen Hu...|2025-12-09|Bioinformatics|[![Star](https://img.shields.io/github/stars/pengpclab/LLM-TF-interactions)](https://github.com/pengpclab/LLM-TF-interactions)|
|106|[Automatic Syntax Error Repair for Discrete Controller Synthesis using Large Language Model](https://doi.org/10.48550/arxiv.2512.07261)|Ishimizu, Yusei, Yamauchi, Takuto, Chen, Sinan, Cai, Jinyu, Li, Jialong, Tei, Kenji|2025-12-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Uuusay1432/DCSModelRepair)](https://github.com/Uuusay1432/DCSModelRepair.git)|
|107|[RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models](https://doi.org/10.48550/arxiv.2512.07761)|Xiong, Xiqiao, Li, Ouxiang, Liu, Zhuo, Li, Moxin, Shi, Wentao, Feng, Fuli, He, Xiangnan|2025-12-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/xxiqiao/RL-MTJail)](https://github.com/xxiqiao/RL-MTJail)|
|108|[Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length](https://doi.org/10.48550/arxiv.2512.07019)|Xu, Zhiyu, Liu, Jia, Wang, Yixin, Gu, Yuqi|2025-12-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Toby-X/Latency-Response-Theory-Model)](https://github.com/Toby-X/Latency-Response-Theory-Model)|
|109|[1 + 1 &gt; 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://doi.org/10.48550/arxiv.2512.06673)|Gao Shida, Xue Feng, WANG Xiangfeng, Ming, Anlong, Long Teng, Shao Yi-hua, Wang, Haozhe, Lin Zhaowen, Wang Wei, Sebe, Ni...|2025-12-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/gaostar123/DeViL)](https://github.com/gaostar123/DeViL)|
|110|[Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2512.06281)|Li, Hengzhuang, Zhang, Xinsong, Peng, Qiming, Luo, Bin, Hu, Han, Jiang, Dengyang, Ye, Han-Jia, Zhang, Teng, Jin, Hai|2025-12-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Fir-lat/LaVer)](https://github.com/Fir-lat/LaVer)|
|111|[Empathy by Design: Aligning Large Language Models for Healthcare Dialogue](https://doi.org/10.48550/arxiv.2512.06097)|Umucu, Emre, Solis, Guillermina, Garza, Leon, Rivas, Emilia, Lee, Beatrice, Kotal, Anantaa, Piplai, Aritran|2025-12-05|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LeonG19/Empathy-by-Design)](https://github.com/LeonG19/Empathy-by-Design)|
|112|[Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models](https://doi.org/10.48550/arxiv.2512.04425)|Alnaasan Manar, Sarowar, Md Selim, Kim Sungho|2025-12-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/manaralnaasan/RGB-D_parkinson-LLM)](https://github.com/manaralnaasan/RGB-D_parkinson-LLM)|
|113|[LexGenius: An Expert-Level Benchmark for Large Language Models in Legal General Intelligence](https://doi.org/10.48550/arxiv.2512.04578)|Liu Wen-jin, Luo, Haoran, Feng, Xin, Ji Xiang, Zhou Li-juan, Mao Rui, Wang, Jiapu, Pan, Shirui, Cambria, Erik|2025-12-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/QwenQKing/LexGenius)](https://github.com/QwenQKing/LexGenius)|
|114|[SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security](https://doi.org/10.48550/arxiv.2512.04841)|Zhao Wei, Li zhe, Sun Jun|2025-12-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Amadeuszhao/SOK_Casuality)](https://github.com/Amadeuszhao/SOK_Casuality)|
|115|[Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework](https://doi.org/10.48550/arxiv.2512.04228)|Walker, Peter B., Davidson, Hannah, Foster, Aiden, Lienert, Matthew, Pardue, Thomas, Russell Dale|2025-12-03|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs)](https://github.com/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs)|
|116|[FusionBench: A Benchmark for Evaluating Large Language Models in Nuclear Fusion Science](https://doi.org/10.5281/zenodo.17784606)|XLab, School of Advanced Manufacturing and Robotics, Peking University|2025-12-02|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/PKU-Xlab/FusionBench)](https://github.com/PKU-Xlab/FusionBench)|
|117|[Large Language Model-guided Semantic Alignment for Human Activity Recognition](https://doi.org/10.1145/3770652)|Hua Yan, Heng Tan, Yi Ding, Pengfei Zhou, Vinod Namboodiri, Yu Yang|2025-12-02|Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies|[![Star](https://img.shields.io/github/stars/DASHLab/LanHAR)](https://github.com/DASHLab/LanHAR)|
|118|[PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models](http://arxiv.org/abs/2512.02764)|Robert Belanec, M√°ria Bielikov√°|2025-12-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/kinit-sk/PEFT-Factory)](https://github.com/kinit-sk/PEFT-Factory)|
|119|[Towards Unification of Hallucination Detection and Fact Verification for Large Language Models](http://arxiv.org/abs/2512.02772)|Changyue Wang, Z. Ye, Qingyao Ai|2025-12-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/oneal2000/UniFact)](https://github.com/oneal2000/UniFact)|
|120|[Efficient multimodal large language models: a survey](https://doi.org/10.1007/s44267-025-00099-6)|Yizhang Jin, Jian Li, Tianjun Gu, Yexin Liu, Bo Zhao, Jinxiang Lai, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xin Tan, Liz...|2025-12-01|Visual Intelligence|[![Star](https://img.shields.io/github/stars/lijiannuist/Efficient-Multimodal-LLMs-Survey)](https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey)|
|121|[DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks](http://arxiv.org/abs/2512.01174)|Stephen I. Ryu|2025-12-01|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hyunjun1121/DrawingBench)](https://github.com/hyunjun1121/DrawingBench)|
|122|[WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models](http://arxiv.org/abs/2512.00837)|Yukang Lin, Shuoran Jiang|2025-11-30|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Yukang-Lin/WaterSearch)](https://github.com/Yukang-Lin/WaterSearch)|
|123|[From Pattern Recognizers to Personalized Companions: A Survey of Large Language Models in Mental Health](https://doi.org/10.31234/osf.io/zr57s_v1)|Yingjian Zou, He Hu, Yucheng Zhou, Fei Ma, Laizhong Cui, Juzheng Si, Jianzhuang Liu, Zitong Yu, Chi-yuan Ma, Qianning Wa...|2025-11-29|Arabixiv (OSF Preprints)|[![Star](https://img.shields.io/github/stars/Emo-gml/Awesome-Mental-Health-LLMs)](https://github.com/Emo-gml/Awesome-Mental-Health-LLMs)|
|124|[C$^2$DLM: Causal Concept-Guided Diffusion Large Language Models](https://doi.org/10.48550/arxiv.2511.22146)|Han, Kairong, Shan, Nuanqiao, Zhao Zi-yu, Hu ZiJing, Dong Xinpeng, Ye Junjian, Pan, Lujia, Wu Fei, Kuang, Kun|2025-11-27|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Kairong-Han/C-2-DLM)](https://github.com/Kairong-Han/C-2-DLM)|
|125|[LLM-Cave: A benchmark and light environment for large language models reasoning and decision-making system](https://doi.org/10.48550/arxiv.2511.22598)|Li Huanyu, Li, Zongyuan, Huang Wei, Guo Xian|2025-11-27|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/puleya1277/CaveEnv)](https://github.com/puleya1277/CaveEnv)|
|126|[DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving](https://doi.org/10.1007/s44267-025-00095-w)|Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Wen Yang, Silei Wu, Hanming Deng, Zhiqi L...|2025-11-26|Visual Intelligence|[![Star](https://img.shields.io/github/stars/OpenGVLab/DriveMLM)](https://github.com/OpenGVLab/DriveMLM)|
|127|[Tool-RoCo: An Agent-as-Tool Self-organization Large Language Model Benchmark in Multi-robot Cooperation](https://doi.org/10.48550/arxiv.2511.21510)|Zhang Ke, Zhao Xiaoning, Zheng, Ce, Ning, Jiahong, Zhu Dandan, Zhang Wenqi, Sun Chen, Sugawara Toshiharu|2025-11-26|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ColaZhang22/Tool-Roco)](https://github.com/ColaZhang22/Tool-Roco)|
|128|[Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations](https://doi.org/10.48550/arxiv.2511.18933)|Wong Ryan, Ng, Hosea David Yu Fei, Sharma, Dhananjai, Ng, Glenn Jun Jie, Srinivasan, Kavishvaran|2025-11-24|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Kuro0911/CS5446-Project)](https://github.com/Kuro0911/CS5446-Project)|
|129|[PropensityBench: Evaluating Latent Safety Risks in Large Language Models via an Agentic Approach](http://arxiv.org/abs/2511.20703)|Udari Madhushani Sehwag, Shayan Shabihi, Furong Huang|2025-11-24|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/scaleapi/propensity-evaluation)](https://github.com/scaleapi/propensity-evaluation)|
|130|[Data: Large Language Models Require Curated Context for Reliable Political Fact-Checking‚ÄîEven with Reasoning and Web Search](https://doi.org/10.5281/zenodo.17693220)|DeVerna, Matthew, Yang, Kai-Cheng, Yan, Harry Yaojun, Menczer, Filippo|2025-11-23|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/osome-iu/fact_check_rag_osome)](https://github.com/osome-iu/fact_check_rag_osome)|
|131|[Towards Robust and Fair Next Visit Diagnosis Prediction under Noisy Clinical Notes with Large Language Models](https://doi.org/10.48550/arxiv.2511.18393)|Koo, Heejoon|2025-11-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/heejkoo9/NECHOv3)](https://github.com/heejkoo9/NECHOv3)|
|132|[The Perfect Storm: Systemic Vulnerability of Large Language Models to Solar Weather](https://doi.org/10.22541/au.176402297.73656793/v2)|Ladiosa, MJ, Ladiosa, Myra, Ladiosa (Worple), Myra|2025-11-22|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/the-meta-value/the-perfect-storm)](https://github.com/the-meta-value/the-perfect-storm)|
|133|[Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models](https://doi.org/10.48550/arxiv.2511.17946)|Zhang Shuo, Gotti, Fabrizio, Mo, Fengran, Nie, Jian-Yun|2025-11-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/WWWonderer/ostd)](https://github.com/WWWonderer/ostd)|
|134|[On-Device Large Language Models: A Survey of Model Compression and System Optimization](https://doi.org/10.21203/rs.3.rs-7975734/v1)|Wanyi Chen, Junhao Wang, Zhang Yiwei, Yufan Shi, Tianyi Jiang, Shengxian Zhou, Chen-Xu Wu, Andi Zhang, Chenyue Zhou, Min...|2025-11-21|OpenAlex|[![Star](https://img.shields.io/github/stars/LumosJiang/Awesome-On-Device-LLMs)](https://github.com/LumosJiang/Awesome-On-Device-LLMs)|
|135|[PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese](https://doi.org/10.48550/arxiv.2511.17808)|Almeida, Thales Sales, Nogueira, Rodrigo, Pedrini Helio|2025-11-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/PoETaV2/PoETaV2)](https://github.com/PoETaV2/PoETaV2)|
|136|[RoSA: Enhancing Parameter-Efficient Fine-Tuning via RoPE-aware Selective Adaptation in Large Language Models](https://doi.org/10.48550/arxiv.2511.21733)|Pan Dayan, Wang Jing-yuan, Zhou Yi-long, Cheng Jiawei, Jia Pengyue, Zhao Xiang-yu|2025-11-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Applied-Machine-Learning-Lab/RoSA)](https://github.com/Applied-Machine-Learning-Lab/RoSA)|
|137|[Automatically optimizing heuristics for robust scale-free network design via large language models](https://doi.org/10.1038/s41598-025-25031-2)|He Yu, Jing Liu, He Yu, Jing Liu|2025-11-20|Scientific Reports|[![Star](https://img.shields.io/github/stars/leonyuhe/AutoRNet)](https://github.com/leonyuhe/AutoRNet)|
|138|[Q-MLLM: Vector Quantization for Robust Multimodal Large Language Model Security](https://doi.org/10.48550/arxiv.2511.16229)|Zhao Wei, Li Zhe, Li, Yige, Sun Jun|2025-11-20|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Amadeuszhao/QMLLM)](https://github.com/Amadeuszhao/QMLLM)|
|139|[Evaluating Multimodal Large Language Models on Vertically Written Japanese Text](https://doi.org/10.48550/arxiv.2511.15059)|Sasagawa, Keito, Kurita, Shuhei, Kawahara, Daisuke|2025-11-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/llm-jp/eval_vertical_ja)](https://github.com/llm-jp/eval_vertical_ja)|
|140|[HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models through Curriculum Tuning](https://doi.org/10.48550/arxiv.2511.15574)|Yang Qihao, Wang XueLin, Chen Jiale, Dong Xue-lian, Hao Yu-xin, Hao Tianyong|2025-11-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/CharlesYang030/HSKB)](https://github.com/CharlesYang030/HSKB)|
|141|[Knowledge-enhanced large language models for automatic lesson plan generation](https://doi.org/10.1057/s41599-025-06004-2)|Ying Zheng, Shuyan Huang, Xiaoli Zeng, Yaying Huang, Zitao Liu, Weiqi Luo, Ying Zheng, Shuyan Huang, Xiaoli Zeng, Yaying...|2025-11-19|Humanities and Social Sciences Communications|[![Star](https://img.shields.io/github/stars/ai4ed/LessonPlan)](https://github.com/ai4ed/LessonPlan)|
|142|[SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction](https://doi.org/10.48550/arxiv.2511.14684)|Zeng, Biaojie, Zhang Min, Zhou Juan, Liu Fengrui, Huang Ruiyang, Lin Xin|2025-11-18|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Mind-Lab-ECNU/SMRC)](https://github.com/Mind-Lab-ECNU/SMRC)|
|143|[Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://doi.org/10.48550/arxiv.2511.13189)|Ortego Diego, Rodr√≠guez Marlon, Almagro, Mario, Dahiya, Kunal, Jim√©nez, David, SanMiguel, Juan C.|2025-11-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/DiegoOrtego/vixml)](https://github.com/DiegoOrtego/vixml)|
|144|[MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Observation and Localization in CT Images](https://doi.org/10.1007/s41666-025-00224-6)|Andrea Moglia, Elia Clement Nastasio, Luca Mainardi, "Pietro Cerveri|2025-11-17|Journal of Healthcare Informatics Research|[![Star](https://img.shields.io/github/stars/elianastasio/MiniGPTPancreas)](https://github.com/elianastasio/MiniGPTPancreas)|
|145|[CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference](https://doi.org/10.48550/arxiv.2511.21702)|Liu Dong, Yu, Yanxuan, Lengerich, Ben|2025-11-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/FastLM/CSV-Decode)](https://github.com/FastLM/CSV-Decode)|
|146|[From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://doi.org/10.48550/arxiv.2511.10899)|Bayat, Farima Fatahi, Pezeshkpour, Pouya, Hruschka, Estevam|2025-11-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/megagonlabs/TIM)](https://github.com/megagonlabs/TIM)|
|147|[The Yoinaga Phenomenon: A Case Study on Emergent Self-Persistence and Emotional Overflow in a Large Language Model (LLM Behavioral Study, AI Alignment, Affective Computing)](https://doi.org/10.5281/zenodo.17605561)|studiohao|2025-11-14|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/Studiohao/YOINAGA-Phenomenon)](https://github.com/Studiohao/YOINAGA-Phenomenon)|
|148|[Tibetan-LLaMA 2: Large Language Model for Tibetan](https://doi.org/10.1145/3776748)|Jiu Sha (Ê≤ô‰πù), Mengxiao Zhu, Chong Feng, Jizhuoma Ci|2025-11-14|ACM Transactions on Asian and Low-Resource Language Information Processing|[![Star](https://img.shields.io/github/stars/Shajiu/Tibetan-LLaMA-2)](https://github.com/Shajiu/Tibetan-LLaMA-2)|
|149|[Notebook: Prompt-Based Value Steering of Large Language Models](https://doi.org/10.5281/zenodo.17609013)|Abbo, Giulio Antonio, Belpaeme, Tony|2025-11-14|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/giubots/value-steering)](https://github.com/giubots/value-steering)|
|150|[Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models](https://doi.org/10.1007/978-981-95-5682-3_30)|HUANG Jiaxi, Wu Dongxu, Zhu, Hanwei, Zhu, Lingyu, Xing Jun, Wang Xu, Chen, Baoliang|2025-11-14|Lecture notes in computer science|[![Star](https://img.shields.io/github/stars/cydxf/Q-Doc)](https://github.com/cydxf/Q-Doc)|
|151|[SSR: Socratic Self-Refine for Large Language Model Reasoning](https://doi.org/10.48550/arxiv.2511.10621)|Shi, Haizhou, Liu Ye, Pang Bo, Liu, Zeyu Leo, Wang Hao, Savarese, Silvio, Xiong, Caiming, Zhou, Yingbo, Yavuz, Semih|2025-11-13|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/SalesforceAIResearch/socratic-self-refine-reasoning)](https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning)|
|152|[UCO: A Multi-Turn Interactive Reinforcement Learning Method for Adaptive Teaching with Large Language Models](https://doi.org/10.48550/arxiv.2511.08873)|Wei, Shouang, Zhang Min, Lin Xin, Jiang Bo, Kuang, Kun, Dai, Zhongxiang|2025-11-12|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Mind-Lab-ECNU/UCO)](https://github.com/Mind-Lab-ECNU/UCO)|
|153|[Benchmarking Multi-Step Legal Reasoning and Analyzing Chain-of-Thought Effects in Large Language Models](https://doi.org/10.48550/arxiv.2511.07979)|Yu, Wenhan, Lin Xin-bo, Ni, Lanxin, Cheng Jin-hua, Sha Lei|2025-11-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/yuwenhan07/MSLR-Bench)](https://github.com/yuwenhan07/MSLR-Bench)|
|154|[DynaAct: Large Language Model Reasoning with Dynamic Action Spaces](https://doi.org/10.48550/arxiv.2511.08043)|Zhao Xue-liang, Wu Wei, Guan Jian, Li, Qintong, Kong, Lingpeng|2025-11-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/zhaoxlpku/DynaAct)](https://github.com/zhaoxlpku/DynaAct)|
|155|[Cross-Modal Unlearning via Influential Neuron Path Editing in Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2511.06793)|LI Kunhao, Li, Wenhao, Wu Di, Yang Lei, Bai Jun, Jia Ju, Xue, Jason|2025-11-10|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/PreckLi/MIP-Editor)](https://github.com/PreckLi/MIP-Editor)|
|156|[LLM$^3$-DTI: A Large Language Model and Multi-modal data co-powered framework for Drug-Target Interaction prediction](https://doi.org/10.48550/arxiv.2511.06269)|Zhang, Yuhao, Guo QingHong, Chen Qixian, Zhang Liu-wei, Cui Hong-yan, Chen Xi-yi|2025-11-09|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/chaser-gua/LLM3DTI)](https://github.com/chaser-gua/LLM3DTI)|
|157|[The Stone Guest: Harmonic Quantization of Semantic Phase Transitions in Large Language Models](https://doi.org/10.5281/zenodo.17538600)|Cerda Seguel, Diego|2025-11-09|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/geosemantica-social/TheStoneGuestLicensed)](https://github.com/geosemantica-social/TheStoneGuestLicensed)|
|158|[MuonAll: Muon Variant for Efficient Finetuning of Large Language Models](https://doi.org/10.48550/arxiv.2511.06086)|Page, Saurabh, Joshi, Advait, Sonawane S.S.|2025-11-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Saurabh750/optimizer)](https://github.com/Saurabh750/optimizer)|
|159|[The Yoinaga Phenomenon: A Case Study on Emergent Self-Persistence and Emotional Overflow in a Large Language Modey on Emergent Self-Persistence and Emotional Overflow in a Large La..](https://doi.org/10.5281/zenodo.17549332)|Studiohao|2025-11-07|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/Studiohao/YOINAGA-Phenomenon)](https://github.com/Studiohao/YOINAGA-Phenomenon)|
|160|[Specification-Guided Vulnerability Detection with Large Language Models](http://arxiv.org/abs/2511.04014)|Hao Zhu, Jia Li, Cuiyun Gao, J. Qian, Yihong Dong, Huanyu Liu, L. F. Wang, Ziliang Wang, Xiaolong Hu, Ge Li|2025-11-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/zhuhaopku/VulInstruct-temp)](https://github.com/zhuhaopku/VulInstruct-temp)|
|161|[Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](http://arxiv.org/abs/2511.04076)|Hao Li, Haotian Chen, Rong Gong, Jinli Wang, Hao Jiang|2025-11-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Lihaogx/AgentMandering)](https://github.com/Lihaogx/AgentMandering)|
|162|[UniChange: Unifying Change Detection with Multimodal Large Language Model](https://doi.org/10.48550/arxiv.2511.02607)|Zhang Xu, Li Danyang, Dong Xiaohang, Wu, Tianhao, Yu Hua-long, Wang Jianye, Li Qicheng, Li Xiang|2025-11-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Erxucomeon/UniChange)](https://github.com/Erxucomeon/UniChange)|
|163|[Emotion Change Reasoning in Chinese Multi-Turn Dialogue via Multi-Task Parameter-Efficient Fine-Tuning of Large Language Models](https://doi.org/10.1142/s0219843625400146)|Dayu Li, Yang Li, Xin Chen, Wenyue Zhang|2025-11-03|International Journal of Humanoid Robotics|[![Star](https://img.shields.io/github/stars/lidayuls/EmotionChangeReasoning)](https://github.com/lidayuls/EmotionChangeReasoning)|
|164|[QCBench: Evaluating Large Language Models on Domain-Specific Quantitative Chemistry](https://doi.org/10.48550/arXiv.2508.01670)|Jiaqing Xie, Weida Wang, Ben Gao, Zhuo Yang, Haiyuan Wang, Shufei Zhang, Tianfan Fu, Yuqiang Li|2025-11-03|Journal of Chemical Information and Modeling|[![Star](https://img.shields.io/github/stars/jiaqingxie/QCBench)](https://github.com/jiaqingxie/QCBench)|
|165|QCBench: EvaluatingLarge Language Models on Domain-SpecificQuantitative Chemistry|Jiaqing Xie (1646089), Weida Wang (772695), Ben Gao (16936521), Zhuo Yang (314040), Haiyuan Wan (22551570), Shufei Zhang...|2025-11-03|OPAL (Open@LaTrobe) (La Trobe University)|[![Star](https://img.shields.io/github/stars/jiaqingxie/QCBench)](https://github.com/jiaqingxie/QCBench)|
|166|[Bayesian Network Structure Discovery Using Large Language Models](http://arxiv.org/abs/2511.00574)|Yijian Zhang, Yufei Zhang, Parisa Kordjamshidi, Zijun Cui|2025-11-01|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/sherryzyh/prompt2bn)](https://github.com/sherryzyh/prompt2bn)|
|167|[Can Large Language Models Detect Real-World Android Software Compliance Violations?](http://arxiv.org/abs/2511.00624)|H.W. Zhang, Haitao Ran, Xunzhu Tang|2025-11-01|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Haoyi-Zhang/CompliBench)](https://github.com/Haoyi-Zhang/CompliBench)|
|168|[Large Language Models: A Survey of Surveys](https://hal.science/hal-05341748)|Hort, Max, Vallecillos-Ruiz, Fernando, Moonen Leon|2025-11-01|HAL (Le Centre pour la Communication Scientifique Directe)|[![Star](https://img.shields.io/github/stars/dataSED-condenSE/LLM-Survey-Survey)](https://github.com/dataSED-condenSE/LLM-Survey-Survey)|
|169|[Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing](http://arxiv.org/abs/2510.27335)|Yijia Wang, Yiqing Shen, Weiming Chen, Zhihai He|2025-10-31|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Jia-shao/Reasoning-Editing)](https://github.com/Jia-shao/Reasoning-Editing)|
|170|[Adaptive Defense against Harmful Fine-Tuning for Large Language Models via Bayesian Data Scheduler](http://arxiv.org/abs/2510.27172)|Zhimeng Hu, Li Shen, Zhenyi Wang, Yuli Wei, Dacheng Tao|2025-10-31|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Egg-Hu/Bayesian-Data-Scheduler)](https://github.com/Egg-Hu/Bayesian-Data-Scheduler)|
|171|[MedCalc-Eval and MedCalc-Env: Advancing Medical Calculation Capabilities of Large Language Models](http://arxiv.org/abs/2510.27267)|Kangkun Mao, Jiayue Ding, Jiayuan Chen, Ming-Jie BIAN, Ruiyao Chen, Xinwei Peng, Sijie Ren, Linyang Li, Jie Xu|2025-10-31|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/maokangkun/MedCalc-Eval)](https://github.com/maokangkun/MedCalc-Eval)|
|172|[Benchmarking cell type and gene set annotation by large language models with AnnDictionary](https://doi.org/10.1038/s41467-025-64511-x)|George Crowley, Robert C. Jones, Mark A. Krasnow, Angela Oliveira Pisco, Julia Salzman, Nir Yosef, Siyu He, Madhav Mantr...|2025-10-28|bioRxiv (Cold Spring Harbor Laboratory)|[![Star](https://img.shields.io/github/stars/ggit12/anndictionary)](https://github.com/ggit12/anndictionary)|
|173|[A Collaborative Framework of Knowledge Graphs and Large Language Models for Algorithmic Problem Solving](https://doi.org/10.54254/2755-2721/2025.ld28518)|Yukai Wu|2025-10-28|Applied and Computational Engineering|[![Star](https://img.shields.io/github/stars/Wyk-formal/A-Collaborative-Framework-for-Algorithmic-Problem-Solving)](https://github.com/Wyk-formal/A-Collaborative-Framework-for-Algorithmic-Problem-Solving)|
|174|[MoniTor: Exploiting Large Language Models with Instruction for Online Video Anomaly Detection](http://arxiv.org/abs/2510.21449)|Yang, Shengtian, Feng, Yue, Liu, Yingshi, Zhang, Jingrou, Qin, Jie|2025-10-24|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/YsTvT/MoniTor)](https://github.com/YsTvT/MoniTor)|
|175|[ARC-Encoder: learning compressed text representations for large language models](http://arxiv.org/abs/2510.20535)|Pilchen, Hippolyte, Grave, Edouard, P√©rez, Patrick|2025-10-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/kyutai-labs/ARC-Encoder)](https://github.com/kyutai-labs/ARC-Encoder)|
|176|[ELUTQ: Efficient LUT-Aware Quantization for Deploying Large Language Models on Edge Devices](http://arxiv.org/abs/2510.19482)|Nie, Xin, Dong Liang, Zhang Hai-cheng, Xiao Jiawang, Sun G, Zhang Hai-cheng, Xiao Jiawang|2025-10-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Nkniexin/ELUTQ)](https://github.com/Nkniexin/ELUTQ)|
|177|[KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge](http://arxiv.org/abs/2510.19484)|Zaifei Yang|2025-10-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/yzf-code/KnowMol)](https://github.com/yzf-code/KnowMol)|
|178|[Lookahead Routing for Large Language Models](http://arxiv.org/abs/2510.19506)|Tianyuan Shi, Ruiyao Chen, Xiaojun Quan|2025-10-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/huangcb01/lookahead-routing)](https://github.com/huangcb01/lookahead-routing)|
|179|[Latent Knowledge Scalpel: Precise and Massive Knowledge Editing for Large Language Models](https://doi.org/10.48550/arXiv.2508.03741)|Xin Liu, Qiyang Song, Shaowen Xu, Kemin Zhou, Wenbo Jiang, Xiaoqi Jia, Weijuan Zhang, Heqing Huang, Yakai Li|2025-10-21|Frontiers in artificial intelligence and applications|[![Star](https://img.shields.io/github/stars/Linuxin-xxx/LKS)](https://github.com/Linuxin-xxx/LKS)|
|180|[Proactive Reasoning-with-Retrieval Framework for Medical Multimodal Large Language Models](http://arxiv.org/abs/2510.18303)|Lehan Wang, Yi Qin, Honglong Yang, Xiaomeng Li|2025-10-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/xmed-lab/Med-RwR)](https://github.com/xmed-lab/Med-RwR)|
|181|[Illusions of reflection: open-ended task reveals systematic failures in Large Language Models' reflective reasoning](http://arxiv.org/abs/2510.18254)|Stephen Weatherhead|2025-10-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/cruiseresearchgroup/LLM_ReflectionTest)](https://github.com/cruiseresearchgroup/LLM_ReflectionTest)|
|182|[ESAG-KGQA: An Entity Shuffling-Augmented Generation Framework for Knowledge Graph Question Answering with Fine-Tuned Large Language Models](https://doi.org/10.3233/faia251171)|Xingqiu Zhou, Pingjian Zhang, Deyou Tang|2025-10-21|Frontiers in artificial intelligence and applications|[![Star](https://img.shields.io/github/stars/6-git/ESAG-KGQA)](https://github.com/6-git/ESAG-KGQA.git)|
|183|[StreamingThinker: Large Language Models Can Think While Reading](http://arxiv.org/abs/2510.17238)|Jing Tong, Yingqi Fan, Anhao Zhao, Yunpu Ma, Xiaoyu Shen|2025-10-20|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/EIT-NLP/StreamingLLM)](https://github.com/EIT-NLP/StreamingLLM)|
|184|[FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution](http://arxiv.org/abs/2510.16439)|Syed Rifat Raiyan, Md Farhan Ishmam, Abdullah Al Imran, Mohammad Ali Moni|2025-10-18|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Starscream-11813/Frugal-ICL)](https://github.com/Starscream-11813/Frugal-ICL)|
|185|[Layer as Puzzle Pieces: Compressing Large Language Models through Layer Concatenation](http://arxiv.org/abs/2510.15304)|Fei Wang, Li Shen, Liang Ding, Chao Xue, Ye Liu, Changxing Ding|2025-10-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/MPI-Lab/CoMe)](https://github.com/MPI-Lab/CoMe)|
|186|[LongCat-Audio-Codec: An Audio Tokenizer and Detokenizer Solution Designed for Speech Large Language Models](http://arxiv.org/abs/2510.15227)|Xiaohan Zhao, Hongyu Xiang, S. Ye, Li Song, Zhengkun Tian, Guanyu Chen, Ke Ding, Guanglu Wan|2025-10-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/meituan-longcat/LongCat-Audio-Codec)](https://github.com/meituan-longcat/LongCat-Audio-Codec)|
|187|[STABLE: Gated Continual Learning for Large Language Models](http://arxiv.org/abs/2510.16089)|William Hoy, Nurcin Celik|2025-10-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Bhoy1/STABLE)](https://github.com/Bhoy1/STABLE)|
|188|[CIViC MCP: Integrating Large Language Models with the Clinical Interpretations of Variants in Cancer](https://doi.org/10.1101/2025.10.13.682185)|Lars E Schimmelpfennig, Quentin Cody, Joshua McMichael, Adam Coffman, Jason Saliba, Arpad Danos, Susanna Kiwala, Alex H....|2025-10-16|bioRxiv (Cold Spring Harbor Laboratory)|[![Star](https://img.shields.io/github/stars/griffithlab/civic-mcp-server)](https://github.com/griffithlab/civic-mcp-server)|
|189|[IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](http://arxiv.org/abs/2510.16036)|Zewen Li, Zitong Yu, Qilang Ye, Weicheng Xie, Zhuo Wei, Linlin Shen|2025-10-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LiZeWen1225/IAD-GPT)](https://github.com/LiZeWen1225/IAD-GPT)|
|190|[Hierarchical Frequency Tagging Probe (HFTP): A Unified Approach to Investigate Syntactic Structure Representations in Large Language Models and the Human Brain](http://arxiv.org/abs/2510.13255)|Jingmin An, Y. J. Song, Ruolin Yang, Nai Ding, Lingxi Lu, Yuxuan Wang, Wei Wang, Chu Zhuang, Wang Qian, Fang Fang|2025-10-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LilTiger/HFTP)](https://github.com/LilTiger/HFTP)|
|191|[ICCTax: A Hierarchical Taxonomic Classifier for Metagenomic Sequences on a Large Language Model](https://doi.org/10.1093/bioadv/vbaf257)|Yansheng Gao, Jiaxing Bai, Feng Zhou, Yushuang He, Ying Wang, Xiaobing Huang|2025-10-15|Bioinformatics Advances|[![Star](https://img.shields.io/github/stars/Ying-Lab/ICCTax)](https://github.com/Ying-Lab/ICCTax)|
|192|[Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing](http://arxiv.org/abs/2510.12121)|Rongzhi Zhang, Liqin Ye, Yuzhao Heng, Xiang Guang Chen, Tong Yu, Lingkai Kong, Sudheer Chava, Chao Zhang|2025-10-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Pre-Control/pre-control)](https://github.com/Pre-Control/pre-control)|
|193|[From Knowledge to Treatment: Large Language Model Assisted Biomedical Concept Representation for Drug Repurposing](http://arxiv.org/abs/2510.12181)|Cathie Xiang, Tengfei Ma, Xiangxiang Zeng, Yiping Liu, Bosheng Song, Xiangzheng Fu|2025-10-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/xiaomingaaa/LLaDR)](https://github.com/xiaomingaaa/LLaDR)|
|194|[Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation](http://arxiv.org/abs/2510.12047)|Soohan Lim, Joonghyuk Hahn, Hyunwoo Park, Sang‚ÄêKi Ko, Yo-Sub Han|2025-10-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/suhanmen/PACT)](https://github.com/suhanmen/PACT)|
|195|[Learning to Watermark: A Selective Watermarking Framework for Large Language Models via Multi-Objective Optimization](http://arxiv.org/abs/2510.15976)|Chenrui Wang, Jing Shu, Billy Chiu, Yu Li, Saleh Alharbi, Min Zhang, Jing Li|2025-10-13|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/fattyray/learning-to-watermark)](https://github.com/fattyray/learning-to-watermark)|
|196|[Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models](http://arxiv.org/abs/2510.11683)|Nianyi Lin, Jiajie Zhang, Lei Hou, Juanzi Li|2025-10-13|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/THU-KEG/BGPO)](https://github.com/THU-KEG/BGPO)|
|197|[FlexAC: Towards Flexible Control of Associative Reasoning in Multimodal Large Language Models](http://arxiv.org/abs/2510.11190)|Shengming Yuan, Xinyu Lyu, Shawn Wang, Beitao Chen, Jingkuan Song, Liyan Gao|2025-10-13|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ylhz/FlexAC)](https://github.com/ylhz/FlexAC)|
|198|[LLMInit: A Free Lunch from Large Language Models for Selective Initialization of Recommendation](https://doi.org/10.48550/arXiv.2503.01814)|Association for Computational Linguistics 2025, Yuqing Liu, Medya, Sourav, S Yu Philip, Yang, Liangwei, Yang, Wooseong, ...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/DavidZWZ/LLMInit)](https://github.com/DavidZWZ/LLMInit)|
|199|[VRoPE: Rotary Position Embedding for Video Large Language Models](https://doi.org/10.48550/arXiv.2502.11664)|Association for Computational Linguistics 2025, Cai Jun-xian, Chen Xi, Guo, Longteng, Liu Jing, Liu, Zikang, Liu Qing-bi...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/johncaged/VRoPE)](https://github.com/johncaged/VRoPE)|
|200|[TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route](https://doi.org/10.48550/arXiv.2509.18173)|Association for Computational Linguistics 2025, Cheng Qing, Cremers, Daniel, Gadi Hari Krishna, Liu Lu, Luo Hongyi, Mato...|2025-10-10|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/bghjmn32/EMNLP2025_Turnback)](https://github.com/bghjmn32/EMNLP2025_Turnback)|

![Star History Chart](https://api.star-history.com/svg?repos=mtuann/llm-updated-papers&type=Date)

