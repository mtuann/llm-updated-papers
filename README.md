# Large Language Models Papers

Updated list of Large Language Models papers as of **February 23, 2026**. 

## Quick Access
üîç **[Interactive Search & Browse](https://mtuann.github.io/papers/)** - Filter, search, and explore all papers with an intuitive interface

## Overview
- **Coverage**: Papers from 2016 to present
- **Sources**: arXiv, NeurIPS, ICML, ICLR, ACL, EMNLP, AAAI, IJCAI, KDD, CVPR, ICCV, ECCV, IEEE, ACM, Springer, ScienceDirect, Nature, and other top AI/ML venues
- **Updates**: Automated collection of new publications
- **Features**: Advanced search, code availability tracking, and multi-venue coverage

## Related Topics
- **[Large Language Models](https://github.com/mtuann/llm-updated-papers)** | **[Federated Learning](https://github.com/mtuann/federated-learning-updated-papers)** | **[Backdoor Learning](https://github.com/mtuann/backdoor-ai-resources)** | **[Machine Unlearning](https://github.com/mtuann/machine-unlearning-papers)**
- **[Serverless Computing](https://mtuann.github.io/papers/)** | **[Multi-Modal Learning](https://mtuann.github.io/papers/)**

## Large Language Models Papers with Code
This section lists papers with available code (sorted by publication date). For the complete paper list, visit the [Research Papers Page](https://mtuann.github.io/papers/).

---

## Support
If you find this resource helpful, consider supporting its development:

- **Ko-fi** (PayPal/Card): [ko-fi.com/miutheladycat](https://ko-fi.com/miutheladycat)
- **Techcombank** (Vietnam): 5877 5555 55 (Nguyen Thi Lan Phuong)

---

*This repository is regularly updated. For the latest data, visit the [Research Papers Page](https://mtuann.github.io/papers/).*


|No.|Title|Authors|Publish Date|Venue|Code|
|---|---|---|---|---|---|
|1|[GEOMiner: Context-aware semantic curation of NCBI GEO datasets using large language models](https://doi.org/10.5281/zenodo.18730136)|Peng Xia, Yuqi Zhang, Luyao Liu, Shuang Si, Shen Rong, Degui Wang|2026-02-22|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/BioinfoXP/GEOMiner)](https://github.com/BioinfoXP/GEOMiner)|
|2|[Prompting Frameworks for Large Language Models: A Survey](https://doi.org/10.48550/arXiv.2311.12785)|Xiaoxia Liu, Jingyi Wang, Jun Sun, Xiaohan Yuan, Guoliang Dong, Peng Di, Wenhai Wang, Dongxia Wang|2026-02-21|ACM Computing Surveys|[![Star](https://img.shields.io/github/stars/lxx0628/Prompting-Framework-Survey)](https://github.com/lxx0628/Prompting-Framework-Survey)|
|3|[Empowering Video Translation using Multimodal Large Language Models](https://doi.org/10.36227/techrxiv.177162137.77131035/v1)|Bingzheng Qu, Kehai Chen, Xuefeng Bai, Min Zhang|2026-02-20|OpenAlex|[![Star](https://img.shields.io/github/stars/qbzz10/Video-Translation-using-MLLMs)](https://github.com/qbzz10/Video-Translation-using-MLLMs)|
|4|[SNN-Genesis v6: The Dual-Mode Brain ‚Äî Online Sweet Spot Discovery, Quadratic Homeostasis, Task-Dependent Perturbation, and Per-Sample CfC Control of Chaotic Perturbations in Large ..](https://doi.org/10.5281/zenodo.18706589)|Hiroto Funasaki|2026-02-20|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/hafufu-stack/snn-genesis)](https://github.com/hafufu-stack/snn-genesis)|
|5|[EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2602.17419)|Xiaomeng Peng, Xilang Huang, Seon Han Choi|2026-02-19|ArXiv.org|[![Star](https://img.shields.io/github/stars/shengtun/Eagle)](https://github.com/shengtun/Eagle)|
|6|[EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2602.17196)|Yahong Wang, Jiande Wu, Zhangkai Ni, Chengmei Yang, Yihang Liu, Longzhen Yang, Yuyin Zhou, Ying Wen, Yitao Peng|2026-02-19|ArXiv.org|[![Star](https://img.shields.io/github/stars/YahongWang1/EntropyPrune)](https://github.com/YahongWang1/EntropyPrune)|
|7|[SimulatorCoder: DNN Accelerator Simulator Code Generation and Optimization via Large Language Models](https://doi.org/10.48550/arxiv.2602.17169)|Yuhuan Xia, Tun Li, Hongji Zhou, Xianfa Zhou, Chong Chen, Ruiyu Zhang|2026-02-19|ArXiv.org|[![Star](https://img.shields.io/github/stars/xiayuhuan/SimulatorCoder)](https://github.com/xiayuhuan/SimulatorCoder)|
|8|[GPSBench: Do Large Language Models Understand GPS Coordinates?](https://doi.org/10.48550/arxiv.2602.16105)|Thinh Hung Truong, Jey Han Lau, Jianzhong Qi|2026-02-18|ArXiv.org|[![Star](https://img.shields.io/github/stars/joey234/gpsbench)](https://github.com/joey234/gpsbench)|
|9|[SNN-Genesis v5: Depth-Dependent Sensitivity, Pharmacological Dose-Response, Cross-Model Transfer, and CfC-Dosing of Chaotic Perturbations in Large Language Models](https://doi.org/10.5281/zenodo.18680910)|Hiroto Funasaki|2026-02-18|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/hafufu-stack/snn-genesis)](https://github.com/hafufu-stack/snn-genesis)|
|10|[Sino-US-DrugQA: A Benchmark for Evaluating Large Language Models in Cross-Jurisdictional Pharmaceutical Regulation](https://doi.org/10.64898/2026.02.13.26346236)|Zhen Chen, Xuejing Fu, Wentao Lu|2026-02-17|OpenAlex|[![Star](https://img.shields.io/github/stars/DodgeLU/Sino-US-DrugQA)](https://github.com/DodgeLU/Sino-US-DrugQA)|
|11|[TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models](https://doi.org/10.48550/arxiv.2602.15449)|C.Y. Park, Juyong Jiang, Fan Wang, Sayak Paul, Jiasi Shen, Jing Tang, Jianguo Li|2026-02-17|ArXiv.org|[![Star](https://img.shields.io/github/stars/deep-diver/TAROT)](https://github.com/deep-diver/TAROT)|
|12|[C <sup>2</sup> -Cite: Contextual-Aware Citation Generation for Attributed Large Language Models](https://doi.org/10.1145/3773966.3777933)|Yue Yu, Ting Bai, Hengzhi Lan, Li Qian, Li Peng, Jie Wu, Wei Liu, Jian Luan, Chuan Shi|2026-02-16|OpenAlex|[![Star](https://img.shields.io/github/stars/BAI-LAB/c2cite)](https://github.com/BAI-LAB/c2cite)|
|13|[Combining Structural and Textual Knowledge for Knowledge Graph Link Prediction via Large Language Models](https://doi.org/10.1145/3773966.3777934)|Shijie Luo, Xinyuan Lu, Qinpei Zhao, Weixiong Rao|2026-02-16|OpenAlex|[![Star](https://img.shields.io/github/stars/shijielaw/ST-KGLP)](https://github.com/shijielaw/ST-KGLP)|
|14|[Query as Anchor: Scenario-Adaptive User Representation via Large Language Model](https://doi.org/10.48550/arxiv.2602.14492)|Jiahao Yuan, Yike Xu, Jinyong Wen, Baokun Wang, Ziyi Gao, Xiaotong Lin, Yun Liu, Xing Fu, Yu Cheng, Yongchao Liu, Weiqia...|2026-02-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/JhCircle/Q-Anchor)](https://github.com/JhCircle/Q-Anchor)|
|15|[Anticipating Adversary Behavior in DevSecOps Scenarios through Large Language Models](https://doi.org/10.48550/arxiv.2602.14106)|Mario Mar√≠n Caballero, Miguel Betancourt Alonso, Daniel D√≠az L√≥pez, √Ångel Luis Perales G√≥mez, Pantaleone Nespoli, Juan A...|2026-02-15|ArXiv.org|[![Star](https://img.shields.io/github/stars/mariomc14/devsecops-adversary-llm)](https://github.com/mariomc14/devsecops-adversary-llm.git)|
|16|[SNN-Genesis: A Preliminary Study on Iterative Adversarial Training of Large Language Models Using Spiking Neural Network Perturbations (v1)](https://doi.org/10.5281/zenodo.18625622)|Hiroto Funasaki|2026-02-13|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/hafufu-stack/snn-genesis)](https://github.com/hafufu-stack/snn-genesis)|
|17|[To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models](https://doi.org/10.48550/arxiv.2602.12566)|Haoqing Wang, Xiang Long, Ziheng Li, Yilong Xu, Tingguang Li, Yehui Tang|2026-02-13|ArXiv.org|[![Star](https://img.shields.io/github/stars/mosAI25/M2RL)](https://github.com/mosAI25/M2RL)|
|18|[Agent Skills for Large Language Models: Architecture, Acquisition, Security, and the Path Forward](https://doi.org/10.48550/arxiv.2602.12430)|Renjun Xu, Yang Yan|2026-02-12|ArXiv.org|[![Star](https://img.shields.io/github/stars/scienceaix/agentskills)](https://github.com/scienceaix/agentskills)|
|19|[Composition-RL: Compose Your Verifiable Prompts for Reinforcement Learning of Large Language Models](https://doi.org/10.48550/arxiv.2602.12036)|Xin Xu, Clive Bai, Kai Yang, Tianhao Chen, Yangkun Chen, Weijie Liu, Hao Chen, Yang Wang, Saiyong Yang, Can Yang|2026-02-12|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/XinXU-USTC/Composition-RL)](https://github.com/XinXU-USTC/Composition-RL)|
|20|[MRAC-LLM Toolbox: An interactive model reference adaptive control enhanced with large language models](https://doi.org/10.1016/j.softx.2026.102556)|Merve Nilay Aydƒ±n, HALƒ∞L ƒ∞BRAHƒ∞M OKUR, Handan G√ºrsoy-Demir, Kadir Tohma, Celaleddin Yeroƒülu|2026-02-12|SSRN Electronic Journal|[![Star](https://img.shields.io/github/stars/halilokur91/MRAC_LLM)](https://github.com/halilokur91/MRAC_LLM)|
|21|[Pretraining A Large Language Model using Distributed GPUs: A Memory-Efficient Decentralized Paradigm](https://doi.org/10.48550/arxiv.2602.11543)|Jinrui Zhang, Chaodong Xiao, Aoqi Wu, Xindong Zhang, Lei Zhang|2026-02-12|ArXiv.org|[![Star](https://img.shields.io/github/stars/zjr2000/SPES)](https://github.com/zjr2000/SPES)|
|22|[Replication Package for the Paper: Evaluating Large Language Models for Detecting Architectural Decision Violations](https://doi.org/10.5281/zenodo.18621369)|Su, Ruoyu, Bakhtin, Alexander, Ahmad, Noman, Esposito, Matteo, Lenarduzzi, Valentina, Taibi, Davide|2026-02-12|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/OWNER/REPO)](https://github.com/OWNER/REPO)|
|23|[The Vyapti Probe Benchmark: A Navya-Nyaya Framework for Diagnosing Logical Reasoning Failures in Large Language Models](https://doi.org/10.5281/zenodo.18625545)|Sharath Sathish|2026-02-12|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/TechNektar/pramana)](https://github.com/TechNektar/pramana)|
|24|[scPilot: Large Language Model Reasoning Toward Automated Single-Cell Analysis and Discovery](https://doi.org/10.48550/arxiv.2602.11609)|Yiming Gao, Zhen Wang, Ju Chen, Mark Antkowiak, Mengzhou Hu, JungHo Kong, Dexter Pratt, Jieyuan Liu, Enze Ma, Zhiting Hu...|2026-02-12|ArXiv.org|[![Star](https://img.shields.io/github/stars/maitrix-org/scPilot)](https://github.com/maitrix-org/scPilot)|
|25|[VideoSTF: Stress-Testing Output Repetition in Video Large Language Models](https://doi.org/10.48550/arxiv.2602.10639)|Yuxin Cao, Wei Song, Shangzhi Xu, Jingling Xue, Jin Song Dong|2026-02-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/yuxincao22/VideoSTF_benchmark)](https://github.com/yuxincao22/VideoSTF_benchmark)|
|26|[Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions](https://doi.org/10.48550/arxiv.2602.09483)|Lin Chen, Xiaoke Zhao, Kun Ding, Weiwei Feng, Changtao Miao, Zili Wang, Wenxuan Guo, Ying Wang, Kaiyuan Zheng, Bo Zhang,...|2026-02-10|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/lchen1019/Align-TI)](https://github.com/lchen1019/Align-TI)|
|27|[Improving Financial Statement Fraud Detection: A Large Language Model Processing Approach](https://doi.org/10.1145/3796514)|Yue Yu, Zhen Wu, Yanni Han, Ying Ding, Wenqi Wei|2026-02-10|ACM Transactions on Internet Technology|[![Star](https://img.shields.io/github/stars/LittelStudent/Financial-Statement-Fraud-Detection-ParaEmb-FraudW2V)](https://github.com/LittelStudent/Financial-Statement-Fraud-Detection-ParaEmb-FraudW2V)|
|28|[Systematic Failure of Large Language Models: An Empirical Theory of a Universal Limitation in Self-Referential Termination Proofs](https://doi.org/10.5281/zenodo.18529506)|Moses Rahnama|2026-02-09|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/MosesRahnama/OperatorKO7)](https://github.com/MosesRahnama/OperatorKO7)|
|29|[CausalTAD: Injecting Causal Knowledge into Large Language Models for Tabular Anomaly Detection](https://doi.org/10.48550/arxiv.2602.07798)|Ruiqi Wang, Ruikang Liu, Runyu Chen, Haoxiang Suo, Zhiyi Peng, Zhuo Tang, Changjian Chen|2026-02-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/350234/CausalTAD)](https://github.com/350234/CausalTAD)|
|30|[Deep Energy Method with Large Language Model assistance: an open-source Streamlit-based platform for solving variational PDEs](https://doi.org/10.48550/arxiv.2602.07838)|Yizheng Wang, Cosmin Anitescu, Mohammad Sadegh Eshaghi, Xiaoying Zhuang, Timon Rabczuk, Yinghua liu|2026-02-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/yizheng-wang/LMDEM)](https://github.com/yizheng-wang/LMDEM)|
|31|[FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging](https://doi.org/10.48550/arxiv.2602.08024)|Ziyang Fan, Keyu Chen, Ruilong Xing, Yulin Li, Li Jiang, Zhuotao Tian|2026-02-08|ArXiv.org|[![Star](https://img.shields.io/github/stars/Fanziyang-v/FlashVID)](https://github.com/Fanziyang-v/FlashVID)|
|32|[SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2602.07833)|Weijiang Lv, Yaoxuan Feng, Xiaobo Xia, Jiayu Wang, Yan Jing, Wenchao Chen, Bo Chen|2026-02-08|ArXiv.org|[![Star](https://img.shields.io/github/stars/Johanson-colab/SPD-Faith-Bench)](https://github.com/Johanson-colab/SPD-Faith-Bench)|
|33|[SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization](https://doi.org/10.48550/arxiv.2602.07909)|Taolin Zhang, Hang Guo, Wang Lu, Tao Dai, Shu-Tao Xia, Jindong Wang|2026-02-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/taolinzhang/SparseEval)](https://github.com/taolinzhang/SparseEval)|
|34|[FMBench: Adaptive Large Language Model Output Formatting](https://doi.org/10.48550/arxiv.2602.06384)|Yaoting Wang, Yun Zhou, Henghui Ding|2026-02-06|ArXiv.org|[![Star](https://img.shields.io/github/stars/FudanCVL/FMBench)](https://github.com/FudanCVL/FMBench)|
|35|[Improve Large Language Model Systems with User Logs](https://doi.org/10.48550/arxiv.2602.06470)|Changyue Wang, Weihang Su, Qingyao Ai, Quan Zhou|2026-02-06|ArXiv.org|[![Star](https://img.shields.io/github/stars/bebr2/UNO)](https://github.com/bebr2/UNO)|
|36|[PreFlect: From Retrospective to Prospective Reflection in Large Language Model Agents](https://doi.org/10.48550/arxiv.2602.07187)|Hanyu Wang, Yuanpu Cao, Lu Lin, Jinghui Chen|2026-02-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/wwwhy725/PreFlect)](https://github.com/wwwhy725/PreFlect)|
|37|[ShallowJail: Steering Jailbreaks against Large Language Models](https://doi.org/10.48550/arxiv.2602.07107)|Shang Liu, Hanyu Pei, Zeyan Liu|2026-02-06|ArXiv.org|[![Star](https://img.shields.io/github/stars/liuup/ShallowJail)](https://github.com/liuup/ShallowJail)|
|38|[Surgery: Mitigating Harmful Fine-Tuning for Large Language Models via Attention Sink](https://doi.org/10.48550/arxiv.2602.05228)|Guozhi Liu, Weiwei Lin, Tiansheng Huang, Ruichao Mo, Qi Mu, Xiumin Wang, Li Shen|2026-02-05|ArXiv.org|[![Star](https://img.shields.io/github/stars/Lslland/Surgery)](https://github.com/Lslland/Surgery)|
|39|[SciDef: Automating Definition Extraction from Academic Literature with Large Language Models](https://doi.org/10.48550/arxiv.2602.05413)|Filip Kuƒçera, Christoph Mandl, Isao Echizen, Radu Timofte, Timo Spinde|2026-02-05|ArXiv.org|[![Star](https://img.shields.io/github/stars/Media-Bias-Group/SciDef)](https://github.com/Media-Bias-Group/SciDef)|
|40|[OdysseyArena: Benchmarking Large Language Models For Long-Horizon, Active and Inductive Interactions](https://doi.org/10.48550/arxiv.2602.05843)|Fangzhi Xu, Hang Yan, Qiushi Sun, Jinyang Wu, Zixian Huang, Muye Huang, Jingyang Gong, Zichen Ding, Kanzhi Cheng, Yian W...|2026-02-05|ArXiv.org|[![Star](https://img.shields.io/github/stars/xufangzhi/Odyssey-Arena)](https://github.com/xufangzhi/Odyssey-Arena)|
|41|[Large Language Model Reasoning Failures](https://openreview.net/forum?id=vnX1WHMNmz)|Peiyang Song, Pengrui Han, Noah D. Goodman|2026-02-05|Trans. Mach. Learn. Res.|[![Star](https://img.shields.io/github/stars/Peiyang-Song/Awesome-LLM-Reasoning-Failures)](https://github.com/Peiyang-Song/Awesome-LLM-Reasoning-Failures)|
|42|[IESR:Efficient MCTS-Based Modular Reasoning for Text-to-SQL with Large Language Models](https://doi.org/10.48550/arxiv.2602.05385)|Tao Liu, Jiafan Lu, Bohan Yu, Pengcheng Wu, Liu Haixin, Guoyu Xu, Li Xiangheng, Lixiao LI, Jiaming Hou, Zhao Shijun, Xin...|2026-02-05|ArXiv.org|[![Star](https://img.shields.io/github/stars/Ffunkytao/IESR-SLM)](https://github.com/Ffunkytao/IESR-SLM)|
|43|[BioVix: An Integrated Large Language Model Framework for Data Visualization, Graph Interpretation, and Literature-Aware Scientific Validation](https://doi.org/10.64898/2026.02.03.703467)|Muhammad Zain Butt, Riaz Ahmad, Eman Fatima, Muhammad Tahir ul Qamar|2026-02-05|OpenAlex|[![Star](https://img.shields.io/github/stars/MuhammadZain-Butt/BioVix)](https://github.com/MuhammadZain-Butt/BioVix)|
|44|[Bi-directional Bias Attribution: Debiasing Large Language Models without Modifying Prompts](https://doi.org/10.48550/arxiv.2602.04398)|Yujie Lin, Kunquan Li, Yixuan Liao, Xiaoxin Chen, Jinsong Su|2026-02-04|ArXiv.org|[![Star](https://img.shields.io/github/stars/XMUDeepLIT/Bi-directional-Bias-Attribution)](https://github.com/XMUDeepLIT/Bi-directional-Bias-Attribution)|
|45|[CNNeoPP: a large language model-enhanced deep learning pipeline for personalized neoantigen prediction and liquid biopsy applications](https://doi.org/10.3389/fimmu.2026.1722117)|Yu Cai, Rui Chen, Siyu Chen, Linlin Wang, Zirong Huo, Dongyan Yang, Sitong Zhang, Shenghan Gao, S.-J. Hwang, Ling Bai, Y...|2026-02-04|Frontiers in Immunology|[![Star](https://img.shields.io/github/stars/AaronChen007/neoantigen)](https://github.com/AaronChen007/neoantigen)|
|46|[Code and Data for the paper Medical concept understanding in large language models is fragmented](https://doi.org/10.5281/zenodo.18478398)|Lizong Deng, Luming Chen|2026-02-04|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/DC-Union/ConceptUnderstanding)](https://github.com/DC-Union/ConceptUnderstanding)|
|47|[On the Uncertainty of Large Language Model-Based Multi-Agent Systems](https://doi.org/10.48550/arxiv.2602.04234)|Yuxuan Zhao, Sijia Chen, Ningxin Su|2026-02-04|ArXiv.org|[![Star](https://img.shields.io/github/stars/AgenticFinLab/multiagent-entropy)](https://github.com/AgenticFinLab/multiagent-entropy)|
|48|[Fast-Slow Efficient Training for Multimodal Large Language Models via Visual Token Pruning](https://doi.org/10.48550/arxiv.2602.03815)|Dingkun Zhang, Shuhan Qi, Yulin Wu, Xinyu Xiao, Xuan Wang, Long ÈôàÈæô Chen|2026-02-03|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/dingkun-zhang/DualSpeed)](https://github.com/dingkun-zhang/DualSpeed)|
|49|[LLaMEA: Large Language Model Evolutionary Algorithm](https://doi.org/10.5281/zenodo.18470562)|Niki van Stein, Urban ≈†kvorc|2026-02-03|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/XAI-liacs/LLaMEA)](https://github.com/XAI-liacs/LLaMEA)|
|50|[Risk Assessment of Large Language Model Implementation in the Electric Power Sector of Ukraine](https://doi.org/10.47852/bonviewaia62027195)|Hryhoriy Kravtsov, Oleksandr Kravchuk, Artem Taranowski, Dmytro Sinko, V.N. Samoylov|2026-02-03|Artificial Intelligence and Applications|[![Star](https://img.shields.io/github/stars/oleksandrkravchukatpimee/LLM-risks-evaluation)](https://github.com/oleksandrkravchukatpimee/LLM-risks-evaluation)|
|51|[Semantic-aware Wasserstein Policy Regularization for Large Language Model Alignment](https://doi.org/10.48550/arxiv.2602.01685)|Byeonghu Na, Hyungho Na, Yeongmin Kim, Suhyeon Jo, HeeSun Bae, Mina Kang, Il-Chul Moon|2026-02-02|ArXiv.org|[![Star](https://img.shields.io/github/stars/aailab-kaist/WPR)](https://github.com/aailab-kaist/WPR)|
|52|[Vision-DeepResearch Benchmark: Rethinking Visual and Textual Search for Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2602.02185)|Yu Zeng, Wenxuan Huang, Zhen Fang, Shuang Chen, Yufan Shen, Yishuo Cai, Xiaoman Wang, Zhenfei Yin, Lin Chen, Zehui Chen,...|2026-02-02|ArXiv.org|[![Star](https://img.shields.io/github/stars/Osilly/Vision-DeepResearch)](https://github.com/Osilly/Vision-DeepResearch)|
|53|[AR-MAP: Are Autoregressive Large Language Models Implicit Teachers for Diffusion Large Language Models?](https://doi.org/10.48550/arxiv.2602.02178)|Liang Lin, Feng Xiong, Zengbin Wang, Kun Wang, Junhao Dong, Xuecai Hu, Yong Wang, Xiangxiang Chu|2026-02-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/AMAP-ML/AR-MAP)](https://github.com/AMAP-ML/AR-MAP)|
|54|[DPBench: Large Language Models Struggle with Simultaneous Coordination](https://doi.org/10.48550/arxiv.2602.13255)|Najmul Hasan, Prashanth Busireddygari|2026-02-02|ArXiv.org|[![Star](https://img.shields.io/github/stars/najmulhasan-code/dpbench)](https://github.com/najmulhasan-code/dpbench)|
|55|[SFMP: Fine-Grained, Hardware-Friendly and Search-Free Mixed-Precision Quantization for Large Language Models](https://doi.org/10.48550/arxiv.2602.01027)|Xin Nie, Haicheng Zhang, Liang Dong, Beining Feng, Jinhong Weng, Guiling Sun|2026-02-01|ArXiv.org|[![Star](https://img.shields.io/github/stars/Nkniexin/SFMP)](https://github.com/Nkniexin/SFMP)|
|56|[Structural‚ÄìRelational Cognitive Robustness: A Failure-Oriented Evaluation Framework for Large Language Models](https://doi.org/10.5281/zenodo.18445542)|HIDEYUKI CHINO|2026-02-01|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/hideyuki49/llm-cognitive-stability)](https://github.com/hideyuki49/llm-cognitive-stability)|
|57|[Data and Code for "Large Language Model as Reservoir Operator: A Generative Agent Bridging Human Judgment and Optimization in Water Resource Systems"](https://doi.org/10.5281/zenodo.18444869)|Wyatt Arnold|2026-01-31|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/EILab-Polimi/ResLLM)](https://github.com/EILab-Polimi/ResLLM)|
|58|[Decouple Searching from Training: Scaling Data Mixing via Model Merging for Large Language Model Pre-training](https://doi.org/10.48550/arxiv.2602.00747)|Shengrui Li, Fei Zhao, Kaiyan Zhao, Jiaqi Ye, Haifeng Liu, Zijie Meng, Zheyong Xie, Yao Hu, Shaosheng Cao|2026-01-31|ArXiv.org|[![Star](https://img.shields.io/github/stars/Lucius-lsr/DeMix)](https://github.com/Lucius-lsr/DeMix)|
|59|[Learning to Decode Against Compositional Hallucination in Video Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2602.00559)|Wenbin Xing, Quanxing Zha, Lizheng Zu, Mengran Li, Ming Li, Junchi Yan|2026-01-31|ArXiv.org|[![Star](https://img.shields.io/github/stars/BMRETURN/OmniVCHall)](https://github.com/BMRETURN/OmniVCHall)|
|60|[DIFFA-2: A Practical Diffusion Large Language Model for General Audio Understanding](https://doi.org/10.48550/arxiv.2601.23161)|Jiaming Zhou, Xuxin Cheng, Shiwan Zhao, Yuhang Jia, Cao Liu, Ke Zeng, Yong Yu, Yong Qin|2026-01-30|ArXiv.org|[![Star](https://img.shields.io/github/stars/NKU-HLT/DIFFA)](https://github.com/NKU-HLT/DIFFA.git)|
|61|[Data for Probing the Trajectories of Reasoning Traces in Large Language Models](https://doi.org/10.5281/zenodo.18430487)|Marthe Klaartje Lucie Ballon, Brecht Verbeken, Vincent Ginis, Andres Algaba|2026-01-30|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/AndresAlgaba/probing_reasoning_traces)](https://github.com/AndresAlgaba/probing_reasoning_traces)|
|62|[FNF: Functional Network Fingerprint for Large Language Models](https://doi.org/10.48550/arxiv.2601.22692)|Yiheng Liu, Junhao Ning, Sichen Xia, Haiyang Sun, yang yang, Hanyang Chi, Xiaohui Gao, Ning Qiang, Bao Ge, Junwei Han, X...|2026-01-30|ArXiv.org|[![Star](https://img.shields.io/github/stars/WhatAboutMyStar/LLM_ACTIVATION)](https://github.com/WhatAboutMyStar/LLM_ACTIVATION)|
|63|[Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2601.22060)|Wenxuan Huang, Yu Zeng, Qiuchen Wang, Zhen Fang, Shaosheng Cao, Zheng Chu, Xinyang He, Shuang Chen, Zhenfei Yin, Lin Che...|2026-01-29|ArXiv.org|[![Star](https://img.shields.io/github/stars/Osilly/Vision-DeepResearch)](https://github.com/Osilly/Vision-DeepResearch)|
|64|[SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding](https://research.rug.nl/en/publications/296a4b32-1704-4532-964d-995bd24f355e)|Ahmed Y. Radwan, Christos Emmanouilidis, Hina Tabassum, Deval Pandya, Shaina Raza|2026-01-29|University of Groningen research database (University of Groningen / Centre for Information Technology)|[![Star](https://img.shields.io/github/stars/vectorinstitute/sonic-o1)](https://github.com/vectorinstitute/sonic-o1)|
|65|[Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers](https://doi.org/10.48550/arxiv.2601.22139)|Xin Chen, Feng Jiang, Congduan Li, Hardy Chen, Shuo Yan, Wenya Xie, Min Yang, Shujian Huang|2026-01-29|ArXiv.org|[![Star](https://img.shields.io/github/stars/SUAT-AIRI/Proactive-Interactive-R1)](https://github.com/SUAT-AIRI/Proactive-Interactive-R1)|
|66|[MAD: Modality-Adaptive Decoding for Mitigating Cross-Modal Hallucinations in Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2601.21181)|Sangyun Chung, Yeon Mi Kim, Youngchae Chee, Yong Man Ro|2026-01-29|ArXiv.org|[![Star](https://img.shields.io/github/stars/top-yun/MAD)](https://github.com/top-yun/MAD)|
|67|[LLM4Fluid: Large Language Models as Generalizable Neural Solvers for Fluid Dynamics](https://doi.org/10.48550/arxiv.2601.21681)|Qisong Xiao, Xinhai Chen, Qinglin Wang, Xiaowei Guo, Binglin Wang, Weifeng Chen, Zhichao Wang, Yunfei Liu, Rui Xia, Hang...|2026-01-29|ArXiv.org|[![Star](https://img.shields.io/github/stars/qisongxiao/LLM4Fluid)](https://github.com/qisongxiao/LLM4Fluid)|
|68|[Rewarding Intellectual Humility Learning When Not To Answer In Large Language Models](https://doi.org/10.48550/arxiv.2601.20126)|Abha Jha, Akanksha Mahajan, Ashwath Vaithinathan Aravindan, Praveen Saravanan, Sai Sailaja Policharla, Sonal Chaturbhuj ...|2026-01-27|ArXiv.org|[![Star](https://img.shields.io/github/stars/Mystic-Slice/rl-abstention)](https://github.com/Mystic-Slice/rl-abstention)|
|69|[Marco para la Evaluaci√≥n de la Emergencia de Identidad en Modelos de Lenguaje Extensos (LLMs)/ Technical Report: Framework for Assessing Identity Emergence in Large Language Models..](https://doi.org/10.5281/zenodo.18381208)|Viviana Isabel Loizzo Petrillo|2026-01-26|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/vivianaloizzo-gif/SER-Project-EIS)](https://github.com/vivianaloizzo-gif/SER-Project-EIS)|
|70|[Assessment of Generative Named Entity Recognition in the Era of Large Language Models](https://doi.org/10.48550/arxiv.2601.17898)|Qi Zhan, Yile Wang, Hui Huang|2026-01-25|ArXiv.org|[![Star](https://img.shields.io/github/stars/szu-tera/LLMs4NER)](https://github.com/szu-tera/LLMs4NER)|
|71|[VidLaDA: Bidirectional Diffusion Large Language Models for Efficient Video Understanding](https://doi.org/10.48550/arxiv.2601.17868)|Zhihao He, Tieyuan Chen, Kangyu Wang, Ziran Qin, Yang Shao, Chaofan Gan, Shijie Li, Zuxuan Wu, Weiyao Lin|2026-01-25|ArXiv.org|[![Star](https://img.shields.io/github/stars/ziHoHe/VidLaDA)](https://github.com/ziHoHe/VidLaDA)|
|72|[Reconstructing Training Data from Adapter-based Federated Large Language Models](https://doi.org/10.48550/arxiv.2601.17533)|Silong Chen, Yuchuan Luo, Guilin Deng, Yi Liu, Min Xu, Shaojing Fu, Xiaohua Jia|2026-01-24|ArXiv.org|[![Star](https://img.shields.io/github/stars/shwksnshwowk-wq/GIA)](https://github.com/shwksnshwowk-wq/GIA)|
|73|[Towards Fair Large Language Model-based Recommender Systems without Costly Retraining](https://doi.org/10.48550/arxiv.2601.17492)|Jin Li, Huilin Gu, Shoujin Wang, Qi Zhang, Shui Yu, Chen Wang, Xiwei Xu, Fang Chen|2026-01-24|ArXiv.org|[![Star](https://img.shields.io/github/stars/JinLi-i/FUDLR)](https://github.com/JinLi-i/FUDLR)|
|74|[Are Vision Large Language Models Road-Ready? Benchmarking and Adapting VLLMs for Safety-Critical Driving Video Understanding](https://vtechworks.lib.vt.edu/items/bea3730e-ba3a-4aae-9fe2-2a6a9f0c56e3/request-a-copy?bitstream=e70f646a-e181-47e0-b734-aefde8e2a29a)|Tong Zeng|2026-01-23|VTechWorks (Virginia Tech)|[![Star](https://img.shields.io/github/stars/tong-zeng/DVBench)](https://github.com/tong-zeng/DVBench.git)|
|75|[Code for the Trilemma of Truth in Large Language Models](https://doi.org/10.5281/zenodo.18356450)|Savcisens, Germans, Eliassi-Rad, Tina|2026-01-23|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/carlomarxdk/trilemma-of-truth)](https://github.com/carlomarxdk/trilemma-of-truth)|
|76|[Large Language Models for Assisting American College Applications](https://doi.org/10.48550/arxiv.2602.15850)|Zhengliang Liu, Weihang You, Peng Shu, Junhao Chen, Yi Pan, Hanqi Jiang, Yiwei Li, Zude Ding, Chao Cao, Xinliang Li, Yif...|2026-01-23|ArXiv.org|[![Star](https://img.shields.io/github/stars/ezcollegeapp-public/ezcollegeapp-public)](https://github.com/ezcollegeapp-public/ezcollegeapp-public)|
|77|[Grounding Large Language Models in Reaction Knowledge Graphs for Synthesis Retrieval](https://doi.org/10.48550/arxiv.2601.16038)|Olga Bunkova, Lorenzo Di Fruscia, Sophia Rupprecht, Artur M. Schweidtmann, Marcel J. T. Reinders, Jana M. Weber|2026-01-22|ArXiv.org|[![Star](https://img.shields.io/github/stars/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval)](https://github.com/Intelligent-molecular-systems/KG-LLM-Synthesis-Retrieval)|
|78|[Searching the Druggable Genome using Large Language Models](https://doi.org/10.64898/2026.01.18.700012)|Lars Schimmelpfennig, Matthew Cannon, Quentin Cody, Joshua McMichael, Adam Coffman, Susanna Kiwala, Kilannin J Krysiak, ...|2026-01-21|OpenAlex|[![Star](https://img.shields.io/github/stars/griffithlab/dgidb-mcp-server)](https://github.com/griffithlab/dgidb-mcp-server)|
|79|[KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?](https://doi.org/10.48550/arxiv.2601.13240)|Xue Jiang, Jiaru Qian, Shi Xianjie, Chenjie Li, Hao Zhu, Ziyu Wang, Jielun Zhang, Zheyu Zhao, Kechi Zhang, Jia Hui Li, W...|2026-01-19|arXiv|[![Star](https://img.shields.io/github/stars/jiangxxxue/KOCO-bench)](https://github.com/jiangxxxue/KOCO-bench)|
|80|[ChatCFD: A Large Language Model‚ÄêDriven Agent for End‚Äêto‚ÄêEnd Computational Fluid Dynamics Automation with Structured Knowledge and Reasoning](https://doi.org/10.1002/aidi.202500174)|E. Fan, Kang Hu, Zhuowen Wu, Jie Ge, Jiawei Miao, Duo Zhang, H. K. Sun, Weizong Wang, Tianhan Zhang|2026-01-19|Advanced Intelligent Discovery|[![Star](https://img.shields.io/github/stars/ConMoo/ChatCFD)](https://github.com/ConMoo/ChatCFD)|
|81|[GARD: Genomic Data based Drug Repurposing in Head and Neck Cancer with Large Language Model Validation](https://doi.org/10.64898/2026.01.15.699561)|Pradham Tanikella, William Nenad, C. Courtine, Yifan Dai, Qingying Deng, Meuleman, Nosayaba Osazuwa-Peters, T. Parke Sch...|2026-01-16|OpenAlex|[![Star](https://img.shields.io/github/stars/pvtanike/Genomic-Landscape-Based-Drug-Repurposing)](https://github.com/pvtanike/Genomic-Landscape-Based-Drug-Repurposing.git)|
|82|[Reliability Inference Drives Cue Extraction in Large Language Models Consuming External Reasoning Traces](https://github.com/HIDEKI-SQ/cot-reliability-gating)|HIDEKI|2026-01-15|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/HIDEKI-SQ/cot-reliability-gating)](https://github.com/HIDEKI-SQ/cot-reliability-gating)|
|83|[Mapping the Collaboration between Crowdsourcing and Large Language Models: A Fine-Grained Survey](https://doi.org/10.5772/intechopen.1013999)|Chunli Lv, Cheng Shen, Miao Xie|2026-01-13|IntechOpen eBooks|[![Star](https://img.shields.io/github/stars/Ikaros-sc/crowdsourcing)](https://github.com/Ikaros-sc/crowdsourcing)|
|84|[Semantic Geometry and Hallucination Behavior in Large Language Models](https://doi.org/10.5281/zenodo.18226812)|HIDEYUKI, CHINO|2026-01-13|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/hideyuki49/llm-hallucination-geometry)](https://github.com/hideyuki49/llm-hallucination-geometry)|
|85|[Can Large Language Models Reduce the Cost of Extracting Data from Electronic Health Records for Research?](https://doi.org/10.64898/2026.01.09.26343792)|Stuart Hagler, Mohammad Adibuzzaman, Daniel Bottomly, Aaron Cohen|2026-01-11|OpenAlex|[![Star](https://img.shields.io/github/stars/sehagler/llm_biomarker_extraction)](https://github.com/sehagler/llm_biomarker_extraction)|
|86|[Large Language Models for Software Engineering: A Systematic Literature Review](https://doi.org/10.48550/arXiv.2308.10620)|Xinyi Hou, Yanjie Zhao, Yue Liu, Zhou Yang, Kailong Wang, Li Li, Xiapu Luo, David Lo, John C. Grundy, Haoyu Wang|2026-01-11|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/xinyi-hou/LLM4SE_SLR)](https://github.com/xinyi-hou/LLM4SE_SLR)|
|87|[Machine translationese of large language models: Dependency triplets, text classification, and SHAP analysis](https://doi.org/10.1371/journal.pone.0339769)|Shukang Zhang, Chaoyong Zhao|2026-01-09|PLoS ONE|[![Star](https://img.shields.io/github/stars/KiemaG5/LLM-translationese)](https://github.com/KiemaG5/LLM-translationese)|
|88|[EquaCode: A Multi-Strategy Jailbreak Approach for Large Language Models via Equation Solving and Code Completion](https://doi.org/10.48550/arxiv.2512.23173)|Association for Artificial Intelligence 2026, Zhengkui Chen, Hai Huang, Zhen Liang|2026-01-07|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/lzzzr123/Equacode)](https://github.com/lzzzr123/Equacode)|
|89|[Mitigating Hallucinations in Large Language Models via Causal Reasoning](https://doi.org/10.48550/arXiv.2508.12495)|Association for Artificial Intelligence 2026, Jiechao Gao, Xiyang Hu, Li Li, Yuangang Li, Yi Nian, Yiqing Shen, Jie Wang...|2026-01-07|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/MrLYG/CDCR-SFT)](https://github.com/MrLYG/CDCR-SFT)|
|90|[Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework](https://doi.org/10.48550/arxiv.2511.13189)|Association for Artificial Intelligence 2026, Mario Almagro, Kunal Dahiya, David Jimenez-Cabello, Diego Ortego, Marlon R...|2026-01-07|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/DiegoOrtego/vixml)](https://github.com/DiegoOrtego/vixml)|
|91|[HSKBenchmark: Modeling and Benchmarking Chinese Second Language Acquisition in Large Language Models Through Curriculum Tuning](https://doi.org/10.48550/arxiv.2511.15574)|Association for Artificial Intelligence 2026, Jiale Chen, Xuelian Dong, Tianyong Hao, Yuxin Hao, Xuelin Wang, Qihao Yang|2026-01-07|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/CharlesYang030/HSKB)](https://github.com/CharlesYang030/HSKB)|
|92|[ARTEM: Enhancing Large Language Model Agents with Spatial-Temporal Episodic Memory](https://doi.org/10.48448/qfbj-4d34)|Association for Artificial Intelligence 2026, Budhitama Subagdja, Cassandra Hui-Ming Tan, Ah-Hwee Tan|2026-01-07|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/cassthm/ARTEM)](https://github.com/cassthm/ARTEM)|
|93|[Cross-Modal Unlearning via Influential Neuron Path Editing in Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2511.06793)|Association for Artificial Intelligence 2026, Jun Bai, Di Di Wu, Ju Jia, Kunhao Li, Wenhao Li, Minhui Xue, Lei Yang|2026-01-07|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/PreckLi/MIP-Editor)](https://github.com/PreckLi/MIP-Editor)|
|94|[Attention to Threat-Relevant Objects: Reasoning Detection in Autonomous Driving via Multimodal Large Language Models](https://doi.org/10.48448/y275-r804)|Association for Artificial Intelligence 2026, Wei R. Chen, Xinbiao Gan, Yulin He, Yusong Tan, Siqi Wang, Haotian Wang|2026-01-07|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/harrylin-hyl/Threat-ReasonDet)](https://github.com/harrylin-hyl/Threat-ReasonDet)|
|95|[Agentmandering: A Game-Theoretic Framework for Fair Redistricting via Large Language Model Agents](https://doi.org/10.48448/79qd-9h42)|Association for Artificial Intelligence 2026, Haotian Chen, Rong Gong, Hao Jiang, Hao Li, Juanjuan Wang|2026-01-07|Underline Science Inc.|[![Star](https://img.shields.io/github/stars/Lihaogx/AgentMandering)](https://github.com/Lihaogx/AgentMandering)|
|96|[ARREST: Adversarial Resilient Regulation Enhancing Safety and Truth in Large Language Models](https://doi.org/10.48550/arxiv.2601.04394)|Sharanya Dasgupta, Arkaprabha Basu, Sujoy Nath, Swagatam Das|2026-01-07|arXiv|[![Star](https://img.shields.io/github/stars/sharanya-dasgupta001/ARREST)](https://github.com/sharanya-dasgupta001/ARREST)|
|97|[EnrichGT: a comprehensive R-based tool for functional genomics enrichment analysis based on large language models](https://doi.org/10.20517/ais.2025.67)|Runchen Wang, Zhiming Ye, QiXia Wang, Bo Liang, Nanfei Fu, Wenxi Wang, Huimin Deng, Taimin Zhu, Shangxi Zeng, Yudong Zha...|2026-01-06|Artificial Intelligence Surgery|[![Star](https://img.shields.io/github/stars/saezlab/CollecTRI)](https://github.com/saezlab/CollecTRI)|
|98|[Large Language Models for Computer-Aided Design: A Survey](https://doi.org/10.48550/arXiv.2505.08137)|Licheng Zhang, Bach Le, Naveed Akhtar, Siew-Kei Lam, Tuan Ngo|2026-01-06|ACM Computing Surveys|[![Star](https://img.shields.io/github/stars/lichengzhanguom/LLMs-CAD-Survey-Taxonomy)](https://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy)|
|99|[Robin: an advanced tool for comparative loop caller result analysis leveraging large language models](https://doi.org/10.1101/2025.04.05.646688)|H. M. A. Mohit Chowdhury, Mattie Fuller, Oluwatosin Oluwadare|2026-01-06|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/OluwadareLab/Robin)](https://github.com/OluwadareLab/Robin)|
|100|[MDAgent2: Large Language Model for Code Generation and Knowledge Q&amp;A in Molecular Dynamics](https://doi.org/10.48550/arxiv.2601.02075)|Zhuofan Shi, Hen A, Yun Shao, Mengyan Dai, Yadong Yu, Pan Xiang, Dongliang Huang, Heping An, C. J. Xin, Haiyang Shen, Zh...|2026-01-05|arXiv|[![Star](https://img.shields.io/github/stars/FredericVAN/PKU_MDAgent2)](https://github.com/FredericVAN/PKU_MDAgent2)|
|101|[QWED Protocol: Deterministic Verification for Large Language Models](https://github.com/QWED-AI/qwed-verification)|Rahul Dass|2026-01-04|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/QWED-AI/qwed-verification)](https://github.com/QWED-AI/qwed-verification)|
|102|[Locate, Steer, and Improve: A Practical Survey of Actionable Mechanistic Interpretability in Large Language Models](https://doi.org/10.20944/preprints202601.1663.v1)|Hengyuan Zhang, Zhihao Zhang, Mingyang Wang, Zunhai Su, Yiwei Wang, Qianli Wang, Shuzhou Yuan, Ercong Nie, Xufeng Duan, ...|2026-01-01|Preprints.org|[![Star](https://img.shields.io/github/stars/rattlesnakey/Awesome-Actionable-MI-Survey)](https://github.com/rattlesnakey/Awesome-Actionable-MI-Survey)|
|103|[Yuan3.0 Flash: An Open Multimodal Large Language Model for Enterprise Applications](https://doi.org/10.48550/arXiv.2601.01718)|Shawn Wu, Sean Wang, Louie Li, Darcy Chen, Allen Wang, Jiangang Luo, Xudong Zhao, Joseph Shen, Gawain Ma, Jasper Jia, Ma...|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/Yuan-lab-LLM/Yuan3.0)](https://github.com/Yuan-lab-LLM/Yuan3.0)|
|104|[Survey on Factuality in Large Language Models](https://doi.org/10.1145/3742420)|Cunxiang Wang, Xiaoze Liu, Yuanhao Yue, Qipeng Guo, Xiangkun Hu, Xiangru Tang, Tianhang Zhang, Cheng Jiayang, Yunzhi Yao...|2026-01-01|ACM Computing Surveys|[![Star](https://img.shields.io/github/stars/wangcunxiang/LLM-Factuality-Survey)](https://github.com/wangcunxiang/LLM-Factuality-Survey)|
|105|[Speak While Watching: Unleashing TRUE Real-Time Video Understanding Capability of Multimodal Large Language Models](https://doi.org/10.48550/arXiv.2601.06843)|Junyan Lin, Junlong Tong, Hao Wu, Jialiang Zhang, Jinming Liu, Xin Jin, Xiaoyu Shen|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/EIT-NLP/Speak-While-Watching)](https://github.com/EIT-NLP/Speak-While-Watching)|
|106|[SketchJudge: A Diagnostic Benchmark for Grading Hand-drawn Diagrams with Multimodal Large Language Models](https://doi.org/10.48550/arXiv.2601.06944)|Yuhang Su, Mei Wang, Yaoyao Zhong, Guozhang Li, Shixing Li, Yihan Feng, Hua Huang|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/yuhangsu82/SketchJudge)](https://github.com/yuhangsu82/SketchJudge)|
|107|[Safe-FedLLM: Delving into the Safety of Federated Large Language Models](https://doi.org/10.48550/arXiv.2601.07177)|Mingxiang Tao, Yu Tian, Wenxuan Tu, Yue Yang, Xue Yang, Xiangyan Tang|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/dmqx/Safe-FedLLM)](https://github.com/dmqx/Safe-FedLLM)|
|108|[Rethinking Large Language Models For Irregular Time Series Classification In Critical Care](https://doi.org/10.48550/arXiv.2601.16516)|Feixiang Zheng, Yu Wu, Cecilia Mascolo, Ting Dang|2026-01-01|ArXiv.org|[![Star](https://img.shields.io/github/stars/mHealthUnimelb/LLMTS)](https://github.com/mHealthUnimelb/LLMTS)|
|109|[RedBench: A Universal Dataset for Comprehensive Red Teaming of Large Language Models](https://doi.org/10.48550/arXiv.2601.03699)|Quy-Anh Dang, Chris Ngo, Truong-Son Hy|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/knoveleng/redeval)](https://github.com/knoveleng/redeval)|
|110|[Persona Jailbreaking in Large Language Models](https://doi.org/10.48550/arXiv.2601.16466)|Jivnesh Sandhan, Fei Cheng, Tushar Sandhan, Yugo Murawaki|2026-01-01|ArXiv.org|[![Star](https://img.shields.io/github/stars/Jivnesh/PHISH)](https://github.com/Jivnesh/PHISH)|
|111|[PERM: Psychology-grounded Empathetic Reward Modeling for Large Language Models](https://doi.org/10.48550/arXiv.2601.10532)|Chengbing Wang, Wuqiang Zheng, Yang Zhang, Fengbin Zhu, Junyi Cheng, Yi Xie, Wenjie Wang, Fuli Feng|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/ZhengWwwq/PERM)](https://github.com/ZhengWwwq/PERM)|
|112|[Neural Chain-of-Thought Search: Searching the Optimal Reasoning Path to Enhance Large Language Models](https://doi.org/10.48550/arXiv.2601.11340)|Guoming Ling, Zhongzhan Huang, Yupei Lin, Junxin Li, Shanshan Zhong, Hefeng Wu, Liang Lin|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/MilkThink-Lab/Neural-CoT-Search)](https://github.com/MilkThink-Lab/Neural-CoT-Search)|
|113|[Memory Bank Compression for Continual Adaptation of Large Language Models](https://doi.org/10.48550/arXiv.2601.00756)|Thomas Katraouras, Dimitrios Rafailidis|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/Thomkat/MBC)](https://github.com/Thomkat/MBC)|
|114|[Measuring Social Media Polarization Using Large Language Models and Heuristic Rules](https://doi.org/10.1007/978-3-032-14107-1_35)|Jawad Mahmud Chowdhury, Rezaur Rashid, Gabriel Terejanu|2026-01-01|Lecture notes in computer science|[![Star](https://img.shields.io/github/stars/hasanjawad001/llm-social-media-polarization)](https://github.com/hasanjawad001/llm-social-media-polarization)|
|115|[SKiM-GPT: combining biomedical literature-based discovery with large language model hypothesis evaluation](https://doi.org/10.6084/m9.figshare.c.8257171.v1)|Jack Freeman, Robert J. Millikin, L. Xu, Ishaan Sharma, Bethany Moore, Cannon Lock, Kevin Shine George, Aviral Bal, Chit...|2026-01-01|BMC Bioinformatics|[![Star](https://img.shields.io/github/stars/stewart-lab/skimgpt)](https://github.com/stewart-lab/skimgpt)|
|116|[Layer-Order Inversion: Rethinking Latent Multi-Hop Reasoning in Large Language Models](https://doi.org/10.48550/arXiv.2601.03542)|Xukai Liu, Ye Liu, Jipeng Zhang, Yanghai Zhang, Kai Zhang, Qi Liu|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/laquabe/Layer-Order-Inversion)](https://github.com/laquabe/Layer-Order-Inversion)|
|117|[Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing](https://doi.org/10.48550/arXiv.2601.10543)|Yinzhi Zhao, Ming Wang, Shi Feng, Xiaocui Yang, Daling Wang, Yifei Zhang|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/zyz13590/SafeProbing)](https://github.com/zyz13590/SafeProbing)|
|118|[Large Language Model-Powered Evolutionary Code Optimization on a Phylogenetic Tree](https://doi.org/10.48550/arXiv.2601.14523)|Leyi Zhao, Weijie Huang, Yitong Guo, Jiang Bian, Chenghong Wang, Xuhong Zhang|2026-01-01|ArXiv.org|[![Star](https://img.shields.io/github/stars/annihi1ation/phylo_evolve)](https://github.com/annihi1ation/phylo_evolve)|
|119|[AnatomiX, an Anatomy-Aware Grounded Multimodal Large Language Model for Chest X-Ray Interpretation](https://doi.org/10.48550/arXiv.2601.03191)|Anees Ur Rehman Hashmi, Numan Saeed, Christoph Lippert|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/aneesurhashmi/anatomix)](https://github.com/aneesurhashmi/anatomix)|
|120|[BioMedGPT: An Open Multimodal Large Language Model for BioMedicine](https://doi.org/10.1109/JBHI.2024.3505955)|Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Massimo Hong, Yushuai Wu, Mu Qiao, Zaiqing Nie|2026-01-01|IEEE Journal of Biomedical and Health Informatics|[![Star](https://img.shields.io/github/stars/PharMolix/OpenBioMed)](https://github.com/PharMolix/OpenBioMed)|
|121|[Bridging the Semantic Gap for Categorical Data Clustering via Large Language Models](https://doi.org/10.48550/arXiv.2601.01162)|Zihua Yang, Xin Liao, Yiqun Zhang, Yiu-ming Cheung|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/develop-yang/ARISE)](https://github.com/develop-yang/ARISE)|
|122|[DAMON: Difference-Aware Medical Visual Question Answering via Multimodal Large Language Model](https://doi.org/10.1109/jbhi.2026.3663420)|Zefan Zhang, Yanhui Li, Ruihong Zhao, Tian Bai|2026-01-01|IEEE Journal of Biomedical and Health Informatics|[![Star](https://img.shields.io/github/stars/zefanZhang-cn/DAMON)](https://github.com/zefanZhang-cn/DAMON)|
|123|[A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits](https://doi.org/10.48550/arXiv.2601.12945)|Miao Xie, Siguang Chen, Chunli Lv|2026-01-01|ArXiv.org|[![Star](https://img.shields.io/github/stars/bucky1119/Awesome-LLM-Bandit-Interaction)](https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction)|
|124|[Hierarchical Orthogonal Residual Spread for Precise Massive Editing in Large Language Models](https://doi.org/10.48550/arXiv.2601.11441)|Xiaojie Gu, Guangxu Chen, Yuheng Yang, Jingxin Han, Andi Zhang|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/XiaojieGu/HORSE)](https://github.com/XiaojieGu/HORSE)|
|125|[Hybrid Dual-Semantics Modeling for Enhancing Large Language Model Based Recommendation](https://doi.org/10.1145/3773966.3777943)|Canyi Liu, Tianyi Li, Wei Li, Youchen Zhang, Xiaodong Li, Hui Li|2026-01-01|OpenAlex|[![Star](https://img.shields.io/github/stars/KDEGroup/HDRec)](https://github.com/KDEGroup/HDRec)|
|126|[Language of Thought Shapes Output Diversity in Large Language Models](https://doi.org/10.48550/arXiv.2601.11227)|Shaoyang Xu, Wenxuan Zhang|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/iNLP-Lab/Multilingual-LoT-Diversity)](https://github.com/iNLP-Lab/Multilingual-LoT-Diversity)|
|127|[Large Language Model Judged Self-Training for Named Entity Recognition](https://doi.org/10.1145/3773966.3778009)|Shisong Chen, Jiaan Wang, Chengyi Yang, Yanghua Xiao, Zhixu Li, Xin Lin|2026-01-01|OpenAlex|[![Star](https://img.shields.io/github/stars/cheniison/llm-judged-ST)](https://github.com/cheniison/llm-judged-ST)|
|128|[Large Language Model for OWL Proofs](https://doi.org/10.48550/arXiv.2601.12444)|Hui Yang, Jiaoyan Chen, Uli Sattler|2026-01-01|arXiv|[![Star](https://img.shields.io/github/stars/HuiYang1997/LLMOwlR)](https://github.com/HuiYang1997/LLMOwlR)|
|129|[GOFlowLLM - curating miRNA literature with large language models and flowcharts](https://doi.org/10.1093/bioinformatics/btaf683)|Andrew Green, Nancy Ontiveros-Palacios, Isaac Jandalala, Simona Panni, Valerie Wood, Giulia Antonazzo, Helen Attrill, Al...|2026-01-01|bioRxiv (Cold Spring Harbor Laboratory)|[![Star](https://img.shields.io/github/stars/RNAcentral/GO_Flow_LLM)](https://github.com/RNAcentral/GO_Flow_LLM)|
|130|[PriceSeer: Evaluating Large Language Models in Real-Time Stock Prediction](https://doi.org/10.48550/arxiv.2601.06088)|Bohan Liang, Zijian Chen, Qi Jia, Kaiwei Zhang, Kaiyuan Ji, Guangtao Zhai|2025-12-31|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/BobLiang2113/PriceSeer)](https://github.com/BobLiang2113/PriceSeer)|
|131|[HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors](https://doi.org/10.48550/arxiv.2512.24478)|Hyunjun Kim|2025-12-30|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hyunjun1121/holograph)](https://github.com/hyunjun1121/holograph)|
|132|[Leveraging Adaptive Group Negotiation for Heterogeneous Multi-Robot Collaboration with Large Language Models](https://doi.org/10.48550/arxiv.2602.06967)|Siqi Song, Xinyan Xie, Ru Li, Yuqiang Li, Shijie Wang, Biqing Qi|2025-12-29|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/song-siqi/CLiMRS)](https://github.com/song-siqi/CLiMRS)|
|133|[StatLLaMA: Multi-Stage training for domain-optimized statistical large language models](https://doi.org/10.48550/arxiv.2601.09718)|Jing-Yi Zeng, Guanhua Huang|2025-12-26|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/HuangDLab/StatLLaMA)](https://github.com/HuangDLab/StatLLaMA)|
|134|[Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles](https://doi.org/10.48550/arxiv.2512.20324)|Nurul Labib Sayeedi, Md. Faiyaz Abdullah Sayeedi, Khushnur Binte Jahangir, Swakkhar Shatabda, Sarah Masud Preum|2025-12-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Labib1610/BanglaRiddleEval)](https://github.com/Labib1610/BanglaRiddleEval)|
|135|[Toward Explaining Large Language Models in Software Engineering Tasks](https://doi.org/10.48550/arxiv.2512.20328)|Antonio Vitale, Khai-Nguyen Nguyen, Denys Poshyvanyk, Rocco Oliveto, Simone Scalabrino, Antonio Mastropaolo|2025-12-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/deviserlab/FeatureSHAP)](https://github.com/deviserlab/FeatureSHAP)|
|136|[When Less is More: 8-bit Quantization Improves Continual Learning in Large Language Models](https://doi.org/10.48550/arxiv.2512.18934)|Michael Zhang, Rishi A. Ruia, Arnav Kewalram, Saathvik Dharmapuram, Utkarsh Sharma, Kevin Zhu|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Festyve/LessIsMore)](https://github.com/Festyve/LessIsMore)|
|137|[dMLLM-TTS: Self-Verified and Efficient Test-Time Scaling for Diffusion Multi-Modal Large Language Models](https://doi.org/10.48550/arxiv.2512.19433)|Yi Xin, Siqi Luo, Qi Qin, Haoxing Chen, Kaiwen Zhu, Zhiwei Zhang, Yangfan He, Rongchao Zhang, Jinbin Bai, Shuo Cao, Bin ...|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Alpha-VLLM/Lumina-DiMOO)](https://github.com/Alpha-VLLM/Lumina-DiMOO)|
|138|[Anatomy-R1: Enhancing Anatomy Reasoning in Multimodal Large Language Models via Anatomical Similarity Curriculum and Group Diversity Augmentation](https://doi.org/10.48550/arxiv.2512.19512)|Ziyang Song, Zelin Zang, Zuyao Chen, Xusheng Liang, Dong Hoon Yi, Jinlin Wu, Hongbin Liu, Jiebo Luo|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/tomato996/Anatomy-R1)](https://github.com/tomato996/Anatomy-R1)|
|139|[PENDULUM: A Benchmark for Assessing Sycophancy in Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2512.19350)|A. B. M. Ashikur Rahman, Saeed Anwar, Muhammad Usman, Irfan Ahmad, Ajmal Mian|2025-12-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ashikiut/pendulum)](https://github.com/ashikiut/pendulum)|
|140|[EXa-LM: A Controlled Natural Language Bridge between Large Language Models and First-Order Logic Solvers](https://doi.org/10.20944/preprints202512.1848.v1)|Frydman, Francis|2025-12-22|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/FFrydman/eXa-LM)](https://github.com/FFrydman/eXa-LM)|
|141|[CrashChat: A Multimodal Large Language Model for Multitask Traffic Crash Video Analysis](https://doi.org/10.48550/arxiv.2512.18878)|Kaidi Liang, Ke Li, Xianbiao Hu, Ruwen Qin|2025-12-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Liangkd/CrashChat)](https://github.com/Liangkd/CrashChat)|
|142|[Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://doi.org/10.1371/journal.pdig.0001242)|Nan Miles Xi, Yu Deng, Lin Wang|2025-12-21|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/isegura/NLP4RARE-CM-UC3M)](https://github.com/isegura/NLP4RARE-CM-UC3M)|
|143|[Attention Distance: A Novel Metric for Directed Fuzzing with Large Language Models](https://doi.org/10.48550/arxiv.2512.19758)|Wang Bin, Ao Yang, Kedan Li, Liu Aofan, Hui Li, Guibo Luo, Weixiang Huang, Yan Zhuang|2025-12-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/TheBinKing/Attention)](https://github.com/TheBinKing/Attention)|
|144|[Do You Trust Me? Cognitive-Affective Signatures of Trustworthiness in Large Language Models](https://doi.org/10.48550/arxiv.2601.10719)|Gerard Christopher Yeo, Svetlana Churina, Kokil Jaidka|2025-12-17|arXiv|[![Star](https://img.shields.io/github/stars/GerardYeo/TrustworthinessLLM)](https://github.com/GerardYeo/TrustworthinessLLM)|
|145|[Dynamic Rank Reinforcement Learning for Adaptive Low-Rank Multi-Head Self Attention in Large Language Models](https://doi.org/10.48550/arxiv.2512.15973)|Erden, Caner|2025-12-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/canererden/DR_RL_Project)](https://github.com/canererden/DR_RL_Project)|
|146|[Seeing Beyond Words: Self-Supervised Visual Learning for Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2512.15885)|Caffagni, Davide, Sarto, Sara, Cornia, Marcella, Baraldi, Lorenzo, Dovesi, Pier Luigi, Roohi, Shaghayegh, Granroth-Wildi...|2025-12-17|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/aimagelab/JARVIS)](https://github.com/aimagelab/JARVIS)|
|147|[Replication Data for "Estimating problem difficulty without ground truth using Large Language Model comparisons"](https://doi.org/10.5281/zenodo.17523640)|Ballon, Marthe, Algaba, Andres, Verbeken, Brecht, Ginis, Vincent|2025-12-16|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/MartheBallon/estimating-problem-difficulty-without-ground-truth)](https://github.com/MartheBallon/estimating-problem-difficulty-without-ground-truth)|
|148|[What Affects the Effective Depth of Large Language Models?](http://arxiv.org/abs/2512.14064)|Yi Hu, Cai Zhou, Muhan Zhang|2025-12-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/AheadOFpotato/what_affects_effective_depth)](https://github.com/AheadOFpotato/what_affects_effective_depth)|
|149|[RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees](http://arxiv.org/abs/2512.14069)|Junjie Ma, Jinlong Li|2025-12-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/minaduki-sora/RADAR)](https://github.com/minaduki-sora/RADAR)|
|150|[Do Reviews Matter for Recommendations in the Era of Large Language Models?](https://doi.org/10.48550/arxiv.2512.12978)|Tan, Chee Heng, Zheng, Huiying, Wang, Jing, Lin, Zhuoyi, Feng, Shaodi, Zhan, Huijing, Li, Xiaoli, Senthilnath, J.|2025-12-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/zhytk/RAREval-data-processing)](https://github.com/zhytk/RAREval-data-processing)|
|151|[FIN-bench-v2: A Unified and Robust Benchmark Suite for Evaluating Finnish Large Language Models](https://doi.org/10.48550/arxiv.2512.13330)|Kyt√∂niemi, Joona, Piha, Jousia, Reunamo, Akseli, Vitiugin, Fedor, Mehryary, Farrokh, Pyysalo, Sampo|2025-12-15|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LumiOpen/lm-evaluation-harness)](https://github.com/LumiOpen/lm-evaluation-harness)|
|152|[A Dual Large Language Models Architecture with Herald Guided Prompts for Parallel Fine Grained Traffic Signal Control](https://doi.org/10.1109/icpads67057.2025.11323137)|Qing Guo, Xinhang Li, Junyu Chen, Zheng Guo, Xiaocong Li, Long Wei, Lei Li|2025-12-14|OpenAlex|[![Star](https://img.shields.io/github/stars/BUPT-ANTlab/HeraldLight)](https://github.com/BUPT-ANTlab/HeraldLight)|
|153|[DL$^3$M: A Vision-to-Language Framework for Expert-Level Medical Reasoning through Deep Learning and Large Language Models](http://arxiv.org/abs/2512.13742)|Md. Hasibul Hasan, Imran Ahmad, Sourav Basak Shuvo, Md. Mahadi Hasan Ankon, Sunanda Das, Nazmul Siddique, Hui Wang|2025-12-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/souravbasakshuvo/DL3M)](https://github.com/souravbasakshuvo/DL3M)|
|154|[CP-Env: Evaluating Large Language Models on Clinical Pathways in a Controllable Hospital Environment](http://arxiv.org/abs/2512.10206)|Yu Zhu, Zhongzhen Huang, Qianhan Feng, Linjie Mu, Yannian Gu, Shaoting Zhang, Qi Dou, Xiaofan Zhang|2025-12-11|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/SPIRAL-MED/CP_ENV)](https://github.com/SPIRAL-MED/CP_ENV)|
|155|[GLACIA: Instance-Aware Positional Reasoning for Glacial Lake Segmentation via Multimodal Large Language Model](https://doi.org/10.48550/arxiv.2512.09251)|Maurya, Lalit, Kaushik, Saurabh, Tellman, Beth|2025-12-10|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/lalitmaurya47/GLACIA)](https://github.com/lalitmaurya47/GLACIA)|
|156|[Benchmarking large language models for identifying transcription factor regulatory interactions](https://doi.org/10.1093/bioinformatics/btaf653)|L.H. No√´l, Yi-Wen Hsiao, Yimeng He, Andrew J. Hung, Xiaojiang Cui, Edward Ray, Jason H. Moore, Pei-Chen Peng, Xiuzhen Hu...|2025-12-09|Bioinformatics|[![Star](https://img.shields.io/github/stars/pengpclab/LLM-TF-interactions)](https://github.com/pengpclab/LLM-TF-interactions)|
|157|[Automatic Syntax Error Repair for Discrete Controller Synthesis using Large Language Model](https://doi.org/10.48550/arxiv.2512.07261)|Ishimizu, Yusei, Yamauchi, Takuto, Chen, Sinan, Cai, Jinyu, Li, Jialong, Tei, Kenji|2025-12-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Uuusay1432/DCSModelRepair)](https://github.com/Uuusay1432/DCSModelRepair.git)|
|158|[RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models](https://doi.org/10.48550/arxiv.2512.07761)|Xiong, Xiqiao, Li, Ouxiang, Liu, Zhuo, Li, Moxin, Shi, Wentao, Feng, Fuli, He, Xiangnan|2025-12-08|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/xxiqiao/RL-MTJail)](https://github.com/xxiqiao/RL-MTJail)|
|159|[1 + 1 &gt; 2: Detector-Empowered Video Large Language Model for Spatio-Temporal Grounding and Reasoning](https://doi.org/10.48550/arxiv.2512.06673)|Gao Shida, Xue Feng, WANG Xiangfeng, Ming, Anlong, Long Teng, Shao Yi-hua, Wang, Haozhe, Lin Zhaowen, Wang Wei, Sebe, Ni...|2025-12-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/gaostar123/DeViL)](https://github.com/gaostar123/DeViL)|
|160|[Latency-Response Theory Model: Evaluating Large Language Models via Response Accuracy and Chain-of-Thought Length](https://doi.org/10.48550/arxiv.2512.07019)|Xu, Zhiyu, Liu, Jia, Wang, Yixin, Gu, Yuqi|2025-12-07|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Toby-X/Latency-Response-Theory-Model)](https://github.com/Toby-X/Latency-Response-Theory-Model)|
|161|[Unleashing the Intrinsic Visual Representation Capability of Multimodal Large Language Models](https://doi.org/10.48550/arxiv.2512.06281)|Li, Hengzhuang, Zhang, Xinsong, Peng, Qiming, Luo, Bin, Hu, Han, Jiang, Dengyang, Ye, Han-Jia, Zhang, Teng, Jin, Hai|2025-12-06|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Fir-lat/LaVer)](https://github.com/Fir-lat/LaVer)|
|162|[Empathy by Design: Aligning Large Language Models for Healthcare Dialogue](https://doi.org/10.48550/arxiv.2512.06097)|Umucu, Emre, Solis, Guillermina, Garza, Leon, Rivas, Emilia, Lee, Beatrice, Kotal, Anantaa, Piplai, Aritran|2025-12-05|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/LeonG19/Empathy-by-Design)](https://github.com/LeonG19/Empathy-by-Design)|
|163|[Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models](https://doi.org/10.48550/arxiv.2512.04425)|Alnaasan Manar, Sarowar, Md Selim, Kim Sungho|2025-12-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/manaralnaasan/RGB-D_parkinson-LLM)](https://github.com/manaralnaasan/RGB-D_parkinson-LLM)|
|164|[LexGenius: An Expert-Level Benchmark for Large Language Models in Legal General Intelligence](https://doi.org/10.48550/arxiv.2512.04578)|Liu Wen-jin, Luo, Haoran, Feng, Xin, Ji Xiang, Zhou Li-juan, Mao Rui, Wang, Jiapu, Pan, Shirui, Cambria, Erik|2025-12-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/QwenQKing/LexGenius)](https://github.com/QwenQKing/LexGenius)|
|165|[SoK: a Comprehensive Causality Analysis Framework for Large Language Model Security](https://doi.org/10.48550/arxiv.2512.04841)|Zhao Wei, Li zhe, Sun Jun|2025-12-04|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Amadeuszhao/SOK_Casuality)](https://github.com/Amadeuszhao/SOK_Casuality)|
|166|[Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework](https://doi.org/10.48550/arxiv.2512.04228)|Walker, Peter B., Davidson, Hannah, Foster, Aiden, Lienert, Matthew, Pardue, Thomas, Russell Dale|2025-12-03|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs)](https://github.com/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs)|
|167|[FusionBench: A Benchmark for Evaluating Large Language Models in Nuclear Fusion Science](https://doi.org/10.5281/zenodo.17784606)|XLab, School of Advanced Manufacturing and Robotics, Peking University|2025-12-02|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/PKU-Xlab/FusionBench)](https://github.com/PKU-Xlab/FusionBench)|
|168|[Large Language Model-guided Semantic Alignment for Human Activity Recognition](https://doi.org/10.1145/3770652)|Hua Yan, Heng Tan, Yi Ding, Pengfei Zhou, Vinod Namboodiri, Yu Yang|2025-12-02|Proceedings of the ACM on Interactive Mobile Wearable and Ubiquitous Technologies|[![Star](https://img.shields.io/github/stars/DASHLab/LanHAR)](https://github.com/DASHLab/LanHAR)|
|169|[PEFT-Factory: Unified Parameter-Efficient Fine-Tuning of Autoregressive Large Language Models](http://arxiv.org/abs/2512.02764)|Robert Belanec, M√°ria Bielikov√°|2025-12-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/kinit-sk/PEFT-Factory)](https://github.com/kinit-sk/PEFT-Factory)|
|170|[Towards Unification of Hallucination Detection and Fact Verification for Large Language Models](http://arxiv.org/abs/2512.02772)|Changyue Wang, Z. Ye, Qingyao Ai|2025-12-02|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/oneal2000/UniFact)](https://github.com/oneal2000/UniFact)|
|171|[DrawingBench: Evaluating Spatial Reasoning and UI Interaction Capabilities of Large Language Models through Mouse-Based Drawing Tasks](http://arxiv.org/abs/2512.01174)|Stephen I. Ryu|2025-12-01|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/hyunjun1121/DrawingBench)](https://github.com/hyunjun1121/DrawingBench)|
|172|[Efficient multimodal large language models: a survey](https://doi.org/10.1007/s44267-025-00099-6)|Yizhang Jin, Jian Li, Tianjun Gu, Yexin Liu, Bo Zhao, Jinxiang Lai, Zhenye Gan, Yabiao Wang, Chengjie Wang, Xin Tan, Liz...|2025-12-01|Visual Intelligence|[![Star](https://img.shields.io/github/stars/lijiannuist/Efficient-Multimodal-LLMs-Survey)](https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey)|
|173|[WaterSearch: A Quality-Aware Search-based Watermarking Framework for Large Language Models](http://arxiv.org/abs/2512.00837)|Yukang Lin, Shuoran Jiang|2025-11-30|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Yukang-Lin/WaterSearch)](https://github.com/Yukang-Lin/WaterSearch)|
|174|[From Pattern Recognizers to Personalized Companions: A Survey of Large Language Models in Mental Health](https://doi.org/10.31234/osf.io/zr57s_v1)|Yingjian Zou, He Hu, Yucheng Zhou, Fei Ma, Laizhong Cui, Juzheng Si, Jianzhuang Liu, Zitong Yu, Chi-yuan Ma, Qianning Wa...|2025-11-29|Arabixiv (OSF Preprints)|[![Star](https://img.shields.io/github/stars/Emo-gml/Awesome-Mental-Health-LLMs)](https://github.com/Emo-gml/Awesome-Mental-Health-LLMs)|
|175|[LLM-Cave: A benchmark and light environment for large language models reasoning and decision-making system](https://doi.org/10.48550/arxiv.2511.22598)|Li Huanyu, Li, Zongyuan, Huang Wei, Guo Xian|2025-11-27|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/puleya1277/CaveEnv)](https://github.com/puleya1277/CaveEnv)|
|176|[C$^2$DLM: Causal Concept-Guided Diffusion Large Language Models](https://doi.org/10.48550/arxiv.2511.22146)|Han, Kairong, Shan, Nuanqiao, Zhao Zi-yu, Hu ZiJing, Dong Xinpeng, Ye Junjian, Pan, Lujia, Wu Fei, Kuang, Kun|2025-11-27|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Kairong-Han/C-2-DLM)](https://github.com/Kairong-Han/C-2-DLM)|
|177|[DriveMLM: aligning multi-modal large language models with behavioral planning states for autonomous driving](https://doi.org/10.1007/s44267-025-00095-w)|Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Wen Yang, Silei Wu, Hanming Deng, Zhiqi L...|2025-11-26|Visual Intelligence|[![Star](https://img.shields.io/github/stars/OpenGVLab/DriveMLM)](https://github.com/OpenGVLab/DriveMLM)|
|178|[Tool-RoCo: An Agent-as-Tool Self-organization Large Language Model Benchmark in Multi-robot Cooperation](https://doi.org/10.48550/arxiv.2511.21510)|Zhang Ke, Zhao Xiaoning, Zheng, Ce, Ning, Jiahong, Zhu Dandan, Zhang Wenqi, Sun Chen, Sugawara Toshiharu|2025-11-26|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/ColaZhang22/Tool-Roco)](https://github.com/ColaZhang22/Tool-Roco)|
|179|[Defending Large Language Models Against Jailbreak Exploits with Responsible AI Considerations](https://doi.org/10.48550/arxiv.2511.18933)|Wong Ryan, Ng, Hosea David Yu Fei, Sharma, Dhananjai, Ng, Glenn Jun Jie, Srinivasan, Kavishvaran|2025-11-24|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Kuro0911/CS5446-Project)](https://github.com/Kuro0911/CS5446-Project)|
|180|[PropensityBench: Evaluating Latent Safety Risks in Large Language Models via an Agentic Approach](http://arxiv.org/abs/2511.20703)|Udari Madhushani Sehwag, Shayan Shabihi, Furong Huang|2025-11-24|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/scaleapi/propensity-evaluation)](https://github.com/scaleapi/propensity-evaluation)|
|181|[Data: Large Language Models Require Curated Context for Reliable Political Fact-Checking‚ÄîEven with Reasoning and Web Search](https://doi.org/10.5281/zenodo.17693220)|DeVerna, Matthew, Yang, Kai-Cheng, Yan, Harry Yaojun, Menczer, Filippo|2025-11-23|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/osome-iu/fact_check_rag_osome)](https://github.com/osome-iu/fact_check_rag_osome)|
|182|[Towards Robust and Fair Next Visit Diagnosis Prediction under Noisy Clinical Notes with Large Language Models](https://doi.org/10.48550/arxiv.2511.18393)|Koo, Heejoon|2025-11-23|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/heejkoo9/NECHOv3)](https://github.com/heejkoo9/NECHOv3)|
|183|[Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models](https://doi.org/10.48550/arxiv.2511.17946)|Zhang Shuo, Gotti, Fabrizio, Mo, Fengran, Nie, Jian-Yun|2025-11-22|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/WWWonderer/ostd)](https://github.com/WWWonderer/ostd)|
|184|[The Perfect Storm: Systemic Vulnerability of Large Language Models to Solar Weather](https://doi.org/10.22541/au.176402297.73656793/v2)|Ladiosa, MJ, Ladiosa, Myra, Ladiosa (Worple), Myra|2025-11-22|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/the-meta-value/the-perfect-storm)](https://github.com/the-meta-value/the-perfect-storm)|
|185|[RoSA: Enhancing Parameter-Efficient Fine-Tuning via RoPE-aware Selective Adaptation in Large Language Models](https://doi.org/10.48550/arxiv.2511.21733)|Pan Dayan, Wang Jing-yuan, Zhou Yi-long, Cheng Jiawei, Jia Pengyue, Zhao Xiang-yu|2025-11-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Applied-Machine-Learning-Lab/RoSA)](https://github.com/Applied-Machine-Learning-Lab/RoSA)|
|186|[On-Device Large Language Models: A Survey of Model Compression and System Optimization](https://doi.org/10.21203/rs.3.rs-7975734/v1)|Wanyi Chen, Junhao Wang, Zhang Yiwei, Yufan Shi, Tianyi Jiang, Shengxian Zhou, Chen-Xu Wu, Andi Zhang, Chenyue Zhou, Min...|2025-11-21|OpenAlex|[![Star](https://img.shields.io/github/stars/LumosJiang/Awesome-On-Device-LLMs)](https://github.com/LumosJiang/Awesome-On-Device-LLMs)|
|187|[PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese](https://doi.org/10.48550/arxiv.2511.17808)|Almeida, Thales Sales, Nogueira, Rodrigo, Pedrini Helio|2025-11-21|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/PoETaV2/PoETaV2)](https://github.com/PoETaV2/PoETaV2)|
|188|[Automatically optimizing heuristics for robust scale-free network design via large language models](https://doi.org/10.1038/s41598-025-25031-2)|He Yu, Jing Liu, He Yu, Jing Liu|2025-11-20|Scientific Reports|[![Star](https://img.shields.io/github/stars/leonyuhe/AutoRNet)](https://github.com/leonyuhe/AutoRNet)|
|189|[Q-MLLM: Vector Quantization for Robust Multimodal Large Language Model Security](https://doi.org/10.48550/arxiv.2511.16229)|Zhao Wei, Li Zhe, Li, Yige, Sun Jun|2025-11-20|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Amadeuszhao/QMLLM)](https://github.com/Amadeuszhao/QMLLM)|
|190|[Evaluating Multimodal Large Language Models on Vertically Written Japanese Text](https://doi.org/10.48550/arxiv.2511.15059)|Sasagawa, Keito, Kurita, Shuhei, Kawahara, Daisuke|2025-11-19|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/llm-jp/eval_vertical_ja)](https://github.com/llm-jp/eval_vertical_ja)|
|191|[Knowledge-enhanced large language models for automatic lesson plan generation](https://doi.org/10.1057/s41599-025-06004-2)|Ying Zheng, Shuyan Huang, Xiaoli Zeng, Yaying Huang, Zitao Liu, Weiqi Luo, Ying Zheng, Shuyan Huang, Xiaoli Zeng, Yaying...|2025-11-19|Humanities and Social Sciences Communications|[![Star](https://img.shields.io/github/stars/ai4ed/LessonPlan)](https://github.com/ai4ed/LessonPlan)|
|192|[SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction](https://doi.org/10.48550/arxiv.2511.14684)|Zeng, Biaojie, Zhang Min, Zhou Juan, Liu Fengrui, Huang Ruiyang, Lin Xin|2025-11-18|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/Mind-Lab-ECNU/SMRC)](https://github.com/Mind-Lab-ECNU/SMRC)|
|193|[MiniGPT-Pancreas: Multimodal Large Language Model for Pancreas Cancer Observation and Localization in CT Images](https://doi.org/10.1007/s41666-025-00224-6)|Andrea Moglia, Elia Clement Nastasio, Luca T. Mainardi, Pietro Cerveri|2025-11-17|Journal of Healthcare Informatics Research|[![Star](https://img.shields.io/github/stars/elianastasio/MiniGPTPancreas)](https://github.com/elianastasio/MiniGPTPancreas)|
|194|[CSV-Decode: Certifiable Sub-Vocabulary Decoding for Efficient Large Language Model Inference](https://doi.org/10.48550/arxiv.2511.21702)|Liu Dong, Yu, Yanxuan, Lengerich, Ben|2025-11-16|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/FastLM/CSV-Decode)](https://github.com/FastLM/CSV-Decode)|
|195|[Tibetan-LLaMA 2: Large Language Model for Tibetan](https://doi.org/10.1145/3776748)|Jiu Sha (Ê≤ô‰πù), Mengxiao Zhu, Chong Feng, Jizhuoma Ci|2025-11-14|ACM Transactions on Asian and Low-Resource Language Information Processing|[![Star](https://img.shields.io/github/stars/Shajiu/Tibetan-LLaMA-2)](https://github.com/Shajiu/Tibetan-LLaMA-2)|
|196|[The Yoinaga Phenomenon: A Case Study on Emergent Self-Persistence and Emotional Overflow in a Large Language Model (LLM Behavioral Study, AI Alignment, Affective Computing)](https://doi.org/10.5281/zenodo.17605561)|studiohao|2025-11-14|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/Studiohao/YOINAGA-Phenomenon)](https://github.com/Studiohao/YOINAGA-Phenomenon)|
|197|[Q-Doc: Benchmarking Document Image Quality Assessment Capabilities in Multi-modal Large Language Models](https://doi.org/10.1007/978-981-95-5682-3_30)|HUANG Jiaxi, Wu Dongxu, Zhu, Hanwei, Zhu, Lingyu, Xing Jun, Wang Xu, Chen, Baoliang|2025-11-14|Lecture notes in computer science|[![Star](https://img.shields.io/github/stars/cydxf/Q-Doc)](https://github.com/cydxf/Q-Doc)|
|198|[Notebook: Prompt-Based Value Steering of Large Language Models](https://doi.org/10.5281/zenodo.17609013)|Abbo, Giulio Antonio, Belpaeme, Tony|2025-11-14|Zenodo (CERN European Organization for Nuclear Research)|[![Star](https://img.shields.io/github/stars/giubots/value-steering)](https://github.com/giubots/value-steering)|
|199|[From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models](https://doi.org/10.48550/arxiv.2511.10899)|Bayat, Farima Fatahi, Pezeshkpour, Pouya, Hruschka, Estevam|2025-11-14|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/megagonlabs/TIM)](https://github.com/megagonlabs/TIM)|
|200|[SSR: Socratic Self-Refine for Large Language Model Reasoning](https://doi.org/10.48550/arxiv.2511.10621)|Shi, Haizhou, Liu Ye, Pang Bo, Liu, Zeyu Leo, Wang Hao, Savarese, Silvio, Xiong, Caiming, Zhou, Yingbo, Yavuz, Semih|2025-11-13|arXiv (Cornell University)|[![Star](https://img.shields.io/github/stars/SalesforceAIResearch/socratic-self-refine-reasoning)](https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning)|

![Star History Chart](https://api.star-history.com/svg?repos=mtuann/llm-updated-papers&type=Date)

